# **Uma Abordagem em Etapa de Processamento para ReduÃ§Ã£o do** **ViÃ©s de Popularidade**

## Rodrigo Ferrari de Souza
#### Universidade de SÃ£o Paulo SÃ£o Carlos-SP, Brasil rodrigofsouza@usp.br
### **ABSTRACT**

Recommendation systems are designed to provide personalized
suggestions to each user to enhance user experience and satisfaction across various applications. However, despite their widespread
adoption and benefits, such as increased user retention and profits, certain challenges persist, particularly popularity bias, which
impacts the quality of recommendations. This bias introduces inconsistencies among user groups, resulting in issues such as lack
of calibration, unfairness, and filter bubbles. To address these challenges, several studies have proposed calibration strategies to improve the quality of recommendations and achieve consistency
among user groups, focusing on mitigating popularity bias. However, integrating these approaches into a unified model remains
a challenge. This study proposes an innovative approach combining popularity-based personalized calibration with the Bayesian
Personalized Ranking (BPR) method in the processing step. Our
approach aims to provide consistent and fair recommendations
while leveraging the efficiency gains of the BPR method. Experimental results on different datasets demonstrate the effectiveness of
our modified approach in achieving comparable or superior results
to state-of-the-art methods in terms of ranking, popularity, and
fairness metrics.
### **KEYWORDS**

Sistemas de RecomendaÃ§Ã£o, ViÃ©s de Popularidade, JustiÃ§a, CalibraÃ§Ã£o
### **1 INTRODUÃ‡ÃƒO**

Os sistemas de recomendaÃ§Ã£o sÃ£o projetados para gerar sugestÃµes
personalizadas para cada usuÃ¡rio com o objetivo de melhorar sua
experiÃªncia e satisfaÃ§Ã£o em diversas aplicaÃ§Ãµes. Consequentemente,
estes sistemas sÃ£o cada vez mais predominantes nos diversos contextos atuais, incluindo comÃ©rcio eletrÃ´nico, vÃ­deos, mÃºsica e muito
mais. Como resultado de sua ampla adoÃ§Ã£o, os sistemas de recomendaÃ§Ã£o tornaram-se um tÃ³pico altamente relevante tanto na
indÃºstria quanto na academia [10].
Assim, embora estes sistemas produzam benefÃ­cios como maior
retenÃ§Ã£o de usuÃ¡rios e lucro por meio da venda de itens recomendados, certos problemas persistem. Notavelmente, a presenÃ§a de viÃ©s
de popularidade afeta significativamente a usabilidade e a qualidade
das recomendaÃ§Ãµes, levando a possÃ­veis consequÃªncias, como falta
de calibraÃ§Ã£o e injustiÃ§a [ 5 ]. Nesse contexto, o viÃ©s de popularidade

In: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe
diaâ€™2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.
Â© 2024 SBC â€“ Sociedade Brasileira de ComputaÃ§Ã£o.
ISSN 2966-2753

## Marcelo Garcia Manzato
#### Universidade de SÃ£o Paulo SÃ£o Carlos-SP, Brasil mmanzato@icmc.usp.br

introduz inconsistÃªncia e injustiÃ§a nas recomendaÃ§Ãµes entre diferentes grupos de indivÃ­duos, fazendo com que aqueles que preferem
itens de nicho recebam sugestÃµes de itens populares e vice-versa

[1, 10].
Estas questÃµes nas recomendaÃ§Ãµes podem ter ainda mais consequÃªncias, tais como bolhas de filtro, que recentemente ganharam
atenÃ§Ã£o considerÃ¡vel em sistemas como as redes sociais. Nestes
casos, usuÃ¡rios sÃ£o expostos exclusivamente a recomendaÃ§Ãµes alinhadas com seus interesses e crenÃ§as, sem receber conteÃºdos com
ideias opostas. Consequentemente, nota-se atualmente um aumento
considerÃ¡vel na polarizaÃ§Ã£o social, levando a tensÃµes significativas
em paÃ­ses democrÃ¡ticos [13].
A Figura 1 exemplifica o problema em questÃ£o apresentando trÃªs
possÃ­veis cenÃ¡rios de recomendaÃ§Ãµes geradas para o usuÃ¡rio. Para
isso, foi considerada a popularidade dos itens como a caracterÃ­stica
de preferÃªncia para o perfil do usuÃ¡rio, sendo dividida em trÃªs tipos:
itens populares, itens diversos e itens de nicho.
O cenÃ¡rio (A) mostra uma distribuiÃ§Ã£o onde o tipo de popularidade preferida (itens populares) supera o menos preferido (itens de
nicho) nas recomendaÃ§Ãµes. O cenÃ¡rio (B) mostra que uma preferÃªncia por itens diversos nÃ£o estÃ¡ representada nas recomendaÃ§Ãµes. JÃ¡ o
cenÃ¡rio (C) apresenta um conjunto de recomendaÃ§Ãµes calibrado, retornando uma lista coerente e justa com as preferÃªncias do usuÃ¡rio.
Os principais algoritmos de recomendaÃ§Ã£o criam rankings semelhantes a (A) e (B), enquanto para recomendar itens semelhantes a
(C), os algoritmos mais utilizados precisam passar por uma etapa
extra de calibraÃ§Ã£o.
A literatura existente indica que vÃ¡rios estudos tentam calibrar
sistemas de recomendaÃ§Ã£o para melhorar a qualidade das sugestÃµes e alcanÃ§ar consistÃªncia entre diferentes grupos de usuÃ¡rios

[ 8, 9, 17, 24, 27 ]. Neste contexto, alguns trabalhos se concentram
na implementaÃ§Ã£o de tÃ©cnicas de calibraÃ§Ã£o para mitigar o viÃ©s de
popularidade [ 7, 21, 25, 29, 31 ]. Conforme descrito em [ 19 ], os trabalhos que propÃµem a calibraÃ§Ã£o de recomendaÃ§Ãµes podem empregar
esta estratÃ©gia em trÃªs momentos distintos:

  - **PrÃ©-processamento:** Normalmente, a calibraÃ§Ã£o nesta etapa
considera que inconsistÃªncias surgem dos dados usados para
treinar o modelo de recomendaÃ§Ã£o. Portanto, visa ajustar
ou mesmo desconsiderar determinados aspectos dos dados
de treinamento. Por outro lado, esses ajustes podem levar a
perda de informaÃ§Ãµes importantes dos dados a serem utilizados.

  - **PÃ³s-processamento:** A calibraÃ§Ã£o neste Ãºltimo estÃ¡gio altera a saÃ­da gerada pelo algoritmo de recomendaÃ§Ã£o, permitindo que ele resolva os vieses existentes na recomendaÃ§Ã£o
produzida pelo modelo. PorÃ©m, esta tÃ©cnica pode comprometer a precisÃ£o do sistema, modificando os resultados iniciais.


310


-----

WebMediaâ€™2024, Juiz de Fora, Brazil Rodrigo Ferrari de Souza and Marcelo Garcia Manzato

**Figura 1: RepresentaÃ§Ã£o de trÃªs possÃ­veis cenÃ¡rios de geraÃ§Ã£o de recomendaÃ§Ãµes.**



  - **Em processamento:** A calibraÃ§Ã£o nesta etapa envolve a modificaÃ§Ã£o ou introduÃ§Ã£o de novos algoritmos com o objetivo
de reduzir vieses no modelo durante o treinamento.

Dessa forma, a estratÃ©gia estudada neste trabalho foi a estratÃ©gia
de calibraÃ§Ã£o em etapa de processamento. Esse tipo de estratÃ©gia
visa modificar algoritmos existentes ou introduzir novos algoritmos
que resultem em classificaÃ§Ãµes e recomendaÃ§Ãµes justas, por exemplo,
removendo preconceitos e discriminaÃ§Ã£o durante o processo de
treinamento do modelo. Normalmente, tais mÃ©todos visam aprender
um modelo sem vieses, ao mesmo tempo que consideram a justiÃ§a,
incorporando mudanÃ§as na funÃ§Ã£o objetivo de um algoritmo por
um termo de justiÃ§a ou impondo restriÃ§Ãµes de justiÃ§a [19].
A abordagem BPR *(Bayesian Personalized Ranking for Implicit Fe-*
*edback)* [ 20 ] Ã© uma tÃ©cnica LTR ( *Learning to Rank* ) do tipo *pairwise*
que procura posicionar itens relevantes no topo da lista de recomendaÃ§Ã£o. Para isso, sÃ£o feitas comparaÃ§Ãµes entre pares de itens
â€“ um conhecido e outro desconhecido pelo usuÃ¡rio â€“ de modo a
maximizar a diferenÃ§a de suas respectivas representaÃ§Ãµes. Apesar
de nÃ£o lidar com injustiÃ§a e vieses, o BPR possui uma flexibilidade em sua construÃ§Ã£o que permite utilizaÃ§Ã£o com outros modelos
de recomendaÃ§Ã£o, e tambÃ©m extensÃµes para que outras condiÃ§Ãµes
(como vieses e injustiÃ§a) sejam impostas durante seu treinamento

[ 3 ]. PorÃ©m, conforme relatado na seÃ§Ã£o de trabalhos relacionados,
hÃ¡ uma deficiÃªncia de trabalhos em etapa de processamento que
sejam capazes de lidar com diferentes aspectos de justiÃ§a e vieses.
Assim, a proposta deste trabalho Ã© combinar uma forma de calibraÃ§Ã£o personalizada baseada na popularidade dos itens com o
mÃ©todo BPR [ 20 ] em etapa de processamento. O objetivo dessa
combinaÃ§Ã£o Ã© trazer um sistema eficiente que traga recomendaÃ§Ãµes
coerentes com as preferÃªncias dos usuÃ¡rios, reduza o viÃ©s de popularidade e se aproveite do mecanismo de otimizaÃ§Ã£o de ranking das
recomendaÃ§Ãµes de acordo com a relevÃ¢ncia dos itens.
A estrutura deste trabalho Ã© a seguinte: na SeÃ§Ã£o 2, discutimos
trabalhos relacionados e comparamos as abordagens existentes com
o nosso trabalho. A SeÃ§Ã£o 3 descreve a estrutura para nossas proposta de calibraÃ§Ã£o. A SeÃ§Ã£o 4 detalha a metodologia de avaliaÃ§Ã£o
do sistema proposto. A SeÃ§Ã£o 5 discute os resultados obtidos. Finalmente, na SeÃ§Ã£o 6, concluÃ­mos nosso estudo, apresentando algumas
direÃ§Ãµes futuras para pesquisas.
### **2 TRABALHOS RELACIONADOS**

A literatura recente apresenta diversas propostas destinadas a calibrar recomendaÃ§Ãµes para alinhÃ¡-las de forma mais consistente com


os perfis dos usuÃ¡rios. Conforme [ 19 ], Ã© possÃ­vel aplicar a calibraÃ§Ã£o
em trÃªs diferentes etapas, que serÃ£o detalhadas a seguir.
### **2.1 Etapa de PrÃ©-Processamento**

Essa etapa consiste em alterar os dados a serem utilizados pelo
sistema antes das recomendaÃ§Ãµes serem geradas. Possui vantagens
como: ajuda a melhorar a qualidade dos dados removendo as inconsistÃªncias antes de serem utilizados e pode melhorar a eficiÃªncia
do tempo de processamento e adaptar os dados Ã s necessidades do
algoritmo. Apesar disso, esses ajustes podem aumentar o tempo de
preparaÃ§Ã£o do sistema e levar a perda de informaÃ§Ãµes importantes
dos dados a serem utilizados.

O trabalho [ 14 ] apresenta uma abordagem de omissÃ£o de atributos para tentar remover o viÃ©s nos dados. AlÃ©m disso, o mesmo
trabalho mostra uma estratÃ©gia de alteraÃ§Ã£o dos rÃ³tulos dos itens
para remover os vieses. Por outro lado, essas tÃ©cnicas podem ser insuficientes para garantir justiÃ§a nas recomendaÃ§Ãµes, alÃ©m de existir
a possibilidade de tais mÃ©todos reduzirem a acurÃ¡cia do sistema.
Uma outra estratÃ©gia Ã© apresentada no trabalho [ 22 ], onde Ã©
proposto um algoritmo baseado em seleÃ§Ã£o de amostras para um
treinamento justo e robusto. Para tanto, Ã© formulado um problema
de otimizaÃ§Ã£o combinatÃ³ria para a seleÃ§Ã£o imparcial de amostras na
presenÃ§a de problemas nos dados de treinamento. Um dos principais
riscos dessa seleÃ§Ã£o de amostras Ã© a introduÃ§Ã£o de injustiÃ§as, caso o
procedimento de amostragem nÃ£o seja cuidadosamente projetado,
jÃ¡ que se a seleÃ§Ã£o for tendenciosa para determinados grupos de
dados, o modelo treinado tambÃ©m poderÃ¡ apresentar vieses. Um
outro problema Ã© o custo computacional dessa seleÃ§Ã£o de amostras
e a complexidade para realizar essa seleÃ§Ã£o dependendo do contexto
dos dados.
### **2.2 Etapa de PÃ³s-Processamento**

A etapa de pÃ³s-processamento consiste em calibrar o sistema apÃ³s o
modelo ter gerado as recomendaÃ§Ãµes e tem as seguintes vantagens:
Ã© independente do algoritmo de recomendaÃ§Ã£o, pois pode ser feita
uma reclassificaÃ§Ã£o da lista gerada por qualquer outro modelo; tambÃ©m Ã© mais eficiente, porque nÃ£o afeta o tempo de processamento
do algoritmo que gera as recomendaÃ§Ãµes. Todavia, essa etapa pode
reduzir a precisÃ£o do sistema jÃ¡ que altera a lista calibrada gerada
pelo modelo de recomendaÃ§Ã£o.
Trabalhos como [ 24 ] e [ 9 ] fazem ajustes com base nos interesses
dos usuÃ¡rios nos gÃªneros de itens para alcanÃ§ar um sistema mais
consistente. Apesar do foco na consistÃªncia, esses trabalhos nÃ£o
abordam a questÃ£o do viÃ©s de popularidade.


311


-----

Uma Abordagem em Etapa de Processamento para ReduÃ§Ã£o do ViÃ©s de Popularidade WebMediaâ€™2024, Juiz de Fora, Brazil


Para abordar o viÃ©s de popularidade, alguns trabalhos implementam estratÃ©gias de calibraÃ§Ã£o nessa etapa, como o artigo [ 23 ] que
apresenta uma proposta de calibraÃ§Ã£o baseada em chaveamento.
Esta abordagem opta pela calibraÃ§Ã£o com base na popularidade dos
itens ou nos gÃªneros dos itens. Por outro lado, conforme mencionado anteriormente, a precisÃ£o do sistema pode ser afetada, jÃ¡
que hÃ¡ uma reordenaÃ§Ã£o na lista de itens sugeridos apÃ³s a etapa de
recomendaÃ§Ã£o.
### **2.3 Etapa de Processamento**

Na etapa de processamento, a calibraÃ§Ã£o ocorre junto com o treinamento e geraÃ§Ã£o das recomendaÃ§Ãµes, e pode levar a um melhor
desempenho do algoritmo em termos de precisÃ£o. AlÃ©m disso, tambÃ©m permite aplicar diferentes tÃ©cnicas de mitigaÃ§Ã£o da injustiÃ§a
durante o treinamento do algoritmo. Entretanto, esses ajustes podem aumentar a complexidade e o tempo de treinamento do sistema.
As abordagens existentes na literatura normalmente usam aprendizado de mÃ¡quina para construir modelos de classificaÃ§Ã£o. Em geral,
esses modelos categorizam listas nÃ£o vistas de forma semelhante Ã 
classificaÃ§Ã£o dos dados de treinamento. O objetivo geral Ã© ter um
modelo que minimize uma funÃ§Ã£o de perda que capture a distÃ¢ncia
entre o que foi aprendido e a classificaÃ§Ã£o de entrada [19].
O trabalho [ 30 ] segue essa ideia adicionando termos de regularizaÃ§Ã£o, que expressam medidas de injustiÃ§a que o modelo deve
minimizar alÃ©m da minimizaÃ§Ã£o da funÃ§Ã£o de perda original. Outra
abordagem apresentada Ã© vista em [16], que utiliza a estratÃ©gia de
rede neural artificial denominada *variational autoencoders* (VAE).
Nessa abordagem a filtragem colaborativa Ã© feita juntamente com
parÃ¢metros de regularizaÃ§Ã£o que melhoram a representaÃ§Ã£o dos
dados implementados pelo modelo.
O trabalho [ 3 ] propÃµe uma abordagem para reduzir o viÃ©s de
popularidade em sistemas de recomendaÃ§Ã£o. O mÃ©todo apresentado conecta as perspectivas do usuÃ¡rio e do item para minimizar a
correlaÃ§Ã£o entre a popularidade de um item e sua relevÃ¢ncia para
um usuÃ¡rio especÃ­fico. Isso Ã© feito por meio de um modelo probabilÃ­stico que aprende os fatores latentes dos usuÃ¡rios e dos itens. O
algoritmo atualiza os fatores do usuÃ¡rio com base na diferenÃ§a entre
a avaliaÃ§Ã£o do usuÃ¡rio para dois itens (popular e menos popular) e
a probabilidade prevista de o usuÃ¡rio preferir o item menos popular.
Devido Ã  similaridade com a proposta deste trabalho, o mÃ©todo foi
usado como um dos *baselines* desta pesquisa.
Existem estratÃ©gias alternativas para gerar recomendaÃ§Ãµes mais
consistentes. A proposta [ 12 ] emprega grafos para mitigar injustiÃ§as
no sistema, considerando o gÃªnero do usuÃ¡rio. O trabalho [ 6 ] implementa uma abordagem pareada considerando a justiÃ§a entre grupos
de itens, ajustando a lista de recomendaÃ§Ãµes durante o treinamento.
A abordagem [ 17 ] sugere a utilizaÃ§Ã£o de grafos e redes neurais para
atribuir pesos aos itens recomendados, equilibrando-os de acordo
com as preferÃªncias do usuÃ¡rio.
Embora essas abordagens produzam resultados interessantes,
os estudos nÃ£o abordam o viÃ©s de popularidade de forma a trazer
recomendaÃ§Ãµes que atendam os interesses dos usuÃ¡rios por esse
aspecto, havendo uma lacuna na Ã¡rea com relaÃ§Ã£o a sistemas que
tragam recomendaÃ§Ãµes coerentes e que lidem com o viÃ©s de popularidade em etapa de processamento. AlÃ©m disso, o trabalho [ 11 ]
destaca a importÃ¢ncia dos vieses e como eles afetam os sistemas,


o que faz ser necessÃ¡rio selecionar mÃ©todos adequados para lidar
com o viÃ©s presente no sistema.
Dessa forma, a estratÃ©gia deste trabalho Ã© combinar uma forma
de calibraÃ§Ã£o personalizada baseada na popularidade dos itens com
o mÃ©todo BPR [ 20 ] em etapa de processamento. O objetivo dessa
combinaÃ§Ã£o Ã© trazer um sistema eficiente que traga recomendaÃ§Ãµes
coerentes, reduza o viÃ©s de popularidade e seja capaz de fornecer
recomendaÃ§Ãµes relevantes de acordo com o perfil de cada usuÃ¡rio.
As prÃ³ximas seÃ§Ãµes descrevem a implementaÃ§Ã£o dessa combinaÃ§Ã£o
e os resultados dessa abordagem.
### **3 ESTRATÃ‰GIA DE CALIBRAÃ‡ÃƒO**

Supondo que hÃ¡ um conjunto de itens *ğ¼* = { *ğ‘–* 1 *,ğ‘–* 2 *, ...,ğ‘–* | *ğ¼* | }, um conjunto de usuÃ¡rios *ğ‘ˆ* = { *ğ‘¢* 1 *,ğ‘¢* 2 *, ...,ğ‘¢* | *ğ‘ˆ* | } e um conjunto de itens
candidatos para cada usuÃ¡rio *ğ¶ğ¼* *ğ‘¢* = { *ğ‘–* 1 *,ğ‘–* 2 *, ...,ğ‘–* *ğ‘* }, onde *ğ‘* Ã© o nÃºmero de itens sugeridos pelo sistema de recomendaÃ§Ã£o. AlÃ©m disso,
existem as informaÃ§Ãµes dos usuÃ¡rios sobre as preferÃªncias de popularidade. A tarefa Ã© explorar essas preferÃªncias para gerar uma lista
de recomendaÃ§Ãµes que aumente a justiÃ§a em relaÃ§Ã£o a popularidade
dos itens.

Para tanto, propÃµe-se uma abordagem de calibraÃ§Ã£o em etapa
de processamento. Na prÃ¡tica, o mÃ©todo utiliza medidas de divergÃªncia na etapa de geraÃ§Ã£o de recomendaÃ§Ãµes para realizar uma
calibraÃ§Ã£o de acordo com diferentes nÃ­veis de popularidade de interesse do usuÃ¡rio. Como resultado, os usuÃ¡rios recebem uma lista
de recomendaÃ§Ãµes prÃ³xima ao seu perfil de interesse em termos
de popularidade. Essa calibraÃ§Ã£o Ã© incorporada ao BPR. A Figura 2
apresenta a estrutura de calibraÃ§Ã£o de popularidade, cujos detalhes
sÃ£o descritos a seguir.

**Figura 2: Estrutura de calibraÃ§Ã£o proposta. A calibraÃ§Ã£o por**
**popularidade Ã© aplicada de forma combinada ao treinamento**
**do BPR, resultando em uma lista calibrada de recomendaÃ§Ãµes**
**de acordo com as preferÃªncias do usuÃ¡rio sobre popularidade**
**e gÃªneros.**
### **3.1 DivisÃ£o de Popularidade**

A calibraÃ§Ã£o da lista de recomendaÃ§Ãµes com base na popularidade
dos itens jÃ¡ consumidos pelo usuÃ¡rio Ã© feita por meio de uma divisÃ£o
de popularidade para agrupar os itens com base na quantidade


312


-----

WebMediaâ€™2024, Juiz de Fora, Brazil Rodrigo Ferrari de Souza and Marcelo Garcia Manzato

VÃ¡rias mÃ©tricas avaliam a imparcialidade em sistemas de recomendaÃ§Ã£o [ 26 ]. PorÃ©m, nesse caso, utiliza-se a medida de divergÃªncia Kullback-Leibler pelas mesmas razÃµes apontadas por [ 24 ] e
exploradas por [ 9 ]. O Kullback-Leibler quantifica a desigualdade
no intervalo [ 0 *,* âˆ], onde 0 significa que ambas as distribuiÃ§Ãµes sÃ£o
quase iguais e valores mais altos indicam injustiÃ§a.
Adicionalmente, Ã© adotada a regularizaÃ§Ã£o proposta por [ 24 ],
que definiu *ğ›¼* = 0 *.* 01 como uma variÃ¡vel de regularizaÃ§Ã£o para
evitar divisÃ£o por zero quando *ğ‘¦* ( *ğ‘¡* | *ğ‘¢* ) vai para zero. Embora existam outras mÃ©tricas de divergÃªncia, como Hellinger e Person QuiSquare, propostas por [ 4 ] e exploradas por [ 9 ], foi utilizada apenas
a Kullback-Leibler devido Ã  sua simplicidade:


*ğ·* *ğ¾ğ¿* ( *ğ‘¥* âˆ¥ *ğ‘¦* ) =

*ğ‘¡*

âˆ‘ï¸


*ğ‘¥* ( *ğ‘¡* | *ğ‘¢* )
*ğ‘¡* *ğ‘¥* ( *ğ‘¡* | *ğ‘¢* ) Â· *ğ‘™ğ‘œğ‘”* (1 âˆ’ *ğ›¼* ) Â· *ğ‘¦* ( *ğ‘¡* | *ğ‘¢* ) + *ğ›¼* - *ğ‘¥* ( *ğ‘¡* | *ğ‘¢* ) (3)

âˆ‘ï¸


**Figura 3: Curva representando a divisÃ£o dos itens em grupos**
**de popularidade.**

de interaÃ§Ãµes. A divisÃ£o de popularidade, introduzida em [ 2 ], Ã©
baseada no conceito de cauda longa dos sistemas de recomendaÃ§Ã£o,
conforme pode ser visualizado na Figura 3. A curva foi dividida
em trÃªs partes. O **Head (H)**, com itens representando 20% do total
de interaÃ§Ãµes. A **Tail (T)** com itens que somam menos de 20%
das interaÃ§Ãµes, e o grupo **Mid (M)**, que contÃ©m itens que nÃ£o sÃ£o
nem **Head (H)** nem **Tail (T)** . Vale ressaltar que esta divisÃ£o por
percentual foi escolhida com base no princÃ­pio de Pareto [18].
### **3.2 CalibraÃ§Ã£o**

A calibraÃ§Ã£o por popularidade foi uma adaptaÃ§Ã£o da fÃ³rmula proposta por [ 24 ]. Seu trabalho pressupÃµe que os itens podem ter mais
de um gÃªnero, o que nÃ£o Ã© vÃ¡lido no contexto de popularidade,
onde um item possui apenas um nÃ­vel de popularidade. EntÃ£o, ao
invÃ©s disso, foram calculadas as somas dos pesos de cada tipo de
popularidade sobre a soma de todos os pesos.
Assim, *ğ‘¥* ( *ğ‘¡* | *ğ‘¢* ) Ã© definido como a distribuiÃ§Ã£o alvo baseada na
popularidade dos itens com os quais o usuÃ¡rio interagiu no passado. Na EquaÃ§Ã£o 1 os pesos *ğ‘Ÿ* *ğ‘¢ğ‘–* sÃ£o definidos como a classificaÃ§Ã£o
explÃ­cita ou implÃ­cita que o usuÃ¡rio *ğ‘¢* deu ao item *ğ‘–* :


A divergÃªncia de Kullback-Leibler Ã© uma medida que quantifica a
diferenÃ§a entre duas distribuiÃ§Ãµes de probabilidade, neste caso, entre
a distribuiÃ§Ã£o observada *ğ‘¥* ( *ğ‘¡* | *ğ‘¢* ) e a distribuiÃ§Ã£o de referÃªncia *ğ‘¦* ( *ğ‘¡* | *ğ‘¢* ) .
No contexto da calibraÃ§Ã£o por popularidade, *ğ‘¥* ( *ğ‘¡* | *ğ‘¢* ) representa a
distribuiÃ§Ã£o empÃ­rica dos itens observados pelo usuÃ¡rio *ğ‘¢*, enquanto
*ğ‘¦* ( *ğ‘¡* | *ğ‘¢* ) representa uma distribuiÃ§Ã£o de referÃªncia desejada, que Ã©
baseada na popularidade dos itens na base de dados.

### **3.3 O MÃ©todo BPR**

O BPR [ 20 ] Ã© uma abordagem eficaz para recomendaÃ§Ã£o de itens
em sistemas de recomendaÃ§Ã£o baseados em feedback implÃ­cito. Ao
modelar as preferÃªncias dos usuÃ¡rios por meio de caracterÃ­sticas
latentes e otimizar a funÃ§Ã£o de perda, o modelo Ã© capaz de aprender
efetivamente as preferÃªncias dos usuÃ¡rios e gerar recomendaÃ§Ãµes
personalizadas, levando em consideraÃ§Ã£o a ordem de preferÃªncia
dos itens.

Mantendo a notaÃ§Ã£o utilizada na seÃ§Ã£o anterior, as letras de indexaÃ§Ã£o especial distinguem usuÃ¡rios e itens: um usuÃ¡rio Ã© indicado
como *ğ‘¢* e um item Ã© referido como *ğ‘–*, *ğ‘—* ; *ğ‘Ÿ* *ğ‘¢ğ‘–* refere-se ao feedback explÃ­cito ou implÃ­cito de um usuÃ¡rio *ğ‘¢* para um item *ğ‘–* . No primeiro caso,
Ã© um nÃºmero inteiro fornecido pelo usuÃ¡rio indicando o quanto
ele gostou do conteÃºdo; no segundo caso, Ã© apenas um booleano
mostrando se o usuÃ¡rio consumiu ou visitou o conteÃºdo ou nÃ£o. A

prediÃ§Ã£o do sistema sobre a preferÃªncia do usuÃ¡rio *ğ‘¢* para o item *ğ‘–*
Ã© representada por Ë† *ğ‘Ÿ* *ğ‘¢ğ‘–*, que Ã© um valor de ponto flutuante estimado
pelo algoritmo de recomendaÃ§Ã£o. O conjunto de pares ( *ğ‘¢,ğ‘–* ) para
os quais *ğ‘Ÿ* *ğ‘¢ğ‘–* Ã© conhecido Ã© representado por *ğ¾* = {( *ğ‘¢,ğ‘–* )| *ğ‘Ÿ* *ğ‘¢ğ‘–* }.
Em um modelo de fatorizaÃ§Ã£o tradicional, cada usuÃ¡rio *ğ‘¢* Ã© associado a um vetor de fatores *ğ‘* *ğ‘¢* âˆˆ R *[ğ‘“]* e cada item *ğ‘–* com um vetor de
fatores *ğ‘* *ğ‘–* âˆˆ R *[ğ‘“]* . Uma regra de previsÃ£o seria:

*ğ‘Ÿ* Ë† *ğ‘¢ğ‘–* = *ğ‘* *ğ‘¢* *[ğ‘‡]* *[ğ‘]* *[ğ‘–]* (1) (4)

Conjuntos adicionais sÃ£o *ğ‘* ( *ğ‘¢* ), que indica o conjunto de itens
para os quais o usuÃ¡rio *ğ‘¢* forneceu um feedback implÃ­cito, e *ğ‘* ( *ğ‘¢* ),
que indica o conjunto de itens desconhecidos para o usuÃ¡rio *ğ‘¢* . Uma
caracterÃ­stica importante desse tipo de feedback Ã© que apenas as
observaÃ§Ãµes positivas sÃ£o conhecidas; os pares usuÃ¡rio-item nÃ£o
observados sÃ£o interpretados como feedback negativo.
O trabalho [ 20 ] discute um problema que surge quando um modelo de recomendaÃ§Ã£o de itens Ã© treinado apenas com esses dados


*ğ‘¥* ( *ğ‘¡* | *ğ‘¢* ) =


ï¿½ *ğ‘–* âˆˆ *ğ¼* *ğ‘¢* *[ğ‘Ÿ]* *ğ‘¢ğ‘–* [Â·] *[ ğ‘¥]* [(] *[ğ‘¡]* [|] *[ğ‘–]* [)]
(1)
ï¿½ *ğ‘–* âˆˆ *ğ¼* *ğ‘¢* *[ğ‘Ÿ]* *ğ‘¢ğ‘–*


onde *ğ¼* *ğ‘¢* Ã© o conjunto de itens interagidos pelo usuÃ¡rio *ğ‘¢*, e *ğ‘¥* ( *ğ‘¡* | *ğ‘–* ) Ã©
definido como 1 se o item *ğ‘–* estiver na categoria de popularidade *ğ‘¡* .
EntÃ£o, para lidar com a distribuiÃ§Ã£o de lista recomendada, a EquaÃ§Ã£o
2 define *ğ‘¦* ( *ğ‘¡* | *ğ‘¢* ) como:


*ğ‘¦* ( *ğ‘¡* | *ğ‘¢* ) =


ï¿½ *ğ‘–* âˆˆ *ğ‘…* *ğ‘¢* [âˆ—] *[ğ‘¤]* *ğ‘* [(] *[ğ‘¢,ğ‘–]* [) Â·] *[ ğ‘¥]* [(] *[ğ‘¡]* [|] *[ğ‘–]* [)]

(2)
ï¿½ *ğ‘–* âˆˆ *ğ‘…* *ğ‘¢* [âˆ—] *[ğ‘¤]* *ğ‘* [(] *[ğ‘¢,ğ‘–]* [)]


Neste caso, usamos os pesos *ğ‘¤* *ğ‘* ( *ğ‘¢,ğ‘–* ) como a posiÃ§Ã£o de classificaÃ§Ã£o do item *ğ‘–* na lista reordenada recomendada *ğ‘…* *ğ‘¢* [âˆ—] para o usuÃ¡rio

*ğ‘¢* .


313


-----

Uma Abordagem em Etapa de Processamento para ReduÃ§Ã£o do ViÃ©s de Popularidade WebMediaâ€™2024, Juiz de Fora, Brazil

**Algorithm 1:** Aprendizado via LearnBPR.

**Input:** *ğ·* *ğ¾*
**Output:** ParÃ¢metros ajustados Î˜

**1** Inicializar Î˜ com valores aleatÃ³rios

**2** **for** *cont = 1,...,#IteraÃ§Ãµes* **do**

**3** obtenha ( *ğ‘¢,ğ‘–, ğ‘—* ) a partir de *ğ‘†* *ğ¾*

**4** *ğ‘ * Ë† *ğ‘¢ğ‘–ğ‘—* â† *ğ‘Ÿ* Ë† *ğ‘¢ğ‘–* âˆ’ *ğ‘Ÿ* Ë† *ğ‘¢ğ‘—*


*ğ‘’* [âˆ’] *[ğ‘ ğ‘¢ğ‘–ğ‘—]* [Ë†]
**5** Î˜ â† Î˜ + *ğ›¼*
ï¿½ 1+ *ğ‘’* [âˆ’] *[ğ‘ ğ‘¢ğ‘–ğ‘—]* [Ë†]


*ğœ•* *[ğœ•]* Î˜ *[ğ‘ ]* [Ë†] *[ğ‘¢ğ‘–ğ‘—]* [âˆ’] [Î›] [Î˜] [Î˜] ï¿½


*ğ‘’* *[ğ‘ ğ‘¢ğ‘–ğ‘—]* *[ğœ•]*

1+ *ğ‘’* [âˆ’] *[ğ‘ ğ‘¢ğ‘–ğ‘—]* [Ë†] *[.]* *ğœ•*


**Figura 4: O quadro Ã  esquerda representa os dados observados.**
**A abordagem cria uma relaÃ§Ã£o par de itens especÃ­fica para o**
**usuÃ¡rio** *ğ‘–* â‰» *ğ‘¢* *ğ‘—* **entre dois itens. No lado direito da tabela, o**
**sinal de mais indica que o usuÃ¡rio** *ğ‘¢* **estÃ¡ mais interessado no**
**item** *ğ‘–* **do que no item** *ğ‘—* **; o sinal de menos indica que o usuÃ¡rio**
**prefere o item** *ğ‘—* **ao** *ğ‘–* **; o ponto de interrogaÃ§Ã£o indica que nÃ£o**
**se pode inferir nenhuma conclusÃ£o entre os itens.**

positivos/negativos. Como as entradas observadas sÃ£o positivas e
as restantes sÃ£o negativas, o modelo serÃ¡ ajustado para fornecer
apenas pontuaÃ§Ãµes positivas para os itens observados. Os elementos
restantes, incluindo aqueles que podem ser de interesse para o usuÃ¡rio, serÃ£o classificados pelo modelo como pontuaÃ§Ãµes negativas e
a classificaÃ§Ã£o nÃ£o poderÃ¡ ser otimizada, pois as previsÃµes estarÃ£o
em torno de zero.

Os autores propuseram um mÃ©todo genÃ©rico para aprender o
comportamento do usuÃ¡rio para classificaÃ§Ã£o personalizada [ 20 ].
Em vez de treinar o modelo usando apenas os pares usuÃ¡rio-item,
eles tambÃ©m consideraram a ordem relativa entre um par de itens, de
acordo com as preferÃªncias do usuÃ¡rio. Se um item *ğ‘–* foi visualizado
pelo usuÃ¡rio *ğ‘¢* e *ğ‘—* nÃ£o ( *ğ‘–* âˆˆ *ğ‘* ( *ğ‘¢* ) e *ğ‘—* âˆˆ *ğ‘* ( *ğ‘¢* ) ), entÃ£o *ğ‘–* Ã© preferido
a *ğ‘—* . A Figura 4 mostra um exemplo do mÃ©todo. Quando *ğ‘–* e *ğ‘—* sÃ£o
desconhecidos para o usuÃ¡rio, ou equivalentemente, ambos sÃ£o
conhecidos, nenhuma conclusÃ£o sobre sua importÃ¢ncia relativa
para o usuÃ¡rio pode ser inferida.
Para estimar se um usuÃ¡rio prefere um item a outro, [ 20 ] propuseram uma anÃ¡lise Bayesiana usando uma funÃ§Ã£o de probabilidade
*ğ‘ğ‘Ÿğ‘œğ‘* ( *ğ‘–* â‰» *ğ‘¢* *ğ‘—* | *ğ‘¢,* Î˜) e a probabilidade anterior para o parÃ¢metro do
modelo *ğ‘ğ‘Ÿğ‘œğ‘* (Î˜) . O critÃ©rio final de otimizaÃ§Ã£o, BPR-Opt, Ã© definido

como:


**6** **end**

{ *ğ‘* *ğ‘¢* *,ğ‘* *ğ‘–* *,ğ‘* *ğ‘—* }, que devem ser aprendidos. Calculamos as derivadas
parciais em relaÃ§Ã£o a Ë† *ğ‘ * *ğ‘¢ğ‘–ğ‘—* :


*ğœ•*
*ğœ•* Î˜ *[ğ‘ ]* [Ë†] *[ğ‘¢ğ‘–ğ‘—]* [=]


*ğ‘* *ğ‘–* âˆ’ *ğ‘* *ğ‘—* quando Î˜ = *ğ‘* *ğ‘¢*
*ğ‘* *ğ‘¢* quando Î˜ = *ğ‘* *ğ‘–*

ï£±ï£´ï£´ï£´ï£´ï£´ï£²âˆ’ *ğ‘* *ğ‘¢* quando Î˜ = *ğ‘* *ğ‘—*

ï£´ï£´ï£´ï£´ï£´ï£³0 caso contrÃ¡rio


Esses gradientes sÃ£o entÃ£o usados para atualizar os fatores de
usuÃ¡rio e item em direÃ§Ã£o ao mÃ­nimo da funÃ§Ã£o de perda, iterativamente, atÃ© que a convergÃªncia seja alcanÃ§ada ou um nÃºmero fixo
de iteraÃ§Ãµes seja concluÃ­do. Desse modo, o SGD permite ajustar
os fatores de usuÃ¡rio e item de forma a maximizar a diferenÃ§a entre as pontuaÃ§Ãµes dos itens positivos e negativos, resultando em
recomendaÃ§Ãµes mais precisas e personalizadas.
### **3.4 BPR com CalibraÃ§Ã£o por Popularidade**

Com o objetivo de combinar um modelo em etapa de processamento com uma forma de calibraÃ§Ã£o de popularidade para trazer
recomendaÃ§Ãµes que reduzam o viÃ©s de popularidade no sistema, a
estratÃ©gia adotada foi alterar o algoritmo de aprendizado *LearnBPR*
(Algoritmo 1). Assim, pode-se combinar o BPR com a calibraÃ§Ã£o
por popularidade, acrescentando no algoritmo a divergÃªncia de
Kullback-Leibler implementada na calibraÃ§Ã£o de popularidade.
Essa combinaÃ§Ã£o pode possibilitar uma maior justiÃ§a nas recomendaÃ§Ãµes em termos de popularidade, jÃ¡ que esse aspecto seria
levado em conta na funÃ§Ã£o de perda do BPR. A alteraÃ§Ã£o Ã© realizada
na linha 5 do Algoritmo 1 somente quando Î˜ = *ğ‘* *ğ‘¢* :


*ğµğ‘ƒğ‘…* - *ğ‘‚ğ‘ğ‘¡* ï¿½

( *ğ‘¢,ğ‘–,ğ‘—* )âˆˆ

âˆ‘ï¸


ln *ğœ* ( *ğ‘ * Ë† *ğ‘¢ğ‘–ğ‘—* ) âˆ’ Î› Î˜ âˆ¥Î˜âˆ¥ [2]
( *ğ‘¢,ğ‘–,ğ‘—* )âˆˆ *ğ‘†* *ğ¾*

âˆ‘ï¸


*ğ‘’* [âˆ’] *[ğ‘ ]* [Ë†] *[ğ‘¢ğ‘–ğ‘—]*
*ğ‘* *ğ‘¢* â† *ğ‘* *ğ‘¢* + *ğ›¼*
ï¿½ 1 + *ğ‘’* [âˆ’] *[ğ‘ ]* [Ë†]


1 + *ğ‘’* [âˆ’] *[ğ‘ ]* [Ë†] *[ğ‘¢ğ‘–ğ‘—]* *[.]* [(] *[ğ‘]* *[ğ‘–]* [âˆ’] *[ğ‘]* *[ğ‘—]* [)]


âˆ’ Î› *ğ‘* *ğ‘¢* *ğ‘* *ğ‘¢*
ï¿½


(5)
ï¿½


onde Ë† *ğ‘ * *ğ‘¢ğ‘–ğ‘—* ï¿½ *ğ‘Ÿ* Ë† *ğ‘¢ğ‘–* âˆ’ *ğ‘Ÿ* Ë† *ğ‘¢ğ‘—* e *ğ‘†* *ğ¾* Ã© o conjunto de triplas ( *ğ‘¢,ğ‘–, ğ‘—* ) onde *ğ‘–*
estÃ¡ em *ğ‘* ( *ğ‘¢* ) e *ğ‘—* nÃ£o estÃ¡. O sÃ­mbolo Î˜ representa os parÃ¢metros
do modelo, Î› Î˜ Ã© o conjunto de constantes de regularizaÃ§Ã£o, e *ğœ* Ã© a
1
funÃ§Ã£o logÃ­stica definida como *ğœ* ( *ğ‘¥* ) = 1+ *ğ‘’* [âˆ’] *[ğ‘¥]* [.]
Os autores tambÃ©m propuseram uma variaÃ§Ã£o na tÃ©cnica de
descida de gradiente estocÃ¡stico, denominada LearnBPR, que amostra aleatoriamente de *ğ‘†* *ğ¾* para ajustar Î˜ . O Algoritmo 1 mostra
uma visÃ£o geral do mÃ©todo de aprendizagem, onde *ğ›¼* Ã© a taxa de
aprendizado.
No presente estudo, definimos a abordagem BPR para considerar
a regra de prediÃ§Ã£o Ë† *ğ‘Ÿ* *ğ‘¢ğ‘–* do modelo de fatorizaÃ§Ã£o simples definido
na EquaÃ§Ã£o 4. Portanto, aplicar a EquaÃ§Ã£o 4 em Ë† *ğ‘ * *ğ‘¢ğ‘–ğ‘—* resulta em Î˜ =


+ *ğœ†* 1 âˆ’ *[ğ·]* *[ğ¾ğ¿]* [(] *[ğ‘¥]* [âˆ¥] *[ğ‘¦]* [)]
ï¿½ *ğ·* *ğ¾ğ¿ğ‘£ğ‘œğ‘–ğ‘‘*


onde *ğœ†* Ã© utilizado como coeficiente do impacto que a divergÃªncia
terÃ¡ no sistema, e *ğ·* *ğ¾ğ¿ğ‘£ğ‘œğ‘–ğ‘‘* Ã© definido como:


*ğ·* *ğ¾ğ¿ğ‘£ğ‘œğ‘–ğ‘‘* =

*ğ‘¡*

âˆ‘ï¸


*ğ‘¡* *ğ‘¥* ( *ğ‘¡* | *ğ‘¢* ) Â· *ğ‘™ğ‘œğ‘”* *ğ›¼* *[ğ‘¥]* - *ğ‘¥* [(] *[ğ‘¡]* ( [|] *[ğ‘¢]* *ğ‘¡* | [)] *ğ‘¢*

âˆ‘ï¸


*ğ›¼* - *ğ‘¥* ( *ğ‘¡* | *ğ‘¢* ) (6)


A razÃ£o para dividir a divergÃªncia *ğ·* *ğ¾ğ¿* ( *ğ‘¥,ğ‘¦* ) por *ğ·* *ğ¾ğ¿ğ‘£ğ‘œğ‘–ğ‘‘* Ã© normalizar o valor da divergÃªncia, deixando o valor ajustado para uma
escala especÃ­fica. Essa normalizaÃ§Ã£o pode ser Ãºtil para realizar a
calibraÃ§Ã£o por popularidade entre diferentes usuÃ¡rios ou grupos,


314


-----

WebMediaâ€™2024, Juiz de Fora, Brazil Rodrigo Ferrari de Souza and Marcelo Garcia Manzato


independentemente do nÃºmero total de itens ou da escala de popularidade na base de dados, possibilitando a aplicaÃ§Ã£o em diferentes

contextos.

Ao considerar nÃ£o apenas as preferÃªncias individuais dos usuÃ¡rios, mas tambÃ©m a popularidade relativa dos itens, a abordagem
modificada pode levar a recomendaÃ§Ãµes mais relevantes e personalizadas. Isso pode resultar em uma melhor experiÃªncia do usuÃ¡rio e
maior satisfaÃ§Ã£o com o sistema de recomendaÃ§Ã£o.
### **4 AVALIAÃ‡ÃƒO**

A execuÃ§Ã£o do experimento foi realizada trÃªs vezes em dois conjuntos de dados do domÃ­nio de filmes para garantir a confiabilidade dos
resultados. A repetiÃ§Ã£o dos testes ajuda a mitigar o impacto de variaÃ§Ãµes aleatÃ³rias, assegurando que a mÃ©dia dos valores obtidos seja
representativa do desempenho do modelo. O t-test de Student foi
escolhido para anÃ¡lise por ser amplamente utilizado na comparaÃ§Ã£o
de mÃ©dias entre dois grupos, especialmente com amostras pequenas
ou moderadas [ 15 ], onde as variÃ¡veis seguem uma distribuiÃ§Ã£o aproximadamente normal. Como as bases de dados utilizadas possuem
dados contÃ­nuos e distribuÃ­dos de forma aproximada Ã  normal, este
teste Ã© adequado para a anÃ¡lise estatÃ­stica. A Tabela 1 resume as
informaÃ§Ãµes dos conjuntos de dados utilizados.

  - **Yahoo Movies** [1] : Este conjunto de dados Ã© uma classificaÃ§Ã£o
de filmes do usuÃ¡rio, onde o usuÃ¡rio atribui notas de um a
cinco aos filmes que assistiu. Na etapa de prÃ©-processamento,
foram removidos apenas filmes sem gÃªnero nos metadados.
Em vez de binarizar a classificaÃ§Ã£o como feito por [ 24 ], foi
utilizado o feedback explÃ­cito como o peso *ğ‘Ÿ* *ğ‘¢ğ‘–* na EquaÃ§Ã£o 1.

  - **MovieLens-20M** [2] : Neste conjunto de dados, semelhante
a [ 24 ] e em contraste com o conjunto de dados do Yahoo
Movies, foi feita a binarizaÃ§Ã£o das classificaÃ§Ãµes retendo as
interaÃ§Ãµes onde a classificaÃ§Ã£o era superior a 4. AlÃ©m disso,
devido a limitaÃ§Ãµes de hardware, o tamanho do conjunto
de dados foi reduzido, removendo filmes com menos de dez
interaÃ§Ãµes e usuÃ¡rios com menos de 180 filmes.

**Tabela 1: EstatÃ­sticas dos conjuntos de dados apÃ³s realizaÃ§Ã£o**
**do prÃ©-processamento.**

Con j unto de dados # UsuÃ¡rios # Intera Ã§ Ãµes # Itens
Yahoo Movies 7,642 211,231 11,916

MovieLens 20M 12,603 3,984,599 10,417

O experimento foi executado trÃªs vezes em cada conjunto de
dados para obter a mÃ©dia dos valores gerados pelas mÃ©tricas e
garantir a estabilidade dos resultados. Os conjuntos de dados de
teste e treinamento foram escolhidos dividindo aleatoriamente o

conjunto de dados em 70/30% de interaÃ§Ãµes, seguindo respectivamente [ 2, 9 ]. O desempenho da abordagem foi comparado com os
seguintes trabalhos do estado da arte:

(1) **BPR** : Proposta em [ 20 ], Ã© um algoritmo de recomendaÃ§Ã£o
projetado para lidar com dados de feedback implÃ­cito, onde
as interaÃ§Ãµes entre usuÃ¡rios e itens sÃ£o representadas como

1 https://webscope.sandbox.yahoo.com/
2 https://grouplens.org/datasets/movielens/20m/


preferÃªncias binÃ¡rias. Para os trÃªs conjuntos de dados, foi
aplicado o *batch* = 1024.
(2) ***PairWise*** : Proposto por [ 3 ], este mÃ©todo atua como uma
etapa de processamento para reduÃ§Ã£o do impacto do viÃ©s de
popularidade. Para o conjunto de dados do Yahoo Movies,
foram aplicados *ğ‘’ğ‘ğ‘œğ‘â„* = 100, *ğ‘ğ‘ğ‘¡ğ‘â„* = 1024 e escolhido o
melhor *ğ›¼* variando no intervalo [ 0 *,* 1 ] . Para o conjunto de
dados MovieLens, foram utilizados *ğ‘ğ‘ğ‘¡ğ‘â„* = 2048 e *ğ‘’ğ‘ğ‘œğ‘â„* =
20. A implementaÃ§Ã£o seguiu aquela feita pelos autores [3] .

**MÃ©tricas.** Em nossos experimentos, avaliamos os efeitos da calibraÃ§Ã£o em termos de precisÃ£o, justiÃ§a e viÃ©s de popularidade,
conforme detalhado a seguir:

(1) **PrecisÃ£o e Qualidade** : usamos as mÃ©tricas *Mean Reciprocal*
*Rank* (MRR) e *Mean Average Precision* (MAP) para medir
a qualidade da classificaÃ§Ã£o do item na lista reclassificada.
MAP e MRR variam no intervalo [ 0 *,* 1 ] onde **valores mais**
**altos sÃ£o melhores** .

(2) **JustiÃ§a** : utilizamos uma mÃ©trica proposta por [ 9 ], denominada *Mean Rank Miscalibration* (MRMC), que cobre o intervalo [ 0 *,* 1 ], onde **valores mais baixos sÃ£o melhores** .
Inicialmente, ela foi usada para calcular a justiÃ§a em gÃªneros na lista de recomendaÃ§Ãµes, mas neste trabalho ela foi
adaptada para calcular o erro de calibraÃ§Ã£o de popularidade.
Embora nossa proposta visa reduzir a injustiÃ§a em termos
de popularidade, nÃ³s tambÃ©m medimos a justiÃ§a de gÃªneros
neste trabalho. Para isso, usamos a mÃ©dia harmÃ´nica F1 entre
MRMC de gÃªneros e popularidade, onde **valores mais altos**
**sÃ£o melhores** :

*ğ¹* 1 = 2 [(] [1][ âˆ’] *[ğ‘€ğ‘…ğ‘€ğ¶ğºğ‘’ğ‘›ğ‘’ğ‘Ÿğ‘œ]* [)] [ âˆ—] [(] [1][ âˆ’] *[ğ‘€ğ‘…ğ‘€ğ¶ğ‘ƒğ‘œ]* *[ğ‘]* [)] (7)

(1 âˆ’ *ğ‘€ğ‘…ğ‘€ğ¶ğºğ‘’ğ‘›ğ‘’ğ‘Ÿğ‘œ* ) + (1 âˆ’ *ğ‘€ğ‘…ğ‘€ğ¶ğ‘ƒğ‘œğ‘* )

(3) **ViÃ©s de popularidade** : usamos as mÃ©tricas de cobertura
de cauda longa (LTC) [ 2 ] e popularidade mÃ©dia do grupo
( Î” GAP) [ 2 ] para medir o viÃ©s de popularidade. A mÃ©trica LTC
indica a fraÃ§Ã£o de itens que os usuÃ¡rios recebem nas listas de
recomendaÃ§Ã£o e varia no intervalo [ 0 *,* 1 ], onde 0 significa que
todos os itens recomendados sÃ£o os mais populares e 1 significa que todos os itens recomendados a um usuÃ¡rio estÃ£o nas
categorias menos populares. Assim, **quanto mais prÃ³ximo**
**de 1, mais diversificado serÃ¡ o conteÃºdo recomendado**

[ 2 ]. O Î” GAP varia no intervalo [âˆ’ 1 *,* 1 ], onde valores negativos significam que as recomendaÃ§Ãµes sÃ£o menos populares
do que o esperado segundo as preferÃªncias dos usuÃ¡rios, e
valores positivos significam que as recomendaÃ§Ãµes sÃ£o mais
populares do que o esperado. TambÃ©m adotamos trÃªs divisÃµes de grupos de usuÃ¡rios, com base em [ 2 ] para o Î” GAP:
**BlockBuster (BB)** cujo consumo dos usuÃ¡rios Ã© de pelo
menos 50% dos itens mais populares, **Nicho (N)** onde o consumo dos usuÃ¡rios Ã© de pelo menos 50% dos itens de menor
popularidade e **Diverso (D)** cujas preferÃªncias dos usuÃ¡rios
divergem dos outros dois grupos. Finalmente, como os valores Ã³timos de Î” *ğºğ´ğ‘ƒ* devem ser prÃ³ximos de zero, propomos
neste artigo a utilizaÃ§Ã£o do *Root Mean Squared Error* (RMSE)
entre os trÃªs grupos de usuÃ¡rios, onde **valores mais baixos**
**sÃ£o melhores** :

3 https://github.com/biasinrecsys/wsdm2021


315


-----

Uma Abordagem em Etapa de Processamento para ReduÃ§Ã£o do ViÃ©s de Popularidade WebMediaâ€™2024, Juiz de Fora, Brazil


Î” *ğºğ´ğ‘ƒ* [2]
*ğµğµ* [+][ Î”] *[ğºğ´ğ‘ƒ]* *ğ‘* [2] [+][ Î”] *[ğºğ´ğ‘ƒ]* *ğ·* [2]
(8)
3


reduziu com sucesso o viÃ©s de popularidade para diferentes grupos
de usuÃ¡rios.

A Tabela 2 reporta resultados semelhantes aos do conjunto de
dados Yahoo Movies, indicando que a proposta melhorou a justiÃ§a
dos gÃªneros e a popularidade em ambos os conjuntos de dados. Embora a abordagem de calibraÃ§Ã£o proposta nÃ£o tenha alcanÃ§ado alta
precisÃ£o, obteve o menor erro de calibraÃ§Ã£o de gÃªnero e de popularidade, o que significa que o modelo fornece recomendaÃ§Ãµes que
respeitam o perfil do usuÃ¡rio tanto no gÃªnero quanto no consumo
de popularidade.
### **6 CONCLUSÃƒO**

O objetivo do BPR Ã© aprender representaÃ§Ãµes latentes para usuÃ¡rios
e itens que capturem suas preferÃªncias individuais. O procedimento
de aprendizado do BPR envolve a otimizaÃ§Ã£o de uma funÃ§Ã£o de
perda que visa maximizar a ordenaÃ§Ã£o correta dos pares de itens
positivos e negativos para cada usuÃ¡rio. Isso Ã© feito por meio de
gradiente descendente estocÃ¡stico, onde os gradientes da funÃ§Ã£o de
perda sÃ£o calculados para atualizar os vetores latentes dos usuÃ¡rios
e dos itens.
A modificaÃ§Ã£o proposta, que combina o BPR com a calibraÃ§Ã£o
por popularidade, visa melhorar a justiÃ§a nas recomendaÃ§Ãµes, considerando nÃ£o apenas as preferÃªncias individuais dos usuÃ¡rios, mas
tambÃ©m a popularidade relativa dos itens. Isso Ã© alcanÃ§ado incorporando a divergÃªncia de Kullback-Leibler na funÃ§Ã£o de perda do
BPR, levando a recomendaÃ§Ãµes mais relevantes e personalizadas. Os
experimentos realizados em dois conjuntos de dados mostram que a
abordagem modificada obtÃ©m resultados comparÃ¡veis ou melhores
em relaÃ§Ã£o aos mÃ©todos do estado da arte, tanto em mÃ©tricas de
classificaÃ§Ã£o quanto em mÃ©tricas de popularidade e justiÃ§a.
No entanto, Ã© importante ressaltar que a abordagem proposta
ainda pode ser aprimorada em vÃ¡rios aspectos. Por exemplo, a escolha dos parÃ¢metros do modelo, como o tamanho do lote e o nÃºmero
de Ã©pocas, pode afetar significativamente o desempenho do sistema.
AlÃ©m disso, a implementaÃ§Ã£o de tÃ©cnicas adicionais de regularizaÃ§Ã£o ou otimizaÃ§Ã£o pode ajudar a evitar o sobreajuste e melhorar a
convergÃªncia do modelo. HÃ¡ tambÃ©m a possibilidade de combinar
tÃ©cnicas de reduÃ§Ã£o do viÃ©s de popularidade com os sistemas conversacionais [ 28 ]. Futuras pesquisas podem explorar essas direÃ§Ãµes
para desenvolver ainda mais a abordagem proposta e melhorar sua
eficÃ¡cia em uma variedade de cenÃ¡rios de recomendaÃ§Ã£o.
### **AGRADECIMENTOS**

Os autores gostariam de agradecer o apoio financeiro da FAPESP,
processo nÃºmero 2022/07016-9, e CNPq.
### **REFERÃŠNCIAS**

[1] Himan Abdollahpouri, Masoud Mansoury, Robin Burke, and Bamshad Mobasher. 2020. The connection between popularity bias, calibration, and fairness in recommendation. In *Fourteenth ACM conference on recommender sys-*
*tems* . Association for Computing Machinery, New York, NY, USA, 726â€“731.
https://doi.org/10.1145/3383313.3418487

[2] Himan Abdollahpouri, Masoud Mansoury, Robin Burke, Bamshad Mobasher,
and Edward C. Malthouse. 2021. User-centered Evaluation of Popularity Bias
in Recommender Systems. In *Proceedings of the 29th ACM Conference on User*
*Modeling, Adaptation and Personalization, UMAP 2021, Utrecht, The Netherlands,*
*June, 21-25, 2021*, Judith Masthoff, Eelco Herder, Nava Tintarev, and Marko Tkalcic
(Eds.). ACM, 119â€“129. https://doi.org/10.1145/3450613.3456821


*ğ‘…ğ‘€ğ‘†ğ¸* =


âˆš

### **5 RESULTADOS** **5.1 Yahoo Movies**

A Tabela 2 apresenta os resultados obtidos para o conjunto de
dados Yahoo Movies. Analisando apenas a **precisÃ£o** dos modelos
pela mÃ©trica MAP, notamos que a abordagem *PairWise* [ 3 ] atingiu
o maior valor de MAP. No entanto, esta conquista significa que
os itens nÃ£o sÃ£o muito diversos entre si, como mostram os seus
resultados relativos a LTC, F1 e RMSE.
Em relaÃ§Ã£o Ã  **justiÃ§a dos gÃªneros** atravÃ©s do MRMC de gÃªneros,
a Tabela 2 indica que a proposta de calibraÃ§Ã£o combinada com o BPR
produziu o melhor resultado, indicando que foi capaz de fornecer
itens mais prÃ³ximos do perfil em termos de gÃªnero. O mesmo foi
verificado em relaÃ§Ã£o Ã  **justiÃ§a de popularidade**, com a proposta
tendo o melhor resultado do MRMC Pop.
Em termos de **cobertura de cauda longa**, a tabela indica que
o modelo mais eficaz para recomendar itens diversos foi o BPR. O
*PairWise* com pontuaÃ§Ãµes mais altas no MAP obteve valores mais
baixos para o LTC. Em relaÃ§Ã£o Ã  mÃ©trica **F1**, Ã© possÃ­vel observar que
a proposta conseguiu alcanÃ§ar o melhor resultado, indicando que a
abordagem de calibraÃ§Ã£o foi capaz de calibrar recomendaÃ§Ãµes de
acordo com gÃªneros e popularidade. Este aspecto Ã© ainda validado
ao analisar a mÃ©trica **RMSE**, onde a mesma abordagem obteve
menor erro com a calibraÃ§Ã£o, indicando que ela aborda os pontos
de justiÃ§a mencionados e reduz o viÃ©s de popularidade do sistema.
Os resultados relatados na Tabela 2 mostram que a abordagem
de calibraÃ§Ã£o foi capaz de equilibrar recomendaÃ§Ãµes de acordo com
gÃªneros e popularidade, em oposiÃ§Ã£o aos outros trabalhos, que sÃ£o
mais adequados para um Ãºnico aspecto, como precisÃ£o, gÃªneros
ou popularidade. AlÃ©m disso, os resultados mostram a importÃ¢ncia
de adotar mÃ©tricas alÃ©m da precisÃ£o na anÃ¡lise de algoritmos de
recomendaÃ§Ã£o. Reconhece-se a alta precisÃ£o do *PairWise*, conforme
indicado pela mÃ©trica MAP. No entanto, os usuÃ¡rios que preferem
itens de nicho, diversos e impopulares sÃ£o afetados por recomendaÃ§Ãµes injustas e tendenciosas produzidas por essas abordagens.
### **5.2 MovieLens 20M**

A Tabela 2 apresenta os resultados obtidos para o conjunto de dados
MovieLens 20M. Analisando a **precisÃ£o**, assim como na base de
dados anterior, o *PairWise* [ 3 ] superou as outras abordagens. No
entanto, os resultados tambÃ©m indicam que estas abordagens devolvem recomendaÃ§Ãµes injustas em termos de gÃªnero e popularidade,
e carecem de diversidade.

Em relaÃ§Ã£o Ã  **justiÃ§a dos gÃªneros** e Ã  **justiÃ§a de popularidade**,
a abordagem de calibraÃ§Ã£o proposta obteve os melhores resultados,
fato confirmado pela mÃ©trica F1. Em relaÃ§Ã£o Ã  **cobertura de cauda**
**longa**, o BPR obteve o melhor resultado entre todas as abordagens. AlÃ©m disso, o *PairWise* [ 3 ] alcanÃ§ou um valor baixo para essa
mÃ©trica, apesar de ter uma alta precisÃ£o.
Com relaÃ§Ã£o ao F1, pode-se observar que a proposta obteve os
melhores valores, destacando seu alto desempenho em termos de
justiÃ§a nos gÃªneros e popularidade. Ademais, a proposta tambÃ©m
obteve o melhor resultado em **RMSE**, indicando que o sistema


316


-----

WebMediaâ€™2024, Juiz de Fora, Brazil Rodrigo Ferrari de Souza and Marcelo Garcia Manzato

**Tabela 2: ComparaÃ§Ã£o da abordagem proposta com os outros trabalhos nos conjuntos de dados Yahoo Movies e MovieLens 20M.**
**O sÃ­mbolo** - **significa que a proposta teve um ganho significativo com relaÃ§Ã£o aos outros trabalhos, com um** ***p-value*** *<* 0 *.* 05
**usando o** ***t-test*** **de Student; o sÃ­mbolo** - **significa que nÃ£o houve um ganho ou perda significativo; e o sÃ­mbolo** - **indica que o**
**outro trabalho Ã© estatisticamente melhor que a proposta. Cada par de sÃ­mbolos se refere ao BPR e ao** ***PairWise*** **, respectivamente.**

Yahoo Movies

**Al** **g** **oritmo** **LTC** **MRMC GÃªneros** **MRMC Po** **p** **.** **F1 Score** **MRR** **MAP** Î” *ğºğ´ğ‘ƒ* *ğµğµ* Î” *ğºğ´ğ‘ƒ* *ğ‘* Î” *ğºğ´ğ‘ƒ* *ğ·* **RMSE**

BPR 0.409 0.629 0.687 0.340 0.002 0.001 -0.991 -0.881 -0.978 0.549

*PairWise* 0.140 0.696 0.661 0.321 0.012 0.038 -0.680 3.105 0.043 1.060
BPR Modificado 0.317 â–¼â–² 0.589 0.496 0.444 â–²â–² 0.012 â–² - 0.004 â–²â–¼ -0.934 -0.142 -0.835 0.420 â–²â–²

MovieLens 20M

**Al** **g** **oritmo** **LTC** **MRMC GÃªneros** **MRMC Po** **p** **.** **F1 Score** **MRR** **MAP** Î” *ğºğ´ğ‘ƒ* *ğµğµ* Î” *ğºğ´ğ‘ƒ* *ğ‘* Î” *ğºğ´ğ‘ƒ* *ğ·* **RMSE**

BPR 0.513 0.459 0.409 0.565 0.001 0.001 -0.912 -0.340 -0.790 0.419

*PairWise* 0.110 0.554 0.501 0.452 0.776 0.583 -0.997 -0.997 -0.996 0.575
BPR Modificado 0.464 â–¼â–² 0.453 0.330 0.596 â–²â–² 0.002 â–²â–¼ 0.001 â€¢ â–¼ -0.865 -0.060 -0.693 0.370 â–²â–²



[3] Ludovico Boratto, Gianni Fenu, and Mirko Marras. 2021. Connecting user and
item perspectives in popularity debiasing for collaborative recommendation.
*Information Processing & Management* 58, 1 (2021), 102387.

[4] Sung-Hyuk Cha. 2007. Comprehensive survey on distance/similarity measures
between probability density functions. *City* 1, 2 (2007), 1.

[5] Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan
He. 2023. Bias and debias in recommender system: A survey and future directions.
*ACM Transactions on Information Systems* 41, 3 (2023), 1â€“39.

[6] Xiao Chen, Wenqi Fan, Jingfan Chen, Haochen Liu, Zitao Liu, Zhaoxiang Zhang,
and Qing Li. 2023. Fairly adaptive negative sampling for recommendations. In
*Proceedings of the ACM Web Conference 2023* . 3723â€“3733.

[7] Zhihong Chen, Jiawei Wu, Chenliang Li, Jingxu Chen, Rong Xiao, and Binqiang
Zhao. 2022. Co-training disentangled domain adaptation network for leveraging
popularity bias in recommenders. In *Proceedings of the 45th International ACM*
*SIGIR Conference on Research and Development in Information Retrieval* . 60â€“69.

[8] Diego CorrÃªa da Silva and Frederico AraÃºjo DurÃ£o. 2023. Introducing a framework
and a decision protocol to calibrated recommender systems. *Applied Intelligence*
(2023), 1â€“29.

[9] Diego CorrÃªa da Silva, Marcelo Garcia Manzato, and Frederico AraÃºjo DurÃ£o. 2021.
Exploiting personalized calibration and metrics for fairness recommendation.
*Expert Systems with Applications* 181 (2021), 115112.

[10] Yashar Deldjoo, Dietmar Jannach, Alejandro Bellogin, Alessandro Difonzo, and
Dario Zanzonelli. 2023. Fairness in recommender systems: research landscape
and future directions. *User Modeling and User-Adapted Interaction* (2023), 1â€“50.

[11] Michael FÃ¤rber, Melissa Coutinho, and Shuzhou Yuan. 2023. Biases in scholarly
recommender systems: impact, prevalence, and mitigation. *Scientometrics* 128, 5
(2023), 2703â€“2736.

[12] Alireza Gharahighehi, Celine Vens, and Konstantinos Pliakos. 2021. Fair multistakeholder news recommender system with hypergraph ranking. *Information*
*Processing & Management* 58, 5 (2021), 102663.

[13] Ruben Interian, RuslÃ¡n G. Marzo, Isela Mendoza, and Celso C Ribeiro. 2023.
Network polarization, filter bubbles, and echo chambers: an annotated review
of measures and reduction methods. *International Transactions in Operational*
*Research* 30, 6 (2023), 3122â€“3158.

[14] Faisal Kamiran and Toon Calders. 2012. Data preprocessing techniques for
classification without discrimination. *Knowledge and information systems* 33, 1
(2012), 1â€“33.

[15] Tae Kyun Kim. 2015. T test as a parametric statistic. *Korean journal of anesthesio-*
*logy* 68, 6 (2015), 540â€“546.

[16] Dawen Liang, Rahul G Krishnan, Matthew D Hoffman, and Tony Jebara. 2018.
Variational autoencoders for collaborative filtering. In *Proceedings of the 2018*
*world wide web conference* . 689â€“698.

[17] Haifeng Liu, Nan Zhao, Xiaokun Zhang, Hongfei Lin, Liang Yang, Bo Xu, Yuan
Lin, and Wenqi Fan. 2022. Dual constraints and adversarial learning for fair
recommenders. *Knowledge-Based Systems* 239 (2022), 108058.

[18] MEJ Newman. 2005. Power laws, Pareto distributions and Zipf's law. *Contempo-*
*rary Physics* 46, 5 (sep 2005), 323â€“351. https://doi.org/10.1080/00107510500052444

[19] Evaggelia Pitoura, Kostas Stefanidis, and Georgia Koutrika. 2022. Fairness in
rankings and recommendations: an overview. *The VLDB Journal* (2022), 1â€“28.

[20] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.
2012. BPR: Bayesian personalized ranking from implicit feedback. *arXiv preprint*
*arXiv:1205.2618* (2012).

[21] Wondo Rhee, Sung Min Cho, and Bongwon Suh. 2022. Countering Popularity
Bias by Regularizing Score Differences. In *Proceedings of the 16th ACM Conference*
*on Recommender Systems* . 145â€“155.



[22] Yuji Roh, Kangwook Lee, Steven Whang, and Changho Suh. 2021. Sample selection for fair and robust training. *Advances in Neural Information Processing*
*Systems* 34 (2021), 815â€“827.

[23] Andre Sacilotti., Rodrigo Souza., and Marcelo G. Manzato. 2023. Counteracting
Popularity-Bias and Improving Diversity Through Calibrated Recommendations.
In *Proceedings of the 25th International Conference on Enterprise Information*
*Systems - Volume 1: ICEIS* . INSTICC, SciTePress, 709â€“720. https://doi.org/10.
5220/0011846000003467

[24] Harald Steck. 2018. Calibrated recommendations. In *Proceedings of the 12th ACM*
*conference on recommender systems* . 154â€“162.

[25] Amit Sultan, Avi Segal, Guy Shani, and Yaâ€™akov Gal. 2022. Addressing Popularity
Bias in Citizen Science. In *Proceedings of the 2022 ACM Conference on Information*
*Technology for Social Good* . 17â€“23.

[26] Sahil Verma, Ruoyuan Gao, and Chirag Shah. 2020. Facets of fairness in search
and recommendation. In *Bias and Social Aspects in Search and Recommendation:*
*First International Workshop, BIAS 2020, Lisbon, Portugal, April 14, Proceedings 1* .
Springer, 1â€“11.

[27] Lili Wang, Sunit Mistry, Abdulkadir Abdulahi Hasan, Abdiaziz Omar Hassan,
Yousuf Islam, and Frimpong Atta Junior Osei. 2023. Implementation of a Collaborative Recommendation System Based on Multi-Clustering. *Mathematics* 11, 6
(2023), 1346.

[28] Xi Wang, Hossein A Rahmani, Jiqun Liu, and Emine Yilmaz. 2023. Improving
Conversational Recommendation Systems via Bias Analysis and Language-ModelEnhanced Data Augmentation. *arXiv preprint arXiv:2310.16738* (2023).

[29] Tianxin Wei, Fuli Feng, Jiawei Chen, Ziwei Wu, Jinfeng Yi, and Xiangnan He.
2021. Model-agnostic counterfactual reasoning for eliminating popularity bias
in recommender system. In *Proceedings of the 27th ACM SIGKDD Conference on*
*Knowledge Discovery & Data Mining* . 1791â€“1800.

[30] Meike Zehlike and Carlos Castillo. 2020. Reducing disparate exposure in ranking:
A learning to rank approach. In *Proceedings of the web conference 2020* . 2849â€“2855.

[31] Yang Zhang, Fuli Feng, Xiangnan He, Tianxin Wei, Chonggang Song, Guohui
Ling, and Yongdong Zhang. 2021. Causal intervention for leveraging popularity
bias in recommendation. In *Proceedings of the 44th International ACM SIGIR*
*Conference on Research and Development in Information Retrieval* . 11â€“20.


317


-----

