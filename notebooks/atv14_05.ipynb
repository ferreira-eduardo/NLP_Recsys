{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-12T22:14:24.069066Z",
     "start_time": "2025-05-12T22:14:24.066347Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from process_text import process_pdfs\n",
    "import pymupdf4llm\n",
    "from PyPDF2 import PdfReader\n",
    "from process_text import clean_full_text"
   ],
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T21:50:25.274546Z",
     "start_time": "2025-05-12T21:50:25.064323Z"
    }
   },
   "cell_type": "code",
   "source": "corpus = pd.read_json('../articles/corpus/corpus.json')",
   "id": "49b93678be0305fb",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:44:08.058110Z",
     "start_time": "2025-05-12T22:44:08.055800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_abs(text: str) -> str:\n",
    "    text = text.replace('**', '')\n",
    "    text = text.replace('*', '')\n",
    "    text = text.replace('###', '')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.strip()\n",
    "    return text"
   ],
   "id": "4c0d0608f90cfee8",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T21:55:05.547147Z",
     "start_time": "2025-05-12T21:55:05.544475Z"
    }
   },
   "cell_type": "code",
   "source": "corpus['resumo_clean'] = corpus['resumo'].apply(lambda x: clean_abs(x))",
   "id": "c125e69a4614c5dd",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:05:20.137161Z",
     "start_time": "2025-05-12T22:05:20.133803Z"
    }
   },
   "cell_type": "code",
   "source": "corpus.loc[13]['referencias']",
   "id": "d02fab3bd0beb57b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[1] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al . 2023. Gemini: a family of highly capable multimodal models.',\n",
       " '[2] David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. Journal of machine Learning research 3, Jan (2003), 993–1022.',\n",
       " '[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al . 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877–1901.',\n",
       " '[4] Antonio J. G. Busson, Rafael Rocha, Rennan Gaio, Rafael Miceli, Ivan Pereira, Daniel de S. Moraes, Sérgio Colcher, Alvaro Veiga, Bruno Rizzi, Francisco Evangelista, Leandro Santos, Fellipe Marques, Marcos Rabaioli, Diego Feldberg, Debora Mattos, João Pasqua, and Diogo Dias. 2023. Hierarchical Classification of Financial Transactions Through Context-Fusion of Transformer-based Embeddings and Taxonomy-aware Attention Layer. In Anais do II Brazilian Workshop on Arti- ficial Intelligence in Finance (BWAIF 2023) (BWAIF 2023) . Sociedade Brasileira de Computação. https://doi.org/10.5753/bwaif.2023.229322',\n",
       " '[5] Ricardo Campos, Vítor Mangaravite, Arian Pasquali, Alípio Jorge, Célia Nunes, and Adam Jatowt. 2020. YAKE! Keyword extraction from single documents using multiple local features. Information Sciences 509 (2020), 257–289.',\n",
       " '[6] Boqi Chen, Fandi Yi, and Dániel Varró. 2023. Prompting or Fine-tuning? A Comparative Study of Large Language Models for Taxonomy Construction. In 2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C) . IEEE, 588–596.',\n",
       " '[7] Sabit Ekin. 2023. Prompt engineering for ChatGPT: a quick guide to techniques, tips, and best practices. Authorea Preprints (2023).',\n",
       " '[8] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021).',\n",
       " '[9] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al . 2024. Mixtral of experts. arXiv preprint arXiv:2401.04088 (2024).',\n",
       " '[10] Dongha Lee, Jiaming Shen, SeongKu Kang, Susik Yoon, Jiawei Han, and Hwanjo Yu. 2022. TaxoCom: Topic Taxonomy Completion with Hierarchical Discovery of Novel Topic Clusters. In Proceedings of the ACM Web Conference 2022 . 2819–2829.',\n",
       " '[11] Yinheng Li. 2023. A Practical Survey on Zero-Shot Prompt Design for In-Context Learning. In Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing . 641–647.',\n",
       " '[12] Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. 2023. Unleashing Infinite-Length Input Capacity for Largescale Language Models with Self-Controlled Memory System. arXiv preprint arXiv:2304.13343 (2023).',\n",
       " '[13] Irina Nikishina, Varvara Logacheva, Alexander Panchenko, and Natalia Loukachevitch. 2020. RUSSE’2020: Findings of the First Taxonomy Enrichment   273   -----  WebMedia’2024, Juiz de Fora, Brazil Moraes et al.   Task for the Russian language. arXiv preprint arXiv:2005.11176 (2020).',\n",
       " '[14] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]',\n",
       " '[15] Octavian Popescu and Carlo Strapparava. 2015. Semeval 2015, task 7: Diachronic text evaluation. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015) . 870–878.',\n",
       " '[16] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research 21, 1 (2020), 5485–5551.',\n",
       " '[17] Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended abstracts of the 2021 CHI conference on human factors in computing systems . 1–7.',\n",
       " '[18] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and Aman Chadha. 2024. A systematic survey of prompt engineering in large language models: Techniques and applications. arXiv preprint arXiv:2402.07927 (2024).',\n",
       " '[19] Rion Snow, Daniel Jurafsky, and Andrew Ng. 2004. Learning syntactic patterns for automatic hypernym discovery. Advances in neural information processing systems 17 (2004).',\n",
       " '[20] Kunihiro Takeoka, Kosuke Akimoto, and Masafumi Oyamada. 2021. Low-resource taxonomy enrichment with pretrained language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing . 2747–2758.',\n",
       " '[21] Adrian Tam. [n. d.]. What Are Zero-Shot Prompting and Few-Shot Prompting. https://machinelearningmastery.com/what-are-zero-shot-prompting-andfew-shot-prompting/. Accessed: 2024-07-02.',\n",
       " '[22] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al . 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023).',\n",
       " '[23] Erlend Vollset, Eirik Folkestad, Marius Rise Gallala, and Jon Atle Gulla. 2017. Making use of external company data to improve the classification of bank transactions. In Advanced Data Mining and Applications: 13th International Conference, ADMA 2017, Singapore, November 5–6, 2017, Proceedings 13 . Springer, 767–780.',\n",
       " '[24] Hanna Wallach, David Mimno, and Andrew McCallum. 2009. Rethinking LDA: Why priors matter. Advances in neural information processing systems 22 (2009).',\n",
       " '[25] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171 (2022).',\n",
       " '[26] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, H Chi, Quoc V Le, and Denny Zhou. [n. d.]. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. ([n. d.]).',\n",
       " '[27] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting in Language Models. In International Conference on Learning Representations (ICLR) .',\n",
       " '[28] Chao Zhang, Fangbo Tao, Xiusi Chen, Jiaming Shen, Meng Jiang, Brian Sadler, Michelle Vanni, and Jiawei Han. 2018. TaxoGen: Unsupervised Topic Taxonomy Construction by Adaptive Term Embedding and Clustering. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Min- ing (London, United Kingdom) (KDD ’18) . Association for Computing Machinery, New York, NY, USA, 2701–2709. https://doi.org/10.1145/3219819.3220064   274   -----']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T21:56:03.456251Z",
     "start_time": "2025-05-12T21:56:03.453128Z"
    }
   },
   "cell_type": "code",
   "source": "corpus.loc[13]['text']",
   "id": "bc132a540f1d11f6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9/02/2008 at 19:20:05 Italian Cuisine Pasta Sauce Pizza Wine |name|macrocategory|microcategory| |---|---|---| |Nona Ristorante|Food|Restaurant| Figure 1: Overview of our method. (Note: Merchant and transaction data are fabricated for demonstration only). ### ABSTRACT This work presents an unsupervised method for tagging banking consumers’ transactions using automatically constructed and expanded topic taxonomies. Initially, we enrich the bank transactions via web scraping to collect relevant descriptions, which are then preprocessed using NLP techniques to generate candidate terms. Topic taxonomies are created using instruction-based fine-tuned LLMs (Large Language Models). To expand existing taxonomies with new terms, we use zero-shot prompting to determine where to add new nodes. The resulting taxonomies are used to assign descriptive tags that characterize the transactions in the retail bank dataset. For evaluation, 12 volunteers completed a two-part form In: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe dia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024. © 2024 SBC – Brazilian Computing Society. ISSN 2966-2753 assessing the quality of the taxonomies and the tags assigned to merchants. The evaluation revealed a coherence rate exceeding 90% for the chosen taxonomies. Additionally, taxonomy expansion using LLMs demonstrated promising results for parent node prediction, with F1-scores of 89% and 70% for Food and Shopping taxonomies, respectively. ### KEYWORDS Large Language Models, Natural Language Processing, Web Scrapping, Topic Modeling ###'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T21:57:52.753838Z",
     "start_time": "2025-05-12T21:57:50.542519Z"
    }
   },
   "cell_type": "code",
   "source": "text = pymupdf4llm.to_markdown('../articles/985-24767-1-10-20240923.pdf')",
   "id": "eb3ee17c3bfcd4f9",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:06:25.455209Z",
     "start_time": "2025-05-12T22:06:25.453180Z"
    }
   },
   "cell_type": "code",
   "source": "from process_text import clean_abstract",
   "id": "777bf96d8aa85243",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:20:03.952547Z",
     "start_time": "2025-05-12T22:20:03.950306Z"
    }
   },
   "cell_type": "code",
   "source": "abstract = text.split('**ABSTRACT**')[1].split('KEYWORDS')[0].strip()",
   "id": "2d8743d0c12150ba",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:20:06.618052Z",
     "start_time": "2025-05-12T22:20:06.614961Z"
    }
   },
   "cell_type": "code",
   "source": "abstract",
   "id": "a985819288946be8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This work presents an unsupervised method for tagging banking\\nconsumers’ transactions using automatically constructed and expanded topic taxonomies. Initially, we enrich the bank transactions\\nvia web scraping to collect relevant descriptions, which are then\\npreprocessed using NLP techniques to generate candidate terms.\\nTopic taxonomies are created using instruction-based fine-tuned\\nLLMs (Large Language Models). To expand existing taxonomies\\nwith new terms, we use zero-shot prompting to determine where\\nto add new nodes. The resulting taxonomies are used to assign\\ndescriptive tags that characterize the transactions in the retail bank\\ndataset. For evaluation, 12 volunteers completed a two-part form\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n© 2024 SBC – Brazilian Computing Society.\\nISSN 2966-2753\\n\\n\\nassessing the quality of the taxonomies and the tags assigned to\\nmerchants. The evaluation revealed a coherence rate exceeding 90%\\nfor the chosen taxonomies. Additionally, taxonomy expansion using\\nLLMs demonstrated promising results for parent node prediction,\\nwith F1-scores of 89% and 70% for *Food* and *Shopping* taxonomies,\\nrespectively.\\n### **'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:20:46.846154Z",
     "start_time": "2025-05-12T22:20:46.844092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "abstract = abstract.replace('\\n', ' ')\n",
    "abstract = abstract.replace('In: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMedia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.© 2024 SBC – Brazilian Computing Society.ISSN 2966-2753','')"
   ],
   "id": "6ce226bad5530de4",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:20:47.538191Z",
     "start_time": "2025-05-12T22:20:47.535506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "abstract = clean_abs(abstract)\n",
    "abstract"
   ],
   "id": "d0b38b45ec71894d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This work presents an unsupervised method for tagging banking consumers’ transactions using automatically constructed and expanded topic taxonomies. Initially, we enrich the bank transactions via web scraping to collect relevant descriptions, which are then preprocessed using NLP techniques to generate candidate terms. Topic taxonomies are created using instruction-based fine-tuned LLMs (Large Language Models). To expand existing taxonomies with new terms, we use zero-shot prompting to determine where to add new nodes. The resulting taxonomies are used to assign descriptive tags that characterize the transactions in the retail bank dataset. For evaluation, 12 volunteers completed a two-part form  In: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe dia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024. © 2024 SBC – Brazilian Computing Society. ISSN 2966-2753   assessing the quality of the taxonomies and the tags assigned to merchants. The evaluation revealed a coherence rate exceeding 90% for the chosen taxonomies. Additionally, taxonomy expansion using LLMs demonstrated promising results for parent node prediction, with F1-scores of 89% and 70% for Food and Shopping taxonomies, respectively.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:13:48.624986Z",
     "start_time": "2025-05-12T22:13:48.622679Z"
    }
   },
   "cell_type": "code",
   "source": "text_temp = text.split('INTRODUCTION')[1]",
   "id": "c1180aa4666977e1",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:13:49.152342Z",
     "start_time": "2025-05-12T22:13:49.149925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "pattern = r'REFERENCES|REFERÊNCIAS'\n",
    "text_temp, *_ = re.split(pattern, text_temp, maxsplit=1)\n"
   ],
   "id": "b1bc17820c355872",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:15:21.550809Z",
     "start_time": "2025-05-12T22:15:21.548229Z"
    }
   },
   "cell_type": "code",
   "source": "text_temp = clean_abs(clean_full_text(text_temp.strip()))",
   "id": "cbe63812701aaa30",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:15:22.159117Z",
     "start_time": "2025-05-12T22:15:22.155504Z"
    }
   },
   "cell_type": "code",
   "source": "text_temp",
   "id": "4ecede92c28e2999",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Many recent studies have focused on the application of Machine Learning-based methods for the classification and characterization of financial transactions. For example, Vollset et al . [23] and Busson et al . [4] explored an approach to hierarchically classifying 267 ----- WebMedia’2024, Juiz de Fora, Brazil Moraes et al. financial transactions, employing a predefined set of categories/subcategories that describe purchase types and transactions. However, these methods apply a limited, predefined set of static classes, which restricts the ability to extend classifications based on user experiences when encountering new, previously undefined categories. In this context, to expand the possible set of classes/tags to label a transaction, we developed an unsupervised method based on topic taxonomies . Taxonomies are very useful in the structural and semantic analyses of topics and textual data. However, creating and maintaining them is often costly and challenging to scale manually. Therefore, recent works have tackled the automatic creation and expansion of topic taxonomies, in which each node in a hierarchy represents a conceptual topic composed of semantically coherent terms. We present an unsupervised method for automatically constructing and expanding topic taxonomies with instruction-based LLMs (Large Language Models), in a Zero-Shot manner. Candidate terms for the initial version of the taxonomy are obtained using topic modeling and keyword extraction techniques. Then we apply LLMs to post-process the resulting terms, create a hierarchy, and add new terms to an existing taxonomy. Since the taxonomies are derived from a corpus of unstructured texts describing niches of consuming habits, we opted to investigate the use of LLMs in our approach. LLMs are often pre-trained on a large corpus of text, allowing them to learn contextual representations that capture the intricacies of human language. We applied our method to a private dataset of transactions of a retail bank, enriched with scraped data from food and shopping companies, and evaluated the resulting taxonomies quantitatively. The generated tags of our topic taxonomies are then assigned to the bank transactions characterizing the companies in each transaction, as shown on Figure 1. In total, 58 topic taxonomies were created for the Food category and 6 for the Shopping category. A two-step quantitative evaluation was conducted on a subset of the taxonomies. For this evaluation, we selected the topic taxonomies with the highest number of terms in each category: \"Brazilian Cuisine\" from Food and \"Clothing and Accessories\" from Shopping . Taxonomies with more terms are most likely to result in a deeper hierarchy, which gives more data for evaluation. We asked 12 volunteers to answer a two-part form, which assessed the quality of the created taxonomies and the quality of the tags assigned to label transactions. The evaluation showed an average coherence of tags to transactions above 90%. As more scraped data from food and shopping companies are added to the retail bank’s dataset, the topic taxonomies will need to be updated to include new terms. We used LLMs for this task as well, employing commercial LLMs like Gemini Pro [1] and GPT4 [14], alongside open-source LLM options such as LLaMA-Alpaca (7B) [22], Phi-2 [1], and Mixtral 8x7B [9]. We showcase their results in both taxonomy creation and expansion. For the expansion part, we also compared our method to existing ones (a BERT-based method and Musubu[20]) on the SemEval dataset and our generated taxonomies as well. Gemini Pro achieved the best results, with F1scores of 89% and 70% for parent node prediction on the Food and Shopping taxonomies, respectively. 1 https://huggingface.co/microsoft/phi-2 The remainder of the paper is structured as follows: Section 2, reviews the related work, highlighting existing approaches. Section 3 provides the necessary background, laying the foundation for our methodologies and contextualizing our contributions. Section 4 details the dataset construction process, explaining how we enriched and prepared the data for the taxonomies’ construction. In Section 5, we describe the creation of the taxonomies, outlining the methods used to generate them. Section 6 discusses the expansion of the taxonomies, demonstrating how they can be dynamically extended to accommodate new categories. Section 7 focuses on the evaluation of these taxonomies, presenting the metrics and results that validate their accuracy and also the quality of the tags assigned to the transactions. Finally, Section 8 concludes the paper, summarizing our findings, discussing their implications, and suggesting directions for future research.  2 RELATED WORK Taxonomies represent the structure behind a collection of documents, organizing the hierarchical relationships between terms in a tree structure [13]. They play an essential part in the structural and semantic analysis of textual data, providing valuable content for many applications that involve information retrieval and filtering, such as web searching, recommendation systems, classification, and question answering. Since creating and maintaining taxonomies is a costly task, often difficult to scale if done manually, methods that automatically construct and update them are desirable. Early works on automatic taxonomy creation focused on building hypernym-hyponym taxonomies, where each pair of terms expresses an ‘is-a’ relationship [19]. More recent works have tackled the automatic creation of other taxonomies, such as topic taxonomies. In a topic taxonomy, each node represents a conceptual topic composed of semantically coherent terms. In this context, Zhang et al . [28] developed TaxoGen, an unsupervised method for constructing topic taxonomies. Taxogen uses the SkipGram model from an input text corpus to embed all the concept terms into a latent space that captures their semantics. In this space, the authors applied a clustering method to construct a hierarchy recursively based on a variation of the spherical K-means algorithm. Another work that focuses on topic taxonomies is TaxoCom [10], a framework for automatic taxonomy expansion. TaxoCom is a hierarchical topic discovery framework that recursively expands an initial taxonomy by discovering new sub-topics. It uses locally discriminative embeddings and adaptive clustering, resulting in a low-dimensional embedding space that effectively encodes the textual similarity between terms. One main disadvantage of TaxoCom is that it requires a large set of quality phrases in the target language, and curating these phrases can be costly. The quality of the output taxonomy is highly dependent on those phrases. Regarding the automatic expansion of taxonomies, an important related example is Musubu [20], a framework for low-resource taxonomy enrichment that uses a Language Model (LM) as a knowledge base to infer term relationships. For the taxonomy expansion part of out method, we used Musubu as a baseline for comparison. 268 ----- Tagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies WebMedia’2024, Juiz de Fora, Brazil As to using Large Language Models for taxonomy tasks, Chen et al . [6] investigated how LLMs, like GPT-3, perform in taxonomy construction tasks. The authors compared two approaches: finetuning, which involves training the LLM on a specific dataset to adapt it for taxonomy tasks, and prompt techniques, where the LLM receives instructions and examples to perform a task without being explicitly trained for it. Their findings showed that prompt techniques such as few-shot learning generally outperform finetuning, particularly with smaller datasets. Based on these findings, we applied prompting techniques, specifically zero-shot prompting, across various LLMs to assess their effectiveness in constructing and expanding taxonomies. Section 7 shows the results of our approach, as well as the results of applying Musubu[20] as baseline.  3 BACKGROUND In this section, we provide a comprehensive background on Large Language Models (LLMs), and the concept of Prompt-tuning. These concepts are essential to understanding the construction and editing of taxonomies utilizing LLMs.  3.1 Large Language Models Lately, Large Language Models (LLMs) have garnered significant attention for their exceptional performance in various NLP tasks. LLMs, such as GPT-3[3] and LLAMA[22], are characterized by their massive scale, comprising billions of parameters and being trained on vast amounts of data. These models are often pre-trained in an unsupervised manner on large corpora of textual data, such as books, articles, and web pages, allowing them to learn contextual representations that capture the intricacies of human language. To use LLMs for specific purposes, a highly effective approach is to fine-tune them on task-specific data. Fine-tuning enables LLMs to adapt to specific domains or tasks with minimal labeled data, significantly reducing the need for large annotated datasets. However, in scenarios where labeled data is scarce or difficult to obtain, LLMs can also be used without specific training or additional data, in a Zero-Shot manner [21]. Given the scale of these models and the data they are trained on, LLMs embed vast knowledge that enables them to achieve high generalization capabilities and perform tasks in diverse contexts, even without specific training for those tasks[16]. In our experiments, we tested several types of language models, from private LLMs (GPT 4 [14], Gemini Pro [2] ), to open-source LLMs (Llama 2 [22]), to a Mixture of Experts LLM (Mixtral [9], and a Small Language Model (SLM), Phi 2 [3] .  3.2 Prompt Engineering Prompt Engineering is a fundamental technique used to enhance the performance and adaptability of Large Language Models (LLMs) in specific tasks or domains [7]. It involves optimizing and crafting prompts to efficiently use language models (LMs) [3]. This approach allows researchers and practitioners to tailor the behavior and output of LLMs, making them more suitable for targeted applications. 2 https://deepmind.google/technologies/gemini/pro/ 3 https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-ofsmall-language-models/ Techniques such as Zero-shot prompting, Few-shot prompting, Chain of Thought, ReAct, Self-Consistency etc. have been explored to guide LLMs toward desired responses [18, 21, 25 – 27]. The effectiveness of prompt tuning has been demonstrated in various applications, including question-answering, summarization, and dialogue generation. The choice of prompt greatly influences the generated output, and by carefully crafting prompts, researchers can guide the model’s responses toward desired behaviors. For example, in language translation, a prompt can specify the source language and desired target language to ensure accurate and fluent translations. In our method, we used the Zero-Shot prompting technique.  3.3 Zero-Shot Prompting Since LLMs (Large Language Models) are trained on vast amounts of data, they can follow instructions and perform tasks in contexts where they were not specifically trained, in a Zero-Shot (ZS) manner. This prompting style allows the model to adapt, making it versatile. A Zero-Shot (ZS) prompt directly instructs the model to perform a task without additional examples or demonstrations to guide the LLM’s response, which is why they are also known as task instructions [21]. In a study by Li [11], the authors highlighted several advantages of using ZS prompts, such as the ability to craft highly interpretable prompts, requiring fewer training data or examples, a more straightforward prompt design process, and a flexible prompt structure. Additionally, Reynolds and McDonell [17] noted that carefully engineered zero-shot prompts can outperform few-shot prompts in certain scenarios, as examples can sometimes be interpreted as part of a narrative rather than as a guiding mechanism. This finding also influenced our decision to use zero-shot prompting in our method.  4 DATASET CONSTRUCTION This work uses a proprietary dataset consisting of consumer transactions from a retail bank. Each transaction includes only a merchant name indicating the business where that purchase occurred along with macro and micro categories as illustrated in Figure 1 The macro and micro are originally assigned by [4] using the information from the business activities and products. We focus on two macro-categories from this dataset: Food and Shopping, selecting the top 50,000 businesses with the highest number of transactions for each category. With the limited initial information, assigning detailed tags to transactions is challenging. To address this, we augment the dataset through a data enrichment process involving web scraping. Using tools such as Selenium [4] and Beautiful Soup [5], we gathered activity descriptions for companies in each macro category. For the Food macro category, the search was conducted on specialized platforms for restaurants and food delivery services. For the Shopping macro category, we obtained establishment descriptions directly from internet indexing and search tools. In the context of enrichment for the Food macro category, web scraping was conducted as follows: (1) the centers of all Brazilian state capitals and the Federal District were used as base locations 4 https://www.selenium.dev/about/ 5 https://readthedocs.org/projects/beautiful-soup-4/downloads/pdf/latest/ 269 ----- WebMedia’2024, Juiz de Fora, Brazil Moraes et al. for restaurant searching; (2) for each location, restaurants listed on the first one hundred pages of the platform were extracted. After completing these steps, the information was combined with the merchant database using the merchant’s name and micro categories. For the Shopping macro category, the description of each merchant was obtained using Google Search Engine [6], selecting descriptions from the first ten search results. The final description consists of a concatenation of all the obtained descriptions. The search queries were constructed using the merchant names combined with their micro categories.  5 TAXONOMY CONSTRUCTION To automatically create topic taxonomies for Food and Shopping businesses, we developed a 3-step method. First, we preprocess the descriptions in our enriched dataset to retain only the relevant parts of the text. Next, we apply two techniques to select candidate terms for the topic taxonomies: keyword extraction and topic modeling. In the post-processing phase, we use large language models (LLMs) to refine the results of each step, filtering out unrelated terms. Finally, we use LLMs again to organize the final terms into hierarchies, forming the topic taxonomies.  5.1 Preprocessing We applied a few NLP techniques to refine the businesses’ descriptions in our dataset. At first, we remove stop words to eliminate commonly used words that do not carry significant meaning in our contexts. Then, to retain only the most relevant portions of the descriptions, we employ part-of-speech (POS) tagging to identify and exclude words that belong to specific POS categories. The list of POS tag categories that were removed includes ADV, CCONJ, ADP, AUX, CONJ, DET, INTJ, PART, PRON, PUNCT, SYM, SCONJ, ADJ, VERB, PROPN. [7] After this initial preprocessing step, we run the first iteration of the candidate term selection part to build a filter of generic words, not to create topic taxonomies yet. For this step, we use the entire corpus of descriptions for each macro category, resulting in two corpora ( Food and Shopping ). For each micro category in the macro categories’ corpora, we use Keyword Extraction and Topic Modeling to gather candidate terms for the filter, combining the results of both techniques in a list. Then, We use an LLM to remove the terms it identifies as unrelated to the main topic (each micro category) from the list. The prompt that we used for requesting this separation is illustrated below. prompt= \"Given the terms in the following list: \"+ <wordsList> +\". Separate them into two groups. In group 1 the terms with no relation to the topic \"+ <type> +\". And in group 2 the terms that are related.\" Listing 1: Prompt for separating candidate terms related to the type of establishment By using this prompt, we try to ensure that the model’s response is consistently formatted according to the pattern described in it, facilitating the processing of the resulting string, although, some 6 https://www.google.com 7 https://spacy.io/usage/linguistic-features#pos-tagging of the LLMs we tested did not output the response in the requested format. Once we complete one iteration of this method for each macro category in our dataset, we add the words of group 2 to the corresponding list of generic words. We apply the corresponding filter of generic words for each macro category corpus, resulting in the final preprocessed corpus.  5.2 Candidate Terms Selection For this part of our method, we use each preprocessed corpus separately. For the Food corpus, we group the descriptions based on their micro-categories, creating 58 sub-corpus specific to that domain. We have six micro categories for the Shopping corpus, resulting in 6 specific sub-corpus. The candidate terms selection methods are applied to each sub-corpus, creating topic taxonomies where the main topic is the micro category. 5.2.1 Keyword Extraction. The first approach to candidate term selection was to use an unsupervised keyword selection method called Yake! [5]. This method is based on statistical text features extracted from single documents to select the most relevant keywords from that text. It does not require training on a document set and is not dependent on dictionaries, text size, language, domain, or external corpora. Yake! allows for the specification of parameters such as the language of the text, the maximum size of the n-grams being sought, and others. In our method, we customized only the language to Portuguese, and the maximum number of keywords sought for each set of descriptions was 30 words. After extracting the keywords from each group of descriptions, we obtained a total set of 𝑁 candidate terms. However, these terms are further filtered using an LLM, where we ask it to separate the terms related to the main topic from those unrelated, as explained earlier in subsection 5.1. 5.2.2 Topic Modeling. Our second approach to collecting initial topics and candidate terms was Topic Modeling. We applied the Latent Dirichlet Allocation algorithm [2], available at the Gensim Library [8] . We construct a dictionary for each macro-category corpus in our macro-categories corpora by extracting unique tokens and bigrams. After a few empirical tests, we set the minimum frequency of a bigram to 20 occurrences. Since some corpora have a minimal number of tokens (the micro category \"Greek Cuisine\" from the Food macro category has only five stores marked as such, with a corpus of only 127 tokens), we had to set a reasonably small number so that smaller corpora could also have a few bigrams. With the resulting dictionary of tokens, the LDA algorithm was applied. Three main parameters are to be defined in an LDA algorithm: number of topics, alpha, and beta . The number of topics defines the latent topics to be extracted from the corpus. The parameter alpha is a priori belief in documenttopic distribution, while beta is a priori belief in topic-word distribution. To define the number of topics for each micro category corpus, we tried numbers from 1 to 5, constantly checking which configuration would result in the best average topic coherence for that 8 https://pypi.org/project/gensim/ 270 ----- Tagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies WebMedia’2024, Juiz de Fora, Brazil corpus. Small corpora would have 1 or 2 topics, while bigger ones would have 5. To correctly define the alpha and beta priors, we would have to analyze the distribution for each category corpus [24]. Since this would be rather difficult, we set those priors to be auto-defined by the LDA algorithm, which learns these parameters based on the corpus. We select the terms with the highest coherence with the resulting topics. Each topic returns 20 words with their coherence scores, but we do not use all of them as some have very low coherence. After testing a few configurations, for each topic, we select 60% of the terms with the highest coherence within that topic. With initial terms for each topic taxonomy, we ask an LLM to separate the ones closely related to the main topic from those unrelated, as mentioned earlier.  5.3 Hierarchy Construction Once we have the post-processed lists of candidate terms obtained by each technique mentioned in subsection 5.2, we merge them and remove repetitions. After the merge, for each macro category, we have lists of terms for each micro category, representing each topic taxonomy. However, they do not have any hierarchy level between the terms configuring the taxonomy. To tackle this problem, we use an LLM again, this time with a prompt that searches for sub-categories within the terms of a topic to create these hierarchies. The prompt is illustrated below: prompt=\"Create a dictionary by hierarchically arranging the following words:\" + <wordsList> +.\" Use JSON format as the output such as the following: {\\\\\"key\\\\\": [\\\\\" list of words\\\\\"]}\" Listing 2: Prompt for creating a hierarchy for each list of tags. With this prompt, we seek to ensure that the LLM response has a consistent pattern and facilitates handling the returned string. After this step, we have a hierarchy of terms in each topic taxonomy in the Food and Shopping macro categories.  5.4 Merchant Tagging With the topic taxonomies for both Food and Shopping macrocategories, we can now assign tags to merchants/establishments. To do so, we use the descriptions attached to these establishments, and we see which terms from a taxonomy are mentioned in their descriptions with a reverse index algorithm. We employ the taxonomy whose topic is the same as the establishment’s micro category, as shown in Figure 2.  6 TAXONOMY EXPANSION Another essential part of our method is the automatic expansion of existing taxonomies as new terms arrive, derived from additional merchant scrapped data, as shown in Section 4. In this section, we present our approach to taxonomy expansion by using instructionbased LLMs. As new transactions may include new businesses, new terms can emerge from the descriptions obtained through the scraping process. Therefore, we need to update the taxonomies with these new terms maintaining and enriching the created hierarchies with the potential new terms. After completing the transaction enrichment process, including the search for business descriptions and the selection of candidate terms, if relevant terms not included in the current hierarchies are detected, we initiate the expansion process.  6.1 Prompt engineering instruction for taxonomy representation First, we represent our topic taxonomies in a format that can be interpreted by an LLM. We employed a generic prompt, illustrated below, across all tested methods to convert topics into root nodes and their terms into child nodes. Childs of [ROOT]: [CHILD1,CHILD2,CHILD3] Childs of [CHILD1]: [CHILD4,CHILD5] Childs of [CHILD2]: [CHILD6] ... Listing 3: Prompt for representation of taxonomy  6.2 Predicting the parent of a node To experiment with taxonomies expansion, we used two datasets: our Food and Shopping topic taxonomies and the taxonomies from SemEval-2015 Task 17 [15]. Those are low-resource taxonomies, with thousands of nodes or less, which are appropriate for the current prompt size of LLMs. We used the SemEval dataset to compare the results with well-established methods for taxonomy expansion, such as Musubu [20]. Similar to their experiments, we hid 20% of the terms (chosen randomically) in the taxonomies to predict their respective parent nodes. To verify the parent/root of a new term, we used the following prompt: Listin g 4: Prom p t for searchin g for a node’s p arent prompt=\"Who is the father of \"+<new_term>+\"?\" In Table 1, we see the F1-Scores for parent node prediction. Equation 1 showcases how to calculate the F1-Score. TP is the number of true positives, nodes that were correctly assigned as parents of child nodes. FP is the number of false positives, nodes that were incorrectly assigned as a parent to a child node. FN is the number of false negatives, nodes that should have been assigned as parent nodes but were not. 2 ∗ 𝑇𝑃 𝐹 1 = (1) 2 ∗ 𝑇𝑃 + 𝐹𝑃 + 𝐹𝑁 For baseline models, we used Bert and Musubu; for commercial LLMs, Gemini Pro and GPT-4; and for open-source LLMs, LLamaAlpaca(7B), Phi-2, and Mixtral 8x7B. We evaluate them in 4 taxonomies from the SemEval dataset and our taxonomies. For each taxonomy, the LLMs perform significantly better than Musubu, with GPT-4 and Gemini Pro having the highest F1-Scores, with the latter beating the former by a few points. However, the most recent open-source options (Phi-2 and Mixtral 8x7B) are getting close in performance. 271 ----- WebMedia’2024, Juiz de Fora, Brazil Moraes et al. Tags assigned to establishments from the \"Clothing & Accessories\" micro category Figure 2: Assigning tags to establishments based on a topic taxonomy. It is important to note that while SemEval taxonomies have thousands of nodes, ours have only a few hundred, which we can assume is a significant reason for the degrading performance of Musubu and Bert (LMs or LM-based methods). In contrast, the LLMs have a robust performance in such low-resource settings. This also shows that LLMs have a remarkable understanding of questions and zero-shot performance, generalizing well even for datasets in different languages.  7 TAXONOMY EVALUATION To properly evaluate the topic taxonomies that we created in this work, we developed a two-step qualitative evaluation of a limited part of the results. In total, 58 topic taxonomies were created for the Food set and 6 for the Shopping set. For our evaluation, we selected the topic taxonomies with the highest number of terms in each part (the \"Brazilian Cuisine\" taxonomy for the Food part and the \"Clothing and Accessories\" taxonomy for the Shopping one). First, we assess the quality of removing generic terms from each taxonomy, and then, we evaluate the tags assigned to establishments based on that taxonomy. We asked 12 volunteers to answer a two-part form. Part 1 - Accuracy of the terms that were selected as related to the topic : In this part, we evaluate if the LLMs could correctly group the relevant and non-relevant terms, removing the generic terms. To do so, we defined a ground truth with the relevant terms as true positives and the non-relevant terms as true negatives. Table 3 shows the results. GPT-4 was the best model, followed by Gemini Pro, both scoring over 60% accuracy for the Brazilian Cuisine taxonomy and over 86% accuracy for the Clothing and Accessories taxonomy. Smaller language models such as Phi 2 and Llama 2 7B performed poorly both in removing generic terms and in formatting the response accordingly, with Phi 2 being particularly verbose. Part 2 - Human Evaluation of the Quality of the Tagging Process : In this part, the volunteers were asked to examine if the tags assigned to an establishment were appropriate and coherent to that establishment’s description. We selected the top 5 establishments with the highest transactions for each micro category. We asked our evaluators to analyze the tags assigned to describe that establishment and choose the ones that were not appropriate. This way, we have a coherence ratio for each establishment based on the number of proper tags divided by the total number of tags. We average the results of our 12 evaluators and present them in Table 2. Figure 2 shows the \"Clothing & Accessories\" taxonomy that was evaluated and 2 of the merchants and the tags assigned to them that were included in the evaluation.  8 CONCLUSION In this work, we presented an unsupervised method for automatically creating and expanding topic taxonomies using LLMs. We evaluated some of the generated taxonomies and applied them in transaction tagging in a retailer’s bank dataset. The evaluation showed promising results, with average coherence scores above 90% for the two selected taxonomies. The taxonomies’ expansion with Gemini Pro also showed exciting results for parent node prediction, with F1-scores of 89% and 70% for Food and Shopping taxonomies, respectively. For future work on taxonomy construction, we plan to test more robust term selection methods, such as embedding-based approaches. We also plan on conducting ablation studies to validate whether the keyword extraction and topic modeling parts help improve the quality of the taxonomies created, by using a baseline prompt to ask the LLM to generate child nodes given a parent node. In terms of taxonomy expansion, there are several tasks to explore, ranging from node-level operations to generating entire sub-trees and identifying similar structures. Additionally, we intend to enhance our instruction-tuned LLM for taxonomy tasks |Method|SemEval-2015 Task 17 Chemical Equipment Food Science|Our taxonomies Food Shopping| |---|---|---| |Gemini Pro|0.68 0.80 0.91 0.72|0.89 0.73| |GPT-4|0.65 0.78 0.89 0.70|0.87 0.71| |Mixtral-8x7B|0.59 0.63 0.80 0.57|0.74 0.60| |Phi-2|0.56 0.52 0.68 0.56|0.64 0.54| |LLama 7B|0.51 0.42 0.58 0.46|0.60 0.49| |Musubu|0.35 0.46 0.37 0.42|0.21 0.13| |Bert-Base|0.13 0.14 0.12 0.16|0.11 0.06| Table 1: F1-score for parent node prediction. 272 ----- Tagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies WebMedia’2024, Juiz de Fora, Brazil |Col1|Brazilian Cuisine Taxonomy Average Coherence Number of Tags|Col3|Clothing & Accessories Taxonomy Average Coherence Number of Tags|Col5| |---|---|---|---|---| |Merchant 1|92.30%|10|97.11%|8| |Merchant 2|94.23%|8|83.07%|5| |Merchant 3|89.23%|5|94.38%|5| |Merchant 4|87.17%|6|93.84%|5| |Merchant 5|93.40%|7|97.43%|6| Table 2: Results of evaluating the tags assigned to each merchant/establishment. |Col1|Brazilian Cuisine|Clothing & Accessories| |---|---|---| |Llama 2 7B|29.54%|52.78%| |Phi 2|40.90|73.68%| |Mixtral 8x7B v0.1|46.93%|70.27%| |Gemini Pro|61.36%|86.11%| |GPT 4|68.08%|86.84%| Table 3: Accuracy of using each LLM to remove generic words from each topic taxonomy. by fine-tuning or employing more efficient methods such as LoRA [8].  LIMITATIONS To address the limitations of our work, we begin with the taxonomy construction component. In this phase, we relied on topic modeling and keywords extraction to select candidate terms for our taxonomies. The LDA algorithm used for topic modeling performs suboptimally when the base corpus is small. Some of our topics had corpora with vocabularies of fewer than 100 words, which can result in topics containing irrelevant or incoherent terms. Additionally, we could have further experimented with the LDA hyperparameters for each micro-category corpus. Regarding the evaluation of the generated taxonomies, we did not assess topic completeness. Without a ground truth, it is challenging to quantify how comprehensively the terms in a taxonomy cover the main topic. Furthermore, we evaluated only 2 of the 64 taxonomies generated by our method, leaving a substantial portion unexamined. In the taxonomy expansion experiment, we evaluated only a lowresource setting with fewer than a thousand nodes. Most studies focus on taxonomies with hundreds of thousands or more nodes. This presents a challenge for LLMs due to their limited context. Addressing this contextual limitation could benefit from insights found in other works that tackle similar issues [12].  ETHICS STATEMENT In this work, we ensure the utmost protection of customers and store sensitive data by exclusively using non-sensitive information in our dataset. Our prompts solely rely on selected words from store descriptions, thus avoiding any direct usage of personal or sensitive information. No customer-specific data or store-sensitive details are integrated into the system, upholding privacy and security as top priorities. Moreover, we strictly adhere to ethical guidelines during our experiments involving volunteers, and no personal data is collected from them. Our focus lies solely on analyzing the results of our proposed approach. Participants’ anonymity and confidentiality are maintained throughout the research process, ensuring a responsible and trustworthy approach to data handling.  ACKNOWLEDGMENTS The authors would like to acknowledge BTG Pactual for the partnership and financial support to this research.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:21:03.914582Z",
     "start_time": "2025-05-12T22:21:03.911615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus.loc[13, 'resumo'] = abstract\n",
    "corpus.loc[13, 'text'] = text_temp"
   ],
   "id": "f6f45e6cb5e8cf51",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:21:04.908483Z",
     "start_time": "2025-05-12T22:21:04.900202Z"
    }
   },
   "cell_type": "code",
   "source": "corpus.loc[13]",
   "id": "f88439290285d225",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "titulo               Tagging Enriched Bank Transactions Using LLM-G...\n",
       "informacoes_url                                                       \n",
       "idioma                                                         english\n",
       "storage_key          ../articles/original/english/985-24767-1-10-20...\n",
       "autores              [Daniel de S. Moraes,  Polyana B. da Costa,  P...\n",
       "data_publicacao                                             11-09-2024\n",
       "resumo               This work presents an unsupervised method for ...\n",
       "keywords             Large Language Models, Natural Language Proces...\n",
       "referencias          [[1] Rohan Anil, Sebastian Borgeaud, Yonghui W...\n",
       "text                 Many recent studies have focused on the applic...\n",
       "artigo_tokenizado    [9/02/2008, at, 19:20:05, Italian, Cuisine, Pa...\n",
       "pos_tagger           [[9/02/2008, NUM, CD], [at, ADP, IN], [19:20:0...\n",
       "lema                 [9/02/2008, at, 19:20:05, italian, Cuisine, Pa...\n",
       "resumo_clean         assessing the quality of the taxonomies and th...\n",
       "Name: 13, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:50:56.413984Z",
     "start_time": "2025-05-12T22:50:56.176159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus = pd.read_json('../articles/corpus/corpus (1).json')\n",
    "autores = ['Garibaldi da Silveira Júnior', 'Gilberto Kreisler','Bruno Zatt','Daniel Palomino','Guilherme Correa']"
   ],
   "id": "67d84ff694cfc15c",
   "outputs": [],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:50:57.707210Z",
     "start_time": "2025-05-12T22:50:57.704818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus.loc[2,'titulo'] = 'Multi-Domain Spatio-Temporal Deformable Fusion model for video quality enhancement'\n",
    "corpus.loc[2, 'autores'] = autores"
   ],
   "id": "1e6b3b45be1a8953",
   "outputs": [],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:24:46.895208Z",
     "start_time": "2025-05-12T23:24:44.918096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = '../articles/original/portuguese/985-24773-1-10-20240923.pdf'\n",
    "text = pymupdf4llm.to_markdown(path)\n",
    "# reader = PdfReader(path)\n",
    "# info = dict(reader.metadata)\n",
    "text"
   ],
   "id": "f57e5f7a56295e01",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# **Uma Investigação sobre Técnicas de Data Augmentation** **Aplicadas a Tradução Automática Português-LIBRAS**\\n\\n## Diego Ramon Bezerra da Silva\\n#### Universidade Federal da Paraíba João Pessoa/PB, Brasil diego.silva@lavid.ufpb.br\\n## Tiago Maritan U. de Araújo\\n#### Universidade Federal da Paraíba João Pessoa/PB, Brasil tiagomaritan@lavid.ufpb.br\\n\\n## Marcos André Bezerra da Silva\\n#### Universidade Federal da Paraíba João Pessoa/PB, Brasil marcos.andre@lavid.ufpb.br\\n## Daniel Faustino L. de Souza\\n#### Universidade Federal da Paraíba João Pessoa/PB, Brasil daniel@dcx.ufpb.br\\n### **ABSTRACT**\\n\\n## Manuella Aschoff C. B. Lima\\n#### Universidade Federal da Paraíba João Pessoa/PB, Brasil manuella.lima@lavid.ufpb.br\\n## Rostand Edson O. Costa\\n#### Universidade Federal da Paraíba João Pessoa/PB, Brasil rostand@lavid.ufpb.br\\n\\n\\nThe automatic translation from Portuguese to LIBRAS is extremely important for accessibility and inclusion of deaf individuals\\nin society, but the scarcity of data and the high cost of building an\\nauthentic corpora pose significant challenges. Data Augmentation\\nin Neural Machine Translation is the process of generating synthetic sentences to increase the quantity and diversity of the training\\nset. This work investigates the use of data augmentation techniques to improve the performance of Portuguese-LIBRAS automatic\\ntranslation using the BLEU metric. Among the techniques analyzed,\\nback-translation and its combination with synonym substitution\\nusing part-of-speech tagging stood out as the most effective in\\nenhancing the translation model and can be used to increase the\\ndiversity of underrepresented datasets.\\n### **KEYWORDS**\\n\\nTradução Automática Neural, Aumento de Dados, Libras\\n### **1 INTRODUÇÃO**\\n\\nA evolução da tecnologia tem desempenhado um papel crucial\\nna acessibilidade e inclusão de pessoas surdas, representando um\\nmarco significativo na promoção da igualdade de acesso à informação. Uma parcela considerável da população surda não compreende\\nos textos disponibilizados na forma escrita da língua oral (LO),\\ncomunicando-se, portanto, basicamente através da língua de sinais\\n(LS). Especialmente na *web*, onde o volume e o dinamismo de informações são enormes, com conteúdo textual sendo gerado a todo\\ninstante, a tarefa de interpretar manualmente textos de páginas\\n*web* para língua de sinais é inviável. Diante desse cenário, torna-se\\nindispensável o uso de componentes de tradução automática para\\ntraduzir o conteúdo, por exemplo, do Português Brasileiro (PB) para\\na Língua Brasileira de Sinais (LIBRAS), para que pessoas surdas\\ntenham acesso efetivo à informação online [ 17 ]. Nesse contexto,\\na plataforma VLibras [ 1 ] emerge como uma solução que torna a\\n*web* verdadeiramente acessível para surdos, destacando-se por sua\\nampla adoção em sites governamentais, onde desempenha um papel essencial ao prover acessibilidade em LIBRAS para os serviços\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n© 2024 SBC – Brazilian Computing Society.\\nISSN 2966-2753\\n\\n\\npúblicos. Atualmente, o VLibras está sendo utilizado em mais de\\n500.000 sites, tanto públicos quanto privados, realizando milhões\\nde traduções mensalmente [2].\\nO componente de tradução do VLibras foi inicialmente desenvolvido utilizando regras de tradução advindas da expertise de\\nlinguistas. Na abordagem atual do VLibras, é utilizado um tradutor\\nbaseado em Tradução Automática Neural, que é capaz de inferir as\\nregras da linguagem a partir dos dados de treinamento. A adição\\ndo componente baseado em rede neural possibilitou uma maior capacidade de desambiguação de palavras em PB que possuem sinais\\ndiferentes na LIBRAS, a depender do contexto [17].\\nEntretanto, apesar dos avanços trazidos pelo uso de redes neurais para Tradução Automática, é exigida uma alta quantidade de\\ndados para o treinamento de modelos que geram traduções de boa\\nqualidade [ 14 ], o que é um grande obstáculo, principalmente em\\nLS, como a LIBRAS. Para que isso aconteça é necessário o desenvolvimento de um corpus bilíngue [1], processo que é extremamente\\ncustoso e manual, visto que é feito por intérpretes da LIBRAS. Sendo\\nassim, é um desafio construir um conjunto de dados de treinamento\\ndiverso que represente diferentes contextos de uso da língua [ 17 ].\\nNeste sentido, a geração de dados sintéticos por meio de técnicas de\\naumento de dados ( *data augmentation* ) é uma estratégia importante\\npara superar a escassez e aumentar a quantidade e diversidade dos\\ndados de treinamento [19].\\nExistem vários métodos de aumento de dados para Processamento de Linguagem Natural (PLN) e muitos métodos, apesar de\\npoderem ser utilizados em diversas tarefas, foram elaborados com\\na finalidade de melhorar o desempenho em uma tarefa específica.\\nA escolha certa dos métodos empregados para aumento de dados\\npode trazer um impacto positivo na qualidade dos modelos gerados,\\nvisto que é sabido que métodos projetados para a tarefa de Tradução\\nAutomática para línguas com muitos recursos disponíveis podem\\nnão ser eficazes para línguas com pouco recurso ( *low-resource lan-*\\n*guages* ) [ 4 ]. Sendo assim, este trabalho tem como objetivo investigar\\no impacto de diferentes métodos de aumento de dados, utilizando\\ntécnicas como retrotradução, substituição de palavras alinhadas,\\nreversão de *tokens* e substituição por sinônimos com *part-of-speech*\\n*tagging*, além de analisar a influência da quantidade de dados sintéticos no desempenho do modelo de tradução PB-LIBRAS. Para\\nisso, foram realizados experimentos com um modelo da arquitetura\\n\\n1 Pares de sentenças em português e suas respectivas traduções em LIBRAS.\\n\\n\\n318\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Silva et al.\\n\\n\\n*transformer* e os resultados foram avaliados com base na métrica\\n\\nBLEU.\\n### **2 TRABALHOS RELACIONADOS**\\n\\nA tradução automática PB-LIBRAS envolve a tradução entre línguas\\nde modalidades diferentes (oral-auditiva e visual-espacial) e que\\npossuem estruturas linguísticas não paralelas entre si. Além disso,\\nas LS geralmente têm poucos recursos, ou seja, existem poucos\\nbancos de dados extensos para LS e, quando existem, são limitados\\na algumas LS específicas [ 5, 10, 15 ]. Assim, a escassez de dados é\\num dos principais desafios para a tradução automática para língua\\nde sinais ( *Sign Language Translation* - SLT). Algumas estratégias\\npara lidar com o problema de tradução automática com poucos recursos incluem aumento de dados, aprendizagem por transferência,\\nretrotradução e abordagens híbridas [ 8 ]. No tocante à técnica de\\naumento de dados, foco deste trabalho, destacam-se os trabalhos\\nde Moryossef et al. 2021, Fadaee et al. 2017, Sanchez-Cartagena et\\nal. 2021, Maimaiti et al. 2021, Jang et al. 2022 e Wang, Yang 2022.\\nMoryossef et al. 2021 se concentraram na tarefa de tradução de\\ntexto dentro do SLT e introduziram duas estratégias de aumento de\\ndados baseadas em regras. Eles apresentaram regras abrangentes e\\nespecíficas do idioma para criar pares texto-glosa pseudo-paralelos.\\nEsses pares foram posteriormente empregados no processo de retrotradução, melhorando o desempenho geral do modelo.\\nO trabalho desenvolvido por Fadaee et al. 2017 utilizou uma\\nestratégia de substituição de palavras alinhadas para aumentar\\na frequência de palavras raras no conjunto de treinamento. As\\nsentenças geradas foram posteriormente filtradas por um modelo de\\nlinguagem que foi treinado com o objetivo de avaliar se as sentenças\\nsintéticas são fluentes, ou seja, estão corretas gramaticalmente e\\nfazem sentido semântico. Foi observado um ganho de 2,5 pontos\\nna métrica BLEU na tradução de inglês para alemão.\\nJá Sánchez-Cartagena et al. 2021 aplicaram duas técnicas para\\naumento de dados em sua pesquisa. Uma delas é a substituição\\nde palavras alinhadas semelhante à apresentada por Fadaee et al.\\n2017, porém sem se preocupar com a fluência das sentenças geradas,\\nportanto, não sendo necessário um modelo de linguagem adicional.\\nDe maneira auxiliar, também foi utilizada a tarefa de reversão de\\n*tokens*, tendo os resultados avaliados na tradução entre inglês e\\nalemão, hebreu e vietnamita, resultando em um ganho médio de\\n1,6 BLEU.\\nMaimaiti et al. 2021 em sua pesquisa propuseram a substituição\\nde sinônimos com *part-of-speech tagging* . Esse trabalho aplicou essa\\ntécnica para tradução dos idiomas azerbaijão, hindi, uzbeque, turco,\\nalemão e chinês para inglês. Como resultado, foram observados\\nganhos de BLEU entre 1,16 e 2,39 pontos.\\nNa pesquisa realizada por Jang et al. 2022, foram utilizadas técnicas de aumento de dados para língua de sinais coreana ( *Gloss-level*\\n*Korean Sign Language* ). A proposta utilizou retrotradução, substituição por sinônimos restrita às classes de substantivos, nomes\\npróprios e pronomes, além de substituição de palavras utilizando\\num modelo de linguagem coreano. Foram observados ganhos de\\nBLEU em 10, 12 e 16 pontos, respectivamente.\\nPor fim, destaca-se a pesquisa de Wang, Yang 2022 que trabalharam em modelos de tradução entre os idiomas inglês, chinês e\\ntailandês. Neste trabalho foram propostos aumento de dados por\\n\\n\\nsubstituição de palavras alinhadas e substituição por sinônimos.\\nObservaram ganhos de 1 BLEU ao utilizar substituição de palavras\\nalinhadas na tradução de chinês para inglês e de 4 BLEU na tradução\\nde inglês para chinês. Já a substituição por sinônimos na tradução\\nde chinês para tailandês resultou em um ganho de 2,7 BLEU.\\n### **3 FUNDAMENTAÇÃO TEÓRICA** **3.1 Tradução Automática Neural**\\n\\nTradução Automática Neural ( *Neural Machine Translation* - NMT) é\\na aplicação de redes neurais para a tarefa de tradução de sentenças\\nde uma língua de origem para uma língua de destino. Em geral, é\\nutilizada uma rede *sequence-to-sequence*, onde o *encoder* constrói\\numa representação da sentença no idioma de origem e o *decoder*\\nutiliza essa representação e as palavras geradas anteriormente pela\\nprópria rede para gerar a sentença traduzida no idioma alvo [3].\\nEm contraste à Tradução Automática Baseada em Regras ( *Rule*\\n*Based Machine Translation* - RBMT), que transforma a sentença de\\norigem através de algoritmos de substituição por regras, derivadas\\ndo conhecimento de linguistas, a NMT é baseada em dados. Sendo\\nassim, para que a NMT seja possível, é necessária a construção\\nde um corpus bilíngue. As redes neurais treinadas para resolver\\no problema de tradução são capazes de aprender as regras de tradução diretamente dos dados, eliminando assim a necessidade da\\nconstrução de algoritmos explícitos com regras de tradução. Especificamente no contexto da LIBRAS, modelos NMT oferecem uma\\ncapacidade de desambiguação de palavras que têm grafia igual no\\nPB, porém significado e sinal diferentes na LIBRAS. Essa capacidade de desambiguação não seria alcançada utilizando apenas uma\\nabordagem de RBMT, já que é necessário que o modelo de tradução\\ncompreenda o contexto em que o termo ambíguo está inserido para\\ndecidir a desambiguação correta [17].\\n### **3.2 Data Augmentation**\\n\\nA qualidade de um modelo de Tradução Automática depende principalmente da existência de um corpus bilíngue de alta qualidade e\\nextensão significativa. Este corpus serve como base de treinamento\\npara que o modelo de tradução neural aprenda os padrões linguísticos de ambos idiomas de origem e destino [ 17 ]. A LIBRAS, assim\\ncomo outras LS, pode ser classificada como língua com poucos\\nrecursos ( *low-resource language* ), tendo em vista a baixíssima quantidade de dados disponíveis para o treinamento de componentes de\\nPLN [ 2 ]. Para as línguas com poucos recursos, não há dados autênticos traduzidos por humanos suficientes disponíveis para treinar um\\nmodelo de NMT e obter resultados de alta qualidade. Desta forma,\\na geração de dados sintéticos é uma estratégia interessante para\\ncomplementar o corpus criado pelos linguistas.\\nCom um corpus pequeno, o universo de sentenças a serem traduzidas pelo modelo (quando o tradutor estiver sendo utilizado pelo\\nusuário) será muito maior que os exemplos vistos no treinamento.\\nO objetivo de algoritmos de aumento de dados é, a partir dos dados\\nautênticos, expandir e diversificar o corpus de treinamento com\\ndados sintéticos para que, idealmente, se aproxime da distribuição\\nde dados de todo o universo de pares de sentenças e traduções válidas, de modo que a quantidade e diversidade desses novos dados\\nbeneficiem o modelo de tradução [19].\\n\\n\\n319\\n\\n\\n-----\\n\\nUma Investigação sobre Técnicas de Data Augmentation Aplicadas a Tradução Automática Português-LIBRAS WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\nAlgoritmos de aumento de dados ( *data augmentation* ) geram dados adicionais, sintéticos, a partir dos dados autênticos, através de\\nmodificações sobre as sentenças autênticas. Esses dados adicionais\\nsão então incorporados ao corpus de treinamento original. Esta\\né uma tarefa desafiadora no ramo de PLN e tradução automática,\\nonde qualquer modificação na sentença pode ter impacto no seu\\nsignificado. Por isso, é importante que as traduções na língua de\\ndestino mantenham equivalência com o significado na língua de\\norigem. É uma alternativa mais barata na falta de exemplos curados\\npor linguistas e é extremamente importante em línguas com poucos\\nrecursos. Porém, por ser um procedimento automático, as sentenças\\nsintéticas geradas pelos algoritmos de aumento de dados tendem\\na ser de menor qualidade em relação às sentenças autênticas produzidas por humanos. Deve-se, portanto, utilizar uma quantidade\\nrazoável de dados sintéticos [14].\\nExistem diversas técnicas para *data augmentation* e, neste trabalho, abordaremos as técnicas de retrotradução, reversão e substituição de palavras, as quais serão detalhadas a seguir.\\n\\n*3.2.1* *Retrotradução.* A técnica de retrotradução (tradução reversa\\nou *back-translation* ) é amplamente utilizada para aumento de dados,\\npois tende a gerar sentenças de boa qualidade. Nessa abordagem,\\ncomo pode ser observado na Figura 1, é utilizado um outro modelo\\nde tradução e a sentença é traduzida de um idioma para outro e\\ndepois de volta para o idioma original [2] . Assim, a partir de um par\\nautêntico de uma sentença na língua de origem e sua respectiva\\ntradução, é possível gerar novas sentenças na língua de origem que\\ntenham o mesmo significado da tradução da sentença original. Isso\\né feito ao executar o algoritmo nas sentenças do idioma de origem,\\nresultando em novas sentenças sintéticas no mesmo idioma. Essas\\nnovas sentenças, resultantes da tradução reversa, são adicionadas\\nao corpus original no lado do idioma de origem. As traduções correspondentes, na língua de destino, associadas a essas novas sentenças\\nsintéticas, serão idênticas às traduções das sentenças autênticas\\noriginais devido à tendência de preservação do significado [14].\\n\\n**Figura 1: Exemplo de retrotradução.**\\n\\n*3.2.2* *Reversão.* A reversão dos *tokens* da sentença na língua de\\ndestino, como exemplificado na Tabela 1, é uma tarefa auxiliar e não\\nconvencional. No entanto, faz com que o modelo de tradução utilize\\nmais informações da representação do *encoder* para prever palavras\\nque geralmente aparecem no final da frase, onde a influência do\\n*encoder* tende a diminuir. Dado que as sentenças geradas não são\\nfluentes, é recomendada a adição de um *token* especial no início de\\ncada par de sentenças sintéticas [16].\\n\\n2 Por exemplo, traduzir uma sentença do português para o inglês e, em seguida, de\\nvolta para o português.\\n\\n\\n*3.2.3* *Substituição de Palavras.* Técnicas baseadas em substituição\\nde palavras envolvem gerar novas sentenças sintéticas escolhendo\\numa palavra alvo na sentença autêntica para ser substituída aleatoriamente por outra palavra. Isso pode ser aplicado tanto nas\\nsentenças da língua de origem quanto nas da língua de destino,\\ne as substituições podem ser realizadas independentemente entre\\nsi. Alternativamente, pode-se respeitar o alinhamento entre as palavras e realizar a substituição aos pares, como exemplificado na\\nTabela 1: ao substituir uma palavra na sentença de origem, também é substituída a palavra correspondente na sentença de destino.\\nMesmo que a substituição seja aleatória, a introdução de ruído nas\\nsentenças de destino ajuda o modelo a aprender a prever a próxima\\npalavra corretamente, mesmo após a geração de uma palavra que\\nnão corresponde exatamente ao padrão ouro [3] da tradução humana\\n\\n[19] [3].\\nSubstituir palavras por sinônimos é uma abordagem que gera\\nsentenças sintéticas com significado mais próximo da sentença autêntica. Ao escolher uma palavra para ser substituída, é consultada\\numa tabela de paráfrases que contém sinônimos que são candidatos\\na substituir a palavra original. Através da representação vetorial\\nda palavra original e das palavras candidatas, é calculada a similaridade cosseno a fim selecionar a palavra candidata com maior\\nsimilaridade para substituir a palavra original. Também é possível\\nutilizar marcação de partes do discurso ( *part-of-speech tagging* ) para\\nlimitar as opções de substituição dentro da mesma classe gramatical, como exemplificado na Figura 2. Ao identificar cada classe\\ngramatical presente em uma sentença por meio de *part-of-speech*\\n*tagging*, torna-se viável realizar substituições de palavras mantendo\\na coerência gramatical do texto. Essas restrições também evitam\\nerros que podem ocorrer com o uso da retrotradução, que confia\\ntotalmente na saída do modelo de tradução [11].\\n\\n**Tabela 1: Exemplos de reversão e substituição alinhada.**\\n\\n|Transformação|Idioma|Sentença|\\n|---|---|---|\\n|Nenhuma (sen- tença original)|Origem Destino|Tivemos uma calorosa recepção. TER CALOROSO&ANIMADO RECEPÇÃO [PONTO]|\\n|Reversão|Destino|[PONTO] RECEPÇÃO ANI- MADO&CALOROSO TER|\\n|Substituição ali- nhada|Origem Destino|Tivemos uma calorosa rio. TER CALOROSO&ANIMADO RIO [PONTO]|\\n\\n### **4 METODOLOGIA** **4.1 Técnicas Selecionadas**\\n\\nCom o objetivo de identificar quais métodos de aumento de dados\\ntrazem o melhor ganho de desempenho ao tradutor, foram selecionadas as seguintes técnicas: (a) retrotradução, por ser amplamente\\nutilizada em NMT, sendo uma referência sólida para comparação,\\n(b) a substituição alinhada e a tarefa auxiliar de reversão como descrito por Sánchez-Cartagena et al. 2021 e exemplificado na Tabela\\n\\n3 Tradução realizada por humanos e tomada como referência.\\n\\n\\n320\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Silva et al.\\n\\n\\n**Figura 2: Exemplos de substituição por sinônimos com** ***part-***\\n***of-speech tagging*** **. [11]**\\n\\n1, dado que é uma evolução sobre métodos de substituição aleatória\\nanteriores, sendo selecionada por sua capacidade de melhorar a\\ndiversidade dos dados sem a necessidade de modelos de linguagem\\nadicionais e (c) a substituição por sinônimos com *part-of-speech*\\n*tagging* como proposto por Maimaiti et al. 2021 e observado na\\nFigura 2, por ser uma alternativa à retrotradução que também tende\\na gerar sentenças fluentes.\\nPara a retrotradução, são utilizados dois modelos de tradução\\nautomática disponíveis no *framework Hugging Face Transformers* : o\\nmodelo *Helsinki-NLP/opus-mt-ROMANCE-en*, que traduz do PB para\\no inglês, e o *modelo Helsinki-NLP/opus-mt-en-ROMANCE*, que traduz\\ndo inglês de volta para o PB. Durante a etapa de retrotradução, as\\nsentenças em PB do corpus autêntico são traduzidas para o inglês\\npelo primeiro modelo e, em seguida, traduzidas de volta para o\\nPB pelo segundo modelo. Após remover as duplicatas, o corpus\\nexpandido apresenta um aumento de cerca de 50% no número total\\nde sentenças.\\nA substituição de palavras alinhadas, como apresentada por\\nSánchez-Cartagena et al. 2021, foi realizada utilizando o sistema\\nde tradução automática estatística MOSES [4] que utiliza a biblioteca\\nGIZA para o alinhamento de palavras. O MOSES produz um léxico\\nque contém entradas de palavras na língua de origem, juntamente\\ncom a probabilidade de que a palavra correspondente na língua\\nde destino seja uma tradução apropriada. Para a substituição, foi\\ndeterminado substituir uma palavra por sentença que é sorteada\\naleatoriamente, devendo o par de palavras alinhadas ter uma probabilidade maior que 0.7 no léxico. Em seguida, este par de palavras\\né substituído por outro par de palavras, também de probabilidade\\nmaior que 0.7, proporcionando uma variação semântica na sentença.\\nO método de reversão consiste em inverter a lista dos *tokens* da\\n\\n*string* da sentença original, separados pelo caractere de espaço. Para\\nmitigar o impacto de sentenças potencialmente não fluentes, foi\\nadicionado um *token* especial precedendo a sentença de origem em\\ncada método. Essa medida tem como objetivo diminuir a influência\\ndessas sentenças no resultado final do tradutor.\\n\\n4 O MOSES é um sistema de tradução automática que opera através do alinhamento\\num para um de todos os *tokens* da língua de origem para a língua de destino.\\n\\n\\nA implementação da substituição por sinônimos baseada em\\n*part-of-speech tagging*, conforme descrito por Maimaiti et al. 2021,\\nutiliza uma combinação de técnicas e recursos de PLN, incluindo\\nmodelos de *part-of-speech tagging*, a base de dados *WordNet* para\\nidentificar sinônimos candidatos e *word embeddings* para calcular a\\nsimilaridade entre os sinônimos candidatos e escolher o sinônimo\\n\\ncom maior similaridade cosseno. Para realizar a marcação de partes\\ndo discurso, foi utilizado o modelo *POS_tagger_brill.pkl*, disponível\\nno repositório do *GitHub inoueMashuu/POS-tagger-portuguese-nltk*\\npara o *framework* de PLN *nltk* . Esse modelo é responsável por\\natribuir classes gramaticais às palavras em PB para a identificação\\ndas palavras alvo que serão substituídas por sinônimos. A base de\\ndados *WordNet* [5], acessada através da biblioteca *nltk*, foi empregada\\ncomo fonte de sinônimos para as palavras identificadas pelo *part-of-*\\n*speech tagging* . Por fim, para calcular a similaridade entre as palavras\\nalvo e os sinônimos disponíveis no *WordNet*, foram utilizadas as\\n*embeddings skip-gram* de 100 dimensões do Repositório de *Word*\\n*Embeddings* do NILC [ 6 ]. As *word embeddings* são representações\\nvetoriais das palavras que capturam suas relações semânticas com\\nbase em seu contexto de ocorrência. Essas representações vetoriais\\npermitem calcular a similaridade cosseno entre palavras, necessário\\npara identificar o sinônimo mais adequado para a substituição.\\n### **4.2 Ambiente de Treinamento**\\n\\nO treinamento foi realizado utilizando o *framework Fairseq* [6] . Optouse por uma versão reduzida de um modelo que adota a arquitetura\\n*transformer*, proposta por Vaswani et al. (2017). No *Fairseq*, o modelo\\n*transformer* reduzido *transformer_iwslt_de_en* foi pré-treinado na\\ntradução de alemão para inglês. A escolha desse modelo permite\\num treinamento mais rápido, uma vez que fazer o ajuste fino ( *fine-*\\n*tuning* ) de um modelo de linguagem já treinado, mesmo que em um\\npar de idiomas diferentes, tende a ser mais eficiente do que treinar\\num modelo do zero [15].\\nO tradutor do VLibras possui uma arquitetura híbrida baseada\\nem RBMT e NMT. A sentença em PB é pré-processada por um\\ncomponente RBMT. A saída desse pré-processamento alimenta o\\nmodelo *transformer*, que é treinado com o objetivo de aproximar\\na glosa gerada pelo componente RBMT para a glosa gerada pelos\\nintérpretes humanos [2].\\nO corpus do VLibras possui pares de sentenças em PB como\\nlíngua de origem e uma representação intermediária da LIBRAS,\\ndenominada glosa, para as sentenças no idioma de destino. O corpus\\nmencionado consiste em aproximadamente 65.000 exemplos de\\npares de sentenças [ 2 ], cobrindo uma ampla variedade de tópicos\\ne contextos linguísticos. As glosas são criadas por intérpretes e\\nlinguistas especializados na LS e cada sinal da glosa possui uma\\nanimação associada. O uso de glosas intermediárias como uma\\nforma de representação linguística da LS permite que os algoritmos\\nde PLN trabalhem de forma mais eficaz com a LIBRAS [9].\\n\\n5 O *WordNet* é uma vasta base de dados lexical que organiza palavras em conjuntos\\nde sinônimos, conhecidos como synsets, e fornece informações sobre suas relações\\nsemânticas e gramaticais. Essa base de dados permite a consulta de sinônimos restritos a\\nclasses gramaticais específicas, como sujeitos, adjetivos, advérbios e verbos, permitindo\\nque a substituição gere sentenças de maior qualidade.\\n6 Desenvolvido pelo *Facebook*, projetado com foco no treinamento de redes *sequence-*\\n*to-sequence*, que são adequadas para tarefas de tradução automática.\\n\\n\\n321\\n\\n\\n-----\\n\\nUma Investigação sobre Técnicas de Data Augmentation Aplicadas a Tradução Automática Português-LIBRAS WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\nNo *pipeline* de tradução do VLibras, já existem cinco métodos de\\naumento de dados que geram sentenças fluentes e foram construídos\\ncom o conhecimento de linguistas da LIBRAS.\\n\\n  - **Lugares:** tem como objetivo introduzir variação nas sentenças ao identificar nomes de lugares como cidades, estados\\nou países, e substituí-los por outras localidades disponíveis\\nem tabelas de substituição auxiliares.\\n\\n  - **Negação:** trabalha na identificação de sinais na frase que\\npodem ser negados e gera novas sentenças realizando essa\\nsubstituição. Essa técnica é essencial para aprimorar a capacidade do sistema de tradução em compreender e gerar\\ncorretamente sentenças negativas, um aspecto importante\\nda gramática da LIBRAS.\\n\\n  - **Intensidade:** foca na identificação de advérbios na frase\\nque podem ser substituídos para gerar novas frases com\\ndiferentes níveis de intensidade. Por exemplo, a substituição\\nde \"muito\" por \"pouco\" ou vice-versa.\\n\\n  - **Famosos:** visa diversificar o conteúdo das sentenças ao identificar sinais referentes a pessoas famosas e substituí-los por\\noutros sinais representando outros famosos. Essa técnica\\nmelhora a capacidade de desambiguação do sistema ao lidar\\ncom pessoas conhecidas.\\n\\n  - **Direcionalidade:** visa capturar uma nuance gramatical específica da LS, levando em consideração o emissor e receptor\\nda ação do verbo, assemelhando-se a concordância númeropessoal. O método de Direcionalidade identifica esses sinais\\nna sentença e realiza substituições apropriadas com outros\\nverbos direcionais equivalentes.\\n\\nTodas essas técnicas de aumento de dados são aplicadas sobre as\\nsentenças do corpus autêntico construído por linguistas especializados em LIBRAS. As sentenças sintéticas geradas por essas técnicas\\nsão então anexadas ao corpus autêntico, formando o corpus padrão\\nutilizado no treinamento.\\n### **4.3 Treinamento**\\n\\nOs pares de sentenças do corpus são processados por cada método de aumento de dados já implementado no *pipeline* do VLibras,\\nconforme descrito na seção 4.2. O corpus aumentado gerado pela\\nsaída do *pipeline* do VLibras compõe o conjunto de dados de treinamento que é o padrão ( *baseline* ) para os experimentos realizados,\\npossuindo cerca de 106 mil pares de sentenças.\\nEm seguida, cada método de aumento de dados proposto e implementado neste trabalho é aplicado sobre o conjunto de sentenças\\ndo conjunto padrão. Nesta etapa, quando há combinação de diferentes métodos, a entrada de cada método de aumento de dados é\\nrestrita às sentenças do conjunto padrão, exceto para o método de\\nretrotradução, devido a restrições de recursos computacionais. No\\ncaso da retrotradução, as sentenças sintéticas geradas são anexadas\\nàs sentenças autênticas antes da execução dos métodos de aumento\\nde dados padrão do *pipeline* a fim de otimizar recursos e diminuir o\\ncusto com o uso de GPU.\\nA semente de geração de números aleatórios é fixada em todos\\nos experimentos realizados durante o treinamento do modelo, permitindo a reprodutibilidade dos resultados e uma comparação justa\\nentre diferentes treinamentos.\\n\\n\\nPor fim, o desempenho do modelo é avaliado através de um\\nconjunto de avaliação cuidadosamente selecionado por linguistas,\\ncontendo 50 sentenças para cada grupo relevante no domínio da\\nLIBRAS. Esses grupos incluem sentenças básicas, com números\\ncardinais, com palavras homônimas, relacionadas à direcionalidade,\\na pessoas famosas, à intensidade, a lugares e à negação, permitindo\\numa avaliação do desempenho do modelo em diferentes contextos\\nlinguísticos.\\n### **4.4 Métricas de Interesse**\\n\\nAvaliar a qualidade das traduções geradas por modelos de NMT é\\numa tarefa desafiadora devido à natureza complexa e subjetiva da\\nlinguagem. Diferentes traduções podem ser consideradas aceitáveis\\npara uma mesma sentença de origem, dependendo de uma variedade\\nde fatores como contexto, estilo e preferências individuais.\\nUma das métricas mais amplamente utilizadas para avaliar a\\nqualidade da tradução automática é o BLEU ( *Bilingual Evaluation*\\n*Understudy* ) [ 13 ]. O BLEU é uma métrica que varia de 0 a 100 e\\nbusca automatizar e replicar como um humano julgaria a qualidade\\nda tradução. Essa métrica é baseada na comparação dos n-gramas\\nda sentença gerada com as traduções de referência disponíveis, a\\nfim de calcular a similaridade entre a tradução gerada pelo modelo\\ne a tradução de referência.\\nO BLEU4, por exemplo, é calculado com base em n-gramas de\\nquatro palavras: Isso significa que o modelo é avaliado com base\\nna precisão dos n-gramas de quatro palavras em suas traduções,\\nem comparação com as traduções de referência. Uma das principais vantagens do BLEU é sua rapidez de cálculo, o que o torna\\numa métrica eficiente para avaliação automática. No entanto, é\\nimportante ressaltar que o BLEU apresenta algumas limitações. Por\\nexemplo, não são consideradas as similaridades semânticas entre\\nas traduções, o que pode levar a pontuações imprecisas em alguns\\n\\ncasos.\\n\\nNeste trabalho, a análise dos resultados é realizada com base no\\ndesempenho do modelo de tradução seguindo a métrica BLEU4.\\n### **5 RESULTADOS E DISCUSSÕES**\\n\\nForam realizadas duas rodadas de experimentos com o objetivo\\nde avaliar o impacto de diferentes métodos de aumento de dados\\nno desempenho do modelo de tradução. Na primeira rodada, cada\\nmétodo foi aplicado em sequência sobre o conjunto padrão, sem\\nlimitar a quantidade de sentenças sintéticas geradas. Na segunda\\nrodada, o tamanho do conjunto de treinamento foi restrito a 155\\nmil sentenças a fim de equilibrar a proporção entre sentenças autênticas e sintéticas. A Tabela 2 mostra o desempenho do modelo\\nutilizando exclusivamente o corpus autêntico com 65 mil pares de\\nsentenças, sem a aplicação de estratégias de aumento de dados. Em\\ncontraste com o corpus padrão, que inclui os métodos de aumento\\nde dados do *pipeline* de tradução do VLibras mencionados na seção\\n4.2, totalizando 106 mil pares de sentenças.\\nTodos os métodos de aumento de dados apresentaram melhorias\\nem relação ao conjunto padrão, conforme observado na Tabela 3. A\\nretrotradução trouxe um ganho geral médio significativo (+15,82\\nBLEU), mesmo com o menor acréscimo de dados (totalizando 155 mil\\nsentenças). Tanto a reversão com substituição de palavras alinhadas,\\nquanto a substituição por sinônimos baseada em *part-of-speech*\\n\\n\\n322\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Silva et al.\\n\\n**Tabela 2: Resultado em BLEU do desempenho do modelo**\\n**utilizando apenas o corpus autêntico, sem aumento de dados.**\\n\\n|Categoria|Pontuação BLEU|\\n|---|---|\\n|Básicas|17,35|\\n|Cardinais|6,22|\\n|Contexto|4,71|\\n|Direcionalidade|0,00|\\n|Famosos|0,00|\\n|Intensidade|0,00|\\n|Lugares|10,79|\\n|Negação|0,00|\\n|Romanos|3,92|\\n\\n\\n\\n*tagging* demonstraram resultados próximos (50,73 e 50,32 pontos\\nde BLEU, respectivamente). Observa-se que o aumento de dados traz\\nconsideráveis melhorias, analizando sob a perspectiva da métrica\\nBLEU, para todos os grupos de validação.\\n\\n**Tabela 3: Resultado em BLEU do desempenho do modelo de**\\n**tradução utilizando as técnicas de aumento de dados selecio-**\\n**nadas.**\\n\\n\\n\\n\\n\\n\\n|Col1|Padrão (106 mil)|RT (155 mil)|RV + SA (313 mil)|SS (346 mil)|\\n|---|---|---|---|---|\\n|Básicas|42,05|51,66|47,89|52,67|\\n|Cardinais|50,18|64,78|72,86|70,52|\\n|Contexto|24,37|43,92|47,67|38,86|\\n|Direcionais|0,00|18,57|30,28|29,06|\\n|Famosos|27,34|45,07|43,23|40,70|\\n|Intensidade|38,47|49,38|42,5|36,55|\\n|Lugares|44,04|56,68|51,85|55,04|\\n|Negação|44,04|50,84|57,14|62,42|\\n|Romanos|25,86|57,85|63,15|67,12|\\n|Média|32,93|48,75|50,73|50,32|\\n\\n\\n**Legenda: RT (Retrotradução); RV (Reversão); SA (Substituição**\\n**alinhada); SS (Substituição por sinônimos)**\\n\\nAo adicionar a retrotradução em conjunto com a combinação\\nde reversão com substituição alinhada, conforme a Tabela 4, foi\\nobservada uma pequena piora não significativa no resultado geral.\\nPor outro lado, a combinação de retrotradução com a substituição\\npor sinônimos resultou em um ganho expressivo de desempenho\\n(+19,25 BLEU em relação ao conjunto padrão), possivelmente devido ao aumento substancial na quantidade de sentenças resultante\\ndessa combinação (totalizando 540 mil sentenças). No entanto, ao incluir mais métodos de aumento de dados, mesmo que isso aumente\\nainda mais a quantidade de sentenças no conjunto de treinamento,\\nnão foi observada uma melhoria significativa no desempenho do\\nmodelo. Isso sugere que há uma limitação na melhoria do desempenho proporcionada pela quantidade de sentenças sintéticas geradas,\\nespecialmente em função da quantidade de dados autênticos disponíveis. Esses resultados evidenciam a importância de encontrar\\n\\n\\n**Figura 3: Média em BLEU do desempenho das técnicas de**\\n**aumento de dados sem restrição no tamanho do conjunto de**\\n**treinamento.**\\n\\num equilíbrio adequado entre a quantidade de dados autênticos e\\nsintéticos no conjunto de treinamento.\\nNa segunda fase dos experimentos, conforme exibido na Tabela 5,\\nonde o tamanho do conjunto de treinamento foi limitado, o método\\nde retrotradução obteve os melhores resultados. Esse resultado não\\né surpreendente, uma vez que a retrotradução é um dos métodos\\nmais estabelecidos e amplamente utilizados para aumento de dados\\n\\nem PLN.\\n\\nAo considerar um cenário com aumento de 25% nos dados para\\ncada método empregado, foram testadas diversas combinações de\\nmétodos aos pares, conforme visto na Tabela 6. Notadamente, a\\ncombinação de retrotradução e substituição por sinônimos com\\n*part-for-speech tagging* demonstrou o melhor desempenho quando\\nhá a limitação do tamanho do conjunto de treinamento, apresentando ganho de 16,5 BLEU. Essa combinação já havia apresentado\\nbons resultados quando não há limitação no tamanho do conjunto\\nde treinamento, com 19,25 pontos de BLEU sobre o padrão (ver\\nTabela 4), isso mostra que as duas técnicas produzem resultados\\nbons quando usadas em conjunto. Uma possível explicação para o\\nbom desempenho dessa combinação é o aumento da diversidade do\\nvocabulário proporcionado pela retrotradução. Ao traduzir as sentenças de volta para o idioma original, o conjunto de treinamento\\npassa a contar com um vocabulário mais amplo, o que, por sua vez,\\naumenta as opções de substituição por sinônimos.\\nDevido à limitação na quantidade de sentenças, a eficácia do\\nmétodo de reversão foi reduzida quando combinado com outras\\ntécnicas. Essa é uma desvantagem que o experimento traz para\\nesse método, porque ele é sugerido como uma tarefa destinada a\\n\\n\\n323\\n\\n\\n-----\\n\\nUma Investigação sobre Técnicas de Data Augmentation Aplicadas a Tradução Automática Português-LIBRAS WebMedia’2024, Juiz de Fora, Brazil\\n\\n**Tabela 4: Desempenho em BLEU de combinações de técnicas de aumento de dados sem restrição na quantidade de sentenças.**\\n\\n\\n|Col1|Padrão (106 mil)|RT + RV + SA (467 mil)|RT + SS (540 mil)|RT + RV + SS (695 mil)|Todos (851 mil)|\\n|---|---|---|---|---|---|\\n|Básicas|42,05|48,55|48,14|50,76|53,53|\\n|Cardinais|50,18|61,54|67,29|66,79|66,49|\\n|Contexto|24,37|45,68|43,73|39,36|49,25|\\n|Direcionalidade|0,00|27,66|33,60|36,95|32,66|\\n|Famosos|27,34|43,74|48,09|44,83|42,85|\\n|Intensidade|38,47|45,37|39,61|42,29|42,29|\\n|Lugares|44,04|54,45|48,10|52,42|47,12|\\n|Negação|44,04|53,84|65,94|59,17|59,34|\\n|Romanos|25,86|74,17|75,19|76,07|75,21|\\n|Média|32,93|50,55|52,18|52,07|52,08|\\n\\n\\n**Legenda: RT (Retrotradução); RV (Reversão); SA (Substituição alinhada); SS (Substituição por sinônimos)**\\n\\n\\n\\n\\n\\n\\n**Tabela 5: Resultado em BLEU de cada técnica utilizada isola-**\\n\\n**damente, com restrição de 155 mil sentenças no conjunto de**\\n**treinamento.**\\n\\n|Col1|Padrão|RT|SS|RV|SA|\\n|---|---|---|---|---|---|\\n|Básicas|42,05|51,66|48,74|43,07|48,51|\\n|Cardinais|50,18|64,78|56,29|60,33|60,69|\\n|Contexto|24,37|43,92|35,00|32,80|37,85|\\n|Direcionalidade|0,00|18,57|0|0|22,34|\\n|Famosos|27,34|45,07|47,41|34,36|45,23|\\n|Intensidade|38,47|49,38|38,54|38,65|43,14|\\n|Lugares|44,04|56,68|51,5|47,87|47,73|\\n|Negação|44,04|50,84|58,87|42,95|53,6|\\n|Romanos|25,86|57,85|40,72|31,07|57,25|\\n|Média|32,93|48,75|41,89|36,78|46,26|\\n\\n\\n\\n**Legenda: RT (Retrotradução); RV (Reversão); SA(Substituição**\\n**alinhada); SS (Substituição por sinônimos)**\\n\\nfortalecer o *encoder* de forma independente de outras técnicas de\\naumento de dados. Além das combinações de pares de métodos,\\ntambém foram exploradas outras configurações envolvendo três\\ntécnicas distintas.\\nPor fim, foram testadas mais algumas configurações, conforme\\nmostrado na Tabela 7. Uma combinação de 25% de retrotradução,\\n25% de substituição por sinônimos e 25% de substituição alinhada\\ne uma configuração com 1/3 de contribuição por cada método.\\nObserva-se, no entanto, que essas combinações não resultaram\\nem melhorias significativas em relação aos resultados obtidos anteriormente. Sendo assim, mesmo com a combinação de diversas\\ntécnicas de aumento de dados, a quantidade de sentenças ainda é\\num fator limitante para alcançar melhorias de desempenho.\\n\\n\\n**Figura 4: Média em BLEU do desempenho das técnicas de**\\n**aumento de dados com limitação de 155 mil sentenças no**\\n**conjunto de treinamento.**\\n\\n\\n324\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Silva et al.\\n\\n**Tabela 6: Resultado em BLEU do treinamento do modelo de tradução com contribuição de 25 mil sentenças por método.**\\n\\n|Col1|Padrão|RV + SA|RV + SS|RT + SS|RT + RV|RT + SA|\\n|---|---|---|---|---|---|---|\\n|Básicas|42,05|47,29|40,84|48,37|47,86|46,25|\\n|Cardinais|50,18|66,69|57,17|66,33|67,18|64,80|\\n|Contexto|24,37|43,86|33,08|38,86|38,56|43,28|\\n|Direcionalidade|0,00|18,61|17,12|28,12|21,91|25,01|\\n|Famosos|27,34|52,12|41,58|46,67|42,07|46,46|\\n|Intensidade|38,47|40,21|49,63|43,62|44,40|42,06|\\n|Lugares|44,04|48,65|47,97|50,84|52,17|52,65|\\n|Negação|44,04|57,69|48,55|60,05|46,69|50,59|\\n|Romanos|25,86|49,51|34,07|62,09|48,99|60,08|\\n|Média|32,93|47,18|41,11|49,43|45,53|47,90|\\n\\n\\n\\n**Legenda: RT (Retrotradução); RV (Reversão); SA (Substituição alinhada); SS (Substituição por sinônimos)**\\n\\n\\n**Tabela 7: Resultado em BLEU para combinações envolvendo**\\n**retrotradução, substituição alinhada e substituição por sinô-**\\n**nimos.**\\n\\n\\n\\n\\n\\n|Col1|Padrão|1 RT + 3 1 SA + 1 SS 3 3|1 RT + 2 1 SA + 1 SS 4 4|\\n|---|---|---|---|\\n|Básicas|42,05|54,24|52,80|\\n|Cardinais|50,18|63,44|63,78|\\n|Contexto|24,37|43,20|44,02|\\n|Direcionalidade|0,00|22,35|16,92|\\n|Famosos|27,34|49,94|50,71|\\n|Intensidade|38,47|42,44|33,28|\\n|Lugares|44,04|51,37|51,24|\\n|Negação|44,04|39,94|52,71|\\n|Romanos|25,86|60,28|62,68|\\n|Média|32,93|47,46|47,57|\\n\\n\\n**Legenda: RT (Retrotradução); SA (Substituição alinhada);**\\n**SS (Substituição por sinônimos)**\\n\\n\\n### **6 CONCLUSÃO**\\n\\nA geração de dados sintéticos para aprimorar modelos de NMT em\\ncenários de poucos recursos ( *low-resource language* ), especialmente\\npara LS como a LIBRAS, é de suma importância para a acessibilidade\\ne inclusão de pessoas surdas. Dessa forma, este trabalho explorou\\ndiferentes métodos de aumento de dados a fim de identificar quais\\nabordagens podem melhorar o desempenho do tradutor, com base\\nem testes computacionais amparados pela métrica BLEU.\\nObservou-se que a técnica amplamente utilizada de retrotradução também é eficaz na tradução de PB para LIBRAS, trazendo um\\nganho de 15,82 pontos de BLEU em relação ao conjunto padrão.\\nA combinação de retrotradução e substituição por sinônimos com\\n*part-of-speech tagging* trouxe os melhores resultados em ambos os\\ncenários: sem restrição no tamanho do conjunto de treinamento\\n(+19,25 BLEU sobre o padrão) e também quando o conjunto de\\ntreinamento foi limitado a 155 mil sentenças (+16,5 BLEU sobre\\no padrão). Essas técnicas podem ser utilizadas para aumentar a\\nquantidade de exemplos em conjuntos de sentenças que estejam\\n\\n\\nsub-representados no corpus. Destaca-se também, diante dos resultados alcançados, a importância de manter um equilíbrio entre a\\nquantidade de dados sintéticos gerados em relação à quantidade de\\ndados autênticos no corpus original.\\nPor fim, enxerga-se que a investigação de técnicas mais custosas,\\nporém potencialmente mais eficazes, pode abrir novas possibilidades para aprimorar ainda mais a qualidade da tradução automática.\\nModelos de linguagem podem produzir texto fluente em uma variedade de contextos linguísticos, portanto, utilizar esses modelos\\npara gerar dados sintéticos é uma sugestão para trabalhos futuros.\\n### **AGRADECIMENTOS**\\n\\nOs autores agradecem à Secretaria Nacional dos Direitos da Pessoa\\ncom Deficiência do Ministério dos Direitos Humanos e da Cidadania\\npelo apoio financeiro para a realização desta pesquisa.\\n### **REFERÊNCIAS**\\n\\n[1] T. M. U. Araújo. 2012. *Uma solução para geração automática de trilhas em*\\n*língua brasileira de sinais em conteúdos multimídia* . Tese (Doutorado em Automação e Sistemas). Universidade Federal do Rio Grande do Norte, Natal.\\nhttps://repositorio.ufrn.br/handle/123456789/15190\\n\\n[2] Renan Costa e Diego Ramon Silva e Samuel Moreira e Daniel Faustino Souza\\ne Rostand Edson Costa e Tiago Maritan Araújo. 2024. Avaliação do uso de\\nmodelos de aprendizagem profunda na tradução automática de línguas de sinais.\\n*Revista Principia - Divulgação Científica e Tecnológica do IFPB* 0, 0 (2024). https:\\n//periodicos.ifpb.edu.br/index.php/principia/article/view/8053\\n\\n[3] Marzieh Fadaee, Arianna Bisazza, and Christof Monz. 2017. Data Augmentation\\nfor Low-Resource Neural Machine Translation. In *Proceedings of the 55th An-*\\n*nual Meeting of the Association for Computational Linguistics (Volume 2: Short*\\n*Papers)*, Regina Barzilay and Min-Yen Kan (Eds.). Association for Computational\\nLinguistics, Vancouver, Canada, 567–573. https://doi.org/10.18653/v1/P17-2090\\n\\n[4] Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi,\\nTeruko Mitamura, and Eduard Hovy. 2021. A Survey of Data Augmentation\\nApproaches for NLP. In *Findings of the Association for Computational Linguistics:*\\n*ACL-IJCNLP 2021*, Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.).\\nAssociation for Computational Linguistics, Online, 968–988. https://doi.org/10.\\n18653/v1/2021.findings-acl.84\\n\\n[5] Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor O. K. Li. 2018. Universal\\nNeural Machine Translation for Extremely Low Resource Languages. *CoRR*\\nabs/1802.05368 (2018). arXiv:1802.05368 http://arxiv.org/abs/1802.05368\\n\\n[6] Nathan Hartmann, Erick Fonseca, Christopher Shulby, Marcos Treviso, Jessica\\nRodrigues, and Sandra Aluisio. 2017. Portuguese Word Embeddings: Evaluating\\non Word Analogies and Natural Language Tasks. In *Proceedings of the XI Brazilian*\\n*Symposium in Information and Human Language Technology and Collocated Events*\\n*(STIL 2017)* . Uberlandia, Minas Gerais, Brazil.\\n\\n\\n325\\n\\n\\n-----\\n\\nUma Investigação sobre Técnicas de Data Augmentation Aplicadas a Tradução Automática Português-LIBRAS WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\n\\n[7] Jin Yea Jang, Han-Mu Park, Saim Shin, Suna Shin, Byungcheon Yoon, and Gahgene Gweon. 2022. Automatic Gloss-level Data Augmentation for Sign Language Translation. In *Proceedings of the Thirteenth Language Resources and Eva-*\\n*luation Conference*, Nicoletta Calzolari, Frédéric Béchet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara,\\nBente Maegaard, Joseph Mariani, Hélène Mazo, Jan Odijk, and Stelios Piperidis\\n(Eds.). European Language Resources Association, Marseille, France, 6808–6813.\\nhttps://aclanthology.org/2022.lrec-1.734\\n\\n[8] Z. Liang, H. Li, and J. Chai. 2023. Sign Language Translation: A Survey of\\nApproaches and Techniques. *Electronics* 12, 12 (2023). https://doi.org/10.3390/\\nelectronics12122678\\n\\n[9] Manuella Aschoff C. B. Lima, Tiago Maritan U. de Araújo, Rostand E. O. Costa, and\\nErickson S. Oliveira. 2022. A machine translation mechanism of Brazilian Portuguese to Libras with syntactic-semantic adequacy. *Natural Language Engineering*\\n28, 3 (2022), 271–294. https://doi.org/10.1017/S1351324920000662\\n\\n[10] Alexandre Magueresse, Vincent Carles, and Evan Heetderks. 2020. Low-resource\\nLanguages: A Review of Past Work and Future Challenges. *CoRR* abs/2006.07264\\n(2020). arXiv:2006.07264 https://arxiv.org/abs/2006.07264\\n\\n[11] Mieradilijiang Maimaiti, Yang Liu, Huanbo Luan, Zegao Pan, and Maosong Sun.\\n2021. Improving Data Augmentation for Low-Resource NMT Guided by POSTagging and Paraphrase Embedding. *ACM Trans. Asian Low-Resour. Lang. Inf.*\\n*Process.* 20, 6, Article 107 (aug 2021), 21 pages. https://doi.org/10.1145/3464427\\n\\n[12] Amit Moryossef, Kayo Yin, Graham Neubig, and Yoav Goldberg. 2021. Data\\nAugmentation for Sign Language Gloss Translation. In *Proceedings of the 1st*\\n*International Workshop on Automatic Translation for Signed and Spoken Languages*\\n*(AT4SSL)*, Dimitar Shterionov (Ed.). Association for Machine Translation in the\\nAmericas, Virtual. https://aclanthology.org/2021.mtsummit-at4ssl.1\\n\\n[13] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu:\\na Method for Automatic Evaluation of Machine Translation. In *Proceedings of*\\n*the 40th Annual Meeting of the Association for Computational Linguistics*, Pierre\\nIsabelle, Eugene Charniak, and Dekang Lin (Eds.). Association for Computational\\nLinguistics, Philadelphia, Pennsylvania, USA, 311–318. https://doi.org/10.3115/\\n1073083.1073135\\n\\n\\n\\n[14] Alberto Poncelas, Dimitar Shterionov, Andy Way, Gideon Maillette de Buy Wenniger, and Peyman Passban. 2018. Investigating Backtranslation in Neural Machine Translation. In *Proceedings of the 21st Annual Conference of the European*\\n*Association for Machine Translation*, Juan Antonio Pérez-Ortiz, Felipe SánchezMartínez, Miquel Esplà-Gomis, Maja Popović, Celia Rico, André Martins, Joachim Van den Bogaert, and Mikel L. Forcada (Eds.). Alicante, Spain, 269–278.\\nhttps://aclanthology.org/2018.eamt-main.25\\n\\n[15] Surangika Ranathunga, En-Shiun Annie Lee, Marjana Prifti Skenduli, Ravi\\nShekhar, Mehreen Alam, and Rishemjit Kaur. 2023. Neural Machine Translation for Low-resource Languages: A Survey. *ACM Comput. Surv.* 55, 11, Article\\n229 (feb 2023), 37 pages. https://doi.org/10.1145/3567592\\n\\n[16] Víctor M. Sánchez-Cartagena, Miquel Esplà-Gomis, Juan Antonio Pérez-Ortiz, and\\nFelipe Sánchez-Martínez. 2021. Rethinking Data Augmentation for Low-Resource\\nNeural Machine Translation: A Multi-Task Learning Approach. In *Proceedings of*\\n*the 2021 Conference on Empirical Methods in Natural Language Processing*, MarieFrancine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.).\\nAssociation for Computational Linguistics, Online and Punta Cana, Dominican\\nRepublic, 8502–8516. https://doi.org/10.18653/v1/2021.emnlp-main.669\\n\\n[17] Vinícius Veríssimo, Cecília Silva, Vitor Hanael, Caio Moraes, Rostand Costa,\\nTiago Maritan, Manuella Aschoff, and Thaís Gaudêncio. 2019. A study on the\\nuse of sequence-to-sequence neural networks for automatic translation of brazilian portuguese to libras. In *Proceedings of the 25th Brazillian Symposium on*\\n*Multimedia and the Web* . 101–108.\\n\\n[18] Jing Wang and Lina Yang. 2022. Effective Data Augmentation Methods for CCMT\\n2022. In *Machine Translation*, Tong Xiao and Juan Pino (Eds.). Springer Nature\\nSingapore, Singapore, 135–142.\\n\\n[19] Xinyi Wang, Hieu Pham, Zihang Dai, and Graham Neubig. 2018. SwitchOut:\\nan Efficient Data Augmentation Algorithm for Neural Machine Translation. In\\n*Proceedings of the 2018 Conference on Empirical Methods in Natural Language*\\n*Processing*, Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii\\n(Eds.). Association for Computational Linguistics, Brussels, Belgium, 856–861.\\nhttps://doi.org/10.18653/v1/D18-1100\\n\\n\\n326\\n\\n\\n-----\\n\\n'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 218
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:24:47.734261Z",
     "start_time": "2025-05-12T23:24:47.730614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from process_text import split_references\n",
    "ref = text.split('**REFERÊNCIAS**')[1].split('###')[0]\n",
    "# ref = text.split('\\n\\n\\n\\n')[1].split('###')[0]\n",
    "ref = clean_abs(ref)\n",
    "ref = split_references(ref)\n",
    "ref"
   ],
   "id": "547d12266d24c8c2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[1] T. M. U. Araújo. 2012. Uma solução para geração automática de trilhas em língua brasileira de sinais em conteúdos multimídia . Tese (Doutorado em Automação e Sistemas). Universidade Federal do Rio Grande do Norte, Natal. https://repositorio.ufrn.br/handle/123456789/15190',\n",
       " '[2] Renan Costa e Diego Ramon Silva e Samuel Moreira e Daniel Faustino Souza e Rostand Edson Costa e Tiago Maritan Araújo. 2024. Avaliação do uso de modelos de aprendizagem profunda na tradução automática de línguas de sinais. Revista Principia - Divulgação Científica e Tecnológica do IFPB 0, 0 (2024). https: //periodicos.ifpb.edu.br/index.php/principia/article/view/8053',\n",
       " '[3] Marzieh Fadaee, Arianna Bisazza, and Christof Monz. 2017. Data Augmentation for Low-Resource Neural Machine Translation. In Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Regina Barzilay and Min-Yen Kan (Eds.). Association for Computational Linguistics, Vancouver, Canada, 567–573. https://doi.org/10.18653/v1/P17-2090',\n",
       " '[4] Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi, Teruko Mitamura, and Eduard Hovy. 2021. A Survey of Data Augmentation Approaches for NLP. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, Online, 968–988. https://doi.org/10. 18653/v1/2021.findings-acl.84',\n",
       " '[5] Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor O. K. Li. 2018. Universal Neural Machine Translation for Extremely Low Resource Languages. CoRR abs/1802.05368 (2018). arXiv:1802.05368 http://arxiv.org/abs/1802.05368',\n",
       " '[6] Nathan Hartmann, Erick Fonseca, Christopher Shulby, Marcos Treviso, Jessica Rodrigues, and Sandra Aluisio. 2017. Portuguese Word Embeddings: Evaluating on Word Analogies and Natural Language Tasks. In Proceedings of the XI Brazilian Symposium in Information and Human Language Technology and Collocated Events (STIL 2017) . Uberlandia, Minas Gerais, Brazil.   325   -----  Uma Investigação sobre Técnicas de Data Augmentation Aplicadas a Tradução Automática Português-LIBRAS WebMedia’2024, Juiz de Fora, Brazil',\n",
       " '[7] Jin Yea Jang, Han-Mu Park, Saim Shin, Suna Shin, Byungcheon Yoon, and Gahgene Gweon. 2022. Automatic Gloss-level Data Augmentation for Sign Language Translation. In Proceedings of the Thirteenth Language Resources and Eva- luation Conference, Nicoletta Calzolari, Frédéric Béchet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hélène Mazo, Jan Odijk, and Stelios Piperidis (Eds.). European Language Resources Association, Marseille, France, 6808–6813. https://aclanthology.org/2022.lrec-1.734',\n",
       " '[8] Z. Liang, H. Li, and J. Chai. 2023. Sign Language Translation: A Survey of Approaches and Techniques. Electronics 12, 12 (2023). https://doi.org/10.3390/ electronics12122678',\n",
       " '[9] Manuella Aschoff C. B. Lima, Tiago Maritan U. de Araújo, Rostand E. O. Costa, and Erickson S. Oliveira. 2022. A machine translation mechanism of Brazilian Portuguese to Libras with syntactic-semantic adequacy. Natural Language Engineering 28, 3 (2022), 271–294. https://doi.org/10.1017/S1351324920000662',\n",
       " '[10] Alexandre Magueresse, Vincent Carles, and Evan Heetderks. 2020. Low-resource Languages: A Review of Past Work and Future Challenges. CoRR abs/2006.07264 (2020). arXiv:2006.07264 https://arxiv.org/abs/2006.07264',\n",
       " '[11] Mieradilijiang Maimaiti, Yang Liu, Huanbo Luan, Zegao Pan, and Maosong Sun. 2021. Improving Data Augmentation for Low-Resource NMT Guided by POSTagging and Paraphrase Embedding. ACM Trans. Asian Low-Resour. Lang. Inf. Process. 20, 6, Article 107 (aug 2021), 21 pages. https://doi.org/10.1145/3464427',\n",
       " '[12] Amit Moryossef, Kayo Yin, Graham Neubig, and Yoav Goldberg. 2021. Data Augmentation for Sign Language Gloss Translation. In Proceedings of the 1st International Workshop on Automatic Translation for Signed and Spoken Languages (AT4SSL), Dimitar Shterionov (Ed.). Association for Machine Translation in the Americas, Virtual. https://aclanthology.org/2021.mtsummit-at4ssl.1',\n",
       " '[13] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Pierre Isabelle, Eugene Charniak, and Dekang Lin (Eds.). Association for Computational Linguistics, Philadelphia, Pennsylvania, USA, 311–318. https://doi.org/10.3115/ 1073083.1073135',\n",
       " '[14] Alberto Poncelas, Dimitar Shterionov, Andy Way, Gideon Maillette de Buy Wenniger, and Peyman Passban. 2018. Investigating Backtranslation in Neural Machine Translation. In Proceedings of the 21st Annual Conference of the European Association for Machine Translation, Juan Antonio Pérez-Ortiz, Felipe SánchezMartínez, Miquel Esplà-Gomis, Maja Popović, Celia Rico, André Martins, Joachim Van den Bogaert, and Mikel L. Forcada (Eds.). Alicante, Spain, 269–278. https://aclanthology.org/2018.eamt-main.25',\n",
       " '[15] Surangika Ranathunga, En-Shiun Annie Lee, Marjana Prifti Skenduli, Ravi Shekhar, Mehreen Alam, and Rishemjit Kaur. 2023. Neural Machine Translation for Low-resource Languages: A Survey. ACM Comput. Surv. 55, 11, Article 229 (feb 2023), 37 pages. https://doi.org/10.1145/3567592',\n",
       " '[16] Víctor M. Sánchez-Cartagena, Miquel Esplà-Gomis, Juan Antonio Pérez-Ortiz, and Felipe Sánchez-Martínez. 2021. Rethinking Data Augmentation for Low-Resource Neural Machine Translation: A Multi-Task Learning Approach. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, MarieFrancine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 8502–8516. https://doi.org/10.18653/v1/2021.emnlp-main.669',\n",
       " '[17] Vinícius Veríssimo, Cecília Silva, Vitor Hanael, Caio Moraes, Rostand Costa, Tiago Maritan, Manuella Aschoff, and Thaís Gaudêncio. 2019. A study on the use of sequence-to-sequence neural networks for automatic translation of brazilian portuguese to libras. In Proceedings of the 25th Brazillian Symposium on Multimedia and the Web . 101–108.',\n",
       " '[18] Jing Wang and Lina Yang. 2022. Effective Data Augmentation Methods for CCMT 2022. In Machine Translation, Tong Xiao and Juan Pino (Eds.). Springer Nature Singapore, Singapore, 135–142.',\n",
       " '[19] Xinyi Wang, Hieu Pham, Zihang Dai, and Graham Neubig. 2018. SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii (Eds.). Association for Computational Linguistics, Brussels, Belgium, 856–861. https://doi.org/10.18653/v1/D18-1100   326   -----']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 219
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:24:53.250624Z",
     "start_time": "2025-05-12T23:24:53.247441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus.loc[38, 'referencias'] = ref\n",
    "corpus.loc[38]['referencias']"
   ],
   "id": "c8a052d8a0e6cd94",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[1] T. M. U. Araújo. 2012. Uma solução para geração automática de trilhas em língua brasileira de sinais em conteúdos multimídia . Tese (Doutorado em Automação e Sistemas). Universidade Federal do Rio Grande do Norte, Natal. https://repositorio.ufrn.br/handle/123456789/15190',\n",
       " '[2] Renan Costa e Diego Ramon Silva e Samuel Moreira e Daniel Faustino Souza e Rostand Edson Costa e Tiago Maritan Araújo. 2024. Avaliação do uso de modelos de aprendizagem profunda na tradução automática de línguas de sinais. Revista Principia - Divulgação Científica e Tecnológica do IFPB 0, 0 (2024). https: //periodicos.ifpb.edu.br/index.php/principia/article/view/8053',\n",
       " '[3] Marzieh Fadaee, Arianna Bisazza, and Christof Monz. 2017. Data Augmentation for Low-Resource Neural Machine Translation. In Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Regina Barzilay and Min-Yen Kan (Eds.). Association for Computational Linguistics, Vancouver, Canada, 567–573. https://doi.org/10.18653/v1/P17-2090',\n",
       " '[4] Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi, Teruko Mitamura, and Eduard Hovy. 2021. A Survey of Data Augmentation Approaches for NLP. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, Online, 968–988. https://doi.org/10. 18653/v1/2021.findings-acl.84',\n",
       " '[5] Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor O. K. Li. 2018. Universal Neural Machine Translation for Extremely Low Resource Languages. CoRR abs/1802.05368 (2018). arXiv:1802.05368 http://arxiv.org/abs/1802.05368',\n",
       " '[6] Nathan Hartmann, Erick Fonseca, Christopher Shulby, Marcos Treviso, Jessica Rodrigues, and Sandra Aluisio. 2017. Portuguese Word Embeddings: Evaluating on Word Analogies and Natural Language Tasks. In Proceedings of the XI Brazilian Symposium in Information and Human Language Technology and Collocated Events (STIL 2017) . Uberlandia, Minas Gerais, Brazil.   325   -----  Uma Investigação sobre Técnicas de Data Augmentation Aplicadas a Tradução Automática Português-LIBRAS WebMedia’2024, Juiz de Fora, Brazil',\n",
       " '[7] Jin Yea Jang, Han-Mu Park, Saim Shin, Suna Shin, Byungcheon Yoon, and Gahgene Gweon. 2022. Automatic Gloss-level Data Augmentation for Sign Language Translation. In Proceedings of the Thirteenth Language Resources and Eva- luation Conference, Nicoletta Calzolari, Frédéric Béchet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hélène Mazo, Jan Odijk, and Stelios Piperidis (Eds.). European Language Resources Association, Marseille, France, 6808–6813. https://aclanthology.org/2022.lrec-1.734',\n",
       " '[8] Z. Liang, H. Li, and J. Chai. 2023. Sign Language Translation: A Survey of Approaches and Techniques. Electronics 12, 12 (2023). https://doi.org/10.3390/ electronics12122678',\n",
       " '[9] Manuella Aschoff C. B. Lima, Tiago Maritan U. de Araújo, Rostand E. O. Costa, and Erickson S. Oliveira. 2022. A machine translation mechanism of Brazilian Portuguese to Libras with syntactic-semantic adequacy. Natural Language Engineering 28, 3 (2022), 271–294. https://doi.org/10.1017/S1351324920000662',\n",
       " '[10] Alexandre Magueresse, Vincent Carles, and Evan Heetderks. 2020. Low-resource Languages: A Review of Past Work and Future Challenges. CoRR abs/2006.07264 (2020). arXiv:2006.07264 https://arxiv.org/abs/2006.07264',\n",
       " '[11] Mieradilijiang Maimaiti, Yang Liu, Huanbo Luan, Zegao Pan, and Maosong Sun. 2021. Improving Data Augmentation for Low-Resource NMT Guided by POSTagging and Paraphrase Embedding. ACM Trans. Asian Low-Resour. Lang. Inf. Process. 20, 6, Article 107 (aug 2021), 21 pages. https://doi.org/10.1145/3464427',\n",
       " '[12] Amit Moryossef, Kayo Yin, Graham Neubig, and Yoav Goldberg. 2021. Data Augmentation for Sign Language Gloss Translation. In Proceedings of the 1st International Workshop on Automatic Translation for Signed and Spoken Languages (AT4SSL), Dimitar Shterionov (Ed.). Association for Machine Translation in the Americas, Virtual. https://aclanthology.org/2021.mtsummit-at4ssl.1',\n",
       " '[13] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Pierre Isabelle, Eugene Charniak, and Dekang Lin (Eds.). Association for Computational Linguistics, Philadelphia, Pennsylvania, USA, 311–318. https://doi.org/10.3115/ 1073083.1073135',\n",
       " '[14] Alberto Poncelas, Dimitar Shterionov, Andy Way, Gideon Maillette de Buy Wenniger, and Peyman Passban. 2018. Investigating Backtranslation in Neural Machine Translation. In Proceedings of the 21st Annual Conference of the European Association for Machine Translation, Juan Antonio Pérez-Ortiz, Felipe SánchezMartínez, Miquel Esplà-Gomis, Maja Popović, Celia Rico, André Martins, Joachim Van den Bogaert, and Mikel L. Forcada (Eds.). Alicante, Spain, 269–278. https://aclanthology.org/2018.eamt-main.25',\n",
       " '[15] Surangika Ranathunga, En-Shiun Annie Lee, Marjana Prifti Skenduli, Ravi Shekhar, Mehreen Alam, and Rishemjit Kaur. 2023. Neural Machine Translation for Low-resource Languages: A Survey. ACM Comput. Surv. 55, 11, Article 229 (feb 2023), 37 pages. https://doi.org/10.1145/3567592',\n",
       " '[16] Víctor M. Sánchez-Cartagena, Miquel Esplà-Gomis, Juan Antonio Pérez-Ortiz, and Felipe Sánchez-Martínez. 2021. Rethinking Data Augmentation for Low-Resource Neural Machine Translation: A Multi-Task Learning Approach. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, MarieFrancine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 8502–8516. https://doi.org/10.18653/v1/2021.emnlp-main.669',\n",
       " '[17] Vinícius Veríssimo, Cecília Silva, Vitor Hanael, Caio Moraes, Rostand Costa, Tiago Maritan, Manuella Aschoff, and Thaís Gaudêncio. 2019. A study on the use of sequence-to-sequence neural networks for automatic translation of brazilian portuguese to libras. In Proceedings of the 25th Brazillian Symposium on Multimedia and the Web . 101–108.',\n",
       " '[18] Jing Wang and Lina Yang. 2022. Effective Data Augmentation Methods for CCMT 2022. In Machine Translation, Tong Xiao and Juan Pino (Eds.). Springer Nature Singapore, Singapore, 135–142.',\n",
       " '[19] Xinyi Wang, Hieu Pham, Zihang Dai, and Graham Neubig. 2018. SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii (Eds.). Association for Computational Linguistics, Brussels, Belgium, 856–861. https://doi.org/10.18653/v1/D18-1100   326   -----']"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 220
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:54:18.735951Z",
     "start_time": "2025-05-12T22:54:18.727002Z"
    }
   },
   "cell_type": "code",
   "source": "corpus.loc[4]",
   "id": "908a1e0ac4df7a74",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "titulo               Tagging Enriched Bank Transactions Using LLM-G...\n",
       "informacoes_url                                                       \n",
       "idioma                                                         english\n",
       "storage_key          ../articles/original/english/985-24767-1-10-20...\n",
       "autores              [Daniel de S. Moraes,  Polyana B. da Costa,  P...\n",
       "data_publicacao                                             11-09-2024\n",
       "resumo               avaliar a qualidade das taxonomias e as tags a...\n",
       "keywords             Large Language Models, Natural Language Proces...\n",
       "referencias          [[1] Rohan Anil, Sebastian Borgeaud, Yonghui W...\n",
       "text                 9/02/2008 às 19:20:05 Molho de macarrão de coz...\n",
       "artigo_tokenizado    [9/02/2008, às, 19:20:05, Molho, de, macarrão,...\n",
       "pos_tagger           [[9/02/2008, NUM, NUM], [às, ADP, ADP], [19:20...\n",
       "lema                 [9/02/2008, a o, 19:20:05, Molho, de, macarrão...\n",
       "Name: 4, dtype: object"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 136
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:42:36.271875Z",
     "start_time": "2025-05-12T23:42:34.244756Z"
    }
   },
   "cell_type": "code",
   "source": "text = pymupdf4llm.to_markdown('../articles/original/english/985-24767-1-10-20240923.pdf')",
   "id": "89f5a1755e218b8a",
   "outputs": [],
   "execution_count": 249
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:46:59.688380Z",
     "start_time": "2025-05-12T23:46:59.684537Z"
    }
   },
   "cell_type": "code",
   "source": "text",
   "id": "7c0c9851ccd0df50",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# **Tagging Enriched Bank Transactions Using LLM-Generated Topic** **Taxonomies**\\n\\n## Daniel de S. Moraes\\n#### TeleMídia Lab - PUC-Rio danielmoraes@telemidia.puc-rio.br\\n## Ivan de J. P. Pinto\\n#### TeleMídia Lab - PUC-Rio ivan@telemidia.puc-rio.br\\n## Matheus A. S. Pinto\\n#### BTG Pactual matheus.adler@btgpactual.com\\n## Gabriela Tourinho\\n#### BTG Pactual gabriela.tourinho@btgpactual.com\\n\\n## Polyana B. da Costa\\n#### TeleMídia Lab - PUC-Rio polyana@telemidia.puc-rio.br\\n## Sérgio Colcher\\n#### TeleMídia Lab - PUC-Rio colcher@inf.puc-rio.br\\n## Rafael H. Rocha\\n#### BTG Pactual rafael-h.rocha@btgpactual.com\\n## Marcos Rabaioli\\n#### BTG Pactual marcos.rabaioli@btgpactual.com\\n\\n## Pedro T. C. Santos\\n#### TeleMídia Lab - PUC-Rio thiagocutrim@telemidia.puc-rio.br\\n## Antonio J. G. Busson\\n#### BTG Pactual antonio.busson@btgpactual.com\\n## Rennan Gaio\\n#### BTG Pactual rennan.gaio@btgpactual.com\\n## David Favaro\\n#### BTG Pactual david.favaro@btgpactual.com\\n\\n**Taxonomies**\\n\\n\\n**Transaction: Purchase**\\n\\n**at PAG*distr_nona on**\\n**19/02/2008 at 19:20:05**\\n\\n**Italian Cuisine**\\n\\n**Pasta**\\n\\n**Sauce**\\n\\n**Pizza**\\n\\n**Wine**\\n\\n\\n\\n\\n\\n|name|macrocategory|microcategory|\\n|---|---|---|\\n|Nona Ristorante|Food|Restaurant|\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n**Figure 1: Overview of our method. (Note: Merchant and transaction data are fabricated for demonstration only).**\\n\\n### **ABSTRACT**\\n\\nThis work presents an unsupervised method for tagging banking\\nconsumers’ transactions using automatically constructed and expanded topic taxonomies. Initially, we enrich the bank transactions\\nvia web scraping to collect relevant descriptions, which are then\\npreprocessed using NLP techniques to generate candidate terms.\\nTopic taxonomies are created using instruction-based fine-tuned\\nLLMs (Large Language Models). To expand existing taxonomies\\nwith new terms, we use zero-shot prompting to determine where\\nto add new nodes. The resulting taxonomies are used to assign\\ndescriptive tags that characterize the transactions in the retail bank\\ndataset. For evaluation, 12 volunteers completed a two-part form\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n© 2024 SBC – Brazilian Computing Society.\\nISSN 2966-2753\\n\\n\\nassessing the quality of the taxonomies and the tags assigned to\\nmerchants. The evaluation revealed a coherence rate exceeding 90%\\nfor the chosen taxonomies. Additionally, taxonomy expansion using\\nLLMs demonstrated promising results for parent node prediction,\\nwith F1-scores of 89% and 70% for *Food* and *Shopping* taxonomies,\\nrespectively.\\n### **KEYWORDS**\\n\\nLarge Language Models, Natural Language Processing, Web Scrapping, Topic Modeling\\n### **1 INTRODUCTION**\\n\\nMany recent studies have focused on the application of Machine\\nLearning-based methods for the classification and characterization of financial transactions. For example, Vollset et al . [23] and\\nBusson et al . [4] explored an approach to hierarchically classifying\\n\\n\\n267\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Moraes et al.\\n\\n\\nfinancial transactions, employing a predefined set of categories/subcategories that describe purchase types and transactions. However,\\nthese methods apply a limited, predefined set of static classes, which\\nrestricts the ability to extend classifications based on user experiences when encountering new, previously undefined categories.\\nIn this context, to expand the possible set of classes/tags to label\\na transaction, we developed an unsupervised method based on\\n*topic taxonomies* . Taxonomies are very useful in the structural and\\nsemantic analyses of topics and textual data. However, creating and\\nmaintaining them is often costly and challenging to scale manually.\\nTherefore, recent works have tackled the automatic creation and\\nexpansion of *topic taxonomies*, in which each node in a hierarchy\\nrepresents a conceptual topic composed of semantically coherent\\n\\nterms.\\n\\nWe present an unsupervised method for automatically constructing and expanding topic taxonomies with instruction-based LLMs\\n(Large Language Models), in a Zero-Shot manner. Candidate terms\\nfor the initial version of the taxonomy are obtained using topic\\nmodeling and keyword extraction techniques. Then we apply LLMs\\nto post-process the resulting terms, create a hierarchy, and add new\\nterms to an existing taxonomy. Since the taxonomies are derived\\nfrom a corpus of unstructured texts describing niches of consuming\\nhabits, we opted to investigate the use of LLMs in our approach.\\nLLMs are often pre-trained on a large corpus of text, allowing them\\nto learn contextual representations that capture the intricacies of\\nhuman language.\\nWe applied our method to a private dataset of transactions of\\na retail bank, enriched with scraped data from food and shopping\\ncompanies, and evaluated the resulting taxonomies quantitatively.\\nThe generated tags of our topic taxonomies are then assigned to the\\nbank transactions characterizing the companies in each transaction,\\nas shown on Figure 1. In total, 58 topic taxonomies were created\\nfor the *Food* category and 6 for the *Shopping* category.\\nA two-step quantitative evaluation was conducted on a subset of the taxonomies. For this evaluation, we selected the topic\\ntaxonomies with the highest number of terms in each category:\\n\"Brazilian Cuisine\" from *Food* and \"Clothing and Accessories\" from\\n*Shopping* . Taxonomies with more terms are most likely to result\\nin a deeper hierarchy, which gives more data for evaluation. We\\nasked 12 volunteers to answer a two-part form, which assessed\\nthe quality of the created taxonomies and the quality of the tags\\nassigned to label transactions. The evaluation showed an average\\ncoherence of tags to transactions above 90%.\\nAs more scraped data from food and shopping companies are\\nadded to the retail bank’s dataset, the topic taxonomies will need\\nto be updated to include new terms. We used LLMs for this task as\\nwell, employing commercial LLMs like Gemini Pro [ 1 ] and GPT4 [ 14 ], alongside open-source LLM options such as LLaMA-Alpaca\\n(7B) [ 22 ], Phi-2 [1], and Mixtral 8x7B [ 9 ]. We showcase their results in\\nboth taxonomy creation and expansion. For the expansion part, we\\nalso compared our method to existing ones (a BERT-based method\\nand Musubu[ 20 ]) on the SemEval dataset and our generated taxonomies as well. Gemini Pro achieved the best results, with F1scores of 89% and 70% for parent node prediction on the *Food* and\\n*Shopping* taxonomies, respectively.\\n\\n1 https://huggingface.co/microsoft/phi-2\\n\\n\\nThe remainder of the paper is structured as follows: Section 2,\\nreviews the related work, highlighting existing approaches. Section 3 provides the necessary background, laying the foundation\\nfor our methodologies and contextualizing our contributions. Section 4 details the dataset construction process, explaining how we\\nenriched and prepared the data for the taxonomies’ construction. In\\nSection 5, we describe the creation of the taxonomies, outlining the\\nmethods used to generate them. Section 6 discusses the expansion\\nof the taxonomies, demonstrating how they can be dynamically\\nextended to accommodate new categories. Section 7 focuses on the\\nevaluation of these taxonomies, presenting the metrics and results\\nthat validate their accuracy and also the quality of the tags assigned\\nto the transactions. Finally, Section 8 concludes the paper, summarizing our findings, discussing their implications, and suggesting\\ndirections for future research.\\n### **2 RELATED WORK**\\n\\nTaxonomies represent the structure behind a collection of documents, organizing the hierarchical relationships between terms in a\\ntree structure [ 13 ]. They play an essential part in the structural and\\nsemantic analysis of textual data, providing valuable content for\\nmany applications that involve information retrieval and filtering,\\nsuch as web searching, recommendation systems, classification,\\nand question answering.\\nSince creating and maintaining taxonomies is a costly task, often difficult to scale if done manually, methods that automatically\\nconstruct and update them are desirable. Early works on automatic\\ntaxonomy creation focused on building hypernym-hyponym taxonomies, where each pair of terms expresses an ‘is-a’ relationship\\n\\n[ 19 ]. More recent works have tackled the automatic creation of\\nother taxonomies, such as topic taxonomies. In a topic taxonomy,\\neach node represents a conceptual topic composed of semantically\\ncoherent terms.\\n\\nIn this context, Zhang et al . [28] developed TaxoGen, an unsupervised method for constructing topic taxonomies. Taxogen uses\\nthe SkipGram model from an input text corpus to embed all the\\nconcept terms into a latent space that captures their semantics. In\\nthis space, the authors applied a clustering method to construct a\\nhierarchy recursively based on a variation of the spherical K-means\\nalgorithm.\\nAnother work that focuses on topic taxonomies is TaxoCom\\n\\n[ 10 ], a framework for automatic taxonomy expansion. TaxoCom is\\na hierarchical topic discovery framework that recursively expands\\nan initial taxonomy by discovering new sub-topics. It uses locally\\ndiscriminative embeddings and adaptive clustering, resulting in\\na low-dimensional embedding space that effectively encodes the\\ntextual similarity between terms. One main disadvantage of TaxoCom is that it requires a large set of quality phrases in the target\\nlanguage, and curating these phrases can be costly. The quality of\\nthe output taxonomy is highly dependent on those phrases.\\nRegarding the automatic expansion of taxonomies, an important related example is Musubu [ 20 ], a framework for low-resource\\ntaxonomy enrichment that uses a Language Model (LM) as a knowledge base to infer term relationships. For the taxonomy expansion\\npart of out method, we used Musubu as a baseline for comparison.\\n\\n\\n268\\n\\n\\n-----\\n\\nTagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\nAs to using Large Language Models for taxonomy tasks, Chen\\net al . [6] investigated how LLMs, like GPT-3, perform in taxonomy\\nconstruction tasks. The authors compared two approaches: finetuning, which involves training the LLM on a specific dataset to\\nadapt it for taxonomy tasks, and prompt techniques, where the\\nLLM receives instructions and examples to perform a task without\\nbeing explicitly trained for it. Their findings showed that prompt\\ntechniques such as few-shot learning generally outperform finetuning, particularly with smaller datasets. Based on these findings,\\nwe applied prompting techniques, specifically zero-shot prompting,\\nacross various LLMs to assess their effectiveness in constructing and\\nexpanding taxonomies. Section 7 shows the results of our approach,\\nas well as the results of applying Musubu[20] as baseline.\\n### **3 BACKGROUND**\\n\\nIn this section, we provide a comprehensive background on Large\\nLanguage Models (LLMs), and the concept of Prompt-tuning. These\\nconcepts are essential to understanding the construction and editing\\nof taxonomies utilizing LLMs.\\n### **3.1 Large Language Models**\\n\\nLately, Large Language Models (LLMs) have garnered significant\\nattention for their exceptional performance in various NLP tasks.\\nLLMs, such as GPT-3[ 3 ] and LLAMA[ 22 ], are characterized by their\\nmassive scale, comprising billions of parameters and being trained\\non vast amounts of data. These models are often pre-trained in\\nan unsupervised manner on large corpora of textual data, such as\\nbooks, articles, and web pages, allowing them to learn contextual\\nrepresentations that capture the intricacies of human language.\\nTo use LLMs for specific purposes, a highly effective approach is\\nto fine-tune them on task-specific data. Fine-tuning enables LLMs\\nto adapt to specific domains or tasks with minimal labeled data,\\nsignificantly reducing the need for large annotated datasets. However, in scenarios where labeled data is scarce or difficult to obtain,\\nLLMs can also be used without specific training or additional data,\\nin a Zero-Shot manner [ 21 ]. Given the scale of these models and\\nthe data they are trained on, LLMs embed vast knowledge that enables them to achieve high generalization capabilities and perform\\ntasks in diverse contexts, even without specific training for those\\ntasks[16].\\nIn our experiments, we tested several types of language models,\\nfrom private LLMs (GPT 4 [ 14 ], Gemini Pro [2] ), to open-source LLMs\\n(Llama 2 [ 22 ]), to a Mixture of Experts LLM (Mixtral [ 9 ], and a Small\\nLanguage Model (SLM), Phi 2 [3] .\\n### **3.2 Prompt Engineering**\\n\\nPrompt Engineering is a fundamental technique used to enhance\\nthe performance and adaptability of Large Language Models (LLMs)\\nin specific tasks or domains [ 7 ]. It involves optimizing and crafting\\nprompts to efficiently use language models (LMs) [ 3 ]. This approach\\nallows researchers and practitioners to tailor the behavior and output of LLMs, making them more suitable for targeted applications.\\n\\n2 https://deepmind.google/technologies/gemini/pro/\\n3 https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-ofsmall-language-models/\\n\\n\\nTechniques such as Zero-shot prompting, Few-shot prompting,\\nChain of Thought, ReAct, Self-Consistency etc. have been explored\\nto guide LLMs toward desired responses [ 18, 21, 25 – 27 ]. The effectiveness of prompt tuning has been demonstrated in various\\napplications, including question-answering, summarization, and\\ndialogue generation. The choice of prompt greatly influences the\\ngenerated output, and by carefully crafting prompts, researchers\\ncan guide the model’s responses toward desired behaviors. For example, in language translation, a prompt can specify the source\\nlanguage and desired target language to ensure accurate and fluent translations. In our method, we used the Zero-Shot prompting\\ntechnique.\\n### **3.3 Zero-Shot Prompting**\\n\\nSince LLMs (Large Language Models) are trained on vast amounts\\nof data, they can follow instructions and perform tasks in contexts\\nwhere they were not specifically trained, in a Zero-Shot (ZS) manner.\\nThis prompting style allows the model to adapt, making it versatile.\\nA Zero-Shot (ZS) prompt directly instructs the model to perform\\na task without additional examples or demonstrations to guide\\nthe LLM’s response, which is why they are also known as task\\ninstructions [21].\\nIn a study by Li [11], the authors highlighted several advantages\\nof using ZS prompts, such as the ability to craft highly interpretable\\nprompts, requiring fewer training data or examples, a more straightforward prompt design process, and a flexible prompt structure.\\nAdditionally, Reynolds and McDonell [17] noted that carefully engineered zero-shot prompts can outperform few-shot prompts in\\ncertain scenarios, as examples can sometimes be interpreted as part\\nof a narrative rather than as a guiding mechanism. This finding also\\ninfluenced our decision to use zero-shot prompting in our method.\\n### **4 DATASET CONSTRUCTION**\\n\\nThis work uses a proprietary dataset consisting of consumer transactions from a retail bank. Each transaction includes only a merchant\\nname indicating the business where that purchase occurred along\\nwith macro and micro categories as illustrated in Figure 1 The macro\\nand micro are originally assigned by [ 4 ] using the information from\\nthe business activities and products.\\nWe focus on two macro-categories from this dataset: *Food* and\\n*Shopping*, selecting the top 50,000 businesses with the highest number of transactions for each category.\\nWith the limited initial information, assigning detailed tags to\\ntransactions is challenging. To address this, we augment the dataset\\nthrough a data enrichment process involving web scraping. Using\\ntools such as Selenium [4] and Beautiful Soup [5], we gathered activity\\ndescriptions for companies in each macro category. For the *Food*\\nmacro category, the search was conducted on specialized platforms\\nfor restaurants and food delivery services. For the *Shopping* macro\\ncategory, we obtained establishment descriptions directly from\\ninternet indexing and search tools.\\nIn the context of enrichment for the *Food* macro category, web\\nscraping was conducted as follows: (1) the centers of all Brazilian\\nstate capitals and the Federal District were used as base locations\\n\\n4 https://www.selenium.dev/about/\\n5 https://readthedocs.org/projects/beautiful-soup-4/downloads/pdf/latest/\\n\\n\\n269\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Moraes et al.\\n\\n\\nfor restaurant searching; (2) for each location, restaurants listed on\\nthe first one hundred pages of the platform were extracted. After\\ncompleting these steps, the information was combined with the\\nmerchant database using the merchant’s name and micro categories.\\nFor the *Shopping* macro category, the description of each merchant was obtained using Google Search Engine [6], selecting descriptions from the first ten search results. The final description\\nconsists of a concatenation of all the obtained descriptions. The\\nsearch queries were constructed using the merchant names combined with their micro categories.\\n### **5 TAXONOMY CONSTRUCTION**\\n\\nTo automatically create topic taxonomies for *Food* and *Shopping*\\nbusinesses, we developed a 3-step method. First, we preprocess the\\ndescriptions in our enriched dataset to retain only the relevant parts\\nof the text. Next, we apply two techniques to select candidate terms\\nfor the topic taxonomies: keyword extraction and topic modeling.\\nIn the post-processing phase, we use large language models\\n(LLMs) to refine the results of each step, filtering out unrelated\\nterms. Finally, we use LLMs again to organize the final terms into\\nhierarchies, forming the topic taxonomies.\\n### **5.1 Preprocessing**\\n\\nWe applied a few NLP techniques to refine the businesses’ descriptions in our dataset. At first, we remove stop words to eliminate\\ncommonly used words that do not carry significant meaning in\\nour contexts. Then, to retain only the most relevant portions of the\\ndescriptions, we employ part-of-speech (POS) tagging to identify\\nand exclude words that belong to specific POS categories. The list\\nof POS tag categories that were removed includes ADV, CCONJ,\\nADP, AUX, CONJ, DET, INTJ, PART, PRON, PUNCT, SYM, SCONJ,\\nADJ, VERB, PROPN. [7]\\n\\nAfter this initial preprocessing step, we run the first iteration\\nof the candidate term selection part to build a filter of generic\\nwords, not to create topic taxonomies yet. For this step, we use the\\nentire corpus of descriptions for each macro category, resulting in\\ntwo corpora ( *Food* and *Shopping* ). For each micro category in the\\nmacro categories’ corpora, we use Keyword Extraction and Topic\\nModeling to gather candidate terms for the filter, combining the\\nresults of both techniques in a list. Then, We use an LLM to remove\\nthe terms it identifies as unrelated to the main topic (each micro\\ncategory) from the list. The prompt that we used for requesting this\\nseparation is illustrated below.\\n\\nprompt= \"Given the terms in the following list: \"+\\n<wordsList> +\". Separate them into two groups. In\\ngroup 1 the terms with no relation to the topic \"+\\n<type> +\". And in group 2 the terms that are related.\"\\n\\n**Listing 1: Prompt for separating candidate terms related to**\\n**the type of establishment**\\n\\nBy using this prompt, we try to ensure that the model’s response\\nis consistently formatted according to the pattern described in it,\\nfacilitating the processing of the resulting string, although, some\\n\\n6 https://www.google.com\\n7 https://spacy.io/usage/linguistic-features#pos-tagging\\n\\n\\nof the LLMs we tested did not output the response in the requested\\nformat. Once we complete one iteration of this method for each\\nmacro category in our dataset, we add the words of group 2 to the\\ncorresponding list of generic words. We apply the corresponding\\nfilter of generic words for each macro category corpus, resulting in\\nthe final preprocessed corpus.\\n### **5.2 Candidate Terms Selection**\\n\\nFor this part of our method, we use each preprocessed corpus\\nseparately. For the *Food* corpus, we group the descriptions based\\non their micro-categories, creating 58 sub-corpus specific to that\\ndomain. We have six micro categories for the *Shopping* corpus,\\nresulting in 6 specific sub-corpus. The candidate terms selection\\nmethods are applied to each sub-corpus, creating topic taxonomies\\nwhere the main topic is the micro category.\\n\\n*5.2.1* *Keyword Extraction.* The first approach to candidate term\\nselection was to use an unsupervised keyword selection method\\ncalled Yake! [ 5 ]. This method is based on statistical text features\\nextracted from single documents to select the most relevant keywords from that text. It does not require training on a document set\\nand is not dependent on dictionaries, text size, language, domain,\\nor external corpora.\\nYake! allows for the specification of parameters such as the\\nlanguage of the text, the maximum size of the n-grams being sought,\\nand others. In our method, we customized only the language to\\nPortuguese, and the maximum number of keywords sought for each\\nset of descriptions was 30 words.\\nAfter extracting the keywords from each group of descriptions,\\nwe obtained a total set of *𝑁* candidate terms. However, these terms\\nare further filtered using an LLM, where we ask it to separate the\\nterms related to the main topic from those unrelated, as explained\\nearlier in subsection 5.1.\\n\\n*5.2.2* *Topic Modeling.* Our second approach to collecting initial\\ntopics and candidate terms was Topic Modeling. We applied the\\nLatent Dirichlet Allocation algorithm [ 2 ], available at the Gensim\\nLibrary [8] .\\nWe construct a dictionary for each macro-category corpus in\\nour macro-categories corpora by extracting unique tokens and\\nbigrams. After a few empirical tests, we set the minimum frequency\\nof a bigram to 20 occurrences. Since some corpora have a minimal\\nnumber of tokens (the micro category \"Greek Cuisine\" from the *Food*\\nmacro category has only five stores marked as such, with a corpus\\nof only 127 tokens), we had to set a reasonably small number so that\\nsmaller corpora could also have a few bigrams. With the resulting\\ndictionary of tokens, the LDA algorithm was applied. Three main\\nparameters are to be defined in an LDA algorithm: number of topics,\\n*alpha*, and *beta* .\\nThe number of topics defines the latent topics to be extracted\\nfrom the corpus. The parameter *alpha* is *a priori* belief in documenttopic distribution, while *beta* is *a priori* belief in topic-word distribution.\\nTo define the number of topics for each micro category corpus,\\nwe tried numbers from 1 to 5, constantly checking which configuration would result in the best average topic coherence for that\\n\\n8 https://pypi.org/project/gensim/\\n\\n\\n270\\n\\n\\n-----\\n\\nTagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\ncorpus. Small corpora would have 1 or 2 topics, while bigger ones\\nwould have 5. To correctly define the *alpha* and *beta* priors, we\\nwould have to analyze the distribution for each category corpus\\n\\n[ 24 ]. Since this would be rather difficult, we set those priors to be\\nauto-defined by the LDA algorithm, which learns these parameters\\nbased on the corpus. We select the terms with the highest coherence\\nwith the resulting topics. Each topic returns 20 words with their\\ncoherence scores, but we do not use all of them as some have very\\nlow coherence. After testing a few configurations, for each topic,\\nwe select 60% of the terms with the highest coherence within that\\ntopic.\\nWith initial terms for each topic taxonomy, we ask an LLM\\nto separate the ones closely related to the main topic from those\\nunrelated, as mentioned earlier.\\n### **5.3 Hierarchy Construction**\\n\\nOnce we have the post-processed lists of candidate terms obtained\\nby each technique mentioned in subsection 5.2, we merge them and\\nremove repetitions. After the merge, for each macro category, we\\nhave lists of terms for each micro category, representing each topic\\ntaxonomy. However, they do not have any hierarchy level between\\nthe terms configuring the taxonomy.\\nTo tackle this problem, we use an LLM again, this time with a\\nprompt that searches for sub-categories within the terms of a topic\\nto create these hierarchies. The prompt is illustrated below:\\n\\nprompt=\"Create a dictionary by hierarchically arranging the\\n\\nfollowing words:\" + <wordsList> +.\" Use JSON format as\\nthe output such as the following: {\\\\\"key\\\\\": [\\\\\" list\\nof words\\\\\"]}\"\\n\\n**Listing 2: Prompt for creating a hierarchy for each list of**\\n**tags.**\\n\\nWith this prompt, we seek to ensure that the LLM response has a\\nconsistent pattern and facilitates handling the returned string. After\\nthis step, we have a hierarchy of terms in each topic taxonomy in\\nthe *Food* and *Shopping* macro categories.\\n### **5.4 Merchant Tagging**\\n\\nWith the topic taxonomies for both *Food* and *Shopping* macrocategories, we can now assign tags to merchants/establishments.\\nTo do so, we use the descriptions attached to these establishments,\\nand we see which terms from a taxonomy are mentioned in their\\ndescriptions with a reverse index algorithm. We employ the taxonomy whose topic is the same as the establishment’s micro category,\\nas shown in Figure 2.\\n### **6 TAXONOMY EXPANSION**\\n\\nAnother essential part of our method is the automatic expansion of\\nexisting taxonomies as new terms arrive, derived from additional\\nmerchant scrapped data, as shown in Section 4. In this section, we\\npresent our approach to taxonomy expansion by using instructionbased LLMs.\\n\\nAs new transactions may include new businesses, new terms\\ncan emerge from the descriptions obtained through the scraping\\nprocess. Therefore, we need to update the taxonomies with these\\n\\n\\nnew terms maintaining and enriching the created hierarchies with\\nthe potential new terms.\\nAfter completing the transaction enrichment process, including\\nthe search for business descriptions and the selection of candidate\\nterms, if relevant terms not included in the current hierarchies are\\ndetected, we initiate the expansion process.\\n### **6.1 Prompt engineering instruction for** **taxonomy representation**\\n\\nFirst, we represent our topic taxonomies in a format that can be\\ninterpreted by an LLM. We employed a generic prompt, illustrated\\nbelow, across all tested methods to convert topics into root nodes\\nand their terms into child nodes.\\n\\nChilds of [ROOT]: [CHILD1,CHILD2,CHILD3]\\nChilds of [CHILD1]: [CHILD4,CHILD5]\\n\\nChilds of [CHILD2]: [CHILD6]\\n\\n...\\n\\n**Listing 3: Prompt for representation of taxonomy**\\n### **6.2 Predicting the parent of a node**\\n\\nTo experiment with taxonomies expansion, we used two datasets:\\nour *Food* and *Shopping* topic taxonomies and the taxonomies from\\nSemEval-2015 Task 17 [ 15 ]. Those are low-resource taxonomies,\\nwith thousands of nodes or less, which are appropriate for the current prompt size of LLMs. We used the SemEval dataset to compare\\nthe results with well-established methods for taxonomy expansion,\\nsuch as Musubu [ 20 ]. Similar to their experiments, we hid 20% of\\nthe terms (chosen randomically) in the taxonomies to predict their\\nrespective parent nodes. To verify the parent/root of a new term,\\nwe used the following prompt:\\n\\n**Listin** **g** **4: Prom** **p** **t for searchin** **g** **for a node’s** **p** **arent**\\n\\nprompt=\"Who is the father of \"+<new_term>+\"?\"\\n\\nIn Table 1, we see the F1-Scores for parent node prediction. Equation 1 showcases how to calculate the F1-Score. TP is the number\\n\\nof true positives, nodes that were correctly assigned as parents of\\nchild nodes. FP is the number of false positives, nodes that were\\nincorrectly assigned as a parent to a child node. FN is the number\\nof false negatives, nodes that should have been assigned as parent\\nnodes but were not.\\n\\n2 ∗ *𝑇𝑃*\\n*𝐹* 1 = (1)\\n2 ∗ *𝑇𝑃* + *𝐹𝑃* + *𝐹𝑁*\\n\\nFor baseline models, we used Bert and Musubu; for commercial\\nLLMs, Gemini Pro and GPT-4; and for open-source LLMs, LLamaAlpaca(7B), Phi-2, and Mixtral 8x7B. We evaluate them in 4 taxonomies from the SemEval dataset and our taxonomies. For each\\ntaxonomy, the LLMs perform significantly better than Musubu,\\nwith GPT-4 and Gemini Pro having the highest F1-Scores, with the\\nlatter beating the former by a few points. However, the most recent\\nopen-source options (Phi-2 and Mixtral 8x7B) are getting close in\\nperformance.\\n\\n\\n271\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Moraes et al.\\n\\n\\n\\n\\n**Tags assigned to establishments from the**\\n\\n***\"Clothing & Accessories\"*** **micro category**\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n**Figure 2: Assigning tags to establishments based on a topic**\\n**taxonomy.**\\n\\nIt is important to note that while SemEval taxonomies have\\nthousands of nodes, ours have only a few hundred, which we can\\nassume is a significant reason for the degrading performance of\\nMusubu and Bert (LMs or LM-based methods). In contrast, the LLMs\\nhave a robust performance in such low-resource settings. This also\\nshows that LLMs have a remarkable understanding of questions\\nand zero-shot performance, generalizing well even for datasets in\\ndifferent languages.\\n### **7 TAXONOMY EVALUATION**\\n\\nTo properly evaluate the topic taxonomies that we created in this\\nwork, we developed a two-step qualitative evaluation of a limited\\npart of the results.\\nIn total, 58 topic taxonomies were created for the *Food* set and\\n6 for the *Shopping* set. For our evaluation, we selected the topic\\ntaxonomies with the highest number of terms in each part (the\\n\"Brazilian Cuisine\" taxonomy for the *Food* part and the \"Clothing\\nand Accessories\" taxonomy for the *Shopping* one). First, we assess\\nthe quality of removing generic terms from each taxonomy, and\\nthen, we evaluate the tags assigned to establishments based on that\\ntaxonomy. We asked 12 volunteers to answer a two-part form.\\n*Part 1 - Accuracy of the terms that were selected as related to the*\\n*topic* : In this part, we evaluate if the LLMs could correctly group\\nthe relevant and non-relevant terms, removing the generic terms.\\nTo do so, we defined a ground truth with the relevant terms as\\n\\n\\ntrue positives and the non-relevant terms as true negatives. Table 3\\nshows the results.\\n\\nGPT-4 was the best model, followed by Gemini Pro, both scoring\\nover 60% accuracy for the Brazilian Cuisine taxonomy and over\\n86% accuracy for the Clothing and Accessories taxonomy. Smaller\\nlanguage models such as Phi 2 and Llama 2 7B performed poorly\\nboth in removing generic terms and in formatting the response\\naccordingly, with Phi 2 being particularly verbose.\\n*Part 2 - Human Evaluation of the Quality of the Tagging Process* :\\nIn this part, the volunteers were asked to examine if the tags assigned to an establishment were appropriate and coherent to that\\nestablishment’s description. We selected the top 5 establishments\\nwith the highest transactions for each micro category. We asked our\\nevaluators to analyze the tags assigned to describe that establishment and choose the ones that were not appropriate. This way, we\\nhave a coherence ratio for each establishment based on the number\\n\\nof proper tags divided by the total number of tags. We average the\\nresults of our 12 evaluators and present them in Table 2. Figure 2\\nshows the \"Clothing & Accessories\" taxonomy that was evaluated\\nand 2 of the merchants and the tags assigned to them that were\\nincluded in the evaluation.\\n### **8 CONCLUSION**\\n\\nIn this work, we presented an unsupervised method for automatically creating and expanding topic taxonomies using LLMs. We\\nevaluated some of the generated taxonomies and applied them\\nin transaction tagging in a retailer’s bank dataset. The evaluation\\nshowed promising results, with average coherence scores above 90%\\nfor the two selected taxonomies. The taxonomies’ expansion with\\nGemini Pro also showed exciting results for parent node prediction,\\nwith F1-scores of 89% and 70% for *Food* and *Shopping* taxonomies,\\nrespectively.\\nFor future work on taxonomy construction, we plan to test\\nmore robust term selection methods, such as embedding-based\\napproaches. We also plan on conducting ablation studies to validate whether the keyword extraction and topic modeling parts\\nhelp improve the quality of the taxonomies created, by using a\\nbaseline prompt to ask the LLM to generate child nodes given a\\nparent node. In terms of taxonomy expansion, there are several\\ntasks to explore, ranging from node-level operations to generating\\nentire sub-trees and identifying similar structures. Additionally, we\\nintend to enhance our instruction-tuned LLM for taxonomy tasks\\n\\n\\n|Method|SemEval-2015 Task 17 Chemical Equipment Food Science|Our taxonomies Food Shopping|\\n|---|---|---|\\n|Gemini Pro|0.68 0.80 0.91 0.72|0.89 0.73|\\n|GPT-4|0.65 0.78 0.89 0.70|0.87 0.71|\\n|Mixtral-8x7B|0.59 0.63 0.80 0.57|0.74 0.60|\\n|Phi-2|0.56 0.52 0.68 0.56|0.64 0.54|\\n|LLama 7B|0.51 0.42 0.58 0.46|0.60 0.49|\\n|Musubu|0.35 0.46 0.37 0.42|0.21 0.13|\\n|Bert-Base|0.13 0.14 0.12 0.16|0.11 0.06|\\n\\n\\n**Table 1: F1-score for parent node prediction.**\\n\\n272\\n\\n\\n-----\\n\\nTagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies WebMedia’2024, Juiz de Fora, Brazil\\n\\n|Col1|Brazilian Cuisine Taxonomy Average Coherence Number of Tags|Col3|Clothing & Accessories Taxonomy Average Coherence Number of Tags|Col5|\\n|---|---|---|---|---|\\n|Merchant 1|92.30%|10|97.11%|8|\\n|Merchant 2|94.23%|8|83.07%|5|\\n|Merchant 3|89.23%|5|94.38%|5|\\n|Merchant 4|87.17%|6|93.84%|5|\\n|Merchant 5|93.40%|7|97.43%|6|\\n\\n\\n\\n**Table 2: Results of evaluating the tags assigned to each merchant/establishment.**\\n\\n\\n|Col1|Brazilian Cuisine|Clothing & Accessories|\\n|---|---|---|\\n|Llama 2 7B|29.54%|52.78%|\\n|Phi 2|40.90|73.68%|\\n|Mixtral 8x7B v0.1|46.93%|70.27%|\\n|Gemini Pro|61.36%|86.11%|\\n|GPT 4|68.08%|86.84%|\\n\\n\\n**Table 3: Accuracy of using each LLM to remove generic words**\\n**from each topic taxonomy.**\\n\\nby fine-tuning or employing more efficient methods such as LoRA\\n\\n[8].\\n### **LIMITATIONS**\\n\\nTo address the limitations of our work, we begin with the taxonomy construction component. In this phase, we relied on topic\\nmodeling and keywords extraction to select candidate terms for our\\ntaxonomies. The LDA algorithm used for topic modeling performs\\nsuboptimally when the base corpus is small. Some of our topics had\\ncorpora with vocabularies of fewer than 100 words, which can result\\nin topics containing irrelevant or incoherent terms. Additionally,\\nwe could have further experimented with the LDA hyperparameters\\nfor each micro-category corpus.\\nRegarding the evaluation of the generated taxonomies, we did not\\nassess topic completeness. Without a ground truth, it is challenging\\nto quantify how comprehensively the terms in a taxonomy cover the\\nmain topic. Furthermore, we evaluated only 2 of the 64 taxonomies\\ngenerated by our method, leaving a substantial portion unexamined.\\nIn the taxonomy expansion experiment, we evaluated only a lowresource setting with fewer than a thousand nodes. Most studies\\nfocus on taxonomies with hundreds of thousands or more nodes.\\n\\nThis presents a challenge for LLMs due to their limited context.\\nAddressing this contextual limitation could benefit from insights\\nfound in other works that tackle similar issues [12].\\n### **ETHICS STATEMENT**\\n\\nIn this work, we ensure the utmost protection of customers and\\nstore sensitive data by exclusively using non-sensitive information\\nin our dataset. Our prompts solely rely on selected words from store\\ndescriptions, thus avoiding any direct usage of personal or sensitive\\ninformation. No customer-specific data or store-sensitive details\\nare integrated into the system, upholding privacy and security as\\ntop priorities.\\n\\n\\nMoreover, we strictly adhere to ethical guidelines during our\\nexperiments involving volunteers, and no personal data is collected\\nfrom them. Our focus lies solely on analyzing the results of our\\nproposed approach. Participants’ anonymity and confidentiality are\\nmaintained throughout the research process, ensuring a responsible\\nand trustworthy approach to data handling.\\n### **ACKNOWLEDGMENTS**\\n\\nThe authors would like to acknowledge BTG Pactual for the partnership and financial support to this research.\\n### **REFERENCES**\\n\\n[1] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,\\nRadu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al . 2023. Gemini:\\na family of highly capable multimodal models.\\n\\n[2] David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation.\\n*Journal of machine Learning research* 3, Jan (2003), 993–1022.\\n\\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al . 2020. Language models are few-shot learners. *Advances in neural*\\n*information processing systems* 33 (2020), 1877–1901.\\n\\n[4] Antonio J. G. Busson, Rafael Rocha, Rennan Gaio, Rafael Miceli, Ivan Pereira,\\nDaniel de S. Moraes, Sérgio Colcher, Alvaro Veiga, Bruno Rizzi, Francisco Evangelista, Leandro Santos, Fellipe Marques, Marcos Rabaioli, Diego Feldberg, Debora\\nMattos, João Pasqua, and Diogo Dias. 2023. Hierarchical Classification of Financial Transactions Through Context-Fusion of Transformer-based Embeddings\\nand Taxonomy-aware Attention Layer. In *Anais do II Brazilian Workshop on Arti-*\\n*ficial Intelligence in Finance (BWAIF 2023) (BWAIF 2023)* . Sociedade Brasileira de\\nComputação. https://doi.org/10.5753/bwaif.2023.229322\\n\\n[5] Ricardo Campos, Vítor Mangaravite, Arian Pasquali, Alípio Jorge, Célia Nunes,\\nand Adam Jatowt. 2020. YAKE! Keyword extraction from single documents using\\nmultiple local features. *Information Sciences* 509 (2020), 257–289.\\n\\n[6] Boqi Chen, Fandi Yi, and Dániel Varró. 2023. Prompting or Fine-tuning? A\\nComparative Study of Large Language Models for Taxonomy Construction. In\\n*2023 ACM/IEEE International Conference on Model Driven Engineering Languages*\\n*and Systems Companion (MODELS-C)* . IEEE, 588–596.\\n\\n[7] Sabit Ekin. 2023. Prompt engineering for ChatGPT: a quick guide to techniques,\\ntips, and best practices. *Authorea Preprints* (2023).\\n\\n[8] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\\nWang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large\\nlanguage models. *arXiv preprint arXiv:2106.09685* (2021).\\n\\n[9] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche\\nSavary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou\\nHanna, Florian Bressand, et al . 2024. Mixtral of experts. *arXiv preprint*\\n*arXiv:2401.04088* (2024).\\n\\n[10] Dongha Lee, Jiaming Shen, SeongKu Kang, Susik Yoon, Jiawei Han, and Hwanjo\\nYu. 2022. TaxoCom: Topic Taxonomy Completion with Hierarchical Discovery of\\nNovel Topic Clusters. In *Proceedings of the ACM Web Conference 2022* . 2819–2829.\\n\\n[11] Yinheng Li. 2023. A Practical Survey on Zero-Shot Prompt Design for In-Context\\nLearning. In *Proceedings of the 14th International Conference on Recent Advances*\\n*in Natural Language Processing* . 641–647.\\n\\n[12] Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun\\nMa, and Zhoujun Li. 2023. Unleashing Infinite-Length Input Capacity for Largescale Language Models with Self-Controlled Memory System. *arXiv preprint*\\n*arXiv:2304.13343* (2023).\\n\\n[13] Irina Nikishina, Varvara Logacheva, Alexander Panchenko, and Natalia\\nLoukachevitch. 2020. RUSSE’2020: Findings of the First Taxonomy Enrichment\\n\\n\\n273\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Moraes et al.\\n\\n\\nTask for the Russian language. *arXiv preprint arXiv:2005.11176* (2020).\\n\\n[14] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\\n\\n[15] Octavian Popescu and Carlo Strapparava. 2015. Semeval 2015, task 7: Diachronic\\ntext evaluation. In *Proceedings of the 9th International Workshop on Semantic*\\n*Evaluation (SemEval 2015)* . 870–878.\\n\\n[16] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of\\ntransfer learning with a unified text-to-text transformer. *The Journal of Machine*\\n*Learning Research* 21, 1 (2020), 5485–5551.\\n\\n[17] Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language\\nmodels: Beyond the few-shot paradigm. In *Extended abstracts of the 2021 CHI*\\n*conference on human factors in computing systems* . 1–7.\\n\\n[18] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal,\\nand Aman Chadha. 2024. A systematic survey of prompt engineering in large\\nlanguage models: Techniques and applications. *arXiv preprint arXiv:2402.07927*\\n(2024).\\n\\n[19] Rion Snow, Daniel Jurafsky, and Andrew Ng. 2004. Learning syntactic patterns\\nfor automatic hypernym discovery. *Advances in neural information processing*\\n*systems* 17 (2004).\\n\\n[20] Kunihiro Takeoka, Kosuke Akimoto, and Masafumi Oyamada. 2021. Low-resource\\ntaxonomy enrichment with pretrained language models. In *Proceedings of the*\\n*2021 Conference on Empirical Methods in Natural Language Processing* . 2747–2758.\\n\\n[21] Adrian Tam. [n. d.]. What Are Zero-Shot Prompting and Few-Shot Prompting. https://machinelearningmastery.com/what-are-zero-shot-prompting-andfew-shot-prompting/. Accessed: 2024-07-02.\\n\\n\\n\\n[22] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, et al . 2023. Llama: Open and efficient foundation language models. *arXiv*\\n*preprint arXiv:2302.13971* (2023).\\n\\n[23] Erlend Vollset, Eirik Folkestad, Marius Rise Gallala, and Jon Atle Gulla. 2017.\\nMaking use of external company data to improve the classification of bank transactions. In *Advanced Data Mining and Applications: 13th International Conference,*\\n*ADMA 2017, Singapore, November 5–6, 2017, Proceedings 13* . Springer, 767–780.\\n\\n[24] Hanna Wallach, David Mimno, and Andrew McCallum. 2009. Rethinking LDA:\\nWhy priors matter. *Advances in neural information processing systems* 22 (2009).\\n\\n[25] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang,\\nAakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain\\nof thought reasoning in language models. *arXiv preprint arXiv:2203.11171* (2022).\\n\\n[26] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei\\nXia, H Chi, Quoc V Le, and Denny Zhou. [n. d.]. Chain-of-Thought Prompting\\nElicits Reasoning in Large Language Models. ([n. d.]).\\n\\n[27] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,\\nand Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting in Language\\nModels. In *International Conference on Learning Representations (ICLR)* .\\n\\n[28] Chao Zhang, Fangbo Tao, Xiusi Chen, Jiaming Shen, Meng Jiang, Brian Sadler,\\nMichelle Vanni, and Jiawei Han. 2018. TaxoGen: Unsupervised Topic Taxonomy\\nConstruction by Adaptive Term Embedding and Clustering. In *Proceedings of the*\\n*24th ACM SIGKDD International Conference on Knowledge Discovery & Data Min-*\\n*ing* (London, United Kingdom) *(KDD ’18)* . Association for Computing Machinery,\\nNew York, NY, USA, 2701–2709. https://doi.org/10.1145/3219819.3220064\\n\\n\\n274\\n\\n\\n-----\\n\\n'"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 253
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:47:48.393112Z",
     "start_time": "2025-05-12T23:47:48.389482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tmp = text.split('\\n### **1')[1].split('**REFERENCES')[0]\n",
    "tmp"
   ],
   "id": "cc8c27c94a4d21b9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' INTRODUCTION**\\n\\nMany recent studies have focused on the application of Machine\\nLearning-based methods for the classification and characterization of financial transactions. For example, Vollset et al . [23] and\\nBusson et al . [4] explored an approach to hierarchically classifying\\n\\n\\n267\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Moraes et al.\\n\\n\\nfinancial transactions, employing a predefined set of categories/subcategories that describe purchase types and transactions. However,\\nthese methods apply a limited, predefined set of static classes, which\\nrestricts the ability to extend classifications based on user experiences when encountering new, previously undefined categories.\\nIn this context, to expand the possible set of classes/tags to label\\na transaction, we developed an unsupervised method based on\\n*topic taxonomies* . Taxonomies are very useful in the structural and\\nsemantic analyses of topics and textual data. However, creating and\\nmaintaining them is often costly and challenging to scale manually.\\nTherefore, recent works have tackled the automatic creation and\\nexpansion of *topic taxonomies*, in which each node in a hierarchy\\nrepresents a conceptual topic composed of semantically coherent\\n\\nterms.\\n\\nWe present an unsupervised method for automatically constructing and expanding topic taxonomies with instruction-based LLMs\\n(Large Language Models), in a Zero-Shot manner. Candidate terms\\nfor the initial version of the taxonomy are obtained using topic\\nmodeling and keyword extraction techniques. Then we apply LLMs\\nto post-process the resulting terms, create a hierarchy, and add new\\nterms to an existing taxonomy. Since the taxonomies are derived\\nfrom a corpus of unstructured texts describing niches of consuming\\nhabits, we opted to investigate the use of LLMs in our approach.\\nLLMs are often pre-trained on a large corpus of text, allowing them\\nto learn contextual representations that capture the intricacies of\\nhuman language.\\nWe applied our method to a private dataset of transactions of\\na retail bank, enriched with scraped data from food and shopping\\ncompanies, and evaluated the resulting taxonomies quantitatively.\\nThe generated tags of our topic taxonomies are then assigned to the\\nbank transactions characterizing the companies in each transaction,\\nas shown on Figure 1. In total, 58 topic taxonomies were created\\nfor the *Food* category and 6 for the *Shopping* category.\\nA two-step quantitative evaluation was conducted on a subset of the taxonomies. For this evaluation, we selected the topic\\ntaxonomies with the highest number of terms in each category:\\n\"Brazilian Cuisine\" from *Food* and \"Clothing and Accessories\" from\\n*Shopping* . Taxonomies with more terms are most likely to result\\nin a deeper hierarchy, which gives more data for evaluation. We\\nasked 12 volunteers to answer a two-part form, which assessed\\nthe quality of the created taxonomies and the quality of the tags\\nassigned to label transactions. The evaluation showed an average\\ncoherence of tags to transactions above 90%.\\nAs more scraped data from food and shopping companies are\\nadded to the retail bank’s dataset, the topic taxonomies will need\\nto be updated to include new terms. We used LLMs for this task as\\nwell, employing commercial LLMs like Gemini Pro [ 1 ] and GPT4 [ 14 ], alongside open-source LLM options such as LLaMA-Alpaca\\n(7B) [ 22 ], Phi-2 [1], and Mixtral 8x7B [ 9 ]. We showcase their results in\\nboth taxonomy creation and expansion. For the expansion part, we\\nalso compared our method to existing ones (a BERT-based method\\nand Musubu[ 20 ]) on the SemEval dataset and our generated taxonomies as well. Gemini Pro achieved the best results, with F1scores of 89% and 70% for parent node prediction on the *Food* and\\n*Shopping* taxonomies, respectively.\\n\\n1 https://huggingface.co/microsoft/phi-2\\n\\n\\nThe remainder of the paper is structured as follows: Section 2,\\nreviews the related work, highlighting existing approaches. Section 3 provides the necessary background, laying the foundation\\nfor our methodologies and contextualizing our contributions. Section 4 details the dataset construction process, explaining how we\\nenriched and prepared the data for the taxonomies’ construction. In\\nSection 5, we describe the creation of the taxonomies, outlining the\\nmethods used to generate them. Section 6 discusses the expansion\\nof the taxonomies, demonstrating how they can be dynamically\\nextended to accommodate new categories. Section 7 focuses on the\\nevaluation of these taxonomies, presenting the metrics and results\\nthat validate their accuracy and also the quality of the tags assigned\\nto the transactions. Finally, Section 8 concludes the paper, summarizing our findings, discussing their implications, and suggesting\\ndirections for future research.\\n### **2 RELATED WORK**\\n\\nTaxonomies represent the structure behind a collection of documents, organizing the hierarchical relationships between terms in a\\ntree structure [ 13 ]. They play an essential part in the structural and\\nsemantic analysis of textual data, providing valuable content for\\nmany applications that involve information retrieval and filtering,\\nsuch as web searching, recommendation systems, classification,\\nand question answering.\\nSince creating and maintaining taxonomies is a costly task, often difficult to scale if done manually, methods that automatically\\nconstruct and update them are desirable. Early works on automatic\\ntaxonomy creation focused on building hypernym-hyponym taxonomies, where each pair of terms expresses an ‘is-a’ relationship\\n\\n[ 19 ]. More recent works have tackled the automatic creation of\\nother taxonomies, such as topic taxonomies. In a topic taxonomy,\\neach node represents a conceptual topic composed of semantically\\ncoherent terms.\\n\\nIn this context, Zhang et al . [28] developed TaxoGen, an unsupervised method for constructing topic taxonomies. Taxogen uses\\nthe SkipGram model from an input text corpus to embed all the\\nconcept terms into a latent space that captures their semantics. In\\nthis space, the authors applied a clustering method to construct a\\nhierarchy recursively based on a variation of the spherical K-means\\nalgorithm.\\nAnother work that focuses on topic taxonomies is TaxoCom\\n\\n[ 10 ], a framework for automatic taxonomy expansion. TaxoCom is\\na hierarchical topic discovery framework that recursively expands\\nan initial taxonomy by discovering new sub-topics. It uses locally\\ndiscriminative embeddings and adaptive clustering, resulting in\\na low-dimensional embedding space that effectively encodes the\\ntextual similarity between terms. One main disadvantage of TaxoCom is that it requires a large set of quality phrases in the target\\nlanguage, and curating these phrases can be costly. The quality of\\nthe output taxonomy is highly dependent on those phrases.\\nRegarding the automatic expansion of taxonomies, an important related example is Musubu [ 20 ], a framework for low-resource\\ntaxonomy enrichment that uses a Language Model (LM) as a knowledge base to infer term relationships. For the taxonomy expansion\\npart of out method, we used Musubu as a baseline for comparison.\\n\\n\\n268\\n\\n\\n-----\\n\\nTagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\nAs to using Large Language Models for taxonomy tasks, Chen\\net al . [6] investigated how LLMs, like GPT-3, perform in taxonomy\\nconstruction tasks. The authors compared two approaches: finetuning, which involves training the LLM on a specific dataset to\\nadapt it for taxonomy tasks, and prompt techniques, where the\\nLLM receives instructions and examples to perform a task without\\nbeing explicitly trained for it. Their findings showed that prompt\\ntechniques such as few-shot learning generally outperform finetuning, particularly with smaller datasets. Based on these findings,\\nwe applied prompting techniques, specifically zero-shot prompting,\\nacross various LLMs to assess their effectiveness in constructing and\\nexpanding taxonomies. Section 7 shows the results of our approach,\\nas well as the results of applying Musubu[20] as baseline.\\n### **3 BACKGROUND**\\n\\nIn this section, we provide a comprehensive background on Large\\nLanguage Models (LLMs), and the concept of Prompt-tuning. These\\nconcepts are essential to understanding the construction and editing\\nof taxonomies utilizing LLMs.\\n### **3.1 Large Language Models**\\n\\nLately, Large Language Models (LLMs) have garnered significant\\nattention for their exceptional performance in various NLP tasks.\\nLLMs, such as GPT-3[ 3 ] and LLAMA[ 22 ], are characterized by their\\nmassive scale, comprising billions of parameters and being trained\\non vast amounts of data. These models are often pre-trained in\\nan unsupervised manner on large corpora of textual data, such as\\nbooks, articles, and web pages, allowing them to learn contextual\\nrepresentations that capture the intricacies of human language.\\nTo use LLMs for specific purposes, a highly effective approach is\\nto fine-tune them on task-specific data. Fine-tuning enables LLMs\\nto adapt to specific domains or tasks with minimal labeled data,\\nsignificantly reducing the need for large annotated datasets. However, in scenarios where labeled data is scarce or difficult to obtain,\\nLLMs can also be used without specific training or additional data,\\nin a Zero-Shot manner [ 21 ]. Given the scale of these models and\\nthe data they are trained on, LLMs embed vast knowledge that enables them to achieve high generalization capabilities and perform\\ntasks in diverse contexts, even without specific training for those\\ntasks[16].\\nIn our experiments, we tested several types of language models,\\nfrom private LLMs (GPT 4 [ 14 ], Gemini Pro [2] ), to open-source LLMs\\n(Llama 2 [ 22 ]), to a Mixture of Experts LLM (Mixtral [ 9 ], and a Small\\nLanguage Model (SLM), Phi 2 [3] .\\n### **3.2 Prompt Engineering**\\n\\nPrompt Engineering is a fundamental technique used to enhance\\nthe performance and adaptability of Large Language Models (LLMs)\\nin specific tasks or domains [ 7 ]. It involves optimizing and crafting\\nprompts to efficiently use language models (LMs) [ 3 ]. This approach\\nallows researchers and practitioners to tailor the behavior and output of LLMs, making them more suitable for targeted applications.\\n\\n2 https://deepmind.google/technologies/gemini/pro/\\n3 https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-ofsmall-language-models/\\n\\n\\nTechniques such as Zero-shot prompting, Few-shot prompting,\\nChain of Thought, ReAct, Self-Consistency etc. have been explored\\nto guide LLMs toward desired responses [ 18, 21, 25 – 27 ]. The effectiveness of prompt tuning has been demonstrated in various\\napplications, including question-answering, summarization, and\\ndialogue generation. The choice of prompt greatly influences the\\ngenerated output, and by carefully crafting prompts, researchers\\ncan guide the model’s responses toward desired behaviors. For example, in language translation, a prompt can specify the source\\nlanguage and desired target language to ensure accurate and fluent translations. In our method, we used the Zero-Shot prompting\\ntechnique.\\n### **3.3 Zero-Shot Prompting**\\n\\nSince LLMs (Large Language Models) are trained on vast amounts\\nof data, they can follow instructions and perform tasks in contexts\\nwhere they were not specifically trained, in a Zero-Shot (ZS) manner.\\nThis prompting style allows the model to adapt, making it versatile.\\nA Zero-Shot (ZS) prompt directly instructs the model to perform\\na task without additional examples or demonstrations to guide\\nthe LLM’s response, which is why they are also known as task\\ninstructions [21].\\nIn a study by Li [11], the authors highlighted several advantages\\nof using ZS prompts, such as the ability to craft highly interpretable\\nprompts, requiring fewer training data or examples, a more straightforward prompt design process, and a flexible prompt structure.\\nAdditionally, Reynolds and McDonell [17] noted that carefully engineered zero-shot prompts can outperform few-shot prompts in\\ncertain scenarios, as examples can sometimes be interpreted as part\\nof a narrative rather than as a guiding mechanism. This finding also\\ninfluenced our decision to use zero-shot prompting in our method.\\n### **4 DATASET CONSTRUCTION**\\n\\nThis work uses a proprietary dataset consisting of consumer transactions from a retail bank. Each transaction includes only a merchant\\nname indicating the business where that purchase occurred along\\nwith macro and micro categories as illustrated in Figure 1 The macro\\nand micro are originally assigned by [ 4 ] using the information from\\nthe business activities and products.\\nWe focus on two macro-categories from this dataset: *Food* and\\n*Shopping*, selecting the top 50,000 businesses with the highest number of transactions for each category.\\nWith the limited initial information, assigning detailed tags to\\ntransactions is challenging. To address this, we augment the dataset\\nthrough a data enrichment process involving web scraping. Using\\ntools such as Selenium [4] and Beautiful Soup [5], we gathered activity\\ndescriptions for companies in each macro category. For the *Food*\\nmacro category, the search was conducted on specialized platforms\\nfor restaurants and food delivery services. For the *Shopping* macro\\ncategory, we obtained establishment descriptions directly from\\ninternet indexing and search tools.\\nIn the context of enrichment for the *Food* macro category, web\\nscraping was conducted as follows: (1) the centers of all Brazilian\\nstate capitals and the Federal District were used as base locations\\n\\n4 https://www.selenium.dev/about/\\n5 https://readthedocs.org/projects/beautiful-soup-4/downloads/pdf/latest/\\n\\n\\n269\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Moraes et al.\\n\\n\\nfor restaurant searching; (2) for each location, restaurants listed on\\nthe first one hundred pages of the platform were extracted. After\\ncompleting these steps, the information was combined with the\\nmerchant database using the merchant’s name and micro categories.\\nFor the *Shopping* macro category, the description of each merchant was obtained using Google Search Engine [6], selecting descriptions from the first ten search results. The final description\\nconsists of a concatenation of all the obtained descriptions. The\\nsearch queries were constructed using the merchant names combined with their micro categories.\\n### **5 TAXONOMY CONSTRUCTION**\\n\\nTo automatically create topic taxonomies for *Food* and *Shopping*\\nbusinesses, we developed a 3-step method. First, we preprocess the\\ndescriptions in our enriched dataset to retain only the relevant parts\\nof the text. Next, we apply two techniques to select candidate terms\\nfor the topic taxonomies: keyword extraction and topic modeling.\\nIn the post-processing phase, we use large language models\\n(LLMs) to refine the results of each step, filtering out unrelated\\nterms. Finally, we use LLMs again to organize the final terms into\\nhierarchies, forming the topic taxonomies.\\n### **5.1 Preprocessing**\\n\\nWe applied a few NLP techniques to refine the businesses’ descriptions in our dataset. At first, we remove stop words to eliminate\\ncommonly used words that do not carry significant meaning in\\nour contexts. Then, to retain only the most relevant portions of the\\ndescriptions, we employ part-of-speech (POS) tagging to identify\\nand exclude words that belong to specific POS categories. The list\\nof POS tag categories that were removed includes ADV, CCONJ,\\nADP, AUX, CONJ, DET, INTJ, PART, PRON, PUNCT, SYM, SCONJ,\\nADJ, VERB, PROPN. [7]\\n\\nAfter this initial preprocessing step, we run the first iteration\\nof the candidate term selection part to build a filter of generic\\nwords, not to create topic taxonomies yet. For this step, we use the\\nentire corpus of descriptions for each macro category, resulting in\\ntwo corpora ( *Food* and *Shopping* ). For each micro category in the\\nmacro categories’ corpora, we use Keyword Extraction and Topic\\nModeling to gather candidate terms for the filter, combining the\\nresults of both techniques in a list. Then, We use an LLM to remove\\nthe terms it identifies as unrelated to the main topic (each micro\\ncategory) from the list. The prompt that we used for requesting this\\nseparation is illustrated below.\\n\\nprompt= \"Given the terms in the following list: \"+\\n<wordsList> +\". Separate them into two groups. In\\ngroup 1 the terms with no relation to the topic \"+\\n<type> +\". And in group 2 the terms that are related.\"\\n\\n**Listing 1: Prompt for separating candidate terms related to**\\n**the type of establishment**\\n\\nBy using this prompt, we try to ensure that the model’s response\\nis consistently formatted according to the pattern described in it,\\nfacilitating the processing of the resulting string, although, some\\n\\n6 https://www.google.com\\n7 https://spacy.io/usage/linguistic-features#pos-tagging\\n\\n\\nof the LLMs we tested did not output the response in the requested\\nformat. Once we complete one iteration of this method for each\\nmacro category in our dataset, we add the words of group 2 to the\\ncorresponding list of generic words. We apply the corresponding\\nfilter of generic words for each macro category corpus, resulting in\\nthe final preprocessed corpus.\\n### **5.2 Candidate Terms Selection**\\n\\nFor this part of our method, we use each preprocessed corpus\\nseparately. For the *Food* corpus, we group the descriptions based\\non their micro-categories, creating 58 sub-corpus specific to that\\ndomain. We have six micro categories for the *Shopping* corpus,\\nresulting in 6 specific sub-corpus. The candidate terms selection\\nmethods are applied to each sub-corpus, creating topic taxonomies\\nwhere the main topic is the micro category.\\n\\n*5.2.1* *Keyword Extraction.* The first approach to candidate term\\nselection was to use an unsupervised keyword selection method\\ncalled Yake! [ 5 ]. This method is based on statistical text features\\nextracted from single documents to select the most relevant keywords from that text. It does not require training on a document set\\nand is not dependent on dictionaries, text size, language, domain,\\nor external corpora.\\nYake! allows for the specification of parameters such as the\\nlanguage of the text, the maximum size of the n-grams being sought,\\nand others. In our method, we customized only the language to\\nPortuguese, and the maximum number of keywords sought for each\\nset of descriptions was 30 words.\\nAfter extracting the keywords from each group of descriptions,\\nwe obtained a total set of *𝑁* candidate terms. However, these terms\\nare further filtered using an LLM, where we ask it to separate the\\nterms related to the main topic from those unrelated, as explained\\nearlier in subsection 5.1.\\n\\n*5.2.2* *Topic Modeling.* Our second approach to collecting initial\\ntopics and candidate terms was Topic Modeling. We applied the\\nLatent Dirichlet Allocation algorithm [ 2 ], available at the Gensim\\nLibrary [8] .\\nWe construct a dictionary for each macro-category corpus in\\nour macro-categories corpora by extracting unique tokens and\\nbigrams. After a few empirical tests, we set the minimum frequency\\nof a bigram to 20 occurrences. Since some corpora have a minimal\\nnumber of tokens (the micro category \"Greek Cuisine\" from the *Food*\\nmacro category has only five stores marked as such, with a corpus\\nof only 127 tokens), we had to set a reasonably small number so that\\nsmaller corpora could also have a few bigrams. With the resulting\\ndictionary of tokens, the LDA algorithm was applied. Three main\\nparameters are to be defined in an LDA algorithm: number of topics,\\n*alpha*, and *beta* .\\nThe number of topics defines the latent topics to be extracted\\nfrom the corpus. The parameter *alpha* is *a priori* belief in documenttopic distribution, while *beta* is *a priori* belief in topic-word distribution.\\nTo define the number of topics for each micro category corpus,\\nwe tried numbers from 1 to 5, constantly checking which configuration would result in the best average topic coherence for that\\n\\n8 https://pypi.org/project/gensim/\\n\\n\\n270\\n\\n\\n-----\\n\\nTagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\ncorpus. Small corpora would have 1 or 2 topics, while bigger ones\\nwould have 5. To correctly define the *alpha* and *beta* priors, we\\nwould have to analyze the distribution for each category corpus\\n\\n[ 24 ]. Since this would be rather difficult, we set those priors to be\\nauto-defined by the LDA algorithm, which learns these parameters\\nbased on the corpus. We select the terms with the highest coherence\\nwith the resulting topics. Each topic returns 20 words with their\\ncoherence scores, but we do not use all of them as some have very\\nlow coherence. After testing a few configurations, for each topic,\\nwe select 60% of the terms with the highest coherence within that\\ntopic.\\nWith initial terms for each topic taxonomy, we ask an LLM\\nto separate the ones closely related to the main topic from those\\nunrelated, as mentioned earlier.\\n### **5.3 Hierarchy Construction**\\n\\nOnce we have the post-processed lists of candidate terms obtained\\nby each technique mentioned in subsection 5.2, we merge them and\\nremove repetitions. After the merge, for each macro category, we\\nhave lists of terms for each micro category, representing each topic\\ntaxonomy. However, they do not have any hierarchy level between\\nthe terms configuring the taxonomy.\\nTo tackle this problem, we use an LLM again, this time with a\\nprompt that searches for sub-categories within the terms of a topic\\nto create these hierarchies. The prompt is illustrated below:\\n\\nprompt=\"Create a dictionary by hierarchically arranging the\\n\\nfollowing words:\" + <wordsList> +.\" Use JSON format as\\nthe output such as the following: {\\\\\"key\\\\\": [\\\\\" list\\nof words\\\\\"]}\"\\n\\n**Listing 2: Prompt for creating a hierarchy for each list of**\\n**tags.**\\n\\nWith this prompt, we seek to ensure that the LLM response has a\\nconsistent pattern and facilitates handling the returned string. After\\nthis step, we have a hierarchy of terms in each topic taxonomy in\\nthe *Food* and *Shopping* macro categories.\\n### **5.4 Merchant Tagging**\\n\\nWith the topic taxonomies for both *Food* and *Shopping* macrocategories, we can now assign tags to merchants/establishments.\\nTo do so, we use the descriptions attached to these establishments,\\nand we see which terms from a taxonomy are mentioned in their\\ndescriptions with a reverse index algorithm. We employ the taxonomy whose topic is the same as the establishment’s micro category,\\nas shown in Figure 2.\\n### **6 TAXONOMY EXPANSION**\\n\\nAnother essential part of our method is the automatic expansion of\\nexisting taxonomies as new terms arrive, derived from additional\\nmerchant scrapped data, as shown in Section 4. In this section, we\\npresent our approach to taxonomy expansion by using instructionbased LLMs.\\n\\nAs new transactions may include new businesses, new terms\\ncan emerge from the descriptions obtained through the scraping\\nprocess. Therefore, we need to update the taxonomies with these\\n\\n\\nnew terms maintaining and enriching the created hierarchies with\\nthe potential new terms.\\nAfter completing the transaction enrichment process, including\\nthe search for business descriptions and the selection of candidate\\nterms, if relevant terms not included in the current hierarchies are\\ndetected, we initiate the expansion process.\\n### **6.1 Prompt engineering instruction for** **taxonomy representation**\\n\\nFirst, we represent our topic taxonomies in a format that can be\\ninterpreted by an LLM. We employed a generic prompt, illustrated\\nbelow, across all tested methods to convert topics into root nodes\\nand their terms into child nodes.\\n\\nChilds of [ROOT]: [CHILD1,CHILD2,CHILD3]\\nChilds of [CHILD1]: [CHILD4,CHILD5]\\n\\nChilds of [CHILD2]: [CHILD6]\\n\\n...\\n\\n**Listing 3: Prompt for representation of taxonomy**\\n### **6.2 Predicting the parent of a node**\\n\\nTo experiment with taxonomies expansion, we used two datasets:\\nour *Food* and *Shopping* topic taxonomies and the taxonomies from\\nSemEval-2015 Task 17 [ 15 ]. Those are low-resource taxonomies,\\nwith thousands of nodes or less, which are appropriate for the current prompt size of LLMs. We used the SemEval dataset to compare\\nthe results with well-established methods for taxonomy expansion,\\nsuch as Musubu [ 20 ]. Similar to their experiments, we hid 20% of\\nthe terms (chosen randomically) in the taxonomies to predict their\\nrespective parent nodes. To verify the parent/root of a new term,\\nwe used the following prompt:\\n\\n**Listin** **g** **4: Prom** **p** **t for searchin** **g** **for a node’s** **p** **arent**\\n\\nprompt=\"Who is the father of \"+<new_term>+\"?\"\\n\\nIn Table 1, we see the F1-Scores for parent node prediction. Equation 1 showcases how to calculate the F1-Score. TP is the number\\n\\nof true positives, nodes that were correctly assigned as parents of\\nchild nodes. FP is the number of false positives, nodes that were\\nincorrectly assigned as a parent to a child node. FN is the number\\nof false negatives, nodes that should have been assigned as parent\\nnodes but were not.\\n\\n2 ∗ *𝑇𝑃*\\n*𝐹* 1 = (1)\\n2 ∗ *𝑇𝑃* + *𝐹𝑃* + *𝐹𝑁*\\n\\nFor baseline models, we used Bert and Musubu; for commercial\\nLLMs, Gemini Pro and GPT-4; and for open-source LLMs, LLamaAlpaca(7B), Phi-2, and Mixtral 8x7B. We evaluate them in 4 taxonomies from the SemEval dataset and our taxonomies. For each\\ntaxonomy, the LLMs perform significantly better than Musubu,\\nwith GPT-4 and Gemini Pro having the highest F1-Scores, with the\\nlatter beating the former by a few points. However, the most recent\\nopen-source options (Phi-2 and Mixtral 8x7B) are getting close in\\nperformance.\\n\\n\\n271\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Moraes et al.\\n\\n\\n\\n\\n**Tags assigned to establishments from the**\\n\\n***\"Clothing & Accessories\"*** **micro category**\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n**Figure 2: Assigning tags to establishments based on a topic**\\n**taxonomy.**\\n\\nIt is important to note that while SemEval taxonomies have\\nthousands of nodes, ours have only a few hundred, which we can\\nassume is a significant reason for the degrading performance of\\nMusubu and Bert (LMs or LM-based methods). In contrast, the LLMs\\nhave a robust performance in such low-resource settings. This also\\nshows that LLMs have a remarkable understanding of questions\\nand zero-shot performance, generalizing well even for datasets in\\ndifferent languages.\\n### **7 TAXONOMY EVALUATION**\\n\\nTo properly evaluate the topic taxonomies that we created in this\\nwork, we developed a two-step qualitative evaluation of a limited\\npart of the results.\\nIn total, 58 topic taxonomies were created for the *Food* set and\\n6 for the *Shopping* set. For our evaluation, we selected the topic\\ntaxonomies with the highest number of terms in each part (the\\n\"Brazilian Cuisine\" taxonomy for the *Food* part and the \"Clothing\\nand Accessories\" taxonomy for the *Shopping* one). First, we assess\\nthe quality of removing generic terms from each taxonomy, and\\nthen, we evaluate the tags assigned to establishments based on that\\ntaxonomy. We asked 12 volunteers to answer a two-part form.\\n*Part 1 - Accuracy of the terms that were selected as related to the*\\n*topic* : In this part, we evaluate if the LLMs could correctly group\\nthe relevant and non-relevant terms, removing the generic terms.\\nTo do so, we defined a ground truth with the relevant terms as\\n\\n\\ntrue positives and the non-relevant terms as true negatives. Table 3\\nshows the results.\\n\\nGPT-4 was the best model, followed by Gemini Pro, both scoring\\nover 60% accuracy for the Brazilian Cuisine taxonomy and over\\n86% accuracy for the Clothing and Accessories taxonomy. Smaller\\nlanguage models such as Phi 2 and Llama 2 7B performed poorly\\nboth in removing generic terms and in formatting the response\\naccordingly, with Phi 2 being particularly verbose.\\n*Part 2 - Human Evaluation of the Quality of the Tagging Process* :\\nIn this part, the volunteers were asked to examine if the tags assigned to an establishment were appropriate and coherent to that\\nestablishment’s description. We selected the top 5 establishments\\nwith the highest transactions for each micro category. We asked our\\nevaluators to analyze the tags assigned to describe that establishment and choose the ones that were not appropriate. This way, we\\nhave a coherence ratio for each establishment based on the number\\n\\nof proper tags divided by the total number of tags. We average the\\nresults of our 12 evaluators and present them in Table 2. Figure 2\\nshows the \"Clothing & Accessories\" taxonomy that was evaluated\\nand 2 of the merchants and the tags assigned to them that were\\nincluded in the evaluation.\\n### **8 CONCLUSION**\\n\\nIn this work, we presented an unsupervised method for automatically creating and expanding topic taxonomies using LLMs. We\\nevaluated some of the generated taxonomies and applied them\\nin transaction tagging in a retailer’s bank dataset. The evaluation\\nshowed promising results, with average coherence scores above 90%\\nfor the two selected taxonomies. The taxonomies’ expansion with\\nGemini Pro also showed exciting results for parent node prediction,\\nwith F1-scores of 89% and 70% for *Food* and *Shopping* taxonomies,\\nrespectively.\\nFor future work on taxonomy construction, we plan to test\\nmore robust term selection methods, such as embedding-based\\napproaches. We also plan on conducting ablation studies to validate whether the keyword extraction and topic modeling parts\\nhelp improve the quality of the taxonomies created, by using a\\nbaseline prompt to ask the LLM to generate child nodes given a\\nparent node. In terms of taxonomy expansion, there are several\\ntasks to explore, ranging from node-level operations to generating\\nentire sub-trees and identifying similar structures. Additionally, we\\nintend to enhance our instruction-tuned LLM for taxonomy tasks\\n\\n\\n|Method|SemEval-2015 Task 17 Chemical Equipment Food Science|Our taxonomies Food Shopping|\\n|---|---|---|\\n|Gemini Pro|0.68 0.80 0.91 0.72|0.89 0.73|\\n|GPT-4|0.65 0.78 0.89 0.70|0.87 0.71|\\n|Mixtral-8x7B|0.59 0.63 0.80 0.57|0.74 0.60|\\n|Phi-2|0.56 0.52 0.68 0.56|0.64 0.54|\\n|LLama 7B|0.51 0.42 0.58 0.46|0.60 0.49|\\n|Musubu|0.35 0.46 0.37 0.42|0.21 0.13|\\n|Bert-Base|0.13 0.14 0.12 0.16|0.11 0.06|\\n\\n\\n**Table 1: F1-score for parent node prediction.**\\n\\n272\\n\\n\\n-----\\n\\nTagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies WebMedia’2024, Juiz de Fora, Brazil\\n\\n|Col1|Brazilian Cuisine Taxonomy Average Coherence Number of Tags|Col3|Clothing & Accessories Taxonomy Average Coherence Number of Tags|Col5|\\n|---|---|---|---|---|\\n|Merchant 1|92.30%|10|97.11%|8|\\n|Merchant 2|94.23%|8|83.07%|5|\\n|Merchant 3|89.23%|5|94.38%|5|\\n|Merchant 4|87.17%|6|93.84%|5|\\n|Merchant 5|93.40%|7|97.43%|6|\\n\\n\\n\\n**Table 2: Results of evaluating the tags assigned to each merchant/establishment.**\\n\\n\\n|Col1|Brazilian Cuisine|Clothing & Accessories|\\n|---|---|---|\\n|Llama 2 7B|29.54%|52.78%|\\n|Phi 2|40.90|73.68%|\\n|Mixtral 8x7B v0.1|46.93%|70.27%|\\n|Gemini Pro|61.36%|86.11%|\\n|GPT 4|68.08%|86.84%|\\n\\n\\n**Table 3: Accuracy of using each LLM to remove generic words**\\n**from each topic taxonomy.**\\n\\nby fine-tuning or employing more efficient methods such as LoRA\\n\\n[8].\\n### **LIMITATIONS**\\n\\nTo address the limitations of our work, we begin with the taxonomy construction component. In this phase, we relied on topic\\nmodeling and keywords extraction to select candidate terms for our\\ntaxonomies. The LDA algorithm used for topic modeling performs\\nsuboptimally when the base corpus is small. Some of our topics had\\ncorpora with vocabularies of fewer than 100 words, which can result\\nin topics containing irrelevant or incoherent terms. Additionally,\\nwe could have further experimented with the LDA hyperparameters\\nfor each micro-category corpus.\\nRegarding the evaluation of the generated taxonomies, we did not\\nassess topic completeness. Without a ground truth, it is challenging\\nto quantify how comprehensively the terms in a taxonomy cover the\\nmain topic. Furthermore, we evaluated only 2 of the 64 taxonomies\\ngenerated by our method, leaving a substantial portion unexamined.\\nIn the taxonomy expansion experiment, we evaluated only a lowresource setting with fewer than a thousand nodes. Most studies\\nfocus on taxonomies with hundreds of thousands or more nodes.\\n\\nThis presents a challenge for LLMs due to their limited context.\\nAddressing this contextual limitation could benefit from insights\\nfound in other works that tackle similar issues [12].\\n### **ETHICS STATEMENT**\\n\\nIn this work, we ensure the utmost protection of customers and\\nstore sensitive data by exclusively using non-sensitive information\\nin our dataset. Our prompts solely rely on selected words from store\\ndescriptions, thus avoiding any direct usage of personal or sensitive\\ninformation. No customer-specific data or store-sensitive details\\nare integrated into the system, upholding privacy and security as\\ntop priorities.\\n\\n\\nMoreover, we strictly adhere to ethical guidelines during our\\nexperiments involving volunteers, and no personal data is collected\\nfrom them. Our focus lies solely on analyzing the results of our\\nproposed approach. Participants’ anonymity and confidentiality are\\nmaintained throughout the research process, ensuring a responsible\\nand trustworthy approach to data handling.\\n### **ACKNOWLEDGMENTS**\\n\\nThe authors would like to acknowledge BTG Pactual for the partnership and financial support to this research.\\n### '"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 256
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:48:04.832326Z",
     "start_time": "2025-05-12T23:48:04.828827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tmp = clean_abs(tmp)\n",
    "tmp"
   ],
   "id": "e4175c22a45916db",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INTRODUCTION  Many recent studies have focused on the application of Machine Learning-based methods for the classification and characterization of financial transactions. For example, Vollset et al . [23] and Busson et al . [4] explored an approach to hierarchically classifying   267   -----  WebMedia’2024, Juiz de Fora, Brazil Moraes et al.   financial transactions, employing a predefined set of categories/subcategories that describe purchase types and transactions. However, these methods apply a limited, predefined set of static classes, which restricts the ability to extend classifications based on user experiences when encountering new, previously undefined categories. In this context, to expand the possible set of classes/tags to label a transaction, we developed an unsupervised method based on topic taxonomies . Taxonomies are very useful in the structural and semantic analyses of topics and textual data. However, creating and maintaining them is often costly and challenging to scale manually. Therefore, recent works have tackled the automatic creation and expansion of topic taxonomies, in which each node in a hierarchy represents a conceptual topic composed of semantically coherent  terms.  We present an unsupervised method for automatically constructing and expanding topic taxonomies with instruction-based LLMs (Large Language Models), in a Zero-Shot manner. Candidate terms for the initial version of the taxonomy are obtained using topic modeling and keyword extraction techniques. Then we apply LLMs to post-process the resulting terms, create a hierarchy, and add new terms to an existing taxonomy. Since the taxonomies are derived from a corpus of unstructured texts describing niches of consuming habits, we opted to investigate the use of LLMs in our approach. LLMs are often pre-trained on a large corpus of text, allowing them to learn contextual representations that capture the intricacies of human language. We applied our method to a private dataset of transactions of a retail bank, enriched with scraped data from food and shopping companies, and evaluated the resulting taxonomies quantitatively. The generated tags of our topic taxonomies are then assigned to the bank transactions characterizing the companies in each transaction, as shown on Figure 1. In total, 58 topic taxonomies were created for the Food category and 6 for the Shopping category. A two-step quantitative evaluation was conducted on a subset of the taxonomies. For this evaluation, we selected the topic taxonomies with the highest number of terms in each category: \"Brazilian Cuisine\" from Food and \"Clothing and Accessories\" from Shopping . Taxonomies with more terms are most likely to result in a deeper hierarchy, which gives more data for evaluation. We asked 12 volunteers to answer a two-part form, which assessed the quality of the created taxonomies and the quality of the tags assigned to label transactions. The evaluation showed an average coherence of tags to transactions above 90%. As more scraped data from food and shopping companies are added to the retail bank’s dataset, the topic taxonomies will need to be updated to include new terms. We used LLMs for this task as well, employing commercial LLMs like Gemini Pro [ 1 ] and GPT4 [ 14 ], alongside open-source LLM options such as LLaMA-Alpaca (7B) [ 22 ], Phi-2 [1], and Mixtral 8x7B [ 9 ]. We showcase their results in both taxonomy creation and expansion. For the expansion part, we also compared our method to existing ones (a BERT-based method and Musubu[ 20 ]) on the SemEval dataset and our generated taxonomies as well. Gemini Pro achieved the best results, with F1scores of 89% and 70% for parent node prediction on the Food and Shopping taxonomies, respectively.  1 https://huggingface.co/microsoft/phi-2   The remainder of the paper is structured as follows: Section 2, reviews the related work, highlighting existing approaches. Section 3 provides the necessary background, laying the foundation for our methodologies and contextualizing our contributions. Section 4 details the dataset construction process, explaining how we enriched and prepared the data for the taxonomies’ construction. In Section 5, we describe the creation of the taxonomies, outlining the methods used to generate them. Section 6 discusses the expansion of the taxonomies, demonstrating how they can be dynamically extended to accommodate new categories. Section 7 focuses on the evaluation of these taxonomies, presenting the metrics and results that validate their accuracy and also the quality of the tags assigned to the transactions. Finally, Section 8 concludes the paper, summarizing our findings, discussing their implications, and suggesting directions for future research.  2 RELATED WORK  Taxonomies represent the structure behind a collection of documents, organizing the hierarchical relationships between terms in a tree structure [ 13 ]. They play an essential part in the structural and semantic analysis of textual data, providing valuable content for many applications that involve information retrieval and filtering, such as web searching, recommendation systems, classification, and question answering. Since creating and maintaining taxonomies is a costly task, often difficult to scale if done manually, methods that automatically construct and update them are desirable. Early works on automatic taxonomy creation focused on building hypernym-hyponym taxonomies, where each pair of terms expresses an ‘is-a’ relationship  [ 19 ]. More recent works have tackled the automatic creation of other taxonomies, such as topic taxonomies. In a topic taxonomy, each node represents a conceptual topic composed of semantically coherent terms.  In this context, Zhang et al . [28] developed TaxoGen, an unsupervised method for constructing topic taxonomies. Taxogen uses the SkipGram model from an input text corpus to embed all the concept terms into a latent space that captures their semantics. In this space, the authors applied a clustering method to construct a hierarchy recursively based on a variation of the spherical K-means algorithm. Another work that focuses on topic taxonomies is TaxoCom  [ 10 ], a framework for automatic taxonomy expansion. TaxoCom is a hierarchical topic discovery framework that recursively expands an initial taxonomy by discovering new sub-topics. It uses locally discriminative embeddings and adaptive clustering, resulting in a low-dimensional embedding space that effectively encodes the textual similarity between terms. One main disadvantage of TaxoCom is that it requires a large set of quality phrases in the target language, and curating these phrases can be costly. The quality of the output taxonomy is highly dependent on those phrases. Regarding the automatic expansion of taxonomies, an important related example is Musubu [ 20 ], a framework for low-resource taxonomy enrichment that uses a Language Model (LM) as a knowledge base to infer term relationships. For the taxonomy expansion part of out method, we used Musubu as a baseline for comparison.   268   -----  Tagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies WebMedia’2024, Juiz de Fora, Brazil   As to using Large Language Models for taxonomy tasks, Chen et al . [6] investigated how LLMs, like GPT-3, perform in taxonomy construction tasks. The authors compared two approaches: finetuning, which involves training the LLM on a specific dataset to adapt it for taxonomy tasks, and prompt techniques, where the LLM receives instructions and examples to perform a task without being explicitly trained for it. Their findings showed that prompt techniques such as few-shot learning generally outperform finetuning, particularly with smaller datasets. Based on these findings, we applied prompting techniques, specifically zero-shot prompting, across various LLMs to assess their effectiveness in constructing and expanding taxonomies. Section 7 shows the results of our approach, as well as the results of applying Musubu[20] as baseline.  3 BACKGROUND  In this section, we provide a comprehensive background on Large Language Models (LLMs), and the concept of Prompt-tuning. These concepts are essential to understanding the construction and editing of taxonomies utilizing LLMs.  3.1 Large Language Models  Lately, Large Language Models (LLMs) have garnered significant attention for their exceptional performance in various NLP tasks. LLMs, such as GPT-3[ 3 ] and LLAMA[ 22 ], are characterized by their massive scale, comprising billions of parameters and being trained on vast amounts of data. These models are often pre-trained in an unsupervised manner on large corpora of textual data, such as books, articles, and web pages, allowing them to learn contextual representations that capture the intricacies of human language. To use LLMs for specific purposes, a highly effective approach is to fine-tune them on task-specific data. Fine-tuning enables LLMs to adapt to specific domains or tasks with minimal labeled data, significantly reducing the need for large annotated datasets. However, in scenarios where labeled data is scarce or difficult to obtain, LLMs can also be used without specific training or additional data, in a Zero-Shot manner [ 21 ]. Given the scale of these models and the data they are trained on, LLMs embed vast knowledge that enables them to achieve high generalization capabilities and perform tasks in diverse contexts, even without specific training for those tasks[16]. In our experiments, we tested several types of language models, from private LLMs (GPT 4 [ 14 ], Gemini Pro [2] ), to open-source LLMs (Llama 2 [ 22 ]), to a Mixture of Experts LLM (Mixtral [ 9 ], and a Small Language Model (SLM), Phi 2 [3] .  3.2 Prompt Engineering  Prompt Engineering is a fundamental technique used to enhance the performance and adaptability of Large Language Models (LLMs) in specific tasks or domains [ 7 ]. It involves optimizing and crafting prompts to efficiently use language models (LMs) [ 3 ]. This approach allows researchers and practitioners to tailor the behavior and output of LLMs, making them more suitable for targeted applications.  2 https://deepmind.google/technologies/gemini/pro/ 3 https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-ofsmall-language-models/   Techniques such as Zero-shot prompting, Few-shot prompting, Chain of Thought, ReAct, Self-Consistency etc. have been explored to guide LLMs toward desired responses [ 18, 21, 25 – 27 ]. The effectiveness of prompt tuning has been demonstrated in various applications, including question-answering, summarization, and dialogue generation. The choice of prompt greatly influences the generated output, and by carefully crafting prompts, researchers can guide the model’s responses toward desired behaviors. For example, in language translation, a prompt can specify the source language and desired target language to ensure accurate and fluent translations. In our method, we used the Zero-Shot prompting technique.  3.3 Zero-Shot Prompting  Since LLMs (Large Language Models) are trained on vast amounts of data, they can follow instructions and perform tasks in contexts where they were not specifically trained, in a Zero-Shot (ZS) manner. This prompting style allows the model to adapt, making it versatile. A Zero-Shot (ZS) prompt directly instructs the model to perform a task without additional examples or demonstrations to guide the LLM’s response, which is why they are also known as task instructions [21]. In a study by Li [11], the authors highlighted several advantages of using ZS prompts, such as the ability to craft highly interpretable prompts, requiring fewer training data or examples, a more straightforward prompt design process, and a flexible prompt structure. Additionally, Reynolds and McDonell [17] noted that carefully engineered zero-shot prompts can outperform few-shot prompts in certain scenarios, as examples can sometimes be interpreted as part of a narrative rather than as a guiding mechanism. This finding also influenced our decision to use zero-shot prompting in our method.  4 DATASET CONSTRUCTION  This work uses a proprietary dataset consisting of consumer transactions from a retail bank. Each transaction includes only a merchant name indicating the business where that purchase occurred along with macro and micro categories as illustrated in Figure 1 The macro and micro are originally assigned by [ 4 ] using the information from the business activities and products. We focus on two macro-categories from this dataset: Food and Shopping, selecting the top 50,000 businesses with the highest number of transactions for each category. With the limited initial information, assigning detailed tags to transactions is challenging. To address this, we augment the dataset through a data enrichment process involving web scraping. Using tools such as Selenium [4] and Beautiful Soup [5], we gathered activity descriptions for companies in each macro category. For the Food macro category, the search was conducted on specialized platforms for restaurants and food delivery services. For the Shopping macro category, we obtained establishment descriptions directly from internet indexing and search tools. In the context of enrichment for the Food macro category, web scraping was conducted as follows: (1) the centers of all Brazilian state capitals and the Federal District were used as base locations  4 https://www.selenium.dev/about/ 5 https://readthedocs.org/projects/beautiful-soup-4/downloads/pdf/latest/   269   -----  WebMedia’2024, Juiz de Fora, Brazil Moraes et al.   for restaurant searching; (2) for each location, restaurants listed on the first one hundred pages of the platform were extracted. After completing these steps, the information was combined with the merchant database using the merchant’s name and micro categories. For the Shopping macro category, the description of each merchant was obtained using Google Search Engine [6], selecting descriptions from the first ten search results. The final description consists of a concatenation of all the obtained descriptions. The search queries were constructed using the merchant names combined with their micro categories.  5 TAXONOMY CONSTRUCTION  To automatically create topic taxonomies for Food and Shopping businesses, we developed a 3-step method. First, we preprocess the descriptions in our enriched dataset to retain only the relevant parts of the text. Next, we apply two techniques to select candidate terms for the topic taxonomies: keyword extraction and topic modeling. In the post-processing phase, we use large language models (LLMs) to refine the results of each step, filtering out unrelated terms. Finally, we use LLMs again to organize the final terms into hierarchies, forming the topic taxonomies.  5.1 Preprocessing  We applied a few NLP techniques to refine the businesses’ descriptions in our dataset. At first, we remove stop words to eliminate commonly used words that do not carry significant meaning in our contexts. Then, to retain only the most relevant portions of the descriptions, we employ part-of-speech (POS) tagging to identify and exclude words that belong to specific POS categories. The list of POS tag categories that were removed includes ADV, CCONJ, ADP, AUX, CONJ, DET, INTJ, PART, PRON, PUNCT, SYM, SCONJ, ADJ, VERB, PROPN. [7]  After this initial preprocessing step, we run the first iteration of the candidate term selection part to build a filter of generic words, not to create topic taxonomies yet. For this step, we use the entire corpus of descriptions for each macro category, resulting in two corpora ( Food and Shopping ). For each micro category in the macro categories’ corpora, we use Keyword Extraction and Topic Modeling to gather candidate terms for the filter, combining the results of both techniques in a list. Then, We use an LLM to remove the terms it identifies as unrelated to the main topic (each micro category) from the list. The prompt that we used for requesting this separation is illustrated below.  prompt= \"Given the terms in the following list: \"+ <wordsList> +\". Separate them into two groups. In group 1 the terms with no relation to the topic \"+ <type> +\". And in group 2 the terms that are related.\"  Listing 1: Prompt for separating candidate terms related to the type of establishment  By using this prompt, we try to ensure that the model’s response is consistently formatted according to the pattern described in it, facilitating the processing of the resulting string, although, some  6 https://www.google.com 7 https://spacy.io/usage/linguistic-features#pos-tagging   of the LLMs we tested did not output the response in the requested format. Once we complete one iteration of this method for each macro category in our dataset, we add the words of group 2 to the corresponding list of generic words. We apply the corresponding filter of generic words for each macro category corpus, resulting in the final preprocessed corpus.  5.2 Candidate Terms Selection  For this part of our method, we use each preprocessed corpus separately. For the Food corpus, we group the descriptions based on their micro-categories, creating 58 sub-corpus specific to that domain. We have six micro categories for the Shopping corpus, resulting in 6 specific sub-corpus. The candidate terms selection methods are applied to each sub-corpus, creating topic taxonomies where the main topic is the micro category.  5.2.1 Keyword Extraction. The first approach to candidate term selection was to use an unsupervised keyword selection method called Yake! [ 5 ]. This method is based on statistical text features extracted from single documents to select the most relevant keywords from that text. It does not require training on a document set and is not dependent on dictionaries, text size, language, domain, or external corpora. Yake! allows for the specification of parameters such as the language of the text, the maximum size of the n-grams being sought, and others. In our method, we customized only the language to Portuguese, and the maximum number of keywords sought for each set of descriptions was 30 words. After extracting the keywords from each group of descriptions, we obtained a total set of 𝑁 candidate terms. However, these terms are further filtered using an LLM, where we ask it to separate the terms related to the main topic from those unrelated, as explained earlier in subsection 5.1.  5.2.2 Topic Modeling. Our second approach to collecting initial topics and candidate terms was Topic Modeling. We applied the Latent Dirichlet Allocation algorithm [ 2 ], available at the Gensim Library [8] . We construct a dictionary for each macro-category corpus in our macro-categories corpora by extracting unique tokens and bigrams. After a few empirical tests, we set the minimum frequency of a bigram to 20 occurrences. Since some corpora have a minimal number of tokens (the micro category \"Greek Cuisine\" from the Food macro category has only five stores marked as such, with a corpus of only 127 tokens), we had to set a reasonably small number so that smaller corpora could also have a few bigrams. With the resulting dictionary of tokens, the LDA algorithm was applied. Three main parameters are to be defined in an LDA algorithm: number of topics, alpha, and beta . The number of topics defines the latent topics to be extracted from the corpus. The parameter alpha is a priori belief in documenttopic distribution, while beta is a priori belief in topic-word distribution. To define the number of topics for each micro category corpus, we tried numbers from 1 to 5, constantly checking which configuration would result in the best average topic coherence for that  8 https://pypi.org/project/gensim/   270   -----  Tagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies WebMedia’2024, Juiz de Fora, Brazil   corpus. Small corpora would have 1 or 2 topics, while bigger ones would have 5. To correctly define the alpha and beta priors, we would have to analyze the distribution for each category corpus  [ 24 ]. Since this would be rather difficult, we set those priors to be auto-defined by the LDA algorithm, which learns these parameters based on the corpus. We select the terms with the highest coherence with the resulting topics. Each topic returns 20 words with their coherence scores, but we do not use all of them as some have very low coherence. After testing a few configurations, for each topic, we select 60% of the terms with the highest coherence within that topic. With initial terms for each topic taxonomy, we ask an LLM to separate the ones closely related to the main topic from those unrelated, as mentioned earlier.  5.3 Hierarchy Construction  Once we have the post-processed lists of candidate terms obtained by each technique mentioned in subsection 5.2, we merge them and remove repetitions. After the merge, for each macro category, we have lists of terms for each micro category, representing each topic taxonomy. However, they do not have any hierarchy level between the terms configuring the taxonomy. To tackle this problem, we use an LLM again, this time with a prompt that searches for sub-categories within the terms of a topic to create these hierarchies. The prompt is illustrated below:  prompt=\"Create a dictionary by hierarchically arranging the  following words:\" + <wordsList> +.\" Use JSON format as the output such as the following: {\\\\\"key\\\\\": [\\\\\" list of words\\\\\"]}\"  Listing 2: Prompt for creating a hierarchy for each list of tags.  With this prompt, we seek to ensure that the LLM response has a consistent pattern and facilitates handling the returned string. After this step, we have a hierarchy of terms in each topic taxonomy in the Food and Shopping macro categories.  5.4 Merchant Tagging  With the topic taxonomies for both Food and Shopping macrocategories, we can now assign tags to merchants/establishments. To do so, we use the descriptions attached to these establishments, and we see which terms from a taxonomy are mentioned in their descriptions with a reverse index algorithm. We employ the taxonomy whose topic is the same as the establishment’s micro category, as shown in Figure 2.  6 TAXONOMY EXPANSION  Another essential part of our method is the automatic expansion of existing taxonomies as new terms arrive, derived from additional merchant scrapped data, as shown in Section 4. In this section, we present our approach to taxonomy expansion by using instructionbased LLMs.  As new transactions may include new businesses, new terms can emerge from the descriptions obtained through the scraping process. Therefore, we need to update the taxonomies with these   new terms maintaining and enriching the created hierarchies with the potential new terms. After completing the transaction enrichment process, including the search for business descriptions and the selection of candidate terms, if relevant terms not included in the current hierarchies are detected, we initiate the expansion process.  6.1 Prompt engineering instruction for taxonomy representation  First, we represent our topic taxonomies in a format that can be interpreted by an LLM. We employed a generic prompt, illustrated below, across all tested methods to convert topics into root nodes and their terms into child nodes.  Childs of [ROOT]: [CHILD1,CHILD2,CHILD3] Childs of [CHILD1]: [CHILD4,CHILD5]  Childs of [CHILD2]: [CHILD6]  ...  Listing 3: Prompt for representation of taxonomy  6.2 Predicting the parent of a node  To experiment with taxonomies expansion, we used two datasets: our Food and Shopping topic taxonomies and the taxonomies from SemEval-2015 Task 17 [ 15 ]. Those are low-resource taxonomies, with thousands of nodes or less, which are appropriate for the current prompt size of LLMs. We used the SemEval dataset to compare the results with well-established methods for taxonomy expansion, such as Musubu [ 20 ]. Similar to their experiments, we hid 20% of the terms (chosen randomically) in the taxonomies to predict their respective parent nodes. To verify the parent/root of a new term, we used the following prompt:  Listin g 4: Prom p t for searchin g for a node’s p arent  prompt=\"Who is the father of \"+<new_term>+\"?\"  In Table 1, we see the F1-Scores for parent node prediction. Equation 1 showcases how to calculate the F1-Score. TP is the number  of true positives, nodes that were correctly assigned as parents of child nodes. FP is the number of false positives, nodes that were incorrectly assigned as a parent to a child node. FN is the number of false negatives, nodes that should have been assigned as parent nodes but were not.  2 ∗ 𝑇𝑃 𝐹 1 = (1) 2 ∗ 𝑇𝑃 + 𝐹𝑃 + 𝐹𝑁  For baseline models, we used Bert and Musubu; for commercial LLMs, Gemini Pro and GPT-4; and for open-source LLMs, LLamaAlpaca(7B), Phi-2, and Mixtral 8x7B. We evaluate them in 4 taxonomies from the SemEval dataset and our taxonomies. For each taxonomy, the LLMs perform significantly better than Musubu, with GPT-4 and Gemini Pro having the highest F1-Scores, with the latter beating the former by a few points. However, the most recent open-source options (Phi-2 and Mixtral 8x7B) are getting close in performance.   271   -----  WebMedia’2024, Juiz de Fora, Brazil Moraes et al.     Tags assigned to establishments from the  \"Clothing & Accessories\" micro category           Figure 2: Assigning tags to establishments based on a topic taxonomy.  It is important to note that while SemEval taxonomies have thousands of nodes, ours have only a few hundred, which we can assume is a significant reason for the degrading performance of Musubu and Bert (LMs or LM-based methods). In contrast, the LLMs have a robust performance in such low-resource settings. This also shows that LLMs have a remarkable understanding of questions and zero-shot performance, generalizing well even for datasets in different languages.  7 TAXONOMY EVALUATION  To properly evaluate the topic taxonomies that we created in this work, we developed a two-step qualitative evaluation of a limited part of the results. In total, 58 topic taxonomies were created for the Food set and 6 for the Shopping set. For our evaluation, we selected the topic taxonomies with the highest number of terms in each part (the \"Brazilian Cuisine\" taxonomy for the Food part and the \"Clothing and Accessories\" taxonomy for the Shopping one). First, we assess the quality of removing generic terms from each taxonomy, and then, we evaluate the tags assigned to establishments based on that taxonomy. We asked 12 volunteers to answer a two-part form. Part 1 - Accuracy of the terms that were selected as related to the topic : In this part, we evaluate if the LLMs could correctly group the relevant and non-relevant terms, removing the generic terms. To do so, we defined a ground truth with the relevant terms as   true positives and the non-relevant terms as true negatives. Table 3 shows the results.  GPT-4 was the best model, followed by Gemini Pro, both scoring over 60% accuracy for the Brazilian Cuisine taxonomy and over 86% accuracy for the Clothing and Accessories taxonomy. Smaller language models such as Phi 2 and Llama 2 7B performed poorly both in removing generic terms and in formatting the response accordingly, with Phi 2 being particularly verbose. Part 2 - Human Evaluation of the Quality of the Tagging Process : In this part, the volunteers were asked to examine if the tags assigned to an establishment were appropriate and coherent to that establishment’s description. We selected the top 5 establishments with the highest transactions for each micro category. We asked our evaluators to analyze the tags assigned to describe that establishment and choose the ones that were not appropriate. This way, we have a coherence ratio for each establishment based on the number  of proper tags divided by the total number of tags. We average the results of our 12 evaluators and present them in Table 2. Figure 2 shows the \"Clothing & Accessories\" taxonomy that was evaluated and 2 of the merchants and the tags assigned to them that were included in the evaluation.  8 CONCLUSION  In this work, we presented an unsupervised method for automatically creating and expanding topic taxonomies using LLMs. We evaluated some of the generated taxonomies and applied them in transaction tagging in a retailer’s bank dataset. The evaluation showed promising results, with average coherence scores above 90% for the two selected taxonomies. The taxonomies’ expansion with Gemini Pro also showed exciting results for parent node prediction, with F1-scores of 89% and 70% for Food and Shopping taxonomies, respectively. For future work on taxonomy construction, we plan to test more robust term selection methods, such as embedding-based approaches. We also plan on conducting ablation studies to validate whether the keyword extraction and topic modeling parts help improve the quality of the taxonomies created, by using a baseline prompt to ask the LLM to generate child nodes given a parent node. In terms of taxonomy expansion, there are several tasks to explore, ranging from node-level operations to generating entire sub-trees and identifying similar structures. Additionally, we intend to enhance our instruction-tuned LLM for taxonomy tasks   |Method|SemEval-2015 Task 17 Chemical Equipment Food Science|Our taxonomies Food Shopping| |---|---|---| |Gemini Pro|0.68 0.80 0.91 0.72|0.89 0.73| |GPT-4|0.65 0.78 0.89 0.70|0.87 0.71| |Mixtral-8x7B|0.59 0.63 0.80 0.57|0.74 0.60| |Phi-2|0.56 0.52 0.68 0.56|0.64 0.54| |LLama 7B|0.51 0.42 0.58 0.46|0.60 0.49| |Musubu|0.35 0.46 0.37 0.42|0.21 0.13| |Bert-Base|0.13 0.14 0.12 0.16|0.11 0.06|   Table 1: F1-score for parent node prediction.  272   -----  Tagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies WebMedia’2024, Juiz de Fora, Brazil  |Col1|Brazilian Cuisine Taxonomy Average Coherence Number of Tags|Col3|Clothing & Accessories Taxonomy Average Coherence Number of Tags|Col5| |---|---|---|---|---| |Merchant 1|92.30%|10|97.11%|8| |Merchant 2|94.23%|8|83.07%|5| |Merchant 3|89.23%|5|94.38%|5| |Merchant 4|87.17%|6|93.84%|5| |Merchant 5|93.40%|7|97.43%|6|    Table 2: Results of evaluating the tags assigned to each merchant/establishment.   |Col1|Brazilian Cuisine|Clothing & Accessories| |---|---|---| |Llama 2 7B|29.54%|52.78%| |Phi 2|40.90|73.68%| |Mixtral 8x7B v0.1|46.93%|70.27%| |Gemini Pro|61.36%|86.11%| |GPT 4|68.08%|86.84%|   Table 3: Accuracy of using each LLM to remove generic words from each topic taxonomy.  by fine-tuning or employing more efficient methods such as LoRA  [8].  LIMITATIONS  To address the limitations of our work, we begin with the taxonomy construction component. In this phase, we relied on topic modeling and keywords extraction to select candidate terms for our taxonomies. The LDA algorithm used for topic modeling performs suboptimally when the base corpus is small. Some of our topics had corpora with vocabularies of fewer than 100 words, which can result in topics containing irrelevant or incoherent terms. Additionally, we could have further experimented with the LDA hyperparameters for each micro-category corpus. Regarding the evaluation of the generated taxonomies, we did not assess topic completeness. Without a ground truth, it is challenging to quantify how comprehensively the terms in a taxonomy cover the main topic. Furthermore, we evaluated only 2 of the 64 taxonomies generated by our method, leaving a substantial portion unexamined. In the taxonomy expansion experiment, we evaluated only a lowresource setting with fewer than a thousand nodes. Most studies focus on taxonomies with hundreds of thousands or more nodes.  This presents a challenge for LLMs due to their limited context. Addressing this contextual limitation could benefit from insights found in other works that tackle similar issues [12].  ETHICS STATEMENT  In this work, we ensure the utmost protection of customers and store sensitive data by exclusively using non-sensitive information in our dataset. Our prompts solely rely on selected words from store descriptions, thus avoiding any direct usage of personal or sensitive information. No customer-specific data or store-sensitive details are integrated into the system, upholding privacy and security as top priorities.   Moreover, we strictly adhere to ethical guidelines during our experiments involving volunteers, and no personal data is collected from them. Our focus lies solely on analyzing the results of our proposed approach. Participants’ anonymity and confidentiality are maintained throughout the research process, ensuring a responsible and trustworthy approach to data handling.  ACKNOWLEDGMENTS  The authors would like to acknowledge BTG Pactual for the partnership and financial support to this research.'"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 257
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ca23428dcc6b03b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:56:36.390376Z",
     "start_time": "2025-05-12T22:56:36.387840Z"
    }
   },
   "cell_type": "code",
   "source": "corpus.loc[4,'resumo'] = abs",
   "id": "9077ed2473d7aff4",
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:03:46.264150Z",
     "start_time": "2025-05-12T23:03:46.261748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tmp = text.split('**1 INTRODUCTION')[1].split('**ACKNOWLEDGMENTS**')[0]\n",
    "\n",
    "# head, *_ = re.split(pattern, text, maxsplit=1)\n",
    "tmp = clean_abs(tmp)\n",
    "# tmp"
   ],
   "id": "eaf9e49f7b2b1872",
   "outputs": [],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:49:13.618793Z",
     "start_time": "2025-05-12T23:49:13.615869Z"
    }
   },
   "cell_type": "code",
   "source": "corpus.loc[4,'text'] = tmp",
   "id": "de76bab783c5804f",
   "outputs": [],
   "execution_count": 261
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:50:16.860985Z",
     "start_time": "2025-05-12T23:50:16.483524Z"
    }
   },
   "cell_type": "code",
   "source": "corpus",
   "id": "e31abc706b2c1502",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               titulo informacoes_url  \\\n",
       "0   Recognition of Emotions through Facial Geometr...                   \n",
       "1   Enhancing widget recognition for automated And...                   \n",
       "2   Multi-Domain Spatio-Temporal Deformable Fusion...                   \n",
       "3              Finding Fake News Websites in the Wild                   \n",
       "4   Tagging Enriched Bank Transactions Using LLM-G...                   \n",
       "5   Fast ISP Mode Decision for the Versatile Video...                   \n",
       "6   OntoDrug: Enhancing Brazilian Health System In...                   \n",
       "7   E-BELA: Enhanced Embedding-Based Entity Linkin...                   \n",
       "8   Exploring Visual and Multimodal Interaction in...                   \n",
       "9   Elderly Fall Monitoring in Smart Homes Using W...                   \n",
       "10  A Comprehensive View of the Biases of Toxicity...                   \n",
       "11  Interoperability Testing Guide for the Interne...                   \n",
       "12  An Ensemble Approach to Facial Deepfake Detect...                   \n",
       "13  Twitter and the 2022 Brazilian Elections Portr...                   \n",
       "14  Automatic Time-aware Recognition of Brazilian ...                   \n",
       "15  Investigating User's Attentional Focus in Comp...                   \n",
       "16  Acceptance and Usability of Complex Medical Sy...                   \n",
       "17  Through the Eyes of Instagram: Analyzing Image...                   \n",
       "18  Characterization of the Brazilian musical land...                   \n",
       "19  A Machine-Learning-Driven Fast Video-based Poi...                   \n",
       "20  Constructing a KBQA Framework: Design and Impl...                   \n",
       "21  A Domain-Specific Language for Multimedia Serv...                   \n",
       "22  Why Ignore Content? A Guideline for Intrinsic ...                   \n",
       "23  Impacto da Pandemia na Discussão sobre Saúde M...                   \n",
       "24  Um Framework para Análise Bidimensional de Dis...                   \n",
       "25  Middleware para Aplicações Distribuídas de Víd...                   \n",
       "26  O Impacto de Estratégias de Embeddings de Graf...                   \n",
       "27  Cuidado Ubíquo de Pacientes com Doenças Crônic...                   \n",
       "28  Jogos Digitais Sérios usados para o Exercício ...                   \n",
       "29  Crianças e Propagandas no TikTok: identificand...                   \n",
       "30  Quando as Avaliações Viram Bombas: Explorando ...                   \n",
       "31  Estratégias de Undersampling para Redução de V...                   \n",
       "32  Arquitetura Multicamadas para Coleta e Análise...                   \n",
       "33  Um framework de rastreamento corporal para rea...                   \n",
       "34  Únicos, mas não incomparáveis: abordagens para...                   \n",
       "35  Uma Proposta de Framework para Sistemas de Con...                   \n",
       "36  Uma Abordagem em Etapa de Processamento para R...                   \n",
       "37  Análise de sentimentos de conteúdo compartilha...                   \n",
       "38  Uma Investigação sobre Técnicas de Data Augmen...                   \n",
       "39  Análise da Percepção do Uso de Cigarros Eletrô...                   \n",
       "\n",
       "        idioma                                        storage_key  \\\n",
       "0      english  ../articles/original/english/985-24766-1-10-20...   \n",
       "1      english  ../articles/original/english/985-24752-1-10-20...   \n",
       "2      english  ../articles/original/english/985-24762-1-10-20...   \n",
       "3      english  ../articles/original/english/985-24756-1-10-20...   \n",
       "4      english  ../articles/original/english/985-24767-1-10-20...   \n",
       "5      english  ../articles/original/english/985-24755-1-10-20...   \n",
       "6      english  ../articles/original/english/985-24764-1-10-20...   \n",
       "7      english  ../articles/original/english/985-24750-1-10-20...   \n",
       "8      english  ../articles/original/english/985-24754-1-10-20...   \n",
       "9      english  ../articles/original/english/985-24751-1-10-20...   \n",
       "10     english  ../articles/original/english/985-24737-1-10-20...   \n",
       "11     english  ../articles/original/english/985-24758-1-10-20...   \n",
       "12     english  ../articles/original/english/985-24741-1-10-20...   \n",
       "13     english  ../articles/original/english/985-24769-1-10-20...   \n",
       "14     english  ../articles/original/english/985-24745-1-10-20...   \n",
       "15     english  ../articles/original/english/985-24759-1-10-20...   \n",
       "16     english  ../articles/original/english/985-24740-1-10-20...   \n",
       "17     english  ../articles/original/english/985-24768-1-10-20...   \n",
       "18     english  ../articles/original/english/985-24746-1-10-20...   \n",
       "19     english  ../articles/original/english/985-24739-1-10-20...   \n",
       "20     english  ../articles/original/english/985-24747-1-10-20...   \n",
       "21     english  ../articles/original/english/985-24738-1-10-20...   \n",
       "22     english  ../articles/original/english/985-24776-1-10-20...   \n",
       "23  portuguese  ../articles/original/portuguese/985-24757-1-10...   \n",
       "24  portuguese  ../articles/original/portuguese/985-24771-2-10...   \n",
       "25  portuguese  ../articles/original/portuguese/985-24761-1-10...   \n",
       "26  portuguese  ../articles/original/portuguese/985-24763-2-10...   \n",
       "27  portuguese  ../articles/original/portuguese/985-24749-1-10...   \n",
       "28  portuguese  ../articles/original/portuguese/985-24760-1-10...   \n",
       "29  portuguese  ../articles/original/portuguese/985-24748-1-10...   \n",
       "30  portuguese  ../articles/original/portuguese/985-24765-1-10...   \n",
       "31  portuguese  ../articles/original/portuguese/985-24753-2-10...   \n",
       "32  portuguese  ../articles/original/portuguese/985-24744-1-10...   \n",
       "33  portuguese  ../articles/original/portuguese/985-24770-1-10...   \n",
       "34  portuguese  ../articles/original/portuguese/985-24775-1-10...   \n",
       "35  portuguese  ../articles/original/portuguese/985-24774-1-10...   \n",
       "36  portuguese  ../articles/original/portuguese/985-24772-1-10...   \n",
       "37  portuguese  ../articles/original/portuguese/985-24743-1-10...   \n",
       "38  portuguese  ../articles/original/portuguese/985-24773-1-10...   \n",
       "39  portuguese  ../articles/original/portuguese/985-24742-1-10...   \n",
       "\n",
       "                                              autores data_publicacao  \\\n",
       "0   [Alessandra Alaniz Macedo,  Leandro Persona,  ...      11-09-2024   \n",
       "1   [Yadini Pérez López,  Laís Dib Albuquerque,  G...      11-09-2024   \n",
       "2   [Garibaldi da Silveira Júnior, Gilberto Kreisl...      11-09-2024   \n",
       "3   [Leandro Araujo,  João M. M. Couto,  Luiz Feli...      11-09-2024   \n",
       "4   [Daniel de S. Moraes,  Polyana B. da Costa,  P...      11-09-2024   \n",
       "5   [Larissa Araújo,  Adson Duarte,  Bruno Zatt,  ...      11-09-2024   \n",
       "6   [Nelson Miranda,  Matheus Matos Machado,   Dil...      11-09-2024   \n",
       "7            [Ítalo M. Pereira  Anderson A. Ferreira]      11-09-2024   \n",
       "8   [Paulo Victor Borges,  Daniel de S. Moraes,  J...      11-09-2024   \n",
       "9   [Júlia M. P. Moreira,  Raphael W. Bettio,  And...      11-09-2024   \n",
       "10  [Guilherme Andrade,  Luiz Nery,  Fabrício Bene...      11-09-2024   \n",
       "11  [Karina da Silva Castelo Branco,  Valéria Lell...      11-09-2024   \n",
       "12  [Yan Martins B. Gurevitz Cunha,  José Matheus ...      11-09-2024   \n",
       "13  [Larissa Malagoli,  Giovana Piorino,  Carlos H...      11-09-2024   \n",
       "14  [Lucas de S. Arcanjo,  Lucas F. Coelho,  Silvi...      11-09-2024   \n",
       "15  [Cassiano da Silva Souza,  Milene Selbach Silv...      11-09-2024   \n",
       "16  [Fábio Ap. Cândido da Silva,  André Pimenta Fr...      11-09-2024   \n",
       "17  [João Francisco Hecksher Olivetti  Philipe de ...      11-09-2024   \n",
       "18  [Filipe A. S. Moura,  Carlos H. G. Ferreira,  ...      11-09-2024   \n",
       "19  [Gustavo Rehbein,  Eduardo Costa,  Guilherme C...      11-09-2024   \n",
       "20  [Rômulo Chrispim de Mello,  Jorão Gomes Jr.,  ...      11-09-2024   \n",
       "21  [Franklin Jordan Ventura Quico,  Anselmo L. E....      11-09-2024   \n",
       "22  [Pedro R. Pires,  Bruno B. Rizzi,   Tiago A. A...      11-09-2024   \n",
       "23  [Pedro Bento,  Arthur Buzelin,  Yan Aquino,  I...      11-09-2024   \n",
       "24  [Geovana S. Oliveira,  Otávio Venâncio,  Viníc...      25-09-2024   \n",
       "25  [Otacílio de A. Ramos Neto,  Rafael C. Chaves,...      11-09-2024   \n",
       "26  [André Levi Zanon,  Leonardo Rocha,   Marcelo ...      04-10-2024   \n",
       "27  [Lucas Pfeiffer Salomão Dias,  L.P.S. Dias,  J...      11-09-2024   \n",
       "28  [Katherin Felipa Carhuaz Malpartida  Kamila Ri...      11-09-2024   \n",
       "29  [Raíssa Gonçalves Lopes Carvalho  Humberto Tor...      11-09-2024   \n",
       "30  [Marcus Vinicius Guerra Ribeiro,  Clara Andrad...      11-09-2024   \n",
       "31  [Guilherme Fonseca,  Gabriel Prenassi,  Washin...      04-10-2024   \n",
       "32  [Juan Felipe Souza Oliveira,  Paulo Cesar Salg...      19-09-2024   \n",
       "33  [Elvis Ribeiro,  Alexandre Brandão,  Marcelo G...      11-09-2024   \n",
       "34  [Guilherme O. Aguiar,  Juan P. D. Esteves,  Cl...      11-09-2024   \n",
       "35  [Marcelo Rocha,  Jesus Favela,   Débora C. Muc...      11-09-2024   \n",
       "36  [Rodrigo Ferrari de Souza  Marcelo Garcia Manz...      11-09-2024   \n",
       "37  [Giovana Piorino,  Vitor Moreira,  Luiz Henriq...      11-09-2024   \n",
       "38  [Marcos André Bezerra da Silva,  Manuella Asch...      11-09-2024   \n",
       "39  [Aline Dias,  Richardy R. Tanure,  Jussara M. ...      11-09-2024   \n",
       "\n",
       "                                               resumo  \\\n",
       "0   O reconhecimento de emoção tem um significado ...   \n",
       "1   O reconhecimento de widgets é crucial para tes...   \n",
       "2   A compactação de vídeo com perda de imóvel int...   \n",
       "3   A batalha contra a disseminação de informações...   \n",
       "4   This work presents an unsupervised method for ...   \n",
       "5   O padrão versátil de codificação de vídeo (VVC...   \n",
       "6   Este artigo apresenta ontodrug, uma ontologia ...   \n",
       "7   Entidade vinculando é o processo de conectar m...   \n",
       "8   Este artigo apresenta duas ferramentas inovado...   \n",
       "9   O progresso constante da tecnologia, especialm...   \n",
       "10  A linguagem é um aspecto dinâmico de nossa cul...   \n",
       "11  A Internet das Coisas (IoT) expandiu a Interne...   \n",
       "12  Esforços substanciais foram dedicados ao desen...   \n",
       "13  A influência das redes sociais on -line nas aç...   \n",
       "14  A linguagem de sinais brasileiros (Libras) é u...   \n",
       "15  Manter o foco atencional do usuário se tornou ...   \n",
       "16  A crescente demanda por testes de imagem torno...   \n",
       "17  A comunicação multimídia se tornou uma parte e...   \n",
       "18  Na era digital, serviços de streaming como o S...   \n",
       "19  Nos últimos anos, o conteúdo da nuvem de ponto...   \n",
       "20  O crescimento exponencial dos dados na Interne...   \n",
       "21  A virtualização é uma tecnologia amplamente us...   \n",
       "22  Com o crescimento constante das informações di...   \n",
       "23  The period of social isolation due to COVID-19...   \n",
       "24  As plataformas de mídia social revolucionaram ...   \n",
       "25  Dentro do escopo da indústria 4.0, a visão com...   \n",
       "26  Explicações em sistemas de recomendação são es...   \n",
       "27  As doenças crônicas estão entre as 7 das 10 pr...   \n",
       "28  O pensamento computacional (CT) é um processo ...   \n",
       "29  Este estudo aborda a identificação da publicid...   \n",
       "30  Este estudo investiga o fenômeno do \" * revisã...   \n",
       "31  Classificação de texto automática (ATC) em con...   \n",
       "32  Uma arquitetura multicamada foi desenvolvida p...   \n",
       "33  Aplicações e jogos multimídia desempenham um p...   \n",
       "34  Entender o comportamento emocional humano é um...   \n",
       "35  Há uma lacuna em plataformas robóticas de códi...   \n",
       "36  Os sistemas de recomendação são projetados par...   \n",
       "37  O uso crescente das mídias sociais e seu impac...   \n",
       "38  A tradução automática de português para Libras...   \n",
       "39  A ascensão de plataformas de vídeo como o YouT...   \n",
       "\n",
       "                                             keywords  \\\n",
       "0   Multimedia Processing, Affective Computing, Ma...   \n",
       "1   Automated Android Testing, Test Portability, O...   \n",
       "2   Redes neurais profundas, Melhoria de qualidade...   \n",
       "3   Fake News, Misinformation, Credibility, Websit...   \n",
       "4   Large Language Models, Natural Language Proces...   \n",
       "5        VVC, Intra Prediction, ISP, Machine Learning   \n",
       "6   Medication Ontologies, Drug Management, Semant...   \n",
       "7   Natural Language Processing, Entity Linking, L...   \n",
       "8   Authoring, LLMs, NCL, Code Generation, Visual ...   \n",
       "9   fall detection, wearable technologies, acceler...   \n",
       "10  African American English, AAE, Bias, Toxicity,...   \n",
       "11  Interoperability, Internet of Things, Interope...   \n",
       "12  deep fake detection, self-supervised, vision t...   \n",
       "13  2022 Brazilian Elections, Network Modeling, On...   \n",
       "14  Computer Vision, Sign Language Recognition, Ge...   \n",
       "15  attention monitoring, webcam, data analysis, l...   \n",
       "16  usability issues, radiology systems, qualitati...   \n",
       "17  Instagram, alt-text, social media, image class...   \n",
       "18  Music Preferences, Music Genre Networks, Regio...   \n",
       "19  point clouds, machine learning, V-PCC, complex...   \n",
       "20  KBQA, Complex Questions, Entity Recognition, P...   \n",
       "21                  IoMT, IoT, VNF, SFC, DSL, L-PRISM   \n",
       "22  embeddings, intrinsic evaluation, qualitative ...   \n",
       "23  Redes sociais, COVID-19, pandemia, saúde menta...   \n",
       "24  Mídias Sociais, Disseminação de Informação, Mo...   \n",
       "25  Middleware, Distribuição de Vídeo, Edge Comput...   \n",
       "26  Sistemas de Recomendação, Explicações, Embeddi...   \n",
       "27  Classificação de Comportamento; Doenças Crônic...   \n",
       "28  Pensamento Computacional, Jogos Digitais Sério...   \n",
       "29  Publicidade Infantil, Criança, TikTok, Rede So...   \n",
       "30  jogos, videogames, Metacritic, review bombing,...   \n",
       "31  Classificação de Texto, Transformers, Undersam...   \n",
       "32  internet das coisas, monitoramento remoto de s...   \n",
       "33  Realidade Virtual, Jogos Sérios, Multimídia, R...   \n",
       "34  Computação Afetiva, Respostas Emocionais, Mode...   \n",
       "35  Robôs sociais, Framework para controle de robô...   \n",
       "36  Sistemas de Recomendação, Viés de Popularidade...   \n",
       "37  Análise de Sentimentos, Comunidades do Reddit,...   \n",
       "38  Tradução Automática Neural, Aumento de Dados, ...   \n",
       "39  Cigarros Eletrônicos, Aprendizado de Máquina, ...   \n",
       "\n",
       "                                          referencias  \\\n",
       "0   [[1] Mauricio Alvarez, David Luengo, and Neil ...   \n",
       "1   [[1] A. A. Abdelhamid, S. Alotaibi, and A. Mou...   \n",
       "2   [[1] Aayushi Agarwal, Akshay Agarwal, Sayan Si...   \n",
       "3   [[1] Leandro Araújo, Luiz Felipe Nery, Isadora...   \n",
       "4   [[1] Rohan Anil, Sebastian Borgeaud, Yonghui W...   \n",
       "5   [[1] James Bergstra and Yoshua Bengio. 2012. R...   \n",
       "6   [[1] Burhan Ud Din Abbasi, Iram Fatima, Hamid ...   \n",
       "7   [[1] Moonis Ali and Savvas Zannettou. 2024. Fr...   \n",
       "8   [[1] NBR ABNT. [n. d.]. Digital Terrestrial Te...   \n",
       "9   [[1] Ibukun Awolusi, Eric Marks, and Matthew H...   \n",
       "10  [[1] Abubakar Abid, Maheen Farooqi, and James ...   \n",
       "11  [[1] Home Assistant. 2024. Awaken your home. h...   \n",
       "12  [[1] Redha Ali, Russell C. Hardie, Barath Nara...   \n",
       "13  [[1] Peiman Barnaghi, Parsa Ghaffari, and John...   \n",
       "14  [[1] Sunusi Bala Abdullahi and Kosin Chamnongt...   \n",
       "15  [[1] and the ACM Digital Library, [2], due to ...   \n",
       "16  [[1] Reza Abbasi, Monireh Sadeqi Jabali, Reza ...   \n",
       "17  [[1] Rakesh Agrawal, Ramakrishnan Srikant, et ...   \n",
       "18  [[1] Moonis Ali and Savvas Zannettou. 2024. Fr...   \n",
       "19  [[1] Gisle Bjontegaard. 2001. Calculation of a...   \n",
       "20  [[1] Shuang Chen, Qian Liu, Zhiwei Yu, Chin-Ye...   \n",
       "21  [[1] ETSI GS NFV-IFA 011. 2023. Network Functi...   \n",
       "22  [[1] Gediminas Adomavicius and Alexander Tuzhi...   \n",
       "23  [[1] Elia Abi-Jaoude, Karline Treurnicht Naylo...   \n",
       "24  [[1] Marcelo MR Araujo, Carlos HG Ferreira, Ju...   \n",
       "25  [[1] Ganesh Ananthanarayanan, Paramvir Bahl, P...   \n",
       "26  [[1] Ali, M., Berrendorf, M., Hoyt, C.T., Verm...   \n",
       "27  [[1] Rodrigo Simon Bavaresco and Jorge Luis Vi...   \n",
       "28  [[1] AAIDD. 2021. Defining Criteria for Intell...   \n",
       "29  [[1] Instituto Alana. 2022. Notificação enviad...   \n",
       "30  [[1] 2022. O que é ’woke’ e por que o termo ge...   \n",
       "31  [[1] Lasse F Wolff Anthony, Benjamin Kanding, ...   \n",
       "32  [[1] Zahra Ahmadi, Mostafa Haghi Kashani, Moha...   \n",
       "33  [[1] Alberto Luiz Aramaki, Rosana Ferreira Sam...   \n",
       "34  [[1] Bert Bakker, Gijs Schumacher, Kevin Arcen...   \n",
       "35  [[1] Michel Albonico, Milica Ðorđević, Engel H...   \n",
       "36  [[1] Himan Abdollahpouri, Masoud Mansoury, Rob...   \n",
       "37  [[1] Rafael J. A. Almeida. 2018. LeIA - Léxico...   \n",
       "38  [[1] T. M. U. Araújo. 2012. Uma solução para g...   \n",
       "39  [[1] Shishir Adhikari, Akshay Uppal, Robin Mer...   \n",
       "\n",
       "                                                 text  \\\n",
       "0   Introdução O reconhecimento das emoções é uma ...   \n",
       "1   Introdução O teste automatizado de aplicativos...   \n",
       "2   Introdução A compactação de vídeo desempenha u...   \n",
       "3   Introdução Nos últimos tempos, a sociedade enf...   \n",
       "4   INTRODUCTION  Many recent studies have focused...   \n",
       "5   Introdução Os vídeos digitais têm sido fundame...   \n",
       "6   Introdução O gerenciamento de medicamentos é u...   \n",
       "7   Introdução O volume crescente de dados publica...   \n",
       "8   Introdução Aplicativos de TV digital (DTV) [26...   \n",
       "9   Introdução ao incentivo à inovação e tecnologi...   \n",
       "10  Introdução nas últimas décadas, testemunhamos ...   \n",
       "11  A tecnologia de introdução transformou signifi...   \n",
       "12  Introdução Nos últimos anos, houve um foco cre...   \n",
       "13  Introdução As plataformas de mídia social on -...   \n",
       "14  Introdução O reconhecimento brasileiro de ling...   \n",
       "15  Introdução O avanço das tecnologias de informa...   \n",
       "16  Introdução Os sistemas de informação em saúde ...   \n",
       "17  Introdução A Web se tornou parte integrante da...   \n",
       "18  Introdução Na era digital, os serviços de stre...   \n",
       "19  As nuvens de ponto de introdução podem ser usa...   \n",
       "20  Introdução O grande volume de dados disponívei...   \n",
       "21  Introdução A virtualização é um conceito que a...   \n",
       "22  Introdução Os sistemas de recomendação são fer...   \n",
       "23  INTRODUÇÃO Nos últimos anos, o panorama da com...   \n",
       "24  INTRODUÇÃO As plataformas de mídias sociais re...   \n",
       "25  INTRODUÇÃO Nos últimos anos, a importância de ...   \n",
       "26  INTRODUÇÃO Sistemas de Recomendação (SsR) são ...   \n",
       "27  INTRODUÇÃO A Organização Mundial de Saúde (OMS...   \n",
       "28  INTRODUÇÃO O Pensamento Computacional (PC) é d...   \n",
       "29  INTRODUCTION No cenário contemporâneo, as rede...   \n",
       "30  INTRODUÇÃO Atualmente, plataformas digitais de...   \n",
       "31  INTRODUÇÃO Classificação Automática de Texto (...   \n",
       "32  INTRODUÇÃO O crescimento exponencial dos siste...   \n",
       "33  Um framework de rastreamento corporal para rea...   \n",
       "34  INTRODUÇÃO Num contexto em que as emoções huma...   \n",
       "35  INTRODUÇÃO A adoção de novas tecnologias na ed...   \n",
       "36  INTRODUÇÃO Os sistemas de recomendação são pro...   \n",
       "37  INTRODUÇÃO As redes sociais têm rompido barrei...   \n",
       "38  INTRODUÇÃO A evolução da tecnologia tem desemp...   \n",
       "39  INTRODUÇÃO A ascensão de plataformas de vídeo,...   \n",
       "\n",
       "                                    artigo_tokenizado  \\\n",
       "0   [Introdução, O, reconhecimento, das, emoções, ...   \n",
       "1   [Introdução, O, teste, automatizado, de, aplic...   \n",
       "2   [Introdução, A, compactação, de, vídeo, desemp...   \n",
       "3   [Introdução, Nos, últimos, tempos, ,, a, socie...   \n",
       "4   [9/02/2008, às, 19:20:05, Molho, de, macarrão,...   \n",
       "5   [Introdução, Os, vídeos, digitais, têm, sido, ...   \n",
       "6   [Introdução, O, gerenciamento, de, medicamento...   \n",
       "7   [Introdução, O, volume, crescente, de, dados, ...   \n",
       "8   [Introdução, Aplicativos, de, TV, digital, (, ...   \n",
       "9   [Introdução, ao, incentivo, à, inovação, e, te...   \n",
       "10  [Introdução, nas, últimas, décadas, ,, testemu...   \n",
       "11  [A, tecnologia, de, introdução, transformou, s...   \n",
       "12  [Introdução, Nos, últimos, anos, ,, houve, um,...   \n",
       "13  [Introdução, As, plataformas, de, mídia, socia...   \n",
       "14  [Introdução, O, reconhecimento, brasileiro, de...   \n",
       "15  [Introdução, O, avanço, das, tecnologias, de, ...   \n",
       "16  [Introdução, Os, sistemas, de, informação, em,...   \n",
       "17  [Introdução, A, Web, se, tornou, parte, integr...   \n",
       "18  [Introdução, Na, era, digital, ,, os, serviços...   \n",
       "19  [As, nuvens, de, ponto, de, introdução, podem,...   \n",
       "20  [Introdução, O, grande, volume, de, dados, dis...   \n",
       "21  [Introdução, A, virtualização, é, um, conceito...   \n",
       "22  [Introdução, Os, sistemas, de, recomendação, s...   \n",
       "23  [INTRODUÇÃO, Nos, últimos, anos, ,, o, panoram...   \n",
       "24  [INTRODUÇÃO, As, plataformas, de, mídias, soci...   \n",
       "25  [INTRODUÇÃO, Nos, últimos, anos, ,, a, importâ...   \n",
       "26  [INTRODUÇÃO, Sistemas, de, Recomendação, (, Ss...   \n",
       "27  [INTRODUÇÃO, A, Organização, Mundial, de, Saúd...   \n",
       "28  [INTRODUÇÃO, O, Pensamento, Computacional, (, ...   \n",
       "29  [INTRODUCTION, No, cenário, contemporâneo, ,, ...   \n",
       "30  [INTRODUÇÃO, Atualmente, ,, plataformas, digit...   \n",
       "31  [INTRODUÇÃO, Classificação, Automática, de, Te...   \n",
       "32  [INTRODUÇÃO, O, crescimento, exponencial, dos,...   \n",
       "33  [Um, framework, de, rastreamento, corporal, pa...   \n",
       "34  [INTRODUÇÃO, Num, contexto, em, que, as, emoçõ...   \n",
       "35  [INTRODUÇÃO, A, adoção, de, novas, tecnologias...   \n",
       "36  [INTRODUÇÃO, Os, sistemas, de, recomendação, s...   \n",
       "37  [INTRODUÇÃO, As, redes, sociais, têm, rompido,...   \n",
       "38  [INTRODUÇÃO, A, evolução, da, tecnologia, tem,...   \n",
       "39  [INTRODUÇÃO, A, ascensão, de, plataformas, de,...   \n",
       "\n",
       "                                           pos_tagger  \\\n",
       "0   [[Introdução, NOUN, NOUN], [O, DET, DET], [rec...   \n",
       "1   [[Introdução, NOUN, NOUN], [O, DET, DET], [tes...   \n",
       "2   [[Introdução, NOUN, NOUN], [A, DET, DET], [com...   \n",
       "3   [[Introdução, NOUN, NOUN], [Nos, ADP, ADP], [ú...   \n",
       "4   [[9/02/2008, NUM, NUM], [às, ADP, ADP], [19:20...   \n",
       "5   [[Introdução, NOUN, NOUN], [Os, DET, DET], [ví...   \n",
       "6   [[Introdução, NOUN, NOUN], [O, DET, DET], [ger...   \n",
       "7   [[Introdução, NOUN, NOUN], [O, DET, DET], [vol...   \n",
       "8   [[Introdução, PROPN, PROPN], [Aplicativos, PRO...   \n",
       "9   [[Introdução, NOUN, NOUN], [ao, ADP, ADP], [in...   \n",
       "10  [[Introdução, NOUN, NOUN], [nas, ADP, ADP], [ú...   \n",
       "11  [[A, DET, DET], [tecnologia, NOUN, NOUN], [de,...   \n",
       "12  [[Introdução, NOUN, NOUN], [Nos, ADP, ADP], [ú...   \n",
       "13  [[Introdução, PROPN, PROPN], [As, DET, DET], [...   \n",
       "14  [[Introdução, NOUN, NOUN], [O, DET, DET], [rec...   \n",
       "15  [[Introdução, NOUN, NOUN], [O, DET, DET], [ava...   \n",
       "16  [[Introdução, NOUN, NOUN], [Os, DET, DET], [si...   \n",
       "17  [[Introdução, NOUN, NOUN], [A, DET, DET], [Web...   \n",
       "18  [[Introdução, NOUN, NOUN], [Na, ADP, ADP], [er...   \n",
       "19  [[As, DET, DET], [nuvens, NOUN, NOUN], [de, AD...   \n",
       "20  [[Introdução, NOUN, NOUN], [O, DET, DET], [gra...   \n",
       "21  [[Introdução, NOUN, NOUN], [A, DET, DET], [vir...   \n",
       "22  [[Introdução, NOUN, NOUN], [Os, DET, DET], [si...   \n",
       "23  [[INTRODUÇÃO, PROPN, PROPN], [Nos, ADP, ADP], ...   \n",
       "24  [[INTRODUÇÃO, PROPN, PROPN], [As, DET, DET], [...   \n",
       "25  [[INTRODUÇÃO, PROPN, PROPN], [Nos, ADP, ADP], ...   \n",
       "26  [[INTRODUÇÃO, PROPN, PROPN], [Sistemas, PROPN,...   \n",
       "27  [[INTRODUÇÃO, PROPN, PROPN], [A, DET, DET], [O...   \n",
       "28  [[INTRODUÇÃO, PROPN, PROPN], [O, DET, DET], [P...   \n",
       "29  [[INTRODUCTION, PROPN, PROPN], [No, ADP, ADP],...   \n",
       "30  [[INTRODUÇÃO, PROPN, PROPN], [Atualmente, ADV,...   \n",
       "31  [[INTRODUÇÃO, PROPN, PROPN], [Classificação, P...   \n",
       "32  [[INTRODUÇÃO, PROPN, PROPN], [O, DET, DET], [c...   \n",
       "33  [[Um, DET, DET], [framework, PROPN, PROPN], [d...   \n",
       "34  [[INTRODUÇÃO, PROPN, PROPN], [Num, ADP, ADP], ...   \n",
       "35  [[INTRODUÇÃO, PROPN, PROPN], [A, DET, DET], [a...   \n",
       "36  [[INTRODUÇÃO, PROPN, PROPN], [Os, DET, DET], [...   \n",
       "37  [[INTRODUÇÃO, PROPN, PROPN], [As, DET, DET], [...   \n",
       "38  [[INTRODUÇÃO, PROPN, PROPN], [A, DET, DET], [e...   \n",
       "39  [[INTRODUÇÃO, PROPN, PROPN], [A, DET, DET], [a...   \n",
       "\n",
       "                                                 lema  \n",
       "0   [introdução, o, reconhecimento, de o, emoção, ...  \n",
       "1   [introdução, o, teste, automatizado, de, aplic...  \n",
       "2   [introdução, o, compactação, de, vídeo, desemp...  \n",
       "3   [introdução, em o, último, tempo, o, sociedade...  \n",
       "4   [9/02/2008, a o, 19:20:05, Molho, de, macarrão...  \n",
       "5   [introdução, o, vídeo, digital, ter, ser, fund...  \n",
       "6   [introdução, o, gerenciamento, de, medicamento...  \n",
       "7   [introdução, o, volume, crescente, de, dado, p...  \n",
       "8   [Introdução, Aplicativos, de, tv, digital, DTV...  \n",
       "9   [introdução, a o, incentivo, a o, inovação, e,...  \n",
       "10  [introdução, em o, último, década, testemunham...  \n",
       "11  [o, tecnologia, de, introdução, transformar, s...  \n",
       "12  [introdução, em o, último, ano, haver, um, foc...  \n",
       "13  [introdução, o, plataforma, de, mídia, social,...  \n",
       "14  [introdução, o, reconhecimento, brasileiro, de...  \n",
       "15  [introdução, o, avanço, de o, tecnologia, de, ...  \n",
       "16  [introdução, o, sistema, de, informação, em, s...  \n",
       "17  [introdução, o, Web, se, tornar, parte, integr...  \n",
       "18  [introdução, em o, ser, digital, o, serviço, d...  \n",
       "19  [o, nuvem, de, ponto, de, introdução, poder, s...  \n",
       "20  [introdução, o, grande, volume, de, dado, disp...  \n",
       "21  [introdução, o, virtualização, ser, um, concei...  \n",
       "22  [introdução, o, sistema, de, recomendação, ser...  \n",
       "23  [INTRODUÇÃO, em o, último, ano, o, panorama, d...  \n",
       "24  [INTRODUÇÃO, o, plataforma, de, mídia, social,...  \n",
       "25  [INTRODUÇÃO, em o, último, ano, o, importância...  \n",
       "26  [INTRODUÇÃO, Sistemas, de, Recomendação, SsR, ...  \n",
       "27  [INTRODUÇÃO, o, Organização, Mundial, de, Saúd...  \n",
       "28  [INTRODUÇÃO, o, Pensamento, Computacional, PC,...  \n",
       "29  [INTRODUCTION, em o, cenário, contemporâneo, o...  \n",
       "30  [INTRODUÇÃO, Atualmente, plataforma, digital, ...  \n",
       "31  [INTRODUÇÃO, Classificação, Automática, de, Te...  \n",
       "32  [INTRODUÇÃO, o, crescimento, exponencial, de o...  \n",
       "33  [um, framework, de, rastreamento, corporal, pa...  \n",
       "34  [INTRODUÇÃO, em um, contexto, em, que, o, emoç...  \n",
       "35  [INTRODUÇÃO, o, adoção, de, novo, tecnologia, ...  \n",
       "36  [INTRODUÇÃO, o, sistema, de, recomendação, ser...  \n",
       "37  [INTRODUÇÃO, o, rede, social, ter, rompir, bar...  \n",
       "38  [INTRODUÇÃO, o, evolução, de o, tecnologia, te...  \n",
       "39  [INTRODUÇÃO, o, ascensão, de, plataforma, de, ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titulo</th>\n",
       "      <th>informacoes_url</th>\n",
       "      <th>idioma</th>\n",
       "      <th>storage_key</th>\n",
       "      <th>autores</th>\n",
       "      <th>data_publicacao</th>\n",
       "      <th>resumo</th>\n",
       "      <th>keywords</th>\n",
       "      <th>referencias</th>\n",
       "      <th>text</th>\n",
       "      <th>artigo_tokenizado</th>\n",
       "      <th>pos_tagger</th>\n",
       "      <th>lema</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Recognition of Emotions through Facial Geometr...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24766-1-10-20...</td>\n",
       "      <td>[Alessandra Alaniz Macedo,  Leandro Persona,  ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O reconhecimento de emoção tem um significado ...</td>\n",
       "      <td>Multimedia Processing, Affective Computing, Ma...</td>\n",
       "      <td>[[1] Mauricio Alvarez, David Luengo, and Neil ...</td>\n",
       "      <td>Introdução O reconhecimento das emoções é uma ...</td>\n",
       "      <td>[Introdução, O, reconhecimento, das, emoções, ...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [O, DET, DET], [rec...</td>\n",
       "      <td>[introdução, o, reconhecimento, de o, emoção, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Enhancing widget recognition for automated And...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24752-1-10-20...</td>\n",
       "      <td>[Yadini Pérez López,  Laís Dib Albuquerque,  G...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O reconhecimento de widgets é crucial para tes...</td>\n",
       "      <td>Automated Android Testing, Test Portability, O...</td>\n",
       "      <td>[[1] A. A. Abdelhamid, S. Alotaibi, and A. Mou...</td>\n",
       "      <td>Introdução O teste automatizado de aplicativos...</td>\n",
       "      <td>[Introdução, O, teste, automatizado, de, aplic...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [O, DET, DET], [tes...</td>\n",
       "      <td>[introdução, o, teste, automatizado, de, aplic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Multi-Domain Spatio-Temporal Deformable Fusion...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24762-1-10-20...</td>\n",
       "      <td>[Garibaldi da Silveira Júnior, Gilberto Kreisl...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A compactação de vídeo com perda de imóvel int...</td>\n",
       "      <td>Redes neurais profundas, Melhoria de qualidade...</td>\n",
       "      <td>[[1] Aayushi Agarwal, Akshay Agarwal, Sayan Si...</td>\n",
       "      <td>Introdução A compactação de vídeo desempenha u...</td>\n",
       "      <td>[Introdução, A, compactação, de, vídeo, desemp...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [A, DET, DET], [com...</td>\n",
       "      <td>[introdução, o, compactação, de, vídeo, desemp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Finding Fake News Websites in the Wild</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24756-1-10-20...</td>\n",
       "      <td>[Leandro Araujo,  João M. M. Couto,  Luiz Feli...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A batalha contra a disseminação de informações...</td>\n",
       "      <td>Fake News, Misinformation, Credibility, Websit...</td>\n",
       "      <td>[[1] Leandro Araújo, Luiz Felipe Nery, Isadora...</td>\n",
       "      <td>Introdução Nos últimos tempos, a sociedade enf...</td>\n",
       "      <td>[Introdução, Nos, últimos, tempos, ,, a, socie...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [Nos, ADP, ADP], [ú...</td>\n",
       "      <td>[introdução, em o, último, tempo, o, sociedade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tagging Enriched Bank Transactions Using LLM-G...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24767-1-10-20...</td>\n",
       "      <td>[Daniel de S. Moraes,  Polyana B. da Costa,  P...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>This work presents an unsupervised method for ...</td>\n",
       "      <td>Large Language Models, Natural Language Proces...</td>\n",
       "      <td>[[1] Rohan Anil, Sebastian Borgeaud, Yonghui W...</td>\n",
       "      <td>INTRODUCTION  Many recent studies have focused...</td>\n",
       "      <td>[9/02/2008, às, 19:20:05, Molho, de, macarrão,...</td>\n",
       "      <td>[[9/02/2008, NUM, NUM], [às, ADP, ADP], [19:20...</td>\n",
       "      <td>[9/02/2008, a o, 19:20:05, Molho, de, macarrão...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fast ISP Mode Decision for the Versatile Video...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24755-1-10-20...</td>\n",
       "      <td>[Larissa Araújo,  Adson Duarte,  Bruno Zatt,  ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O padrão versátil de codificação de vídeo (VVC...</td>\n",
       "      <td>VVC, Intra Prediction, ISP, Machine Learning</td>\n",
       "      <td>[[1] James Bergstra and Yoshua Bengio. 2012. R...</td>\n",
       "      <td>Introdução Os vídeos digitais têm sido fundame...</td>\n",
       "      <td>[Introdução, Os, vídeos, digitais, têm, sido, ...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [Os, DET, DET], [ví...</td>\n",
       "      <td>[introdução, o, vídeo, digital, ter, ser, fund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OntoDrug: Enhancing Brazilian Health System In...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24764-1-10-20...</td>\n",
       "      <td>[Nelson Miranda,  Matheus Matos Machado,   Dil...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Este artigo apresenta ontodrug, uma ontologia ...</td>\n",
       "      <td>Medication Ontologies, Drug Management, Semant...</td>\n",
       "      <td>[[1] Burhan Ud Din Abbasi, Iram Fatima, Hamid ...</td>\n",
       "      <td>Introdução O gerenciamento de medicamentos é u...</td>\n",
       "      <td>[Introdução, O, gerenciamento, de, medicamento...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [O, DET, DET], [ger...</td>\n",
       "      <td>[introdução, o, gerenciamento, de, medicamento...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>E-BELA: Enhanced Embedding-Based Entity Linkin...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24750-1-10-20...</td>\n",
       "      <td>[Ítalo M. Pereira  Anderson A. Ferreira]</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Entidade vinculando é o processo de conectar m...</td>\n",
       "      <td>Natural Language Processing, Entity Linking, L...</td>\n",
       "      <td>[[1] Moonis Ali and Savvas Zannettou. 2024. Fr...</td>\n",
       "      <td>Introdução O volume crescente de dados publica...</td>\n",
       "      <td>[Introdução, O, volume, crescente, de, dados, ...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [O, DET, DET], [vol...</td>\n",
       "      <td>[introdução, o, volume, crescente, de, dado, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Exploring Visual and Multimodal Interaction in...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24754-1-10-20...</td>\n",
       "      <td>[Paulo Victor Borges,  Daniel de S. Moraes,  J...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Este artigo apresenta duas ferramentas inovado...</td>\n",
       "      <td>Authoring, LLMs, NCL, Code Generation, Visual ...</td>\n",
       "      <td>[[1] NBR ABNT. [n. d.]. Digital Terrestrial Te...</td>\n",
       "      <td>Introdução Aplicativos de TV digital (DTV) [26...</td>\n",
       "      <td>[Introdução, Aplicativos, de, TV, digital, (, ...</td>\n",
       "      <td>[[Introdução, PROPN, PROPN], [Aplicativos, PRO...</td>\n",
       "      <td>[Introdução, Aplicativos, de, tv, digital, DTV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Elderly Fall Monitoring in Smart Homes Using W...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24751-1-10-20...</td>\n",
       "      <td>[Júlia M. P. Moreira,  Raphael W. Bettio,  And...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O progresso constante da tecnologia, especialm...</td>\n",
       "      <td>fall detection, wearable technologies, acceler...</td>\n",
       "      <td>[[1] Ibukun Awolusi, Eric Marks, and Matthew H...</td>\n",
       "      <td>Introdução ao incentivo à inovação e tecnologi...</td>\n",
       "      <td>[Introdução, ao, incentivo, à, inovação, e, te...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [ao, ADP, ADP], [in...</td>\n",
       "      <td>[introdução, a o, incentivo, a o, inovação, e,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A Comprehensive View of the Biases of Toxicity...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24737-1-10-20...</td>\n",
       "      <td>[Guilherme Andrade,  Luiz Nery,  Fabrício Bene...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A linguagem é um aspecto dinâmico de nossa cul...</td>\n",
       "      <td>African American English, AAE, Bias, Toxicity,...</td>\n",
       "      <td>[[1] Abubakar Abid, Maheen Farooqi, and James ...</td>\n",
       "      <td>Introdução nas últimas décadas, testemunhamos ...</td>\n",
       "      <td>[Introdução, nas, últimas, décadas, ,, testemu...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [nas, ADP, ADP], [ú...</td>\n",
       "      <td>[introdução, em o, último, década, testemunham...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Interoperability Testing Guide for the Interne...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24758-1-10-20...</td>\n",
       "      <td>[Karina da Silva Castelo Branco,  Valéria Lell...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A Internet das Coisas (IoT) expandiu a Interne...</td>\n",
       "      <td>Interoperability, Internet of Things, Interope...</td>\n",
       "      <td>[[1] Home Assistant. 2024. Awaken your home. h...</td>\n",
       "      <td>A tecnologia de introdução transformou signifi...</td>\n",
       "      <td>[A, tecnologia, de, introdução, transformou, s...</td>\n",
       "      <td>[[A, DET, DET], [tecnologia, NOUN, NOUN], [de,...</td>\n",
       "      <td>[o, tecnologia, de, introdução, transformar, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>An Ensemble Approach to Facial Deepfake Detect...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24741-1-10-20...</td>\n",
       "      <td>[Yan Martins B. Gurevitz Cunha,  José Matheus ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Esforços substanciais foram dedicados ao desen...</td>\n",
       "      <td>deep fake detection, self-supervised, vision t...</td>\n",
       "      <td>[[1] Redha Ali, Russell C. Hardie, Barath Nara...</td>\n",
       "      <td>Introdução Nos últimos anos, houve um foco cre...</td>\n",
       "      <td>[Introdução, Nos, últimos, anos, ,, houve, um,...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [Nos, ADP, ADP], [ú...</td>\n",
       "      <td>[introdução, em o, último, ano, haver, um, foc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Twitter and the 2022 Brazilian Elections Portr...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24769-1-10-20...</td>\n",
       "      <td>[Larissa Malagoli,  Giovana Piorino,  Carlos H...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A influência das redes sociais on -line nas aç...</td>\n",
       "      <td>2022 Brazilian Elections, Network Modeling, On...</td>\n",
       "      <td>[[1] Peiman Barnaghi, Parsa Ghaffari, and John...</td>\n",
       "      <td>Introdução As plataformas de mídia social on -...</td>\n",
       "      <td>[Introdução, As, plataformas, de, mídia, socia...</td>\n",
       "      <td>[[Introdução, PROPN, PROPN], [As, DET, DET], [...</td>\n",
       "      <td>[introdução, o, plataforma, de, mídia, social,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Automatic Time-aware Recognition of Brazilian ...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24745-1-10-20...</td>\n",
       "      <td>[Lucas de S. Arcanjo,  Lucas F. Coelho,  Silvi...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A linguagem de sinais brasileiros (Libras) é u...</td>\n",
       "      <td>Computer Vision, Sign Language Recognition, Ge...</td>\n",
       "      <td>[[1] Sunusi Bala Abdullahi and Kosin Chamnongt...</td>\n",
       "      <td>Introdução O reconhecimento brasileiro de ling...</td>\n",
       "      <td>[Introdução, O, reconhecimento, brasileiro, de...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [O, DET, DET], [rec...</td>\n",
       "      <td>[introdução, o, reconhecimento, brasileiro, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Investigating User's Attentional Focus in Comp...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24759-1-10-20...</td>\n",
       "      <td>[Cassiano da Silva Souza,  Milene Selbach Silv...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Manter o foco atencional do usuário se tornou ...</td>\n",
       "      <td>attention monitoring, webcam, data analysis, l...</td>\n",
       "      <td>[[1] and the ACM Digital Library, [2], due to ...</td>\n",
       "      <td>Introdução O avanço das tecnologias de informa...</td>\n",
       "      <td>[Introdução, O, avanço, das, tecnologias, de, ...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [O, DET, DET], [ava...</td>\n",
       "      <td>[introdução, o, avanço, de o, tecnologia, de, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Acceptance and Usability of Complex Medical Sy...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24740-1-10-20...</td>\n",
       "      <td>[Fábio Ap. Cândido da Silva,  André Pimenta Fr...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A crescente demanda por testes de imagem torno...</td>\n",
       "      <td>usability issues, radiology systems, qualitati...</td>\n",
       "      <td>[[1] Reza Abbasi, Monireh Sadeqi Jabali, Reza ...</td>\n",
       "      <td>Introdução Os sistemas de informação em saúde ...</td>\n",
       "      <td>[Introdução, Os, sistemas, de, informação, em,...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [Os, DET, DET], [si...</td>\n",
       "      <td>[introdução, o, sistema, de, informação, em, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Through the Eyes of Instagram: Analyzing Image...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24768-1-10-20...</td>\n",
       "      <td>[João Francisco Hecksher Olivetti  Philipe de ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A comunicação multimídia se tornou uma parte e...</td>\n",
       "      <td>Instagram, alt-text, social media, image class...</td>\n",
       "      <td>[[1] Rakesh Agrawal, Ramakrishnan Srikant, et ...</td>\n",
       "      <td>Introdução A Web se tornou parte integrante da...</td>\n",
       "      <td>[Introdução, A, Web, se, tornou, parte, integr...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [A, DET, DET], [Web...</td>\n",
       "      <td>[introdução, o, Web, se, tornar, parte, integr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Characterization of the Brazilian musical land...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24746-1-10-20...</td>\n",
       "      <td>[Filipe A. S. Moura,  Carlos H. G. Ferreira,  ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Na era digital, serviços de streaming como o S...</td>\n",
       "      <td>Music Preferences, Music Genre Networks, Regio...</td>\n",
       "      <td>[[1] Moonis Ali and Savvas Zannettou. 2024. Fr...</td>\n",
       "      <td>Introdução Na era digital, os serviços de stre...</td>\n",
       "      <td>[Introdução, Na, era, digital, ,, os, serviços...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [Na, ADP, ADP], [er...</td>\n",
       "      <td>[introdução, em o, ser, digital, o, serviço, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>A Machine-Learning-Driven Fast Video-based Poi...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24739-1-10-20...</td>\n",
       "      <td>[Gustavo Rehbein,  Eduardo Costa,  Guilherme C...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Nos últimos anos, o conteúdo da nuvem de ponto...</td>\n",
       "      <td>point clouds, machine learning, V-PCC, complex...</td>\n",
       "      <td>[[1] Gisle Bjontegaard. 2001. Calculation of a...</td>\n",
       "      <td>As nuvens de ponto de introdução podem ser usa...</td>\n",
       "      <td>[As, nuvens, de, ponto, de, introdução, podem,...</td>\n",
       "      <td>[[As, DET, DET], [nuvens, NOUN, NOUN], [de, AD...</td>\n",
       "      <td>[o, nuvem, de, ponto, de, introdução, poder, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Constructing a KBQA Framework: Design and Impl...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24747-1-10-20...</td>\n",
       "      <td>[Rômulo Chrispim de Mello,  Jorão Gomes Jr.,  ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O crescimento exponencial dos dados na Interne...</td>\n",
       "      <td>KBQA, Complex Questions, Entity Recognition, P...</td>\n",
       "      <td>[[1] Shuang Chen, Qian Liu, Zhiwei Yu, Chin-Ye...</td>\n",
       "      <td>Introdução O grande volume de dados disponívei...</td>\n",
       "      <td>[Introdução, O, grande, volume, de, dados, dis...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [O, DET, DET], [gra...</td>\n",
       "      <td>[introdução, o, grande, volume, de, dado, disp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>A Domain-Specific Language for Multimedia Serv...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24738-1-10-20...</td>\n",
       "      <td>[Franklin Jordan Ventura Quico,  Anselmo L. E....</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A virtualização é uma tecnologia amplamente us...</td>\n",
       "      <td>IoMT, IoT, VNF, SFC, DSL, L-PRISM</td>\n",
       "      <td>[[1] ETSI GS NFV-IFA 011. 2023. Network Functi...</td>\n",
       "      <td>Introdução A virtualização é um conceito que a...</td>\n",
       "      <td>[Introdução, A, virtualização, é, um, conceito...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [A, DET, DET], [vir...</td>\n",
       "      <td>[introdução, o, virtualização, ser, um, concei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Why Ignore Content? A Guideline for Intrinsic ...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24776-1-10-20...</td>\n",
       "      <td>[Pedro R. Pires,  Bruno B. Rizzi,   Tiago A. A...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Com o crescimento constante das informações di...</td>\n",
       "      <td>embeddings, intrinsic evaluation, qualitative ...</td>\n",
       "      <td>[[1] Gediminas Adomavicius and Alexander Tuzhi...</td>\n",
       "      <td>Introdução Os sistemas de recomendação são fer...</td>\n",
       "      <td>[Introdução, Os, sistemas, de, recomendação, s...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [Os, DET, DET], [si...</td>\n",
       "      <td>[introdução, o, sistema, de, recomendação, ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Impacto da Pandemia na Discussão sobre Saúde M...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24757-1-10...</td>\n",
       "      <td>[Pedro Bento,  Arthur Buzelin,  Yan Aquino,  I...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>The period of social isolation due to COVID-19...</td>\n",
       "      <td>Redes sociais, COVID-19, pandemia, saúde menta...</td>\n",
       "      <td>[[1] Elia Abi-Jaoude, Karline Treurnicht Naylo...</td>\n",
       "      <td>INTRODUÇÃO Nos últimos anos, o panorama da com...</td>\n",
       "      <td>[INTRODUÇÃO, Nos, últimos, anos, ,, o, panoram...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [Nos, ADP, ADP], ...</td>\n",
       "      <td>[INTRODUÇÃO, em o, último, ano, o, panorama, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Um Framework para Análise Bidimensional de Dis...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24771-2-10...</td>\n",
       "      <td>[Geovana S. Oliveira,  Otávio Venâncio,  Viníc...</td>\n",
       "      <td>25-09-2024</td>\n",
       "      <td>As plataformas de mídia social revolucionaram ...</td>\n",
       "      <td>Mídias Sociais, Disseminação de Informação, Mo...</td>\n",
       "      <td>[[1] Marcelo MR Araujo, Carlos HG Ferreira, Ju...</td>\n",
       "      <td>INTRODUÇÃO As plataformas de mídias sociais re...</td>\n",
       "      <td>[INTRODUÇÃO, As, plataformas, de, mídias, soci...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [As, DET, DET], [...</td>\n",
       "      <td>[INTRODUÇÃO, o, plataforma, de, mídia, social,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Middleware para Aplicações Distribuídas de Víd...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24761-1-10...</td>\n",
       "      <td>[Otacílio de A. Ramos Neto,  Rafael C. Chaves,...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Dentro do escopo da indústria 4.0, a visão com...</td>\n",
       "      <td>Middleware, Distribuição de Vídeo, Edge Comput...</td>\n",
       "      <td>[[1] Ganesh Ananthanarayanan, Paramvir Bahl, P...</td>\n",
       "      <td>INTRODUÇÃO Nos últimos anos, a importância de ...</td>\n",
       "      <td>[INTRODUÇÃO, Nos, últimos, anos, ,, a, importâ...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [Nos, ADP, ADP], ...</td>\n",
       "      <td>[INTRODUÇÃO, em o, último, ano, o, importância...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>O Impacto de Estratégias de Embeddings de Graf...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24763-2-10...</td>\n",
       "      <td>[André Levi Zanon,  Leonardo Rocha,   Marcelo ...</td>\n",
       "      <td>04-10-2024</td>\n",
       "      <td>Explicações em sistemas de recomendação são es...</td>\n",
       "      <td>Sistemas de Recomendação, Explicações, Embeddi...</td>\n",
       "      <td>[[1] Ali, M., Berrendorf, M., Hoyt, C.T., Verm...</td>\n",
       "      <td>INTRODUÇÃO Sistemas de Recomendação (SsR) são ...</td>\n",
       "      <td>[INTRODUÇÃO, Sistemas, de, Recomendação, (, Ss...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [Sistemas, PROPN,...</td>\n",
       "      <td>[INTRODUÇÃO, Sistemas, de, Recomendação, SsR, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Cuidado Ubíquo de Pacientes com Doenças Crônic...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24749-1-10...</td>\n",
       "      <td>[Lucas Pfeiffer Salomão Dias,  L.P.S. Dias,  J...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>As doenças crônicas estão entre as 7 das 10 pr...</td>\n",
       "      <td>Classificação de Comportamento; Doenças Crônic...</td>\n",
       "      <td>[[1] Rodrigo Simon Bavaresco and Jorge Luis Vi...</td>\n",
       "      <td>INTRODUÇÃO A Organização Mundial de Saúde (OMS...</td>\n",
       "      <td>[INTRODUÇÃO, A, Organização, Mundial, de, Saúd...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [A, DET, DET], [O...</td>\n",
       "      <td>[INTRODUÇÃO, o, Organização, Mundial, de, Saúd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Jogos Digitais Sérios usados para o Exercício ...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24760-1-10...</td>\n",
       "      <td>[Katherin Felipa Carhuaz Malpartida  Kamila Ri...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O pensamento computacional (CT) é um processo ...</td>\n",
       "      <td>Pensamento Computacional, Jogos Digitais Sério...</td>\n",
       "      <td>[[1] AAIDD. 2021. Defining Criteria for Intell...</td>\n",
       "      <td>INTRODUÇÃO O Pensamento Computacional (PC) é d...</td>\n",
       "      <td>[INTRODUÇÃO, O, Pensamento, Computacional, (, ...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [O, DET, DET], [P...</td>\n",
       "      <td>[INTRODUÇÃO, o, Pensamento, Computacional, PC,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Crianças e Propagandas no TikTok: identificand...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24748-1-10...</td>\n",
       "      <td>[Raíssa Gonçalves Lopes Carvalho  Humberto Tor...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Este estudo aborda a identificação da publicid...</td>\n",
       "      <td>Publicidade Infantil, Criança, TikTok, Rede So...</td>\n",
       "      <td>[[1] Instituto Alana. 2022. Notificação enviad...</td>\n",
       "      <td>INTRODUCTION No cenário contemporâneo, as rede...</td>\n",
       "      <td>[INTRODUCTION, No, cenário, contemporâneo, ,, ...</td>\n",
       "      <td>[[INTRODUCTION, PROPN, PROPN], [No, ADP, ADP],...</td>\n",
       "      <td>[INTRODUCTION, em o, cenário, contemporâneo, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Quando as Avaliações Viram Bombas: Explorando ...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24765-1-10...</td>\n",
       "      <td>[Marcus Vinicius Guerra Ribeiro,  Clara Andrad...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Este estudo investiga o fenômeno do \" * revisã...</td>\n",
       "      <td>jogos, videogames, Metacritic, review bombing,...</td>\n",
       "      <td>[[1] 2022. O que é ’woke’ e por que o termo ge...</td>\n",
       "      <td>INTRODUÇÃO Atualmente, plataformas digitais de...</td>\n",
       "      <td>[INTRODUÇÃO, Atualmente, ,, plataformas, digit...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [Atualmente, ADV,...</td>\n",
       "      <td>[INTRODUÇÃO, Atualmente, plataforma, digital, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Estratégias de Undersampling para Redução de V...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24753-2-10...</td>\n",
       "      <td>[Guilherme Fonseca,  Gabriel Prenassi,  Washin...</td>\n",
       "      <td>04-10-2024</td>\n",
       "      <td>Classificação de texto automática (ATC) em con...</td>\n",
       "      <td>Classificação de Texto, Transformers, Undersam...</td>\n",
       "      <td>[[1] Lasse F Wolff Anthony, Benjamin Kanding, ...</td>\n",
       "      <td>INTRODUÇÃO Classificação Automática de Texto (...</td>\n",
       "      <td>[INTRODUÇÃO, Classificação, Automática, de, Te...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [Classificação, P...</td>\n",
       "      <td>[INTRODUÇÃO, Classificação, Automática, de, Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Arquitetura Multicamadas para Coleta e Análise...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24744-1-10...</td>\n",
       "      <td>[Juan Felipe Souza Oliveira,  Paulo Cesar Salg...</td>\n",
       "      <td>19-09-2024</td>\n",
       "      <td>Uma arquitetura multicamada foi desenvolvida p...</td>\n",
       "      <td>internet das coisas, monitoramento remoto de s...</td>\n",
       "      <td>[[1] Zahra Ahmadi, Mostafa Haghi Kashani, Moha...</td>\n",
       "      <td>INTRODUÇÃO O crescimento exponencial dos siste...</td>\n",
       "      <td>[INTRODUÇÃO, O, crescimento, exponencial, dos,...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [O, DET, DET], [c...</td>\n",
       "      <td>[INTRODUÇÃO, o, crescimento, exponencial, de o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Um framework de rastreamento corporal para rea...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24770-1-10...</td>\n",
       "      <td>[Elvis Ribeiro,  Alexandre Brandão,  Marcelo G...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Aplicações e jogos multimídia desempenham um p...</td>\n",
       "      <td>Realidade Virtual, Jogos Sérios, Multimídia, R...</td>\n",
       "      <td>[[1] Alberto Luiz Aramaki, Rosana Ferreira Sam...</td>\n",
       "      <td>Um framework de rastreamento corporal para rea...</td>\n",
       "      <td>[Um, framework, de, rastreamento, corporal, pa...</td>\n",
       "      <td>[[Um, DET, DET], [framework, PROPN, PROPN], [d...</td>\n",
       "      <td>[um, framework, de, rastreamento, corporal, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Únicos, mas não incomparáveis: abordagens para...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24775-1-10...</td>\n",
       "      <td>[Guilherme O. Aguiar,  Juan P. D. Esteves,  Cl...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Entender o comportamento emocional humano é um...</td>\n",
       "      <td>Computação Afetiva, Respostas Emocionais, Mode...</td>\n",
       "      <td>[[1] Bert Bakker, Gijs Schumacher, Kevin Arcen...</td>\n",
       "      <td>INTRODUÇÃO Num contexto em que as emoções huma...</td>\n",
       "      <td>[INTRODUÇÃO, Num, contexto, em, que, as, emoçõ...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [Num, ADP, ADP], ...</td>\n",
       "      <td>[INTRODUÇÃO, em um, contexto, em, que, o, emoç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Uma Proposta de Framework para Sistemas de Con...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24774-1-10...</td>\n",
       "      <td>[Marcelo Rocha,  Jesus Favela,   Débora C. Muc...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Há uma lacuna em plataformas robóticas de códi...</td>\n",
       "      <td>Robôs sociais, Framework para controle de robô...</td>\n",
       "      <td>[[1] Michel Albonico, Milica Ðorđević, Engel H...</td>\n",
       "      <td>INTRODUÇÃO A adoção de novas tecnologias na ed...</td>\n",
       "      <td>[INTRODUÇÃO, A, adoção, de, novas, tecnologias...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [A, DET, DET], [a...</td>\n",
       "      <td>[INTRODUÇÃO, o, adoção, de, novo, tecnologia, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Uma Abordagem em Etapa de Processamento para R...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24772-1-10...</td>\n",
       "      <td>[Rodrigo Ferrari de Souza  Marcelo Garcia Manz...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Os sistemas de recomendação são projetados par...</td>\n",
       "      <td>Sistemas de Recomendação, Viés de Popularidade...</td>\n",
       "      <td>[[1] Himan Abdollahpouri, Masoud Mansoury, Rob...</td>\n",
       "      <td>INTRODUÇÃO Os sistemas de recomendação são pro...</td>\n",
       "      <td>[INTRODUÇÃO, Os, sistemas, de, recomendação, s...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [Os, DET, DET], [...</td>\n",
       "      <td>[INTRODUÇÃO, o, sistema, de, recomendação, ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Análise de sentimentos de conteúdo compartilha...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24743-1-10...</td>\n",
       "      <td>[Giovana Piorino,  Vitor Moreira,  Luiz Henriq...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O uso crescente das mídias sociais e seu impac...</td>\n",
       "      <td>Análise de Sentimentos, Comunidades do Reddit,...</td>\n",
       "      <td>[[1] Rafael J. A. Almeida. 2018. LeIA - Léxico...</td>\n",
       "      <td>INTRODUÇÃO As redes sociais têm rompido barrei...</td>\n",
       "      <td>[INTRODUÇÃO, As, redes, sociais, têm, rompido,...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [As, DET, DET], [...</td>\n",
       "      <td>[INTRODUÇÃO, o, rede, social, ter, rompir, bar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Uma Investigação sobre Técnicas de Data Augmen...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24773-1-10...</td>\n",
       "      <td>[Marcos André Bezerra da Silva,  Manuella Asch...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A tradução automática de português para Libras...</td>\n",
       "      <td>Tradução Automática Neural, Aumento de Dados, ...</td>\n",
       "      <td>[[1] T. M. U. Araújo. 2012. Uma solução para g...</td>\n",
       "      <td>INTRODUÇÃO A evolução da tecnologia tem desemp...</td>\n",
       "      <td>[INTRODUÇÃO, A, evolução, da, tecnologia, tem,...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [A, DET, DET], [e...</td>\n",
       "      <td>[INTRODUÇÃO, o, evolução, de o, tecnologia, te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Análise da Percepção do Uso de Cigarros Eletrô...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24742-1-10...</td>\n",
       "      <td>[Aline Dias,  Richardy R. Tanure,  Jussara M. ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A ascensão de plataformas de vídeo como o YouT...</td>\n",
       "      <td>Cigarros Eletrônicos, Aprendizado de Máquina, ...</td>\n",
       "      <td>[[1] Shishir Adhikari, Akshay Uppal, Robin Mer...</td>\n",
       "      <td>INTRODUÇÃO A ascensão de plataformas de vídeo,...</td>\n",
       "      <td>[INTRODUÇÃO, A, ascensão, de, plataformas, de,...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [A, DET, DET], [a...</td>\n",
       "      <td>[INTRODUÇÃO, o, ascensão, de, plataforma, de, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 263
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:53:28.567682Z",
     "start_time": "2025-05-12T23:53:28.394603Z"
    }
   },
   "cell_type": "code",
   "source": "corpus.to_json('corpus.json', orient='records', indent=2)",
   "id": "2a24f3ecc848f8e5",
   "outputs": [],
   "execution_count": 265
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:55:29.164881Z",
     "start_time": "2025-05-12T23:55:29.009482Z"
    }
   },
   "cell_type": "code",
   "source": "corpus = pd.read_json('corpus.json')",
   "id": "e77b5da41583b2ab",
   "outputs": [],
   "execution_count": 266
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:55:33.474202Z",
     "start_time": "2025-05-12T23:55:33.045667Z"
    }
   },
   "cell_type": "code",
   "source": "corpus",
   "id": "737e82d8ae43a725",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               titulo informacoes_url  \\\n",
       "0   Recognition of Emotions through Facial Geometr...                   \n",
       "1   Enhancing widget recognition for automated And...                   \n",
       "2   Multi-Domain Spatio-Temporal Deformable Fusion...                   \n",
       "3              Finding Fake News Websites in the Wild                   \n",
       "4   Tagging Enriched Bank Transactions Using LLM-G...                   \n",
       "5   Fast ISP Mode Decision for the Versatile Video...                   \n",
       "6   OntoDrug: Enhancing Brazilian Health System In...                   \n",
       "7   E-BELA: Enhanced Embedding-Based Entity Linkin...                   \n",
       "8   Exploring Visual and Multimodal Interaction in...                   \n",
       "9   Elderly Fall Monitoring in Smart Homes Using W...                   \n",
       "10  A Comprehensive View of the Biases of Toxicity...                   \n",
       "11  Interoperability Testing Guide for the Interne...                   \n",
       "12  An Ensemble Approach to Facial Deepfake Detect...                   \n",
       "13  Twitter and the 2022 Brazilian Elections Portr...                   \n",
       "14  Automatic Time-aware Recognition of Brazilian ...                   \n",
       "15  Investigating User's Attentional Focus in Comp...                   \n",
       "16  Acceptance and Usability of Complex Medical Sy...                   \n",
       "17  Through the Eyes of Instagram: Analyzing Image...                   \n",
       "18  Characterization of the Brazilian musical land...                   \n",
       "19  A Machine-Learning-Driven Fast Video-based Poi...                   \n",
       "20  Constructing a KBQA Framework: Design and Impl...                   \n",
       "21  A Domain-Specific Language for Multimedia Serv...                   \n",
       "22  Why Ignore Content? A Guideline for Intrinsic ...                   \n",
       "23  Impacto da Pandemia na Discussão sobre Saúde M...                   \n",
       "24  Um Framework para Análise Bidimensional de Dis...                   \n",
       "25  Middleware para Aplicações Distribuídas de Víd...                   \n",
       "26  O Impacto de Estratégias de Embeddings de Graf...                   \n",
       "27  Cuidado Ubíquo de Pacientes com Doenças Crônic...                   \n",
       "28  Jogos Digitais Sérios usados para o Exercício ...                   \n",
       "29  Crianças e Propagandas no TikTok: identificand...                   \n",
       "30  Quando as Avaliações Viram Bombas: Explorando ...                   \n",
       "31  Estratégias de Undersampling para Redução de V...                   \n",
       "32  Arquitetura Multicamadas para Coleta e Análise...                   \n",
       "33  Um framework de rastreamento corporal para rea...                   \n",
       "34  Únicos, mas não incomparáveis: abordagens para...                   \n",
       "35  Uma Proposta de Framework para Sistemas de Con...                   \n",
       "36  Uma Abordagem em Etapa de Processamento para R...                   \n",
       "37  Análise de sentimentos de conteúdo compartilha...                   \n",
       "38  Uma Investigação sobre Técnicas de Data Augmen...                   \n",
       "39  Análise da Percepção do Uso de Cigarros Eletrô...                   \n",
       "\n",
       "        idioma                                        storage_key  \\\n",
       "0      english  ../articles/original/english/985-24766-1-10-20...   \n",
       "1      english  ../articles/original/english/985-24752-1-10-20...   \n",
       "2      english  ../articles/original/english/985-24762-1-10-20...   \n",
       "3      english  ../articles/original/english/985-24756-1-10-20...   \n",
       "4      english  ../articles/original/english/985-24767-1-10-20...   \n",
       "5      english  ../articles/original/english/985-24755-1-10-20...   \n",
       "6      english  ../articles/original/english/985-24764-1-10-20...   \n",
       "7      english  ../articles/original/english/985-24750-1-10-20...   \n",
       "8      english  ../articles/original/english/985-24754-1-10-20...   \n",
       "9      english  ../articles/original/english/985-24751-1-10-20...   \n",
       "10     english  ../articles/original/english/985-24737-1-10-20...   \n",
       "11     english  ../articles/original/english/985-24758-1-10-20...   \n",
       "12     english  ../articles/original/english/985-24741-1-10-20...   \n",
       "13     english  ../articles/original/english/985-24769-1-10-20...   \n",
       "14     english  ../articles/original/english/985-24745-1-10-20...   \n",
       "15     english  ../articles/original/english/985-24759-1-10-20...   \n",
       "16     english  ../articles/original/english/985-24740-1-10-20...   \n",
       "17     english  ../articles/original/english/985-24768-1-10-20...   \n",
       "18     english  ../articles/original/english/985-24746-1-10-20...   \n",
       "19     english  ../articles/original/english/985-24739-1-10-20...   \n",
       "20     english  ../articles/original/english/985-24747-1-10-20...   \n",
       "21     english  ../articles/original/english/985-24738-1-10-20...   \n",
       "22     english  ../articles/original/english/985-24776-1-10-20...   \n",
       "23  portuguese  ../articles/original/portuguese/985-24757-1-10...   \n",
       "24  portuguese  ../articles/original/portuguese/985-24771-2-10...   \n",
       "25  portuguese  ../articles/original/portuguese/985-24761-1-10...   \n",
       "26  portuguese  ../articles/original/portuguese/985-24763-2-10...   \n",
       "27  portuguese  ../articles/original/portuguese/985-24749-1-10...   \n",
       "28  portuguese  ../articles/original/portuguese/985-24760-1-10...   \n",
       "29  portuguese  ../articles/original/portuguese/985-24748-1-10...   \n",
       "30  portuguese  ../articles/original/portuguese/985-24765-1-10...   \n",
       "31  portuguese  ../articles/original/portuguese/985-24753-2-10...   \n",
       "32  portuguese  ../articles/original/portuguese/985-24744-1-10...   \n",
       "33  portuguese  ../articles/original/portuguese/985-24770-1-10...   \n",
       "34  portuguese  ../articles/original/portuguese/985-24775-1-10...   \n",
       "35  portuguese  ../articles/original/portuguese/985-24774-1-10...   \n",
       "36  portuguese  ../articles/original/portuguese/985-24772-1-10...   \n",
       "37  portuguese  ../articles/original/portuguese/985-24743-1-10...   \n",
       "38  portuguese  ../articles/original/portuguese/985-24773-1-10...   \n",
       "39  portuguese  ../articles/original/portuguese/985-24742-1-10...   \n",
       "\n",
       "                                              autores data_publicacao  \\\n",
       "0   [Alessandra Alaniz Macedo,  Leandro Persona,  ...      11-09-2024   \n",
       "1   [Yadini Pérez López,  Laís Dib Albuquerque,  G...      11-09-2024   \n",
       "2   [Garibaldi da Silveira Júnior, Gilberto Kreisl...      11-09-2024   \n",
       "3   [Leandro Araujo,  João M. M. Couto,  Luiz Feli...      11-09-2024   \n",
       "4   [Daniel de S. Moraes,  Polyana B. da Costa,  P...      11-09-2024   \n",
       "5   [Larissa Araújo,  Adson Duarte,  Bruno Zatt,  ...      11-09-2024   \n",
       "6   [Nelson Miranda,  Matheus Matos Machado,   Dil...      11-09-2024   \n",
       "7            [Ítalo M. Pereira  Anderson A. Ferreira]      11-09-2024   \n",
       "8   [Paulo Victor Borges,  Daniel de S. Moraes,  J...      11-09-2024   \n",
       "9   [Júlia M. P. Moreira,  Raphael W. Bettio,  And...      11-09-2024   \n",
       "10  [Guilherme Andrade,  Luiz Nery,  Fabrício Bene...      11-09-2024   \n",
       "11  [Karina da Silva Castelo Branco,  Valéria Lell...      11-09-2024   \n",
       "12  [Yan Martins B. Gurevitz Cunha,  José Matheus ...      11-09-2024   \n",
       "13  [Larissa Malagoli,  Giovana Piorino,  Carlos H...      11-09-2024   \n",
       "14  [Lucas de S. Arcanjo,  Lucas F. Coelho,  Silvi...      11-09-2024   \n",
       "15  [Cassiano da Silva Souza,  Milene Selbach Silv...      11-09-2024   \n",
       "16  [Fábio Ap. Cândido da Silva,  André Pimenta Fr...      11-09-2024   \n",
       "17  [João Francisco Hecksher Olivetti  Philipe de ...      11-09-2024   \n",
       "18  [Filipe A. S. Moura,  Carlos H. G. Ferreira,  ...      11-09-2024   \n",
       "19  [Gustavo Rehbein,  Eduardo Costa,  Guilherme C...      11-09-2024   \n",
       "20  [Rômulo Chrispim de Mello,  Jorão Gomes Jr.,  ...      11-09-2024   \n",
       "21  [Franklin Jordan Ventura Quico,  Anselmo L. E....      11-09-2024   \n",
       "22  [Pedro R. Pires,  Bruno B. Rizzi,   Tiago A. A...      11-09-2024   \n",
       "23  [Pedro Bento,  Arthur Buzelin,  Yan Aquino,  I...      11-09-2024   \n",
       "24  [Geovana S. Oliveira,  Otávio Venâncio,  Viníc...      25-09-2024   \n",
       "25  [Otacílio de A. Ramos Neto,  Rafael C. Chaves,...      11-09-2024   \n",
       "26  [André Levi Zanon,  Leonardo Rocha,   Marcelo ...      04-10-2024   \n",
       "27  [Lucas Pfeiffer Salomão Dias,  L.P.S. Dias,  J...      11-09-2024   \n",
       "28  [Katherin Felipa Carhuaz Malpartida  Kamila Ri...      11-09-2024   \n",
       "29  [Raíssa Gonçalves Lopes Carvalho  Humberto Tor...      11-09-2024   \n",
       "30  [Marcus Vinicius Guerra Ribeiro,  Clara Andrad...      11-09-2024   \n",
       "31  [Guilherme Fonseca,  Gabriel Prenassi,  Washin...      04-10-2024   \n",
       "32  [Juan Felipe Souza Oliveira,  Paulo Cesar Salg...      19-09-2024   \n",
       "33  [Elvis Ribeiro,  Alexandre Brandão,  Marcelo G...      11-09-2024   \n",
       "34  [Guilherme O. Aguiar,  Juan P. D. Esteves,  Cl...      11-09-2024   \n",
       "35  [Marcelo Rocha,  Jesus Favela,   Débora C. Muc...      11-09-2024   \n",
       "36  [Rodrigo Ferrari de Souza  Marcelo Garcia Manz...      11-09-2024   \n",
       "37  [Giovana Piorino,  Vitor Moreira,  Luiz Henriq...      11-09-2024   \n",
       "38  [Marcos André Bezerra da Silva,  Manuella Asch...      11-09-2024   \n",
       "39  [Aline Dias,  Richardy R. Tanure,  Jussara M. ...      11-09-2024   \n",
       "\n",
       "                                               resumo  \\\n",
       "0   O reconhecimento de emoção tem um significado ...   \n",
       "1   O reconhecimento de widgets é crucial para tes...   \n",
       "2   A compactação de vídeo com perda de imóvel int...   \n",
       "3   A batalha contra a disseminação de informações...   \n",
       "4   This work presents an unsupervised method for ...   \n",
       "5   O padrão versátil de codificação de vídeo (VVC...   \n",
       "6   Este artigo apresenta ontodrug, uma ontologia ...   \n",
       "7   Entidade vinculando é o processo de conectar m...   \n",
       "8   Este artigo apresenta duas ferramentas inovado...   \n",
       "9   O progresso constante da tecnologia, especialm...   \n",
       "10  A linguagem é um aspecto dinâmico de nossa cul...   \n",
       "11  A Internet das Coisas (IoT) expandiu a Interne...   \n",
       "12  Esforços substanciais foram dedicados ao desen...   \n",
       "13  A influência das redes sociais on -line nas aç...   \n",
       "14  A linguagem de sinais brasileiros (Libras) é u...   \n",
       "15  Manter o foco atencional do usuário se tornou ...   \n",
       "16  A crescente demanda por testes de imagem torno...   \n",
       "17  A comunicação multimídia se tornou uma parte e...   \n",
       "18  Na era digital, serviços de streaming como o S...   \n",
       "19  Nos últimos anos, o conteúdo da nuvem de ponto...   \n",
       "20  O crescimento exponencial dos dados na Interne...   \n",
       "21  A virtualização é uma tecnologia amplamente us...   \n",
       "22  Com o crescimento constante das informações di...   \n",
       "23  The period of social isolation due to COVID-19...   \n",
       "24  As plataformas de mídia social revolucionaram ...   \n",
       "25  Dentro do escopo da indústria 4.0, a visão com...   \n",
       "26  Explicações em sistemas de recomendação são es...   \n",
       "27  As doenças crônicas estão entre as 7 das 10 pr...   \n",
       "28  O pensamento computacional (CT) é um processo ...   \n",
       "29  Este estudo aborda a identificação da publicid...   \n",
       "30  Este estudo investiga o fenômeno do \" * revisã...   \n",
       "31  Classificação de texto automática (ATC) em con...   \n",
       "32  Uma arquitetura multicamada foi desenvolvida p...   \n",
       "33  Aplicações e jogos multimídia desempenham um p...   \n",
       "34  Entender o comportamento emocional humano é um...   \n",
       "35  Há uma lacuna em plataformas robóticas de códi...   \n",
       "36  Os sistemas de recomendação são projetados par...   \n",
       "37  O uso crescente das mídias sociais e seu impac...   \n",
       "38  A tradução automática de português para Libras...   \n",
       "39  A ascensão de plataformas de vídeo como o YouT...   \n",
       "\n",
       "                                             keywords  \\\n",
       "0   Multimedia Processing, Affective Computing, Ma...   \n",
       "1   Automated Android Testing, Test Portability, O...   \n",
       "2   Redes neurais profundas, Melhoria de qualidade...   \n",
       "3   Fake News, Misinformation, Credibility, Websit...   \n",
       "4   Large Language Models, Natural Language Proces...   \n",
       "5        VVC, Intra Prediction, ISP, Machine Learning   \n",
       "6   Medication Ontologies, Drug Management, Semant...   \n",
       "7   Natural Language Processing, Entity Linking, L...   \n",
       "8   Authoring, LLMs, NCL, Code Generation, Visual ...   \n",
       "9   fall detection, wearable technologies, acceler...   \n",
       "10  African American English, AAE, Bias, Toxicity,...   \n",
       "11  Interoperability, Internet of Things, Interope...   \n",
       "12  deep fake detection, self-supervised, vision t...   \n",
       "13  2022 Brazilian Elections, Network Modeling, On...   \n",
       "14  Computer Vision, Sign Language Recognition, Ge...   \n",
       "15  attention monitoring, webcam, data analysis, l...   \n",
       "16  usability issues, radiology systems, qualitati...   \n",
       "17  Instagram, alt-text, social media, image class...   \n",
       "18  Music Preferences, Music Genre Networks, Regio...   \n",
       "19  point clouds, machine learning, V-PCC, complex...   \n",
       "20  KBQA, Complex Questions, Entity Recognition, P...   \n",
       "21                  IoMT, IoT, VNF, SFC, DSL, L-PRISM   \n",
       "22  embeddings, intrinsic evaluation, qualitative ...   \n",
       "23  Redes sociais, COVID-19, pandemia, saúde menta...   \n",
       "24  Mídias Sociais, Disseminação de Informação, Mo...   \n",
       "25  Middleware, Distribuição de Vídeo, Edge Comput...   \n",
       "26  Sistemas de Recomendação, Explicações, Embeddi...   \n",
       "27  Classificação de Comportamento; Doenças Crônic...   \n",
       "28  Pensamento Computacional, Jogos Digitais Sério...   \n",
       "29  Publicidade Infantil, Criança, TikTok, Rede So...   \n",
       "30  jogos, videogames, Metacritic, review bombing,...   \n",
       "31  Classificação de Texto, Transformers, Undersam...   \n",
       "32  internet das coisas, monitoramento remoto de s...   \n",
       "33  Realidade Virtual, Jogos Sérios, Multimídia, R...   \n",
       "34  Computação Afetiva, Respostas Emocionais, Mode...   \n",
       "35  Robôs sociais, Framework para controle de robô...   \n",
       "36  Sistemas de Recomendação, Viés de Popularidade...   \n",
       "37  Análise de Sentimentos, Comunidades do Reddit,...   \n",
       "38  Tradução Automática Neural, Aumento de Dados, ...   \n",
       "39  Cigarros Eletrônicos, Aprendizado de Máquina, ...   \n",
       "\n",
       "                                          referencias  \\\n",
       "0   [[1] Mauricio Alvarez, David Luengo, and Neil ...   \n",
       "1   [[1] A. A. Abdelhamid, S. Alotaibi, and A. Mou...   \n",
       "2   [[1] Aayushi Agarwal, Akshay Agarwal, Sayan Si...   \n",
       "3   [[1] Leandro Araújo, Luiz Felipe Nery, Isadora...   \n",
       "4   [[1] Rohan Anil, Sebastian Borgeaud, Yonghui W...   \n",
       "5   [[1] James Bergstra and Yoshua Bengio. 2012. R...   \n",
       "6   [[1] Burhan Ud Din Abbasi, Iram Fatima, Hamid ...   \n",
       "7   [[1] Moonis Ali and Savvas Zannettou. 2024. Fr...   \n",
       "8   [[1] NBR ABNT. [n. d.]. Digital Terrestrial Te...   \n",
       "9   [[1] Ibukun Awolusi, Eric Marks, and Matthew H...   \n",
       "10  [[1] Abubakar Abid, Maheen Farooqi, and James ...   \n",
       "11  [[1] Home Assistant. 2024. Awaken your home. h...   \n",
       "12  [[1] Redha Ali, Russell C. Hardie, Barath Nara...   \n",
       "13  [[1] Peiman Barnaghi, Parsa Ghaffari, and John...   \n",
       "14  [[1] Sunusi Bala Abdullahi and Kosin Chamnongt...   \n",
       "15  [[1] and the ACM Digital Library, [2], due to ...   \n",
       "16  [[1] Reza Abbasi, Monireh Sadeqi Jabali, Reza ...   \n",
       "17  [[1] Rakesh Agrawal, Ramakrishnan Srikant, et ...   \n",
       "18  [[1] Moonis Ali and Savvas Zannettou. 2024. Fr...   \n",
       "19  [[1] Gisle Bjontegaard. 2001. Calculation of a...   \n",
       "20  [[1] Shuang Chen, Qian Liu, Zhiwei Yu, Chin-Ye...   \n",
       "21  [[1] ETSI GS NFV-IFA 011. 2023. Network Functi...   \n",
       "22  [[1] Gediminas Adomavicius and Alexander Tuzhi...   \n",
       "23  [[1] Elia Abi-Jaoude, Karline Treurnicht Naylo...   \n",
       "24  [[1] Marcelo MR Araujo, Carlos HG Ferreira, Ju...   \n",
       "25  [[1] Ganesh Ananthanarayanan, Paramvir Bahl, P...   \n",
       "26  [[1] Ali, M., Berrendorf, M., Hoyt, C.T., Verm...   \n",
       "27  [[1] Rodrigo Simon Bavaresco and Jorge Luis Vi...   \n",
       "28  [[1] AAIDD. 2021. Defining Criteria for Intell...   \n",
       "29  [[1] Instituto Alana. 2022. Notificação enviad...   \n",
       "30  [[1] 2022. O que é ’woke’ e por que o termo ge...   \n",
       "31  [[1] Lasse F Wolff Anthony, Benjamin Kanding, ...   \n",
       "32  [[1] Zahra Ahmadi, Mostafa Haghi Kashani, Moha...   \n",
       "33  [[1] Alberto Luiz Aramaki, Rosana Ferreira Sam...   \n",
       "34  [[1] Bert Bakker, Gijs Schumacher, Kevin Arcen...   \n",
       "35  [[1] Michel Albonico, Milica Ðorđević, Engel H...   \n",
       "36  [[1] Himan Abdollahpouri, Masoud Mansoury, Rob...   \n",
       "37  [[1] Rafael J. A. Almeida. 2018. LeIA - Léxico...   \n",
       "38  [[1] T. M. U. Araújo. 2012. Uma solução para g...   \n",
       "39  [[1] Shishir Adhikari, Akshay Uppal, Robin Mer...   \n",
       "\n",
       "                                                 text  \\\n",
       "0   Introdução O reconhecimento das emoções é uma ...   \n",
       "1   Introdução O teste automatizado de aplicativos...   \n",
       "2   Introdução A compactação de vídeo desempenha u...   \n",
       "3   Introdução Nos últimos tempos, a sociedade enf...   \n",
       "4   INTRODUCTION  Many recent studies have focused...   \n",
       "5   Introdução Os vídeos digitais têm sido fundame...   \n",
       "6   Introdução O gerenciamento de medicamentos é u...   \n",
       "7   Introdução O volume crescente de dados publica...   \n",
       "8   Introdução Aplicativos de TV digital (DTV) [26...   \n",
       "9   Introdução ao incentivo à inovação e tecnologi...   \n",
       "10  Introdução nas últimas décadas, testemunhamos ...   \n",
       "11  A tecnologia de introdução transformou signifi...   \n",
       "12  Introdução Nos últimos anos, houve um foco cre...   \n",
       "13  Introdução As plataformas de mídia social on -...   \n",
       "14  Introdução O reconhecimento brasileiro de ling...   \n",
       "15  Introdução O avanço das tecnologias de informa...   \n",
       "16  Introdução Os sistemas de informação em saúde ...   \n",
       "17  Introdução A Web se tornou parte integrante da...   \n",
       "18  Introdução Na era digital, os serviços de stre...   \n",
       "19  As nuvens de ponto de introdução podem ser usa...   \n",
       "20  Introdução O grande volume de dados disponívei...   \n",
       "21  Introdução A virtualização é um conceito que a...   \n",
       "22  Introdução Os sistemas de recomendação são fer...   \n",
       "23  INTRODUÇÃO Nos últimos anos, o panorama da com...   \n",
       "24  INTRODUÇÃO As plataformas de mídias sociais re...   \n",
       "25  INTRODUÇÃO Nos últimos anos, a importância de ...   \n",
       "26  INTRODUÇÃO Sistemas de Recomendação (SsR) são ...   \n",
       "27  INTRODUÇÃO A Organização Mundial de Saúde (OMS...   \n",
       "28  INTRODUÇÃO O Pensamento Computacional (PC) é d...   \n",
       "29  INTRODUCTION No cenário contemporâneo, as rede...   \n",
       "30  INTRODUÇÃO Atualmente, plataformas digitais de...   \n",
       "31  INTRODUÇÃO Classificação Automática de Texto (...   \n",
       "32  INTRODUÇÃO O crescimento exponencial dos siste...   \n",
       "33  Um framework de rastreamento corporal para rea...   \n",
       "34  INTRODUÇÃO Num contexto em que as emoções huma...   \n",
       "35  INTRODUÇÃO A adoção de novas tecnologias na ed...   \n",
       "36  INTRODUÇÃO Os sistemas de recomendação são pro...   \n",
       "37  INTRODUÇÃO As redes sociais têm rompido barrei...   \n",
       "38  INTRODUÇÃO A evolução da tecnologia tem desemp...   \n",
       "39  INTRODUÇÃO A ascensão de plataformas de vídeo,...   \n",
       "\n",
       "                                    artigo_tokenizado  \\\n",
       "0   [Introdução, O, reconhecimento, das, emoções, ...   \n",
       "1   [Introdução, O, teste, automatizado, de, aplic...   \n",
       "2   [Introdução, A, compactação, de, vídeo, desemp...   \n",
       "3   [Introdução, Nos, últimos, tempos, ,, a, socie...   \n",
       "4   [9/02/2008, às, 19:20:05, Molho, de, macarrão,...   \n",
       "5   [Introdução, Os, vídeos, digitais, têm, sido, ...   \n",
       "6   [Introdução, O, gerenciamento, de, medicamento...   \n",
       "7   [Introdução, O, volume, crescente, de, dados, ...   \n",
       "8   [Introdução, Aplicativos, de, TV, digital, (, ...   \n",
       "9   [Introdução, ao, incentivo, à, inovação, e, te...   \n",
       "10  [Introdução, nas, últimas, décadas, ,, testemu...   \n",
       "11  [A, tecnologia, de, introdução, transformou, s...   \n",
       "12  [Introdução, Nos, últimos, anos, ,, houve, um,...   \n",
       "13  [Introdução, As, plataformas, de, mídia, socia...   \n",
       "14  [Introdução, O, reconhecimento, brasileiro, de...   \n",
       "15  [Introdução, O, avanço, das, tecnologias, de, ...   \n",
       "16  [Introdução, Os, sistemas, de, informação, em,...   \n",
       "17  [Introdução, A, Web, se, tornou, parte, integr...   \n",
       "18  [Introdução, Na, era, digital, ,, os, serviços...   \n",
       "19  [As, nuvens, de, ponto, de, introdução, podem,...   \n",
       "20  [Introdução, O, grande, volume, de, dados, dis...   \n",
       "21  [Introdução, A, virtualização, é, um, conceito...   \n",
       "22  [Introdução, Os, sistemas, de, recomendação, s...   \n",
       "23  [INTRODUÇÃO, Nos, últimos, anos, ,, o, panoram...   \n",
       "24  [INTRODUÇÃO, As, plataformas, de, mídias, soci...   \n",
       "25  [INTRODUÇÃO, Nos, últimos, anos, ,, a, importâ...   \n",
       "26  [INTRODUÇÃO, Sistemas, de, Recomendação, (, Ss...   \n",
       "27  [INTRODUÇÃO, A, Organização, Mundial, de, Saúd...   \n",
       "28  [INTRODUÇÃO, O, Pensamento, Computacional, (, ...   \n",
       "29  [INTRODUCTION, No, cenário, contemporâneo, ,, ...   \n",
       "30  [INTRODUÇÃO, Atualmente, ,, plataformas, digit...   \n",
       "31  [INTRODUÇÃO, Classificação, Automática, de, Te...   \n",
       "32  [INTRODUÇÃO, O, crescimento, exponencial, dos,...   \n",
       "33  [Um, framework, de, rastreamento, corporal, pa...   \n",
       "34  [INTRODUÇÃO, Num, contexto, em, que, as, emoçõ...   \n",
       "35  [INTRODUÇÃO, A, adoção, de, novas, tecnologias...   \n",
       "36  [INTRODUÇÃO, Os, sistemas, de, recomendação, s...   \n",
       "37  [INTRODUÇÃO, As, redes, sociais, têm, rompido,...   \n",
       "38  [INTRODUÇÃO, A, evolução, da, tecnologia, tem,...   \n",
       "39  [INTRODUÇÃO, A, ascensão, de, plataformas, de,...   \n",
       "\n",
       "                                           pos_tagger  \\\n",
       "0   [[Introdução, NOUN, NOUN], [O, DET, DET], [rec...   \n",
       "1   [[Introdução, NOUN, NOUN], [O, DET, DET], [tes...   \n",
       "2   [[Introdução, NOUN, NOUN], [A, DET, DET], [com...   \n",
       "3   [[Introdução, NOUN, NOUN], [Nos, ADP, ADP], [ú...   \n",
       "4   [[9/02/2008, NUM, NUM], [às, ADP, ADP], [19:20...   \n",
       "5   [[Introdução, NOUN, NOUN], [Os, DET, DET], [ví...   \n",
       "6   [[Introdução, NOUN, NOUN], [O, DET, DET], [ger...   \n",
       "7   [[Introdução, NOUN, NOUN], [O, DET, DET], [vol...   \n",
       "8   [[Introdução, PROPN, PROPN], [Aplicativos, PRO...   \n",
       "9   [[Introdução, NOUN, NOUN], [ao, ADP, ADP], [in...   \n",
       "10  [[Introdução, NOUN, NOUN], [nas, ADP, ADP], [ú...   \n",
       "11  [[A, DET, DET], [tecnologia, NOUN, NOUN], [de,...   \n",
       "12  [[Introdução, NOUN, NOUN], [Nos, ADP, ADP], [ú...   \n",
       "13  [[Introdução, PROPN, PROPN], [As, DET, DET], [...   \n",
       "14  [[Introdução, NOUN, NOUN], [O, DET, DET], [rec...   \n",
       "15  [[Introdução, NOUN, NOUN], [O, DET, DET], [ava...   \n",
       "16  [[Introdução, NOUN, NOUN], [Os, DET, DET], [si...   \n",
       "17  [[Introdução, NOUN, NOUN], [A, DET, DET], [Web...   \n",
       "18  [[Introdução, NOUN, NOUN], [Na, ADP, ADP], [er...   \n",
       "19  [[As, DET, DET], [nuvens, NOUN, NOUN], [de, AD...   \n",
       "20  [[Introdução, NOUN, NOUN], [O, DET, DET], [gra...   \n",
       "21  [[Introdução, NOUN, NOUN], [A, DET, DET], [vir...   \n",
       "22  [[Introdução, NOUN, NOUN], [Os, DET, DET], [si...   \n",
       "23  [[INTRODUÇÃO, PROPN, PROPN], [Nos, ADP, ADP], ...   \n",
       "24  [[INTRODUÇÃO, PROPN, PROPN], [As, DET, DET], [...   \n",
       "25  [[INTRODUÇÃO, PROPN, PROPN], [Nos, ADP, ADP], ...   \n",
       "26  [[INTRODUÇÃO, PROPN, PROPN], [Sistemas, PROPN,...   \n",
       "27  [[INTRODUÇÃO, PROPN, PROPN], [A, DET, DET], [O...   \n",
       "28  [[INTRODUÇÃO, PROPN, PROPN], [O, DET, DET], [P...   \n",
       "29  [[INTRODUCTION, PROPN, PROPN], [No, ADP, ADP],...   \n",
       "30  [[INTRODUÇÃO, PROPN, PROPN], [Atualmente, ADV,...   \n",
       "31  [[INTRODUÇÃO, PROPN, PROPN], [Classificação, P...   \n",
       "32  [[INTRODUÇÃO, PROPN, PROPN], [O, DET, DET], [c...   \n",
       "33  [[Um, DET, DET], [framework, PROPN, PROPN], [d...   \n",
       "34  [[INTRODUÇÃO, PROPN, PROPN], [Num, ADP, ADP], ...   \n",
       "35  [[INTRODUÇÃO, PROPN, PROPN], [A, DET, DET], [a...   \n",
       "36  [[INTRODUÇÃO, PROPN, PROPN], [Os, DET, DET], [...   \n",
       "37  [[INTRODUÇÃO, PROPN, PROPN], [As, DET, DET], [...   \n",
       "38  [[INTRODUÇÃO, PROPN, PROPN], [A, DET, DET], [e...   \n",
       "39  [[INTRODUÇÃO, PROPN, PROPN], [A, DET, DET], [a...   \n",
       "\n",
       "                                                 lema  \n",
       "0   [introdução, o, reconhecimento, de o, emoção, ...  \n",
       "1   [introdução, o, teste, automatizado, de, aplic...  \n",
       "2   [introdução, o, compactação, de, vídeo, desemp...  \n",
       "3   [introdução, em o, último, tempo, o, sociedade...  \n",
       "4   [9/02/2008, a o, 19:20:05, Molho, de, macarrão...  \n",
       "5   [introdução, o, vídeo, digital, ter, ser, fund...  \n",
       "6   [introdução, o, gerenciamento, de, medicamento...  \n",
       "7   [introdução, o, volume, crescente, de, dado, p...  \n",
       "8   [Introdução, Aplicativos, de, tv, digital, DTV...  \n",
       "9   [introdução, a o, incentivo, a o, inovação, e,...  \n",
       "10  [introdução, em o, último, década, testemunham...  \n",
       "11  [o, tecnologia, de, introdução, transformar, s...  \n",
       "12  [introdução, em o, último, ano, haver, um, foc...  \n",
       "13  [introdução, o, plataforma, de, mídia, social,...  \n",
       "14  [introdução, o, reconhecimento, brasileiro, de...  \n",
       "15  [introdução, o, avanço, de o, tecnologia, de, ...  \n",
       "16  [introdução, o, sistema, de, informação, em, s...  \n",
       "17  [introdução, o, Web, se, tornar, parte, integr...  \n",
       "18  [introdução, em o, ser, digital, o, serviço, d...  \n",
       "19  [o, nuvem, de, ponto, de, introdução, poder, s...  \n",
       "20  [introdução, o, grande, volume, de, dado, disp...  \n",
       "21  [introdução, o, virtualização, ser, um, concei...  \n",
       "22  [introdução, o, sistema, de, recomendação, ser...  \n",
       "23  [INTRODUÇÃO, em o, último, ano, o, panorama, d...  \n",
       "24  [INTRODUÇÃO, o, plataforma, de, mídia, social,...  \n",
       "25  [INTRODUÇÃO, em o, último, ano, o, importância...  \n",
       "26  [INTRODUÇÃO, Sistemas, de, Recomendação, SsR, ...  \n",
       "27  [INTRODUÇÃO, o, Organização, Mundial, de, Saúd...  \n",
       "28  [INTRODUÇÃO, o, Pensamento, Computacional, PC,...  \n",
       "29  [INTRODUCTION, em o, cenário, contemporâneo, o...  \n",
       "30  [INTRODUÇÃO, Atualmente, plataforma, digital, ...  \n",
       "31  [INTRODUÇÃO, Classificação, Automática, de, Te...  \n",
       "32  [INTRODUÇÃO, o, crescimento, exponencial, de o...  \n",
       "33  [um, framework, de, rastreamento, corporal, pa...  \n",
       "34  [INTRODUÇÃO, em um, contexto, em, que, o, emoç...  \n",
       "35  [INTRODUÇÃO, o, adoção, de, novo, tecnologia, ...  \n",
       "36  [INTRODUÇÃO, o, sistema, de, recomendação, ser...  \n",
       "37  [INTRODUÇÃO, o, rede, social, ter, rompir, bar...  \n",
       "38  [INTRODUÇÃO, o, evolução, de o, tecnologia, te...  \n",
       "39  [INTRODUÇÃO, o, ascensão, de, plataforma, de, ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titulo</th>\n",
       "      <th>informacoes_url</th>\n",
       "      <th>idioma</th>\n",
       "      <th>storage_key</th>\n",
       "      <th>autores</th>\n",
       "      <th>data_publicacao</th>\n",
       "      <th>resumo</th>\n",
       "      <th>keywords</th>\n",
       "      <th>referencias</th>\n",
       "      <th>text</th>\n",
       "      <th>artigo_tokenizado</th>\n",
       "      <th>pos_tagger</th>\n",
       "      <th>lema</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Recognition of Emotions through Facial Geometr...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24766-1-10-20...</td>\n",
       "      <td>[Alessandra Alaniz Macedo,  Leandro Persona,  ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O reconhecimento de emoção tem um significado ...</td>\n",
       "      <td>Multimedia Processing, Affective Computing, Ma...</td>\n",
       "      <td>[[1] Mauricio Alvarez, David Luengo, and Neil ...</td>\n",
       "      <td>Introdução O reconhecimento das emoções é uma ...</td>\n",
       "      <td>[Introdução, O, reconhecimento, das, emoções, ...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [O, DET, DET], [rec...</td>\n",
       "      <td>[introdução, o, reconhecimento, de o, emoção, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Enhancing widget recognition for automated And...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24752-1-10-20...</td>\n",
       "      <td>[Yadini Pérez López,  Laís Dib Albuquerque,  G...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O reconhecimento de widgets é crucial para tes...</td>\n",
       "      <td>Automated Android Testing, Test Portability, O...</td>\n",
       "      <td>[[1] A. A. Abdelhamid, S. Alotaibi, and A. Mou...</td>\n",
       "      <td>Introdução O teste automatizado de aplicativos...</td>\n",
       "      <td>[Introdução, O, teste, automatizado, de, aplic...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [O, DET, DET], [tes...</td>\n",
       "      <td>[introdução, o, teste, automatizado, de, aplic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Multi-Domain Spatio-Temporal Deformable Fusion...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24762-1-10-20...</td>\n",
       "      <td>[Garibaldi da Silveira Júnior, Gilberto Kreisl...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A compactação de vídeo com perda de imóvel int...</td>\n",
       "      <td>Redes neurais profundas, Melhoria de qualidade...</td>\n",
       "      <td>[[1] Aayushi Agarwal, Akshay Agarwal, Sayan Si...</td>\n",
       "      <td>Introdução A compactação de vídeo desempenha u...</td>\n",
       "      <td>[Introdução, A, compactação, de, vídeo, desemp...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [A, DET, DET], [com...</td>\n",
       "      <td>[introdução, o, compactação, de, vídeo, desemp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Finding Fake News Websites in the Wild</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24756-1-10-20...</td>\n",
       "      <td>[Leandro Araujo,  João M. M. Couto,  Luiz Feli...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A batalha contra a disseminação de informações...</td>\n",
       "      <td>Fake News, Misinformation, Credibility, Websit...</td>\n",
       "      <td>[[1] Leandro Araújo, Luiz Felipe Nery, Isadora...</td>\n",
       "      <td>Introdução Nos últimos tempos, a sociedade enf...</td>\n",
       "      <td>[Introdução, Nos, últimos, tempos, ,, a, socie...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [Nos, ADP, ADP], [ú...</td>\n",
       "      <td>[introdução, em o, último, tempo, o, sociedade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tagging Enriched Bank Transactions Using LLM-G...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24767-1-10-20...</td>\n",
       "      <td>[Daniel de S. Moraes,  Polyana B. da Costa,  P...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>This work presents an unsupervised method for ...</td>\n",
       "      <td>Large Language Models, Natural Language Proces...</td>\n",
       "      <td>[[1] Rohan Anil, Sebastian Borgeaud, Yonghui W...</td>\n",
       "      <td>INTRODUCTION  Many recent studies have focused...</td>\n",
       "      <td>[9/02/2008, às, 19:20:05, Molho, de, macarrão,...</td>\n",
       "      <td>[[9/02/2008, NUM, NUM], [às, ADP, ADP], [19:20...</td>\n",
       "      <td>[9/02/2008, a o, 19:20:05, Molho, de, macarrão...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fast ISP Mode Decision for the Versatile Video...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24755-1-10-20...</td>\n",
       "      <td>[Larissa Araújo,  Adson Duarte,  Bruno Zatt,  ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O padrão versátil de codificação de vídeo (VVC...</td>\n",
       "      <td>VVC, Intra Prediction, ISP, Machine Learning</td>\n",
       "      <td>[[1] James Bergstra and Yoshua Bengio. 2012. R...</td>\n",
       "      <td>Introdução Os vídeos digitais têm sido fundame...</td>\n",
       "      <td>[Introdução, Os, vídeos, digitais, têm, sido, ...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [Os, DET, DET], [ví...</td>\n",
       "      <td>[introdução, o, vídeo, digital, ter, ser, fund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OntoDrug: Enhancing Brazilian Health System In...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24764-1-10-20...</td>\n",
       "      <td>[Nelson Miranda,  Matheus Matos Machado,   Dil...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Este artigo apresenta ontodrug, uma ontologia ...</td>\n",
       "      <td>Medication Ontologies, Drug Management, Semant...</td>\n",
       "      <td>[[1] Burhan Ud Din Abbasi, Iram Fatima, Hamid ...</td>\n",
       "      <td>Introdução O gerenciamento de medicamentos é u...</td>\n",
       "      <td>[Introdução, O, gerenciamento, de, medicamento...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [O, DET, DET], [ger...</td>\n",
       "      <td>[introdução, o, gerenciamento, de, medicamento...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>E-BELA: Enhanced Embedding-Based Entity Linkin...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24750-1-10-20...</td>\n",
       "      <td>[Ítalo M. Pereira  Anderson A. Ferreira]</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Entidade vinculando é o processo de conectar m...</td>\n",
       "      <td>Natural Language Processing, Entity Linking, L...</td>\n",
       "      <td>[[1] Moonis Ali and Savvas Zannettou. 2024. Fr...</td>\n",
       "      <td>Introdução O volume crescente de dados publica...</td>\n",
       "      <td>[Introdução, O, volume, crescente, de, dados, ...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [O, DET, DET], [vol...</td>\n",
       "      <td>[introdução, o, volume, crescente, de, dado, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Exploring Visual and Multimodal Interaction in...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24754-1-10-20...</td>\n",
       "      <td>[Paulo Victor Borges,  Daniel de S. Moraes,  J...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Este artigo apresenta duas ferramentas inovado...</td>\n",
       "      <td>Authoring, LLMs, NCL, Code Generation, Visual ...</td>\n",
       "      <td>[[1] NBR ABNT. [n. d.]. Digital Terrestrial Te...</td>\n",
       "      <td>Introdução Aplicativos de TV digital (DTV) [26...</td>\n",
       "      <td>[Introdução, Aplicativos, de, TV, digital, (, ...</td>\n",
       "      <td>[[Introdução, PROPN, PROPN], [Aplicativos, PRO...</td>\n",
       "      <td>[Introdução, Aplicativos, de, tv, digital, DTV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Elderly Fall Monitoring in Smart Homes Using W...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24751-1-10-20...</td>\n",
       "      <td>[Júlia M. P. Moreira,  Raphael W. Bettio,  And...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O progresso constante da tecnologia, especialm...</td>\n",
       "      <td>fall detection, wearable technologies, acceler...</td>\n",
       "      <td>[[1] Ibukun Awolusi, Eric Marks, and Matthew H...</td>\n",
       "      <td>Introdução ao incentivo à inovação e tecnologi...</td>\n",
       "      <td>[Introdução, ao, incentivo, à, inovação, e, te...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [ao, ADP, ADP], [in...</td>\n",
       "      <td>[introdução, a o, incentivo, a o, inovação, e,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A Comprehensive View of the Biases of Toxicity...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24737-1-10-20...</td>\n",
       "      <td>[Guilherme Andrade,  Luiz Nery,  Fabrício Bene...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A linguagem é um aspecto dinâmico de nossa cul...</td>\n",
       "      <td>African American English, AAE, Bias, Toxicity,...</td>\n",
       "      <td>[[1] Abubakar Abid, Maheen Farooqi, and James ...</td>\n",
       "      <td>Introdução nas últimas décadas, testemunhamos ...</td>\n",
       "      <td>[Introdução, nas, últimas, décadas, ,, testemu...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [nas, ADP, ADP], [ú...</td>\n",
       "      <td>[introdução, em o, último, década, testemunham...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Interoperability Testing Guide for the Interne...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24758-1-10-20...</td>\n",
       "      <td>[Karina da Silva Castelo Branco,  Valéria Lell...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A Internet das Coisas (IoT) expandiu a Interne...</td>\n",
       "      <td>Interoperability, Internet of Things, Interope...</td>\n",
       "      <td>[[1] Home Assistant. 2024. Awaken your home. h...</td>\n",
       "      <td>A tecnologia de introdução transformou signifi...</td>\n",
       "      <td>[A, tecnologia, de, introdução, transformou, s...</td>\n",
       "      <td>[[A, DET, DET], [tecnologia, NOUN, NOUN], [de,...</td>\n",
       "      <td>[o, tecnologia, de, introdução, transformar, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>An Ensemble Approach to Facial Deepfake Detect...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24741-1-10-20...</td>\n",
       "      <td>[Yan Martins B. Gurevitz Cunha,  José Matheus ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Esforços substanciais foram dedicados ao desen...</td>\n",
       "      <td>deep fake detection, self-supervised, vision t...</td>\n",
       "      <td>[[1] Redha Ali, Russell C. Hardie, Barath Nara...</td>\n",
       "      <td>Introdução Nos últimos anos, houve um foco cre...</td>\n",
       "      <td>[Introdução, Nos, últimos, anos, ,, houve, um,...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [Nos, ADP, ADP], [ú...</td>\n",
       "      <td>[introdução, em o, último, ano, haver, um, foc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Twitter and the 2022 Brazilian Elections Portr...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24769-1-10-20...</td>\n",
       "      <td>[Larissa Malagoli,  Giovana Piorino,  Carlos H...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A influência das redes sociais on -line nas aç...</td>\n",
       "      <td>2022 Brazilian Elections, Network Modeling, On...</td>\n",
       "      <td>[[1] Peiman Barnaghi, Parsa Ghaffari, and John...</td>\n",
       "      <td>Introdução As plataformas de mídia social on -...</td>\n",
       "      <td>[Introdução, As, plataformas, de, mídia, socia...</td>\n",
       "      <td>[[Introdução, PROPN, PROPN], [As, DET, DET], [...</td>\n",
       "      <td>[introdução, o, plataforma, de, mídia, social,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Automatic Time-aware Recognition of Brazilian ...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24745-1-10-20...</td>\n",
       "      <td>[Lucas de S. Arcanjo,  Lucas F. Coelho,  Silvi...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A linguagem de sinais brasileiros (Libras) é u...</td>\n",
       "      <td>Computer Vision, Sign Language Recognition, Ge...</td>\n",
       "      <td>[[1] Sunusi Bala Abdullahi and Kosin Chamnongt...</td>\n",
       "      <td>Introdução O reconhecimento brasileiro de ling...</td>\n",
       "      <td>[Introdução, O, reconhecimento, brasileiro, de...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [O, DET, DET], [rec...</td>\n",
       "      <td>[introdução, o, reconhecimento, brasileiro, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Investigating User's Attentional Focus in Comp...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24759-1-10-20...</td>\n",
       "      <td>[Cassiano da Silva Souza,  Milene Selbach Silv...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Manter o foco atencional do usuário se tornou ...</td>\n",
       "      <td>attention monitoring, webcam, data analysis, l...</td>\n",
       "      <td>[[1] and the ACM Digital Library, [2], due to ...</td>\n",
       "      <td>Introdução O avanço das tecnologias de informa...</td>\n",
       "      <td>[Introdução, O, avanço, das, tecnologias, de, ...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [O, DET, DET], [ava...</td>\n",
       "      <td>[introdução, o, avanço, de o, tecnologia, de, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Acceptance and Usability of Complex Medical Sy...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24740-1-10-20...</td>\n",
       "      <td>[Fábio Ap. Cândido da Silva,  André Pimenta Fr...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A crescente demanda por testes de imagem torno...</td>\n",
       "      <td>usability issues, radiology systems, qualitati...</td>\n",
       "      <td>[[1] Reza Abbasi, Monireh Sadeqi Jabali, Reza ...</td>\n",
       "      <td>Introdução Os sistemas de informação em saúde ...</td>\n",
       "      <td>[Introdução, Os, sistemas, de, informação, em,...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [Os, DET, DET], [si...</td>\n",
       "      <td>[introdução, o, sistema, de, informação, em, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Through the Eyes of Instagram: Analyzing Image...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24768-1-10-20...</td>\n",
       "      <td>[João Francisco Hecksher Olivetti  Philipe de ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A comunicação multimídia se tornou uma parte e...</td>\n",
       "      <td>Instagram, alt-text, social media, image class...</td>\n",
       "      <td>[[1] Rakesh Agrawal, Ramakrishnan Srikant, et ...</td>\n",
       "      <td>Introdução A Web se tornou parte integrante da...</td>\n",
       "      <td>[Introdução, A, Web, se, tornou, parte, integr...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [A, DET, DET], [Web...</td>\n",
       "      <td>[introdução, o, Web, se, tornar, parte, integr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Characterization of the Brazilian musical land...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24746-1-10-20...</td>\n",
       "      <td>[Filipe A. S. Moura,  Carlos H. G. Ferreira,  ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Na era digital, serviços de streaming como o S...</td>\n",
       "      <td>Music Preferences, Music Genre Networks, Regio...</td>\n",
       "      <td>[[1] Moonis Ali and Savvas Zannettou. 2024. Fr...</td>\n",
       "      <td>Introdução Na era digital, os serviços de stre...</td>\n",
       "      <td>[Introdução, Na, era, digital, ,, os, serviços...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [Na, ADP, ADP], [er...</td>\n",
       "      <td>[introdução, em o, ser, digital, o, serviço, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>A Machine-Learning-Driven Fast Video-based Poi...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24739-1-10-20...</td>\n",
       "      <td>[Gustavo Rehbein,  Eduardo Costa,  Guilherme C...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Nos últimos anos, o conteúdo da nuvem de ponto...</td>\n",
       "      <td>point clouds, machine learning, V-PCC, complex...</td>\n",
       "      <td>[[1] Gisle Bjontegaard. 2001. Calculation of a...</td>\n",
       "      <td>As nuvens de ponto de introdução podem ser usa...</td>\n",
       "      <td>[As, nuvens, de, ponto, de, introdução, podem,...</td>\n",
       "      <td>[[As, DET, DET], [nuvens, NOUN, NOUN], [de, AD...</td>\n",
       "      <td>[o, nuvem, de, ponto, de, introdução, poder, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Constructing a KBQA Framework: Design and Impl...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24747-1-10-20...</td>\n",
       "      <td>[Rômulo Chrispim de Mello,  Jorão Gomes Jr.,  ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O crescimento exponencial dos dados na Interne...</td>\n",
       "      <td>KBQA, Complex Questions, Entity Recognition, P...</td>\n",
       "      <td>[[1] Shuang Chen, Qian Liu, Zhiwei Yu, Chin-Ye...</td>\n",
       "      <td>Introdução O grande volume de dados disponívei...</td>\n",
       "      <td>[Introdução, O, grande, volume, de, dados, dis...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [O, DET, DET], [gra...</td>\n",
       "      <td>[introdução, o, grande, volume, de, dado, disp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>A Domain-Specific Language for Multimedia Serv...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24738-1-10-20...</td>\n",
       "      <td>[Franklin Jordan Ventura Quico,  Anselmo L. E....</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A virtualização é uma tecnologia amplamente us...</td>\n",
       "      <td>IoMT, IoT, VNF, SFC, DSL, L-PRISM</td>\n",
       "      <td>[[1] ETSI GS NFV-IFA 011. 2023. Network Functi...</td>\n",
       "      <td>Introdução A virtualização é um conceito que a...</td>\n",
       "      <td>[Introdução, A, virtualização, é, um, conceito...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [A, DET, DET], [vir...</td>\n",
       "      <td>[introdução, o, virtualização, ser, um, concei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Why Ignore Content? A Guideline for Intrinsic ...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24776-1-10-20...</td>\n",
       "      <td>[Pedro R. Pires,  Bruno B. Rizzi,   Tiago A. A...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Com o crescimento constante das informações di...</td>\n",
       "      <td>embeddings, intrinsic evaluation, qualitative ...</td>\n",
       "      <td>[[1] Gediminas Adomavicius and Alexander Tuzhi...</td>\n",
       "      <td>Introdução Os sistemas de recomendação são fer...</td>\n",
       "      <td>[Introdução, Os, sistemas, de, recomendação, s...</td>\n",
       "      <td>[[Introdução, NOUN, NOUN], [Os, DET, DET], [si...</td>\n",
       "      <td>[introdução, o, sistema, de, recomendação, ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Impacto da Pandemia na Discussão sobre Saúde M...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24757-1-10...</td>\n",
       "      <td>[Pedro Bento,  Arthur Buzelin,  Yan Aquino,  I...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>The period of social isolation due to COVID-19...</td>\n",
       "      <td>Redes sociais, COVID-19, pandemia, saúde menta...</td>\n",
       "      <td>[[1] Elia Abi-Jaoude, Karline Treurnicht Naylo...</td>\n",
       "      <td>INTRODUÇÃO Nos últimos anos, o panorama da com...</td>\n",
       "      <td>[INTRODUÇÃO, Nos, últimos, anos, ,, o, panoram...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [Nos, ADP, ADP], ...</td>\n",
       "      <td>[INTRODUÇÃO, em o, último, ano, o, panorama, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Um Framework para Análise Bidimensional de Dis...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24771-2-10...</td>\n",
       "      <td>[Geovana S. Oliveira,  Otávio Venâncio,  Viníc...</td>\n",
       "      <td>25-09-2024</td>\n",
       "      <td>As plataformas de mídia social revolucionaram ...</td>\n",
       "      <td>Mídias Sociais, Disseminação de Informação, Mo...</td>\n",
       "      <td>[[1] Marcelo MR Araujo, Carlos HG Ferreira, Ju...</td>\n",
       "      <td>INTRODUÇÃO As plataformas de mídias sociais re...</td>\n",
       "      <td>[INTRODUÇÃO, As, plataformas, de, mídias, soci...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [As, DET, DET], [...</td>\n",
       "      <td>[INTRODUÇÃO, o, plataforma, de, mídia, social,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Middleware para Aplicações Distribuídas de Víd...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24761-1-10...</td>\n",
       "      <td>[Otacílio de A. Ramos Neto,  Rafael C. Chaves,...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Dentro do escopo da indústria 4.0, a visão com...</td>\n",
       "      <td>Middleware, Distribuição de Vídeo, Edge Comput...</td>\n",
       "      <td>[[1] Ganesh Ananthanarayanan, Paramvir Bahl, P...</td>\n",
       "      <td>INTRODUÇÃO Nos últimos anos, a importância de ...</td>\n",
       "      <td>[INTRODUÇÃO, Nos, últimos, anos, ,, a, importâ...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [Nos, ADP, ADP], ...</td>\n",
       "      <td>[INTRODUÇÃO, em o, último, ano, o, importância...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>O Impacto de Estratégias de Embeddings de Graf...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24763-2-10...</td>\n",
       "      <td>[André Levi Zanon,  Leonardo Rocha,   Marcelo ...</td>\n",
       "      <td>04-10-2024</td>\n",
       "      <td>Explicações em sistemas de recomendação são es...</td>\n",
       "      <td>Sistemas de Recomendação, Explicações, Embeddi...</td>\n",
       "      <td>[[1] Ali, M., Berrendorf, M., Hoyt, C.T., Verm...</td>\n",
       "      <td>INTRODUÇÃO Sistemas de Recomendação (SsR) são ...</td>\n",
       "      <td>[INTRODUÇÃO, Sistemas, de, Recomendação, (, Ss...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [Sistemas, PROPN,...</td>\n",
       "      <td>[INTRODUÇÃO, Sistemas, de, Recomendação, SsR, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Cuidado Ubíquo de Pacientes com Doenças Crônic...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24749-1-10...</td>\n",
       "      <td>[Lucas Pfeiffer Salomão Dias,  L.P.S. Dias,  J...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>As doenças crônicas estão entre as 7 das 10 pr...</td>\n",
       "      <td>Classificação de Comportamento; Doenças Crônic...</td>\n",
       "      <td>[[1] Rodrigo Simon Bavaresco and Jorge Luis Vi...</td>\n",
       "      <td>INTRODUÇÃO A Organização Mundial de Saúde (OMS...</td>\n",
       "      <td>[INTRODUÇÃO, A, Organização, Mundial, de, Saúd...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [A, DET, DET], [O...</td>\n",
       "      <td>[INTRODUÇÃO, o, Organização, Mundial, de, Saúd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Jogos Digitais Sérios usados para o Exercício ...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24760-1-10...</td>\n",
       "      <td>[Katherin Felipa Carhuaz Malpartida  Kamila Ri...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O pensamento computacional (CT) é um processo ...</td>\n",
       "      <td>Pensamento Computacional, Jogos Digitais Sério...</td>\n",
       "      <td>[[1] AAIDD. 2021. Defining Criteria for Intell...</td>\n",
       "      <td>INTRODUÇÃO O Pensamento Computacional (PC) é d...</td>\n",
       "      <td>[INTRODUÇÃO, O, Pensamento, Computacional, (, ...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [O, DET, DET], [P...</td>\n",
       "      <td>[INTRODUÇÃO, o, Pensamento, Computacional, PC,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Crianças e Propagandas no TikTok: identificand...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24748-1-10...</td>\n",
       "      <td>[Raíssa Gonçalves Lopes Carvalho  Humberto Tor...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Este estudo aborda a identificação da publicid...</td>\n",
       "      <td>Publicidade Infantil, Criança, TikTok, Rede So...</td>\n",
       "      <td>[[1] Instituto Alana. 2022. Notificação enviad...</td>\n",
       "      <td>INTRODUCTION No cenário contemporâneo, as rede...</td>\n",
       "      <td>[INTRODUCTION, No, cenário, contemporâneo, ,, ...</td>\n",
       "      <td>[[INTRODUCTION, PROPN, PROPN], [No, ADP, ADP],...</td>\n",
       "      <td>[INTRODUCTION, em o, cenário, contemporâneo, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Quando as Avaliações Viram Bombas: Explorando ...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24765-1-10...</td>\n",
       "      <td>[Marcus Vinicius Guerra Ribeiro,  Clara Andrad...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Este estudo investiga o fenômeno do \" * revisã...</td>\n",
       "      <td>jogos, videogames, Metacritic, review bombing,...</td>\n",
       "      <td>[[1] 2022. O que é ’woke’ e por que o termo ge...</td>\n",
       "      <td>INTRODUÇÃO Atualmente, plataformas digitais de...</td>\n",
       "      <td>[INTRODUÇÃO, Atualmente, ,, plataformas, digit...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [Atualmente, ADV,...</td>\n",
       "      <td>[INTRODUÇÃO, Atualmente, plataforma, digital, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Estratégias de Undersampling para Redução de V...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24753-2-10...</td>\n",
       "      <td>[Guilherme Fonseca,  Gabriel Prenassi,  Washin...</td>\n",
       "      <td>04-10-2024</td>\n",
       "      <td>Classificação de texto automática (ATC) em con...</td>\n",
       "      <td>Classificação de Texto, Transformers, Undersam...</td>\n",
       "      <td>[[1] Lasse F Wolff Anthony, Benjamin Kanding, ...</td>\n",
       "      <td>INTRODUÇÃO Classificação Automática de Texto (...</td>\n",
       "      <td>[INTRODUÇÃO, Classificação, Automática, de, Te...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [Classificação, P...</td>\n",
       "      <td>[INTRODUÇÃO, Classificação, Automática, de, Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Arquitetura Multicamadas para Coleta e Análise...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24744-1-10...</td>\n",
       "      <td>[Juan Felipe Souza Oliveira,  Paulo Cesar Salg...</td>\n",
       "      <td>19-09-2024</td>\n",
       "      <td>Uma arquitetura multicamada foi desenvolvida p...</td>\n",
       "      <td>internet das coisas, monitoramento remoto de s...</td>\n",
       "      <td>[[1] Zahra Ahmadi, Mostafa Haghi Kashani, Moha...</td>\n",
       "      <td>INTRODUÇÃO O crescimento exponencial dos siste...</td>\n",
       "      <td>[INTRODUÇÃO, O, crescimento, exponencial, dos,...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [O, DET, DET], [c...</td>\n",
       "      <td>[INTRODUÇÃO, o, crescimento, exponencial, de o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Um framework de rastreamento corporal para rea...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24770-1-10...</td>\n",
       "      <td>[Elvis Ribeiro,  Alexandre Brandão,  Marcelo G...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Aplicações e jogos multimídia desempenham um p...</td>\n",
       "      <td>Realidade Virtual, Jogos Sérios, Multimídia, R...</td>\n",
       "      <td>[[1] Alberto Luiz Aramaki, Rosana Ferreira Sam...</td>\n",
       "      <td>Um framework de rastreamento corporal para rea...</td>\n",
       "      <td>[Um, framework, de, rastreamento, corporal, pa...</td>\n",
       "      <td>[[Um, DET, DET], [framework, PROPN, PROPN], [d...</td>\n",
       "      <td>[um, framework, de, rastreamento, corporal, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Únicos, mas não incomparáveis: abordagens para...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24775-1-10...</td>\n",
       "      <td>[Guilherme O. Aguiar,  Juan P. D. Esteves,  Cl...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Entender o comportamento emocional humano é um...</td>\n",
       "      <td>Computação Afetiva, Respostas Emocionais, Mode...</td>\n",
       "      <td>[[1] Bert Bakker, Gijs Schumacher, Kevin Arcen...</td>\n",
       "      <td>INTRODUÇÃO Num contexto em que as emoções huma...</td>\n",
       "      <td>[INTRODUÇÃO, Num, contexto, em, que, as, emoçõ...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [Num, ADP, ADP], ...</td>\n",
       "      <td>[INTRODUÇÃO, em um, contexto, em, que, o, emoç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Uma Proposta de Framework para Sistemas de Con...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24774-1-10...</td>\n",
       "      <td>[Marcelo Rocha,  Jesus Favela,   Débora C. Muc...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Há uma lacuna em plataformas robóticas de códi...</td>\n",
       "      <td>Robôs sociais, Framework para controle de robô...</td>\n",
       "      <td>[[1] Michel Albonico, Milica Ðorđević, Engel H...</td>\n",
       "      <td>INTRODUÇÃO A adoção de novas tecnologias na ed...</td>\n",
       "      <td>[INTRODUÇÃO, A, adoção, de, novas, tecnologias...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [A, DET, DET], [a...</td>\n",
       "      <td>[INTRODUÇÃO, o, adoção, de, novo, tecnologia, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Uma Abordagem em Etapa de Processamento para R...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24772-1-10...</td>\n",
       "      <td>[Rodrigo Ferrari de Souza  Marcelo Garcia Manz...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Os sistemas de recomendação são projetados par...</td>\n",
       "      <td>Sistemas de Recomendação, Viés de Popularidade...</td>\n",
       "      <td>[[1] Himan Abdollahpouri, Masoud Mansoury, Rob...</td>\n",
       "      <td>INTRODUÇÃO Os sistemas de recomendação são pro...</td>\n",
       "      <td>[INTRODUÇÃO, Os, sistemas, de, recomendação, s...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [Os, DET, DET], [...</td>\n",
       "      <td>[INTRODUÇÃO, o, sistema, de, recomendação, ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Análise de sentimentos de conteúdo compartilha...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24743-1-10...</td>\n",
       "      <td>[Giovana Piorino,  Vitor Moreira,  Luiz Henriq...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O uso crescente das mídias sociais e seu impac...</td>\n",
       "      <td>Análise de Sentimentos, Comunidades do Reddit,...</td>\n",
       "      <td>[[1] Rafael J. A. Almeida. 2018. LeIA - Léxico...</td>\n",
       "      <td>INTRODUÇÃO As redes sociais têm rompido barrei...</td>\n",
       "      <td>[INTRODUÇÃO, As, redes, sociais, têm, rompido,...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [As, DET, DET], [...</td>\n",
       "      <td>[INTRODUÇÃO, o, rede, social, ter, rompir, bar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Uma Investigação sobre Técnicas de Data Augmen...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24773-1-10...</td>\n",
       "      <td>[Marcos André Bezerra da Silva,  Manuella Asch...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A tradução automática de português para Libras...</td>\n",
       "      <td>Tradução Automática Neural, Aumento de Dados, ...</td>\n",
       "      <td>[[1] T. M. U. Araújo. 2012. Uma solução para g...</td>\n",
       "      <td>INTRODUÇÃO A evolução da tecnologia tem desemp...</td>\n",
       "      <td>[INTRODUÇÃO, A, evolução, da, tecnologia, tem,...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [A, DET, DET], [e...</td>\n",
       "      <td>[INTRODUÇÃO, o, evolução, de o, tecnologia, te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Análise da Percepção do Uso de Cigarros Eletrô...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24742-1-10...</td>\n",
       "      <td>[Aline Dias,  Richardy R. Tanure,  Jussara M. ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A ascensão de plataformas de vídeo como o YouT...</td>\n",
       "      <td>Cigarros Eletrônicos, Aprendizado de Máquina, ...</td>\n",
       "      <td>[[1] Shishir Adhikari, Akshay Uppal, Robin Mer...</td>\n",
       "      <td>INTRODUÇÃO A ascensão de plataformas de vídeo,...</td>\n",
       "      <td>[INTRODUÇÃO, A, ascensão, de, plataformas, de,...</td>\n",
       "      <td>[[INTRODUÇÃO, PROPN, PROPN], [A, DET, DET], [a...</td>\n",
       "      <td>[INTRODUÇÃO, o, ascensão, de, plataforma, de, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 267
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "aeb37c9d6eae86a1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
