{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-12T22:14:24.069066Z",
     "start_time": "2025-05-12T22:14:24.066347Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from process_text import process_pdfs\n",
    "import pymupdf4llm\n",
    "from PyPDF2 import PdfReader\n",
    "from process_text import clean_full_text"
   ],
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T21:50:25.274546Z",
     "start_time": "2025-05-12T21:50:25.064323Z"
    }
   },
   "cell_type": "code",
   "source": "corpus = pd.read_json('../articles/corpus/corpus.json')",
   "id": "49b93678be0305fb",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:44:08.058110Z",
     "start_time": "2025-05-12T22:44:08.055800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_abs(text: str) -> str:\n",
    "    text = text.replace('**', '')\n",
    "    text = text.replace('*', '')\n",
    "    text = text.replace('###', '')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.strip()\n",
    "    return text"
   ],
   "id": "4c0d0608f90cfee8",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T21:55:05.547147Z",
     "start_time": "2025-05-12T21:55:05.544475Z"
    }
   },
   "cell_type": "code",
   "source": "corpus['resumo_clean'] = corpus['resumo'].apply(lambda x: clean_abs(x))",
   "id": "c125e69a4614c5dd",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:05:20.137161Z",
     "start_time": "2025-05-12T22:05:20.133803Z"
    }
   },
   "cell_type": "code",
   "source": "corpus.loc[13]['referencias']",
   "id": "d02fab3bd0beb57b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[1] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al . 2023. Gemini: a family of highly capable multimodal models.',\n",
       " '[2] David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. Journal of machine Learning research 3, Jan (2003), 993‚Äì1022.',\n",
       " '[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al . 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877‚Äì1901.',\n",
       " '[4] Antonio J. G. Busson, Rafael Rocha, Rennan Gaio, Rafael Miceli, Ivan Pereira, Daniel de S. Moraes, S√©rgio Colcher, Alvaro Veiga, Bruno Rizzi, Francisco Evangelista, Leandro Santos, Fellipe Marques, Marcos Rabaioli, Diego Feldberg, Debora Mattos, Jo√£o Pasqua, and Diogo Dias. 2023. Hierarchical Classification of Financial Transactions Through Context-Fusion of Transformer-based Embeddings and Taxonomy-aware Attention Layer. In Anais do II Brazilian Workshop on Arti- ficial Intelligence in Finance (BWAIF 2023) (BWAIF 2023) . Sociedade Brasileira de Computa√ß√£o. https://doi.org/10.5753/bwaif.2023.229322',\n",
       " '[5] Ricardo Campos, V√≠tor Mangaravite, Arian Pasquali, Al√≠pio Jorge, C√©lia Nunes, and Adam Jatowt. 2020. YAKE! Keyword extraction from single documents using multiple local features. Information Sciences 509 (2020), 257‚Äì289.',\n",
       " '[6] Boqi Chen, Fandi Yi, and D√°niel Varr√≥. 2023. Prompting or Fine-tuning? A Comparative Study of Large Language Models for Taxonomy Construction. In 2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C) . IEEE, 588‚Äì596.',\n",
       " '[7] Sabit Ekin. 2023. Prompt engineering for ChatGPT: a quick guide to techniques, tips, and best practices. Authorea Preprints (2023).',\n",
       " '[8] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021).',\n",
       " '[9] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al . 2024. Mixtral of experts. arXiv preprint arXiv:2401.04088 (2024).',\n",
       " '[10] Dongha Lee, Jiaming Shen, SeongKu Kang, Susik Yoon, Jiawei Han, and Hwanjo Yu. 2022. TaxoCom: Topic Taxonomy Completion with Hierarchical Discovery of Novel Topic Clusters. In Proceedings of the ACM Web Conference 2022 . 2819‚Äì2829.',\n",
       " '[11] Yinheng Li. 2023. A Practical Survey on Zero-Shot Prompt Design for In-Context Learning. In Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing . 641‚Äì647.',\n",
       " '[12] Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. 2023. Unleashing Infinite-Length Input Capacity for Largescale Language Models with Self-Controlled Memory System. arXiv preprint arXiv:2304.13343 (2023).',\n",
       " '[13] Irina Nikishina, Varvara Logacheva, Alexander Panchenko, and Natalia Loukachevitch. 2020. RUSSE‚Äô2020: Findings of the First Taxonomy Enrichment   273   -----  WebMedia‚Äô2024, Juiz de Fora, Brazil Moraes et al.   Task for the Russian language. arXiv preprint arXiv:2005.11176 (2020).',\n",
       " '[14] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]',\n",
       " '[15] Octavian Popescu and Carlo Strapparava. 2015. Semeval 2015, task 7: Diachronic text evaluation. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015) . 870‚Äì878.',\n",
       " '[16] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research 21, 1 (2020), 5485‚Äì5551.',\n",
       " '[17] Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended abstracts of the 2021 CHI conference on human factors in computing systems . 1‚Äì7.',\n",
       " '[18] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and Aman Chadha. 2024. A systematic survey of prompt engineering in large language models: Techniques and applications. arXiv preprint arXiv:2402.07927 (2024).',\n",
       " '[19] Rion Snow, Daniel Jurafsky, and Andrew Ng. 2004. Learning syntactic patterns for automatic hypernym discovery. Advances in neural information processing systems 17 (2004).',\n",
       " '[20] Kunihiro Takeoka, Kosuke Akimoto, and Masafumi Oyamada. 2021. Low-resource taxonomy enrichment with pretrained language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing . 2747‚Äì2758.',\n",
       " '[21] Adrian Tam. [n. d.]. What Are Zero-Shot Prompting and Few-Shot Prompting. https://machinelearningmastery.com/what-are-zero-shot-prompting-andfew-shot-prompting/. Accessed: 2024-07-02.',\n",
       " '[22] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al . 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023).',\n",
       " '[23] Erlend Vollset, Eirik Folkestad, Marius Rise Gallala, and Jon Atle Gulla. 2017. Making use of external company data to improve the classification of bank transactions. In Advanced Data Mining and Applications: 13th International Conference, ADMA 2017, Singapore, November 5‚Äì6, 2017, Proceedings 13 . Springer, 767‚Äì780.',\n",
       " '[24] Hanna Wallach, David Mimno, and Andrew McCallum. 2009. Rethinking LDA: Why priors matter. Advances in neural information processing systems 22 (2009).',\n",
       " '[25] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171 (2022).',\n",
       " '[26] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, H Chi, Quoc V Le, and Denny Zhou. [n. d.]. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. ([n. d.]).',\n",
       " '[27] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting in Language Models. In International Conference on Learning Representations (ICLR) .',\n",
       " '[28] Chao Zhang, Fangbo Tao, Xiusi Chen, Jiaming Shen, Meng Jiang, Brian Sadler, Michelle Vanni, and Jiawei Han. 2018. TaxoGen: Unsupervised Topic Taxonomy Construction by Adaptive Term Embedding and Clustering. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Min- ing (London, United Kingdom) (KDD ‚Äô18) . Association for Computing Machinery, New York, NY, USA, 2701‚Äì2709. https://doi.org/10.1145/3219819.3220064   274   -----']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T21:56:03.456251Z",
     "start_time": "2025-05-12T21:56:03.453128Z"
    }
   },
   "cell_type": "code",
   "source": "corpus.loc[13]['text']",
   "id": "bc132a540f1d11f6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9/02/2008 at 19:20:05 Italian Cuisine Pasta Sauce Pizza Wine |name|macrocategory|microcategory| |---|---|---| |Nona Ristorante|Food|Restaurant| Figure 1: Overview of our method. (Note: Merchant and transaction data are fabricated for demonstration only). ### ABSTRACT This work presents an unsupervised method for tagging banking consumers‚Äô transactions using automatically constructed and expanded topic taxonomies. Initially, we enrich the bank transactions via web scraping to collect relevant descriptions, which are then preprocessed using NLP techniques to generate candidate terms. Topic taxonomies are created using instruction-based fine-tuned LLMs (Large Language Models). To expand existing taxonomies with new terms, we use zero-shot prompting to determine where to add new nodes. The resulting taxonomies are used to assign descriptive tags that characterize the transactions in the retail bank dataset. For evaluation, 12 volunteers completed a two-part form In: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe dia‚Äô2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024. ¬© 2024 SBC ‚Äì Brazilian Computing Society. ISSN 2966-2753 assessing the quality of the taxonomies and the tags assigned to merchants. The evaluation revealed a coherence rate exceeding 90% for the chosen taxonomies. Additionally, taxonomy expansion using LLMs demonstrated promising results for parent node prediction, with F1-scores of 89% and 70% for Food and Shopping taxonomies, respectively. ### KEYWORDS Large Language Models, Natural Language Processing, Web Scrapping, Topic Modeling ###'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T21:57:52.753838Z",
     "start_time": "2025-05-12T21:57:50.542519Z"
    }
   },
   "cell_type": "code",
   "source": "text = pymupdf4llm.to_markdown('../articles/985-24767-1-10-20240923.pdf')",
   "id": "eb3ee17c3bfcd4f9",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:06:25.455209Z",
     "start_time": "2025-05-12T22:06:25.453180Z"
    }
   },
   "cell_type": "code",
   "source": "from process_text import clean_abstract",
   "id": "777bf96d8aa85243",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:20:03.952547Z",
     "start_time": "2025-05-12T22:20:03.950306Z"
    }
   },
   "cell_type": "code",
   "source": "abstract = text.split('**ABSTRACT**')[1].split('KEYWORDS')[0].strip()",
   "id": "2d8743d0c12150ba",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:20:06.618052Z",
     "start_time": "2025-05-12T22:20:06.614961Z"
    }
   },
   "cell_type": "code",
   "source": "abstract",
   "id": "a985819288946be8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This work presents an unsupervised method for tagging banking\\nconsumers‚Äô transactions using automatically constructed and expanded topic taxonomies. Initially, we enrich the bank transactions\\nvia web scraping to collect relevant descriptions, which are then\\npreprocessed using NLP techniques to generate candidate terms.\\nTopic taxonomies are created using instruction-based fine-tuned\\nLLMs (Large Language Models). To expand existing taxonomies\\nwith new terms, we use zero-shot prompting to determine where\\nto add new nodes. The resulting taxonomies are used to assign\\ndescriptive tags that characterize the transactions in the retail bank\\ndataset. For evaluation, 12 volunteers completed a two-part form\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia‚Äô2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n¬© 2024 SBC ‚Äì Brazilian Computing Society.\\nISSN 2966-2753\\n\\n\\nassessing the quality of the taxonomies and the tags assigned to\\nmerchants. The evaluation revealed a coherence rate exceeding 90%\\nfor the chosen taxonomies. Additionally, taxonomy expansion using\\nLLMs demonstrated promising results for parent node prediction,\\nwith F1-scores of 89% and 70% for *Food* and *Shopping* taxonomies,\\nrespectively.\\n### **'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:20:46.846154Z",
     "start_time": "2025-05-12T22:20:46.844092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "abstract = abstract.replace('\\n', ' ')\n",
    "abstract = abstract.replace('In: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMedia‚Äô2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.¬© 2024 SBC ‚Äì Brazilian Computing Society.ISSN 2966-2753','')"
   ],
   "id": "6ce226bad5530de4",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:20:47.538191Z",
     "start_time": "2025-05-12T22:20:47.535506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "abstract = clean_abs(abstract)\n",
    "abstract"
   ],
   "id": "d0b38b45ec71894d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This work presents an unsupervised method for tagging banking consumers‚Äô transactions using automatically constructed and expanded topic taxonomies. Initially, we enrich the bank transactions via web scraping to collect relevant descriptions, which are then preprocessed using NLP techniques to generate candidate terms. Topic taxonomies are created using instruction-based fine-tuned LLMs (Large Language Models). To expand existing taxonomies with new terms, we use zero-shot prompting to determine where to add new nodes. The resulting taxonomies are used to assign descriptive tags that characterize the transactions in the retail bank dataset. For evaluation, 12 volunteers completed a two-part form  In: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe dia‚Äô2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024. ¬© 2024 SBC ‚Äì Brazilian Computing Society. ISSN 2966-2753   assessing the quality of the taxonomies and the tags assigned to merchants. The evaluation revealed a coherence rate exceeding 90% for the chosen taxonomies. Additionally, taxonomy expansion using LLMs demonstrated promising results for parent node prediction, with F1-scores of 89% and 70% for Food and Shopping taxonomies, respectively.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:13:48.624986Z",
     "start_time": "2025-05-12T22:13:48.622679Z"
    }
   },
   "cell_type": "code",
   "source": "text_temp = text.split('INTRODUCTION')[1]",
   "id": "c1180aa4666977e1",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:13:49.152342Z",
     "start_time": "2025-05-12T22:13:49.149925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "pattern = r'REFERENCES|REFER√äNCIAS'\n",
    "text_temp, *_ = re.split(pattern, text_temp, maxsplit=1)\n"
   ],
   "id": "b1bc17820c355872",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:15:21.550809Z",
     "start_time": "2025-05-12T22:15:21.548229Z"
    }
   },
   "cell_type": "code",
   "source": "text_temp = clean_abs(clean_full_text(text_temp.strip()))",
   "id": "cbe63812701aaa30",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:15:22.159117Z",
     "start_time": "2025-05-12T22:15:22.155504Z"
    }
   },
   "cell_type": "code",
   "source": "text_temp",
   "id": "4ecede92c28e2999",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Many recent studies have focused on the application of Machine Learning-based methods for the classification and characterization of financial transactions. For example, Vollset et al . [23] and Busson et al . [4] explored an approach to hierarchically classifying 267 ----- WebMedia‚Äô2024, Juiz de Fora, Brazil Moraes et al. financial transactions, employing a predefined set of categories/subcategories that describe purchase types and transactions. However, these methods apply a limited, predefined set of static classes, which restricts the ability to extend classifications based on user experiences when encountering new, previously undefined categories. In this context, to expand the possible set of classes/tags to label a transaction, we developed an unsupervised method based on topic taxonomies . Taxonomies are very useful in the structural and semantic analyses of topics and textual data. However, creating and maintaining them is often costly and challenging to scale manually. Therefore, recent works have tackled the automatic creation and expansion of topic taxonomies, in which each node in a hierarchy represents a conceptual topic composed of semantically coherent terms. We present an unsupervised method for automatically constructing and expanding topic taxonomies with instruction-based LLMs (Large Language Models), in a Zero-Shot manner. Candidate terms for the initial version of the taxonomy are obtained using topic modeling and keyword extraction techniques. Then we apply LLMs to post-process the resulting terms, create a hierarchy, and add new terms to an existing taxonomy. Since the taxonomies are derived from a corpus of unstructured texts describing niches of consuming habits, we opted to investigate the use of LLMs in our approach. LLMs are often pre-trained on a large corpus of text, allowing them to learn contextual representations that capture the intricacies of human language. We applied our method to a private dataset of transactions of a retail bank, enriched with scraped data from food and shopping companies, and evaluated the resulting taxonomies quantitatively. The generated tags of our topic taxonomies are then assigned to the bank transactions characterizing the companies in each transaction, as shown on Figure 1. In total, 58 topic taxonomies were created for the Food category and 6 for the Shopping category. A two-step quantitative evaluation was conducted on a subset of the taxonomies. For this evaluation, we selected the topic taxonomies with the highest number of terms in each category: \"Brazilian Cuisine\" from Food and \"Clothing and Accessories\" from Shopping . Taxonomies with more terms are most likely to result in a deeper hierarchy, which gives more data for evaluation. We asked 12 volunteers to answer a two-part form, which assessed the quality of the created taxonomies and the quality of the tags assigned to label transactions. The evaluation showed an average coherence of tags to transactions above 90%. As more scraped data from food and shopping companies are added to the retail bank‚Äôs dataset, the topic taxonomies will need to be updated to include new terms. We used LLMs for this task as well, employing commercial LLMs like Gemini Pro [1] and GPT4 [14], alongside open-source LLM options such as LLaMA-Alpaca (7B) [22], Phi-2 [1], and Mixtral 8x7B [9]. We showcase their results in both taxonomy creation and expansion. For the expansion part, we also compared our method to existing ones (a BERT-based method and Musubu[20]) on the SemEval dataset and our generated taxonomies as well. Gemini Pro achieved the best results, with F1scores of 89% and 70% for parent node prediction on the Food and Shopping taxonomies, respectively. 1 https://huggingface.co/microsoft/phi-2 The remainder of the paper is structured as follows: Section 2, reviews the related work, highlighting existing approaches. Section 3 provides the necessary background, laying the foundation for our methodologies and contextualizing our contributions. Section 4 details the dataset construction process, explaining how we enriched and prepared the data for the taxonomies‚Äô construction. In Section 5, we describe the creation of the taxonomies, outlining the methods used to generate them. Section 6 discusses the expansion of the taxonomies, demonstrating how they can be dynamically extended to accommodate new categories. Section 7 focuses on the evaluation of these taxonomies, presenting the metrics and results that validate their accuracy and also the quality of the tags assigned to the transactions. Finally, Section 8 concludes the paper, summarizing our findings, discussing their implications, and suggesting directions for future research.  2 RELATED WORK Taxonomies represent the structure behind a collection of documents, organizing the hierarchical relationships between terms in a tree structure [13]. They play an essential part in the structural and semantic analysis of textual data, providing valuable content for many applications that involve information retrieval and filtering, such as web searching, recommendation systems, classification, and question answering. Since creating and maintaining taxonomies is a costly task, often difficult to scale if done manually, methods that automatically construct and update them are desirable. Early works on automatic taxonomy creation focused on building hypernym-hyponym taxonomies, where each pair of terms expresses an ‚Äòis-a‚Äô relationship [19]. More recent works have tackled the automatic creation of other taxonomies, such as topic taxonomies. In a topic taxonomy, each node represents a conceptual topic composed of semantically coherent terms. In this context, Zhang et al . [28] developed TaxoGen, an unsupervised method for constructing topic taxonomies. Taxogen uses the SkipGram model from an input text corpus to embed all the concept terms into a latent space that captures their semantics. In this space, the authors applied a clustering method to construct a hierarchy recursively based on a variation of the spherical K-means algorithm. Another work that focuses on topic taxonomies is TaxoCom [10], a framework for automatic taxonomy expansion. TaxoCom is a hierarchical topic discovery framework that recursively expands an initial taxonomy by discovering new sub-topics. It uses locally discriminative embeddings and adaptive clustering, resulting in a low-dimensional embedding space that effectively encodes the textual similarity between terms. One main disadvantage of TaxoCom is that it requires a large set of quality phrases in the target language, and curating these phrases can be costly. The quality of the output taxonomy is highly dependent on those phrases. Regarding the automatic expansion of taxonomies, an important related example is Musubu [20], a framework for low-resource taxonomy enrichment that uses a Language Model (LM) as a knowledge base to infer term relationships. For the taxonomy expansion part of out method, we used Musubu as a baseline for comparison. 268 ----- Tagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies WebMedia‚Äô2024, Juiz de Fora, Brazil As to using Large Language Models for taxonomy tasks, Chen et al . [6] investigated how LLMs, like GPT-3, perform in taxonomy construction tasks. The authors compared two approaches: finetuning, which involves training the LLM on a specific dataset to adapt it for taxonomy tasks, and prompt techniques, where the LLM receives instructions and examples to perform a task without being explicitly trained for it. Their findings showed that prompt techniques such as few-shot learning generally outperform finetuning, particularly with smaller datasets. Based on these findings, we applied prompting techniques, specifically zero-shot prompting, across various LLMs to assess their effectiveness in constructing and expanding taxonomies. Section 7 shows the results of our approach, as well as the results of applying Musubu[20] as baseline.  3 BACKGROUND In this section, we provide a comprehensive background on Large Language Models (LLMs), and the concept of Prompt-tuning. These concepts are essential to understanding the construction and editing of taxonomies utilizing LLMs.  3.1 Large Language Models Lately, Large Language Models (LLMs) have garnered significant attention for their exceptional performance in various NLP tasks. LLMs, such as GPT-3[3] and LLAMA[22], are characterized by their massive scale, comprising billions of parameters and being trained on vast amounts of data. These models are often pre-trained in an unsupervised manner on large corpora of textual data, such as books, articles, and web pages, allowing them to learn contextual representations that capture the intricacies of human language. To use LLMs for specific purposes, a highly effective approach is to fine-tune them on task-specific data. Fine-tuning enables LLMs to adapt to specific domains or tasks with minimal labeled data, significantly reducing the need for large annotated datasets. However, in scenarios where labeled data is scarce or difficult to obtain, LLMs can also be used without specific training or additional data, in a Zero-Shot manner [21]. Given the scale of these models and the data they are trained on, LLMs embed vast knowledge that enables them to achieve high generalization capabilities and perform tasks in diverse contexts, even without specific training for those tasks[16]. In our experiments, we tested several types of language models, from private LLMs (GPT 4 [14], Gemini Pro [2] ), to open-source LLMs (Llama 2 [22]), to a Mixture of Experts LLM (Mixtral [9], and a Small Language Model (SLM), Phi 2 [3] .  3.2 Prompt Engineering Prompt Engineering is a fundamental technique used to enhance the performance and adaptability of Large Language Models (LLMs) in specific tasks or domains [7]. It involves optimizing and crafting prompts to efficiently use language models (LMs) [3]. This approach allows researchers and practitioners to tailor the behavior and output of LLMs, making them more suitable for targeted applications. 2 https://deepmind.google/technologies/gemini/pro/ 3 https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-ofsmall-language-models/ Techniques such as Zero-shot prompting, Few-shot prompting, Chain of Thought, ReAct, Self-Consistency etc. have been explored to guide LLMs toward desired responses [18, 21, 25 ‚Äì 27]. The effectiveness of prompt tuning has been demonstrated in various applications, including question-answering, summarization, and dialogue generation. The choice of prompt greatly influences the generated output, and by carefully crafting prompts, researchers can guide the model‚Äôs responses toward desired behaviors. For example, in language translation, a prompt can specify the source language and desired target language to ensure accurate and fluent translations. In our method, we used the Zero-Shot prompting technique.  3.3 Zero-Shot Prompting Since LLMs (Large Language Models) are trained on vast amounts of data, they can follow instructions and perform tasks in contexts where they were not specifically trained, in a Zero-Shot (ZS) manner. This prompting style allows the model to adapt, making it versatile. A Zero-Shot (ZS) prompt directly instructs the model to perform a task without additional examples or demonstrations to guide the LLM‚Äôs response, which is why they are also known as task instructions [21]. In a study by Li [11], the authors highlighted several advantages of using ZS prompts, such as the ability to craft highly interpretable prompts, requiring fewer training data or examples, a more straightforward prompt design process, and a flexible prompt structure. Additionally, Reynolds and McDonell [17] noted that carefully engineered zero-shot prompts can outperform few-shot prompts in certain scenarios, as examples can sometimes be interpreted as part of a narrative rather than as a guiding mechanism. This finding also influenced our decision to use zero-shot prompting in our method.  4 DATASET CONSTRUCTION This work uses a proprietary dataset consisting of consumer transactions from a retail bank. Each transaction includes only a merchant name indicating the business where that purchase occurred along with macro and micro categories as illustrated in Figure 1 The macro and micro are originally assigned by [4] using the information from the business activities and products. We focus on two macro-categories from this dataset: Food and Shopping, selecting the top 50,000 businesses with the highest number of transactions for each category. With the limited initial information, assigning detailed tags to transactions is challenging. To address this, we augment the dataset through a data enrichment process involving web scraping. Using tools such as Selenium [4] and Beautiful Soup [5], we gathered activity descriptions for companies in each macro category. For the Food macro category, the search was conducted on specialized platforms for restaurants and food delivery services. For the Shopping macro category, we obtained establishment descriptions directly from internet indexing and search tools. In the context of enrichment for the Food macro category, web scraping was conducted as follows: (1) the centers of all Brazilian state capitals and the Federal District were used as base locations 4 https://www.selenium.dev/about/ 5 https://readthedocs.org/projects/beautiful-soup-4/downloads/pdf/latest/ 269 ----- WebMedia‚Äô2024, Juiz de Fora, Brazil Moraes et al. for restaurant searching; (2) for each location, restaurants listed on the first one hundred pages of the platform were extracted. After completing these steps, the information was combined with the merchant database using the merchant‚Äôs name and micro categories. For the Shopping macro category, the description of each merchant was obtained using Google Search Engine [6], selecting descriptions from the first ten search results. The final description consists of a concatenation of all the obtained descriptions. The search queries were constructed using the merchant names combined with their micro categories.  5 TAXONOMY CONSTRUCTION To automatically create topic taxonomies for Food and Shopping businesses, we developed a 3-step method. First, we preprocess the descriptions in our enriched dataset to retain only the relevant parts of the text. Next, we apply two techniques to select candidate terms for the topic taxonomies: keyword extraction and topic modeling. In the post-processing phase, we use large language models (LLMs) to refine the results of each step, filtering out unrelated terms. Finally, we use LLMs again to organize the final terms into hierarchies, forming the topic taxonomies.  5.1 Preprocessing We applied a few NLP techniques to refine the businesses‚Äô descriptions in our dataset. At first, we remove stop words to eliminate commonly used words that do not carry significant meaning in our contexts. Then, to retain only the most relevant portions of the descriptions, we employ part-of-speech (POS) tagging to identify and exclude words that belong to specific POS categories. The list of POS tag categories that were removed includes ADV, CCONJ, ADP, AUX, CONJ, DET, INTJ, PART, PRON, PUNCT, SYM, SCONJ, ADJ, VERB, PROPN. [7] After this initial preprocessing step, we run the first iteration of the candidate term selection part to build a filter of generic words, not to create topic taxonomies yet. For this step, we use the entire corpus of descriptions for each macro category, resulting in two corpora ( Food and Shopping ). For each micro category in the macro categories‚Äô corpora, we use Keyword Extraction and Topic Modeling to gather candidate terms for the filter, combining the results of both techniques in a list. Then, We use an LLM to remove the terms it identifies as unrelated to the main topic (each micro category) from the list. The prompt that we used for requesting this separation is illustrated below. prompt= \"Given the terms in the following list: \"+ <wordsList> +\". Separate them into two groups. In group 1 the terms with no relation to the topic \"+ <type> +\". And in group 2 the terms that are related.\" Listing 1: Prompt for separating candidate terms related to the type of establishment By using this prompt, we try to ensure that the model‚Äôs response is consistently formatted according to the pattern described in it, facilitating the processing of the resulting string, although, some 6 https://www.google.com 7 https://spacy.io/usage/linguistic-features#pos-tagging of the LLMs we tested did not output the response in the requested format. Once we complete one iteration of this method for each macro category in our dataset, we add the words of group 2 to the corresponding list of generic words. We apply the corresponding filter of generic words for each macro category corpus, resulting in the final preprocessed corpus.  5.2 Candidate Terms Selection For this part of our method, we use each preprocessed corpus separately. For the Food corpus, we group the descriptions based on their micro-categories, creating 58 sub-corpus specific to that domain. We have six micro categories for the Shopping corpus, resulting in 6 specific sub-corpus. The candidate terms selection methods are applied to each sub-corpus, creating topic taxonomies where the main topic is the micro category. 5.2.1 Keyword Extraction. The first approach to candidate term selection was to use an unsupervised keyword selection method called Yake! [5]. This method is based on statistical text features extracted from single documents to select the most relevant keywords from that text. It does not require training on a document set and is not dependent on dictionaries, text size, language, domain, or external corpora. Yake! allows for the specification of parameters such as the language of the text, the maximum size of the n-grams being sought, and others. In our method, we customized only the language to Portuguese, and the maximum number of keywords sought for each set of descriptions was 30 words. After extracting the keywords from each group of descriptions, we obtained a total set of ùëÅ candidate terms. However, these terms are further filtered using an LLM, where we ask it to separate the terms related to the main topic from those unrelated, as explained earlier in subsection 5.1. 5.2.2 Topic Modeling. Our second approach to collecting initial topics and candidate terms was Topic Modeling. We applied the Latent Dirichlet Allocation algorithm [2], available at the Gensim Library [8] . We construct a dictionary for each macro-category corpus in our macro-categories corpora by extracting unique tokens and bigrams. After a few empirical tests, we set the minimum frequency of a bigram to 20 occurrences. Since some corpora have a minimal number of tokens (the micro category \"Greek Cuisine\" from the Food macro category has only five stores marked as such, with a corpus of only 127 tokens), we had to set a reasonably small number so that smaller corpora could also have a few bigrams. With the resulting dictionary of tokens, the LDA algorithm was applied. Three main parameters are to be defined in an LDA algorithm: number of topics, alpha, and beta . The number of topics defines the latent topics to be extracted from the corpus. The parameter alpha is a priori belief in documenttopic distribution, while beta is a priori belief in topic-word distribution. To define the number of topics for each micro category corpus, we tried numbers from 1 to 5, constantly checking which configuration would result in the best average topic coherence for that 8 https://pypi.org/project/gensim/ 270 ----- Tagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies WebMedia‚Äô2024, Juiz de Fora, Brazil corpus. Small corpora would have 1 or 2 topics, while bigger ones would have 5. To correctly define the alpha and beta priors, we would have to analyze the distribution for each category corpus [24]. Since this would be rather difficult, we set those priors to be auto-defined by the LDA algorithm, which learns these parameters based on the corpus. We select the terms with the highest coherence with the resulting topics. Each topic returns 20 words with their coherence scores, but we do not use all of them as some have very low coherence. After testing a few configurations, for each topic, we select 60% of the terms with the highest coherence within that topic. With initial terms for each topic taxonomy, we ask an LLM to separate the ones closely related to the main topic from those unrelated, as mentioned earlier.  5.3 Hierarchy Construction Once we have the post-processed lists of candidate terms obtained by each technique mentioned in subsection 5.2, we merge them and remove repetitions. After the merge, for each macro category, we have lists of terms for each micro category, representing each topic taxonomy. However, they do not have any hierarchy level between the terms configuring the taxonomy. To tackle this problem, we use an LLM again, this time with a prompt that searches for sub-categories within the terms of a topic to create these hierarchies. The prompt is illustrated below: prompt=\"Create a dictionary by hierarchically arranging the following words:\" + <wordsList> +.\" Use JSON format as the output such as the following: {\\\\\"key\\\\\": [\\\\\" list of words\\\\\"]}\" Listing 2: Prompt for creating a hierarchy for each list of tags. With this prompt, we seek to ensure that the LLM response has a consistent pattern and facilitates handling the returned string. After this step, we have a hierarchy of terms in each topic taxonomy in the Food and Shopping macro categories.  5.4 Merchant Tagging With the topic taxonomies for both Food and Shopping macrocategories, we can now assign tags to merchants/establishments. To do so, we use the descriptions attached to these establishments, and we see which terms from a taxonomy are mentioned in their descriptions with a reverse index algorithm. We employ the taxonomy whose topic is the same as the establishment‚Äôs micro category, as shown in Figure 2.  6 TAXONOMY EXPANSION Another essential part of our method is the automatic expansion of existing taxonomies as new terms arrive, derived from additional merchant scrapped data, as shown in Section 4. In this section, we present our approach to taxonomy expansion by using instructionbased LLMs. As new transactions may include new businesses, new terms can emerge from the descriptions obtained through the scraping process. Therefore, we need to update the taxonomies with these new terms maintaining and enriching the created hierarchies with the potential new terms. After completing the transaction enrichment process, including the search for business descriptions and the selection of candidate terms, if relevant terms not included in the current hierarchies are detected, we initiate the expansion process.  6.1 Prompt engineering instruction for taxonomy representation First, we represent our topic taxonomies in a format that can be interpreted by an LLM. We employed a generic prompt, illustrated below, across all tested methods to convert topics into root nodes and their terms into child nodes. Childs of [ROOT]: [CHILD1,CHILD2,CHILD3] Childs of [CHILD1]: [CHILD4,CHILD5] Childs of [CHILD2]: [CHILD6] ... Listing 3: Prompt for representation of taxonomy  6.2 Predicting the parent of a node To experiment with taxonomies expansion, we used two datasets: our Food and Shopping topic taxonomies and the taxonomies from SemEval-2015 Task 17 [15]. Those are low-resource taxonomies, with thousands of nodes or less, which are appropriate for the current prompt size of LLMs. We used the SemEval dataset to compare the results with well-established methods for taxonomy expansion, such as Musubu [20]. Similar to their experiments, we hid 20% of the terms (chosen randomically) in the taxonomies to predict their respective parent nodes. To verify the parent/root of a new term, we used the following prompt: Listin g 4: Prom p t for searchin g for a node‚Äôs p arent prompt=\"Who is the father of \"+<new_term>+\"?\" In Table 1, we see the F1-Scores for parent node prediction. Equation 1 showcases how to calculate the F1-Score. TP is the number of true positives, nodes that were correctly assigned as parents of child nodes. FP is the number of false positives, nodes that were incorrectly assigned as a parent to a child node. FN is the number of false negatives, nodes that should have been assigned as parent nodes but were not. 2 ‚àó ùëáùëÉ ùêπ 1 = (1) 2 ‚àó ùëáùëÉ + ùêπùëÉ + ùêπùëÅ For baseline models, we used Bert and Musubu; for commercial LLMs, Gemini Pro and GPT-4; and for open-source LLMs, LLamaAlpaca(7B), Phi-2, and Mixtral 8x7B. We evaluate them in 4 taxonomies from the SemEval dataset and our taxonomies. For each taxonomy, the LLMs perform significantly better than Musubu, with GPT-4 and Gemini Pro having the highest F1-Scores, with the latter beating the former by a few points. However, the most recent open-source options (Phi-2 and Mixtral 8x7B) are getting close in performance. 271 ----- WebMedia‚Äô2024, Juiz de Fora, Brazil Moraes et al. Tags assigned to establishments from the \"Clothing & Accessories\" micro category Figure 2: Assigning tags to establishments based on a topic taxonomy. It is important to note that while SemEval taxonomies have thousands of nodes, ours have only a few hundred, which we can assume is a significant reason for the degrading performance of Musubu and Bert (LMs or LM-based methods). In contrast, the LLMs have a robust performance in such low-resource settings. This also shows that LLMs have a remarkable understanding of questions and zero-shot performance, generalizing well even for datasets in different languages.  7 TAXONOMY EVALUATION To properly evaluate the topic taxonomies that we created in this work, we developed a two-step qualitative evaluation of a limited part of the results. In total, 58 topic taxonomies were created for the Food set and 6 for the Shopping set. For our evaluation, we selected the topic taxonomies with the highest number of terms in each part (the \"Brazilian Cuisine\" taxonomy for the Food part and the \"Clothing and Accessories\" taxonomy for the Shopping one). First, we assess the quality of removing generic terms from each taxonomy, and then, we evaluate the tags assigned to establishments based on that taxonomy. We asked 12 volunteers to answer a two-part form. Part 1 - Accuracy of the terms that were selected as related to the topic : In this part, we evaluate if the LLMs could correctly group the relevant and non-relevant terms, removing the generic terms. To do so, we defined a ground truth with the relevant terms as true positives and the non-relevant terms as true negatives. Table 3 shows the results. GPT-4 was the best model, followed by Gemini Pro, both scoring over 60% accuracy for the Brazilian Cuisine taxonomy and over 86% accuracy for the Clothing and Accessories taxonomy. Smaller language models such as Phi 2 and Llama 2 7B performed poorly both in removing generic terms and in formatting the response accordingly, with Phi 2 being particularly verbose. Part 2 - Human Evaluation of the Quality of the Tagging Process : In this part, the volunteers were asked to examine if the tags assigned to an establishment were appropriate and coherent to that establishment‚Äôs description. We selected the top 5 establishments with the highest transactions for each micro category. We asked our evaluators to analyze the tags assigned to describe that establishment and choose the ones that were not appropriate. This way, we have a coherence ratio for each establishment based on the number of proper tags divided by the total number of tags. We average the results of our 12 evaluators and present them in Table 2. Figure 2 shows the \"Clothing & Accessories\" taxonomy that was evaluated and 2 of the merchants and the tags assigned to them that were included in the evaluation.  8 CONCLUSION In this work, we presented an unsupervised method for automatically creating and expanding topic taxonomies using LLMs. We evaluated some of the generated taxonomies and applied them in transaction tagging in a retailer‚Äôs bank dataset. The evaluation showed promising results, with average coherence scores above 90% for the two selected taxonomies. The taxonomies‚Äô expansion with Gemini Pro also showed exciting results for parent node prediction, with F1-scores of 89% and 70% for Food and Shopping taxonomies, respectively. For future work on taxonomy construction, we plan to test more robust term selection methods, such as embedding-based approaches. We also plan on conducting ablation studies to validate whether the keyword extraction and topic modeling parts help improve the quality of the taxonomies created, by using a baseline prompt to ask the LLM to generate child nodes given a parent node. In terms of taxonomy expansion, there are several tasks to explore, ranging from node-level operations to generating entire sub-trees and identifying similar structures. Additionally, we intend to enhance our instruction-tuned LLM for taxonomy tasks |Method|SemEval-2015 Task 17 Chemical Equipment Food Science|Our taxonomies Food Shopping| |---|---|---| |Gemini Pro|0.68 0.80 0.91 0.72|0.89 0.73| |GPT-4|0.65 0.78 0.89 0.70|0.87 0.71| |Mixtral-8x7B|0.59 0.63 0.80 0.57|0.74 0.60| |Phi-2|0.56 0.52 0.68 0.56|0.64 0.54| |LLama 7B|0.51 0.42 0.58 0.46|0.60 0.49| |Musubu|0.35 0.46 0.37 0.42|0.21 0.13| |Bert-Base|0.13 0.14 0.12 0.16|0.11 0.06| Table 1: F1-score for parent node prediction. 272 ----- Tagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies WebMedia‚Äô2024, Juiz de Fora, Brazil |Col1|Brazilian Cuisine Taxonomy Average Coherence Number of Tags|Col3|Clothing & Accessories Taxonomy Average Coherence Number of Tags|Col5| |---|---|---|---|---| |Merchant 1|92.30%|10|97.11%|8| |Merchant 2|94.23%|8|83.07%|5| |Merchant 3|89.23%|5|94.38%|5| |Merchant 4|87.17%|6|93.84%|5| |Merchant 5|93.40%|7|97.43%|6| Table 2: Results of evaluating the tags assigned to each merchant/establishment. |Col1|Brazilian Cuisine|Clothing & Accessories| |---|---|---| |Llama 2 7B|29.54%|52.78%| |Phi 2|40.90|73.68%| |Mixtral 8x7B v0.1|46.93%|70.27%| |Gemini Pro|61.36%|86.11%| |GPT 4|68.08%|86.84%| Table 3: Accuracy of using each LLM to remove generic words from each topic taxonomy. by fine-tuning or employing more efficient methods such as LoRA [8].  LIMITATIONS To address the limitations of our work, we begin with the taxonomy construction component. In this phase, we relied on topic modeling and keywords extraction to select candidate terms for our taxonomies. The LDA algorithm used for topic modeling performs suboptimally when the base corpus is small. Some of our topics had corpora with vocabularies of fewer than 100 words, which can result in topics containing irrelevant or incoherent terms. Additionally, we could have further experimented with the LDA hyperparameters for each micro-category corpus. Regarding the evaluation of the generated taxonomies, we did not assess topic completeness. Without a ground truth, it is challenging to quantify how comprehensively the terms in a taxonomy cover the main topic. Furthermore, we evaluated only 2 of the 64 taxonomies generated by our method, leaving a substantial portion unexamined. In the taxonomy expansion experiment, we evaluated only a lowresource setting with fewer than a thousand nodes. Most studies focus on taxonomies with hundreds of thousands or more nodes. This presents a challenge for LLMs due to their limited context. Addressing this contextual limitation could benefit from insights found in other works that tackle similar issues [12].  ETHICS STATEMENT In this work, we ensure the utmost protection of customers and store sensitive data by exclusively using non-sensitive information in our dataset. Our prompts solely rely on selected words from store descriptions, thus avoiding any direct usage of personal or sensitive information. No customer-specific data or store-sensitive details are integrated into the system, upholding privacy and security as top priorities. Moreover, we strictly adhere to ethical guidelines during our experiments involving volunteers, and no personal data is collected from them. Our focus lies solely on analyzing the results of our proposed approach. Participants‚Äô anonymity and confidentiality are maintained throughout the research process, ensuring a responsible and trustworthy approach to data handling.  ACKNOWLEDGMENTS The authors would like to acknowledge BTG Pactual for the partnership and financial support to this research.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:21:03.914582Z",
     "start_time": "2025-05-12T22:21:03.911615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus.loc[13, 'resumo'] = abstract\n",
    "corpus.loc[13, 'text'] = text_temp"
   ],
   "id": "f6f45e6cb5e8cf51",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:21:04.908483Z",
     "start_time": "2025-05-12T22:21:04.900202Z"
    }
   },
   "cell_type": "code",
   "source": "corpus.loc[13]",
   "id": "f88439290285d225",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "titulo               Tagging Enriched Bank Transactions Using LLM-G...\n",
       "informacoes_url                                                       \n",
       "idioma                                                         english\n",
       "storage_key          ../articles/original/english/985-24767-1-10-20...\n",
       "autores              [Daniel de S. Moraes,  Polyana B. da Costa,  P...\n",
       "data_publicacao                                             11-09-2024\n",
       "resumo               This work presents an unsupervised method for ...\n",
       "keywords             Large Language Models, Natural Language Proces...\n",
       "referencias          [[1] Rohan Anil, Sebastian Borgeaud, Yonghui W...\n",
       "text                 Many recent studies have focused on the applic...\n",
       "artigo_tokenizado    [9/02/2008, at, 19:20:05, Italian, Cuisine, Pa...\n",
       "pos_tagger           [[9/02/2008, NUM, CD], [at, ADP, IN], [19:20:0...\n",
       "lema                 [9/02/2008, at, 19:20:05, italian, Cuisine, Pa...\n",
       "resumo_clean         assessing the quality of the taxonomies and th...\n",
       "Name: 13, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:50:56.413984Z",
     "start_time": "2025-05-12T22:50:56.176159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus = pd.read_json('../articles/corpus/corpus (1).json')\n",
    "autores = ['Garibaldi da Silveira J√∫nior', 'Gilberto Kreisler','Bruno Zatt','Daniel Palomino','Guilherme Correa']"
   ],
   "id": "67d84ff694cfc15c",
   "outputs": [],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:50:57.707210Z",
     "start_time": "2025-05-12T22:50:57.704818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus.loc[2,'titulo'] = 'Multi-Domain Spatio-Temporal Deformable Fusion model for video quality enhancement'\n",
    "corpus.loc[2, 'autores'] = autores"
   ],
   "id": "1e6b3b45be1a8953",
   "outputs": [],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:24:46.895208Z",
     "start_time": "2025-05-12T23:24:44.918096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = '../articles/original/portuguese/985-24773-1-10-20240923.pdf'\n",
    "text = pymupdf4llm.to_markdown(path)\n",
    "# reader = PdfReader(path)\n",
    "# info = dict(reader.metadata)\n",
    "text"
   ],
   "id": "f57e5f7a56295e01",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# **Uma Investiga√ß√£o sobre T√©cnicas de Data Augmentation** **Aplicadas a Tradu√ß√£o Autom√°tica Portugu√™s-LIBRAS**\\n\\n## Diego Ramon Bezerra da Silva\\n#### Universidade Federal da Para√≠ba Jo√£o Pessoa/PB, Brasil diego.silva@lavid.ufpb.br\\n## Tiago Maritan U. de Ara√∫jo\\n#### Universidade Federal da Para√≠ba Jo√£o Pessoa/PB, Brasil tiagomaritan@lavid.ufpb.br\\n\\n## Marcos Andr√© Bezerra da Silva\\n#### Universidade Federal da Para√≠ba Jo√£o Pessoa/PB, Brasil marcos.andre@lavid.ufpb.br\\n## Daniel Faustino L. de Souza\\n#### Universidade Federal da Para√≠ba Jo√£o Pessoa/PB, Brasil daniel@dcx.ufpb.br\\n### **ABSTRACT**\\n\\n## Manuella Aschoff C. B. Lima\\n#### Universidade Federal da Para√≠ba Jo√£o Pessoa/PB, Brasil manuella.lima@lavid.ufpb.br\\n## Rostand Edson O. Costa\\n#### Universidade Federal da Para√≠ba Jo√£o Pessoa/PB, Brasil rostand@lavid.ufpb.br\\n\\n\\nThe automatic translation from Portuguese to LIBRAS is extremely important for accessibility and inclusion of deaf individuals\\nin society, but the scarcity of data and the high cost of building an\\nauthentic corpora pose significant challenges. Data Augmentation\\nin Neural Machine Translation is the process of generating synthetic sentences to increase the quantity and diversity of the training\\nset. This work investigates the use of data augmentation techniques to improve the performance of Portuguese-LIBRAS automatic\\ntranslation using the BLEU metric. Among the techniques analyzed,\\nback-translation and its combination with synonym substitution\\nusing part-of-speech tagging stood out as the most effective in\\nenhancing the translation model and can be used to increase the\\ndiversity of underrepresented datasets.\\n### **KEYWORDS**\\n\\nTradu√ß√£o Autom√°tica Neural, Aumento de Dados, Libras\\n### **1 INTRODU√á√ÉO**\\n\\nA evolu√ß√£o da tecnologia tem desempenhado um papel crucial\\nna acessibilidade e inclus√£o de pessoas surdas, representando um\\nmarco significativo na promo√ß√£o da igualdade de acesso √† informa√ß√£o. Uma parcela consider√°vel da popula√ß√£o surda n√£o compreende\\nos textos disponibilizados na forma escrita da l√≠ngua oral (LO),\\ncomunicando-se, portanto, basicamente atrav√©s da l√≠ngua de sinais\\n(LS). Especialmente na *web*, onde o volume e o dinamismo de informa√ß√µes s√£o enormes, com conte√∫do textual sendo gerado a todo\\ninstante, a tarefa de interpretar manualmente textos de p√°ginas\\n*web* para l√≠ngua de sinais √© invi√°vel. Diante desse cen√°rio, torna-se\\nindispens√°vel o uso de componentes de tradu√ß√£o autom√°tica para\\ntraduzir o conte√∫do, por exemplo, do Portugu√™s Brasileiro (PB) para\\na L√≠ngua Brasileira de Sinais (LIBRAS), para que pessoas surdas\\ntenham acesso efetivo √† informa√ß√£o online [ 17 ]. Nesse contexto,\\na plataforma VLibras [ 1 ] emerge como uma solu√ß√£o que torna a\\n*web* verdadeiramente acess√≠vel para surdos, destacando-se por sua\\nampla ado√ß√£o em sites governamentais, onde desempenha um papel essencial ao prover acessibilidade em LIBRAS para os servi√ßos\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia‚Äô2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n¬© 2024 SBC ‚Äì Brazilian Computing Society.\\nISSN 2966-2753\\n\\n\\np√∫blicos. Atualmente, o VLibras est√° sendo utilizado em mais de\\n500.000 sites, tanto p√∫blicos quanto privados, realizando milh√µes\\nde tradu√ß√µes mensalmente [2].\\nO componente de tradu√ß√£o do VLibras foi inicialmente desenvolvido utilizando regras de tradu√ß√£o advindas da expertise de\\nlinguistas. Na abordagem atual do VLibras, √© utilizado um tradutor\\nbaseado em Tradu√ß√£o Autom√°tica Neural, que √© capaz de inferir as\\nregras da linguagem a partir dos dados de treinamento. A adi√ß√£o\\ndo componente baseado em rede neural possibilitou uma maior capacidade de desambigua√ß√£o de palavras em PB que possuem sinais\\ndiferentes na LIBRAS, a depender do contexto [17].\\nEntretanto, apesar dos avan√ßos trazidos pelo uso de redes neurais para Tradu√ß√£o Autom√°tica, √© exigida uma alta quantidade de\\ndados para o treinamento de modelos que geram tradu√ß√µes de boa\\nqualidade [ 14 ], o que √© um grande obst√°culo, principalmente em\\nLS, como a LIBRAS. Para que isso aconte√ßa √© necess√°rio o desenvolvimento de um corpus bil√≠ngue [1], processo que √© extremamente\\ncustoso e manual, visto que √© feito por int√©rpretes da LIBRAS. Sendo\\nassim, √© um desafio construir um conjunto de dados de treinamento\\ndiverso que represente diferentes contextos de uso da l√≠ngua [ 17 ].\\nNeste sentido, a gera√ß√£o de dados sint√©ticos por meio de t√©cnicas de\\naumento de dados ( *data augmentation* ) √© uma estrat√©gia importante\\npara superar a escassez e aumentar a quantidade e diversidade dos\\ndados de treinamento [19].\\nExistem v√°rios m√©todos de aumento de dados para Processamento de Linguagem Natural (PLN) e muitos m√©todos, apesar de\\npoderem ser utilizados em diversas tarefas, foram elaborados com\\na finalidade de melhorar o desempenho em uma tarefa espec√≠fica.\\nA escolha certa dos m√©todos empregados para aumento de dados\\npode trazer um impacto positivo na qualidade dos modelos gerados,\\nvisto que √© sabido que m√©todos projetados para a tarefa de Tradu√ß√£o\\nAutom√°tica para l√≠nguas com muitos recursos dispon√≠veis podem\\nn√£o ser eficazes para l√≠nguas com pouco recurso ( *low-resource lan-*\\n*guages* ) [ 4 ]. Sendo assim, este trabalho tem como objetivo investigar\\no impacto de diferentes m√©todos de aumento de dados, utilizando\\nt√©cnicas como retrotradu√ß√£o, substitui√ß√£o de palavras alinhadas,\\nrevers√£o de *tokens* e substitui√ß√£o por sin√¥nimos com *part-of-speech*\\n*tagging*, al√©m de analisar a influ√™ncia da quantidade de dados sint√©ticos no desempenho do modelo de tradu√ß√£o PB-LIBRAS. Para\\nisso, foram realizados experimentos com um modelo da arquitetura\\n\\n1 Pares de senten√ßas em portugu√™s e suas respectivas tradu√ß√µes em LIBRAS.\\n\\n\\n318\\n\\n\\n-----\\n\\nWebMedia‚Äô2024, Juiz de Fora, Brazil Silva et al.\\n\\n\\n*transformer* e os resultados foram avaliados com base na m√©trica\\n\\nBLEU.\\n### **2 TRABALHOS RELACIONADOS**\\n\\nA tradu√ß√£o autom√°tica PB-LIBRAS envolve a tradu√ß√£o entre l√≠nguas\\nde modalidades diferentes (oral-auditiva e visual-espacial) e que\\npossuem estruturas lingu√≠sticas n√£o paralelas entre si. Al√©m disso,\\nas LS geralmente t√™m poucos recursos, ou seja, existem poucos\\nbancos de dados extensos para LS e, quando existem, s√£o limitados\\na algumas LS espec√≠ficas [ 5, 10, 15 ]. Assim, a escassez de dados √©\\num dos principais desafios para a tradu√ß√£o autom√°tica para l√≠ngua\\nde sinais ( *Sign Language Translation* - SLT). Algumas estrat√©gias\\npara lidar com o problema de tradu√ß√£o autom√°tica com poucos recursos incluem aumento de dados, aprendizagem por transfer√™ncia,\\nretrotradu√ß√£o e abordagens h√≠bridas [ 8 ]. No tocante √† t√©cnica de\\naumento de dados, foco deste trabalho, destacam-se os trabalhos\\nde Moryossef et al. 2021, Fadaee et al. 2017, Sanchez-Cartagena et\\nal. 2021, Maimaiti et al. 2021, Jang et al. 2022 e Wang, Yang 2022.\\nMoryossef et al. 2021 se concentraram na tarefa de tradu√ß√£o de\\ntexto dentro do SLT e introduziram duas estrat√©gias de aumento de\\ndados baseadas em regras. Eles apresentaram regras abrangentes e\\nespec√≠ficas do idioma para criar pares texto-glosa pseudo-paralelos.\\nEsses pares foram posteriormente empregados no processo de retrotradu√ß√£o, melhorando o desempenho geral do modelo.\\nO trabalho desenvolvido por Fadaee et al. 2017 utilizou uma\\nestrat√©gia de substitui√ß√£o de palavras alinhadas para aumentar\\na frequ√™ncia de palavras raras no conjunto de treinamento. As\\nsenten√ßas geradas foram posteriormente filtradas por um modelo de\\nlinguagem que foi treinado com o objetivo de avaliar se as senten√ßas\\nsint√©ticas s√£o fluentes, ou seja, est√£o corretas gramaticalmente e\\nfazem sentido sem√¢ntico. Foi observado um ganho de 2,5 pontos\\nna m√©trica BLEU na tradu√ß√£o de ingl√™s para alem√£o.\\nJ√° S√°nchez-Cartagena et al. 2021 aplicaram duas t√©cnicas para\\naumento de dados em sua pesquisa. Uma delas √© a substitui√ß√£o\\nde palavras alinhadas semelhante √† apresentada por Fadaee et al.\\n2017, por√©m sem se preocupar com a flu√™ncia das senten√ßas geradas,\\nportanto, n√£o sendo necess√°rio um modelo de linguagem adicional.\\nDe maneira auxiliar, tamb√©m foi utilizada a tarefa de revers√£o de\\n*tokens*, tendo os resultados avaliados na tradu√ß√£o entre ingl√™s e\\nalem√£o, hebreu e vietnamita, resultando em um ganho m√©dio de\\n1,6 BLEU.\\nMaimaiti et al. 2021 em sua pesquisa propuseram a substitui√ß√£o\\nde sin√¥nimos com *part-of-speech tagging* . Esse trabalho aplicou essa\\nt√©cnica para tradu√ß√£o dos idiomas azerbaij√£o, hindi, uzbeque, turco,\\nalem√£o e chin√™s para ingl√™s. Como resultado, foram observados\\nganhos de BLEU entre 1,16 e 2,39 pontos.\\nNa pesquisa realizada por Jang et al. 2022, foram utilizadas t√©cnicas de aumento de dados para l√≠ngua de sinais coreana ( *Gloss-level*\\n*Korean Sign Language* ). A proposta utilizou retrotradu√ß√£o, substitui√ß√£o por sin√¥nimos restrita √†s classes de substantivos, nomes\\npr√≥prios e pronomes, al√©m de substitui√ß√£o de palavras utilizando\\num modelo de linguagem coreano. Foram observados ganhos de\\nBLEU em 10, 12 e 16 pontos, respectivamente.\\nPor fim, destaca-se a pesquisa de Wang, Yang 2022 que trabalharam em modelos de tradu√ß√£o entre os idiomas ingl√™s, chin√™s e\\ntailand√™s. Neste trabalho foram propostos aumento de dados por\\n\\n\\nsubstitui√ß√£o de palavras alinhadas e substitui√ß√£o por sin√¥nimos.\\nObservaram ganhos de 1 BLEU ao utilizar substitui√ß√£o de palavras\\nalinhadas na tradu√ß√£o de chin√™s para ingl√™s e de 4 BLEU na tradu√ß√£o\\nde ingl√™s para chin√™s. J√° a substitui√ß√£o por sin√¥nimos na tradu√ß√£o\\nde chin√™s para tailand√™s resultou em um ganho de 2,7 BLEU.\\n### **3 FUNDAMENTA√á√ÉO TE√ìRICA** **3.1 Tradu√ß√£o Autom√°tica Neural**\\n\\nTradu√ß√£o Autom√°tica Neural ( *Neural Machine Translation* - NMT) √©\\na aplica√ß√£o de redes neurais para a tarefa de tradu√ß√£o de senten√ßas\\nde uma l√≠ngua de origem para uma l√≠ngua de destino. Em geral, √©\\nutilizada uma rede *sequence-to-sequence*, onde o *encoder* constr√≥i\\numa representa√ß√£o da senten√ßa no idioma de origem e o *decoder*\\nutiliza essa representa√ß√£o e as palavras geradas anteriormente pela\\npr√≥pria rede para gerar a senten√ßa traduzida no idioma alvo [3].\\nEm contraste √† Tradu√ß√£o Autom√°tica Baseada em Regras ( *Rule*\\n*Based Machine Translation* - RBMT), que transforma a senten√ßa de\\norigem atrav√©s de algoritmos de substitui√ß√£o por regras, derivadas\\ndo conhecimento de linguistas, a NMT √© baseada em dados. Sendo\\nassim, para que a NMT seja poss√≠vel, √© necess√°ria a constru√ß√£o\\nde um corpus bil√≠ngue. As redes neurais treinadas para resolver\\no problema de tradu√ß√£o s√£o capazes de aprender as regras de tradu√ß√£o diretamente dos dados, eliminando assim a necessidade da\\nconstru√ß√£o de algoritmos expl√≠citos com regras de tradu√ß√£o. Especificamente no contexto da LIBRAS, modelos NMT oferecem uma\\ncapacidade de desambigua√ß√£o de palavras que t√™m grafia igual no\\nPB, por√©m significado e sinal diferentes na LIBRAS. Essa capacidade de desambigua√ß√£o n√£o seria alcan√ßada utilizando apenas uma\\nabordagem de RBMT, j√° que √© necess√°rio que o modelo de tradu√ß√£o\\ncompreenda o contexto em que o termo amb√≠guo est√° inserido para\\ndecidir a desambigua√ß√£o correta [17].\\n### **3.2 Data Augmentation**\\n\\nA qualidade de um modelo de Tradu√ß√£o Autom√°tica depende principalmente da exist√™ncia de um corpus bil√≠ngue de alta qualidade e\\nextens√£o significativa. Este corpus serve como base de treinamento\\npara que o modelo de tradu√ß√£o neural aprenda os padr√µes lingu√≠sticos de ambos idiomas de origem e destino [ 17 ]. A LIBRAS, assim\\ncomo outras LS, pode ser classificada como l√≠ngua com poucos\\nrecursos ( *low-resource language* ), tendo em vista a baix√≠ssima quantidade de dados dispon√≠veis para o treinamento de componentes de\\nPLN [ 2 ]. Para as l√≠nguas com poucos recursos, n√£o h√° dados aut√™nticos traduzidos por humanos suficientes dispon√≠veis para treinar um\\nmodelo de NMT e obter resultados de alta qualidade. Desta forma,\\na gera√ß√£o de dados sint√©ticos √© uma estrat√©gia interessante para\\ncomplementar o corpus criado pelos linguistas.\\nCom um corpus pequeno, o universo de senten√ßas a serem traduzidas pelo modelo (quando o tradutor estiver sendo utilizado pelo\\nusu√°rio) ser√° muito maior que os exemplos vistos no treinamento.\\nO objetivo de algoritmos de aumento de dados √©, a partir dos dados\\naut√™nticos, expandir e diversificar o corpus de treinamento com\\ndados sint√©ticos para que, idealmente, se aproxime da distribui√ß√£o\\nde dados de todo o universo de pares de senten√ßas e tradu√ß√µes v√°lidas, de modo que a quantidade e diversidade desses novos dados\\nbeneficiem o modelo de tradu√ß√£o [19].\\n\\n\\n319\\n\\n\\n-----\\n\\nUma Investiga√ß√£o sobre T√©cnicas de Data Augmentation Aplicadas a Tradu√ß√£o Autom√°tica Portugu√™s-LIBRAS WebMedia‚Äô2024, Juiz de Fora, Brazil\\n\\n\\nAlgoritmos de aumento de dados ( *data augmentation* ) geram dados adicionais, sint√©ticos, a partir dos dados aut√™nticos, atrav√©s de\\nmodifica√ß√µes sobre as senten√ßas aut√™nticas. Esses dados adicionais\\ns√£o ent√£o incorporados ao corpus de treinamento original. Esta\\n√© uma tarefa desafiadora no ramo de PLN e tradu√ß√£o autom√°tica,\\nonde qualquer modifica√ß√£o na senten√ßa pode ter impacto no seu\\nsignificado. Por isso, √© importante que as tradu√ß√µes na l√≠ngua de\\ndestino mantenham equival√™ncia com o significado na l√≠ngua de\\norigem. √â uma alternativa mais barata na falta de exemplos curados\\npor linguistas e √© extremamente importante em l√≠nguas com poucos\\nrecursos. Por√©m, por ser um procedimento autom√°tico, as senten√ßas\\nsint√©ticas geradas pelos algoritmos de aumento de dados tendem\\na ser de menor qualidade em rela√ß√£o √†s senten√ßas aut√™nticas produzidas por humanos. Deve-se, portanto, utilizar uma quantidade\\nrazo√°vel de dados sint√©ticos [14].\\nExistem diversas t√©cnicas para *data augmentation* e, neste trabalho, abordaremos as t√©cnicas de retrotradu√ß√£o, revers√£o e substitui√ß√£o de palavras, as quais ser√£o detalhadas a seguir.\\n\\n*3.2.1* *Retrotradu√ß√£o.* A t√©cnica de retrotradu√ß√£o (tradu√ß√£o reversa\\nou *back-translation* ) √© amplamente utilizada para aumento de dados,\\npois tende a gerar senten√ßas de boa qualidade. Nessa abordagem,\\ncomo pode ser observado na Figura 1, √© utilizado um outro modelo\\nde tradu√ß√£o e a senten√ßa √© traduzida de um idioma para outro e\\ndepois de volta para o idioma original [2] . Assim, a partir de um par\\naut√™ntico de uma senten√ßa na l√≠ngua de origem e sua respectiva\\ntradu√ß√£o, √© poss√≠vel gerar novas senten√ßas na l√≠ngua de origem que\\ntenham o mesmo significado da tradu√ß√£o da senten√ßa original. Isso\\n√© feito ao executar o algoritmo nas senten√ßas do idioma de origem,\\nresultando em novas senten√ßas sint√©ticas no mesmo idioma. Essas\\nnovas senten√ßas, resultantes da tradu√ß√£o reversa, s√£o adicionadas\\nao corpus original no lado do idioma de origem. As tradu√ß√µes correspondentes, na l√≠ngua de destino, associadas a essas novas senten√ßas\\nsint√©ticas, ser√£o id√™nticas √†s tradu√ß√µes das senten√ßas aut√™nticas\\noriginais devido √† tend√™ncia de preserva√ß√£o do significado [14].\\n\\n**Figura 1: Exemplo de retrotradu√ß√£o.**\\n\\n*3.2.2* *Revers√£o.* A revers√£o dos *tokens* da senten√ßa na l√≠ngua de\\ndestino, como exemplificado na Tabela 1, √© uma tarefa auxiliar e n√£o\\nconvencional. No entanto, faz com que o modelo de tradu√ß√£o utilize\\nmais informa√ß√µes da representa√ß√£o do *encoder* para prever palavras\\nque geralmente aparecem no final da frase, onde a influ√™ncia do\\n*encoder* tende a diminuir. Dado que as senten√ßas geradas n√£o s√£o\\nfluentes, √© recomendada a adi√ß√£o de um *token* especial no in√≠cio de\\ncada par de senten√ßas sint√©ticas [16].\\n\\n2 Por exemplo, traduzir uma senten√ßa do portugu√™s para o ingl√™s e, em seguida, de\\nvolta para o portugu√™s.\\n\\n\\n*3.2.3* *Substitui√ß√£o de Palavras.* T√©cnicas baseadas em substitui√ß√£o\\nde palavras envolvem gerar novas senten√ßas sint√©ticas escolhendo\\numa palavra alvo na senten√ßa aut√™ntica para ser substitu√≠da aleatoriamente por outra palavra. Isso pode ser aplicado tanto nas\\nsenten√ßas da l√≠ngua de origem quanto nas da l√≠ngua de destino,\\ne as substitui√ß√µes podem ser realizadas independentemente entre\\nsi. Alternativamente, pode-se respeitar o alinhamento entre as palavras e realizar a substitui√ß√£o aos pares, como exemplificado na\\nTabela 1: ao substituir uma palavra na senten√ßa de origem, tamb√©m √© substitu√≠da a palavra correspondente na senten√ßa de destino.\\nMesmo que a substitui√ß√£o seja aleat√≥ria, a introdu√ß√£o de ru√≠do nas\\nsenten√ßas de destino ajuda o modelo a aprender a prever a pr√≥xima\\npalavra corretamente, mesmo ap√≥s a gera√ß√£o de uma palavra que\\nn√£o corresponde exatamente ao padr√£o ouro [3] da tradu√ß√£o humana\\n\\n[19] [3].\\nSubstituir palavras por sin√¥nimos √© uma abordagem que gera\\nsenten√ßas sint√©ticas com significado mais pr√≥ximo da senten√ßa aut√™ntica. Ao escolher uma palavra para ser substitu√≠da, √© consultada\\numa tabela de par√°frases que cont√©m sin√¥nimos que s√£o candidatos\\na substituir a palavra original. Atrav√©s da representa√ß√£o vetorial\\nda palavra original e das palavras candidatas, √© calculada a similaridade cosseno a fim selecionar a palavra candidata com maior\\nsimilaridade para substituir a palavra original. Tamb√©m √© poss√≠vel\\nutilizar marca√ß√£o de partes do discurso ( *part-of-speech tagging* ) para\\nlimitar as op√ß√µes de substitui√ß√£o dentro da mesma classe gramatical, como exemplificado na Figura 2. Ao identificar cada classe\\ngramatical presente em uma senten√ßa por meio de *part-of-speech*\\n*tagging*, torna-se vi√°vel realizar substitui√ß√µes de palavras mantendo\\na coer√™ncia gramatical do texto. Essas restri√ß√µes tamb√©m evitam\\nerros que podem ocorrer com o uso da retrotradu√ß√£o, que confia\\ntotalmente na sa√≠da do modelo de tradu√ß√£o [11].\\n\\n**Tabela 1: Exemplos de revers√£o e substitui√ß√£o alinhada.**\\n\\n|Transforma√ß√£o|Idioma|Senten√ßa|\\n|---|---|---|\\n|Nenhuma (sen- ten√ßa original)|Origem Destino|Tivemos uma calorosa recep√ß√£o. TER CALOROSO&ANIMADO RECEP√á√ÉO [PONTO]|\\n|Revers√£o|Destino|[PONTO] RECEP√á√ÉO ANI- MADO&CALOROSO TER|\\n|Substitui√ß√£o ali- nhada|Origem Destino|Tivemos uma calorosa rio. TER CALOROSO&ANIMADO RIO [PONTO]|\\n\\n### **4 METODOLOGIA** **4.1 T√©cnicas Selecionadas**\\n\\nCom o objetivo de identificar quais m√©todos de aumento de dados\\ntrazem o melhor ganho de desempenho ao tradutor, foram selecionadas as seguintes t√©cnicas: (a) retrotradu√ß√£o, por ser amplamente\\nutilizada em NMT, sendo uma refer√™ncia s√≥lida para compara√ß√£o,\\n(b) a substitui√ß√£o alinhada e a tarefa auxiliar de revers√£o como descrito por S√°nchez-Cartagena et al. 2021 e exemplificado na Tabela\\n\\n3 Tradu√ß√£o realizada por humanos e tomada como refer√™ncia.\\n\\n\\n320\\n\\n\\n-----\\n\\nWebMedia‚Äô2024, Juiz de Fora, Brazil Silva et al.\\n\\n\\n**Figura 2: Exemplos de substitui√ß√£o por sin√¥nimos com** ***part-***\\n***of-speech tagging*** **. [11]**\\n\\n1, dado que √© uma evolu√ß√£o sobre m√©todos de substitui√ß√£o aleat√≥ria\\nanteriores, sendo selecionada por sua capacidade de melhorar a\\ndiversidade dos dados sem a necessidade de modelos de linguagem\\nadicionais e (c) a substitui√ß√£o por sin√¥nimos com *part-of-speech*\\n*tagging* como proposto por Maimaiti et al. 2021 e observado na\\nFigura 2, por ser uma alternativa √† retrotradu√ß√£o que tamb√©m tende\\na gerar senten√ßas fluentes.\\nPara a retrotradu√ß√£o, s√£o utilizados dois modelos de tradu√ß√£o\\nautom√°tica dispon√≠veis no *framework Hugging Face Transformers* : o\\nmodelo *Helsinki-NLP/opus-mt-ROMANCE-en*, que traduz do PB para\\no ingl√™s, e o *modelo Helsinki-NLP/opus-mt-en-ROMANCE*, que traduz\\ndo ingl√™s de volta para o PB. Durante a etapa de retrotradu√ß√£o, as\\nsenten√ßas em PB do corpus aut√™ntico s√£o traduzidas para o ingl√™s\\npelo primeiro modelo e, em seguida, traduzidas de volta para o\\nPB pelo segundo modelo. Ap√≥s remover as duplicatas, o corpus\\nexpandido apresenta um aumento de cerca de 50% no n√∫mero total\\nde senten√ßas.\\nA substitui√ß√£o de palavras alinhadas, como apresentada por\\nS√°nchez-Cartagena et al. 2021, foi realizada utilizando o sistema\\nde tradu√ß√£o autom√°tica estat√≠stica MOSES [4] que utiliza a biblioteca\\nGIZA para o alinhamento de palavras. O MOSES produz um l√©xico\\nque cont√©m entradas de palavras na l√≠ngua de origem, juntamente\\ncom a probabilidade de que a palavra correspondente na l√≠ngua\\nde destino seja uma tradu√ß√£o apropriada. Para a substitui√ß√£o, foi\\ndeterminado substituir uma palavra por senten√ßa que √© sorteada\\naleatoriamente, devendo o par de palavras alinhadas ter uma probabilidade maior que 0.7 no l√©xico. Em seguida, este par de palavras\\n√© substitu√≠do por outro par de palavras, tamb√©m de probabilidade\\nmaior que 0.7, proporcionando uma varia√ß√£o sem√¢ntica na senten√ßa.\\nO m√©todo de revers√£o consiste em inverter a lista dos *tokens* da\\n\\n*string* da senten√ßa original, separados pelo caractere de espa√ßo. Para\\nmitigar o impacto de senten√ßas potencialmente n√£o fluentes, foi\\nadicionado um *token* especial precedendo a senten√ßa de origem em\\ncada m√©todo. Essa medida tem como objetivo diminuir a influ√™ncia\\ndessas senten√ßas no resultado final do tradutor.\\n\\n4 O MOSES √© um sistema de tradu√ß√£o autom√°tica que opera atrav√©s do alinhamento\\num para um de todos os *tokens* da l√≠ngua de origem para a l√≠ngua de destino.\\n\\n\\nA implementa√ß√£o da substitui√ß√£o por sin√¥nimos baseada em\\n*part-of-speech tagging*, conforme descrito por Maimaiti et al. 2021,\\nutiliza uma combina√ß√£o de t√©cnicas e recursos de PLN, incluindo\\nmodelos de *part-of-speech tagging*, a base de dados *WordNet* para\\nidentificar sin√¥nimos candidatos e *word embeddings* para calcular a\\nsimilaridade entre os sin√¥nimos candidatos e escolher o sin√¥nimo\\n\\ncom maior similaridade cosseno. Para realizar a marca√ß√£o de partes\\ndo discurso, foi utilizado o modelo *POS_tagger_brill.pkl*, dispon√≠vel\\nno reposit√≥rio do *GitHub inoueMashuu/POS-tagger-portuguese-nltk*\\npara o *framework* de PLN *nltk* . Esse modelo √© respons√°vel por\\natribuir classes gramaticais √†s palavras em PB para a identifica√ß√£o\\ndas palavras alvo que ser√£o substitu√≠das por sin√¥nimos. A base de\\ndados *WordNet* [5], acessada atrav√©s da biblioteca *nltk*, foi empregada\\ncomo fonte de sin√¥nimos para as palavras identificadas pelo *part-of-*\\n*speech tagging* . Por fim, para calcular a similaridade entre as palavras\\nalvo e os sin√¥nimos dispon√≠veis no *WordNet*, foram utilizadas as\\n*embeddings skip-gram* de 100 dimens√µes do Reposit√≥rio de *Word*\\n*Embeddings* do NILC [ 6 ]. As *word embeddings* s√£o representa√ß√µes\\nvetoriais das palavras que capturam suas rela√ß√µes sem√¢nticas com\\nbase em seu contexto de ocorr√™ncia. Essas representa√ß√µes vetoriais\\npermitem calcular a similaridade cosseno entre palavras, necess√°rio\\npara identificar o sin√¥nimo mais adequado para a substitui√ß√£o.\\n### **4.2 Ambiente de Treinamento**\\n\\nO treinamento foi realizado utilizando o *framework Fairseq* [6] . Optouse por uma vers√£o reduzida de um modelo que adota a arquitetura\\n*transformer*, proposta por Vaswani et al. (2017). No *Fairseq*, o modelo\\n*transformer* reduzido *transformer_iwslt_de_en* foi pr√©-treinado na\\ntradu√ß√£o de alem√£o para ingl√™s. A escolha desse modelo permite\\num treinamento mais r√°pido, uma vez que fazer o ajuste fino ( *fine-*\\n*tuning* ) de um modelo de linguagem j√° treinado, mesmo que em um\\npar de idiomas diferentes, tende a ser mais eficiente do que treinar\\num modelo do zero [15].\\nO tradutor do VLibras possui uma arquitetura h√≠brida baseada\\nem RBMT e NMT. A senten√ßa em PB √© pr√©-processada por um\\ncomponente RBMT. A sa√≠da desse pr√©-processamento alimenta o\\nmodelo *transformer*, que √© treinado com o objetivo de aproximar\\na glosa gerada pelo componente RBMT para a glosa gerada pelos\\nint√©rpretes humanos [2].\\nO corpus do VLibras possui pares de senten√ßas em PB como\\nl√≠ngua de origem e uma representa√ß√£o intermedi√°ria da LIBRAS,\\ndenominada glosa, para as senten√ßas no idioma de destino. O corpus\\nmencionado consiste em aproximadamente 65.000 exemplos de\\npares de senten√ßas [ 2 ], cobrindo uma ampla variedade de t√≥picos\\ne contextos lingu√≠sticos. As glosas s√£o criadas por int√©rpretes e\\nlinguistas especializados na LS e cada sinal da glosa possui uma\\nanima√ß√£o associada. O uso de glosas intermedi√°rias como uma\\nforma de representa√ß√£o lingu√≠stica da LS permite que os algoritmos\\nde PLN trabalhem de forma mais eficaz com a LIBRAS [9].\\n\\n5 O *WordNet* √© uma vasta base de dados lexical que organiza palavras em conjuntos\\nde sin√¥nimos, conhecidos como synsets, e fornece informa√ß√µes sobre suas rela√ß√µes\\nsem√¢nticas e gramaticais. Essa base de dados permite a consulta de sin√¥nimos restritos a\\nclasses gramaticais espec√≠ficas, como sujeitos, adjetivos, adv√©rbios e verbos, permitindo\\nque a substitui√ß√£o gere senten√ßas de maior qualidade.\\n6 Desenvolvido pelo *Facebook*, projetado com foco no treinamento de redes *sequence-*\\n*to-sequence*, que s√£o adequadas para tarefas de tradu√ß√£o autom√°tica.\\n\\n\\n321\\n\\n\\n-----\\n\\nUma Investiga√ß√£o sobre T√©cnicas de Data Augmentation Aplicadas a Tradu√ß√£o Autom√°tica Portugu√™s-LIBRAS WebMedia‚Äô2024, Juiz de Fora, Brazil\\n\\n\\nNo *pipeline* de tradu√ß√£o do VLibras, j√° existem cinco m√©todos de\\naumento de dados que geram senten√ßas fluentes e foram constru√≠dos\\ncom o conhecimento de linguistas da LIBRAS.\\n\\n  - **Lugares:** tem como objetivo introduzir varia√ß√£o nas senten√ßas ao identificar nomes de lugares como cidades, estados\\nou pa√≠ses, e substitu√≠-los por outras localidades dispon√≠veis\\nem tabelas de substitui√ß√£o auxiliares.\\n\\n  - **Nega√ß√£o:** trabalha na identifica√ß√£o de sinais na frase que\\npodem ser negados e gera novas senten√ßas realizando essa\\nsubstitui√ß√£o. Essa t√©cnica √© essencial para aprimorar a capacidade do sistema de tradu√ß√£o em compreender e gerar\\ncorretamente senten√ßas negativas, um aspecto importante\\nda gram√°tica da LIBRAS.\\n\\n  - **Intensidade:** foca na identifica√ß√£o de adv√©rbios na frase\\nque podem ser substitu√≠dos para gerar novas frases com\\ndiferentes n√≠veis de intensidade. Por exemplo, a substitui√ß√£o\\nde \"muito\" por \"pouco\" ou vice-versa.\\n\\n  - **Famosos:** visa diversificar o conte√∫do das senten√ßas ao identificar sinais referentes a pessoas famosas e substitu√≠-los por\\noutros sinais representando outros famosos. Essa t√©cnica\\nmelhora a capacidade de desambigua√ß√£o do sistema ao lidar\\ncom pessoas conhecidas.\\n\\n  - **Direcionalidade:** visa capturar uma nuance gramatical espec√≠fica da LS, levando em considera√ß√£o o emissor e receptor\\nda a√ß√£o do verbo, assemelhando-se a concord√¢ncia n√∫meropessoal. O m√©todo de Direcionalidade identifica esses sinais\\nna senten√ßa e realiza substitui√ß√µes apropriadas com outros\\nverbos direcionais equivalentes.\\n\\nTodas essas t√©cnicas de aumento de dados s√£o aplicadas sobre as\\nsenten√ßas do corpus aut√™ntico constru√≠do por linguistas especializados em LIBRAS. As senten√ßas sint√©ticas geradas por essas t√©cnicas\\ns√£o ent√£o anexadas ao corpus aut√™ntico, formando o corpus padr√£o\\nutilizado no treinamento.\\n### **4.3 Treinamento**\\n\\nOs pares de senten√ßas do corpus s√£o processados por cada m√©todo de aumento de dados j√° implementado no *pipeline* do VLibras,\\nconforme descrito na se√ß√£o 4.2. O corpus aumentado gerado pela\\nsa√≠da do *pipeline* do VLibras comp√µe o conjunto de dados de treinamento que √© o padr√£o ( *baseline* ) para os experimentos realizados,\\npossuindo cerca de 106 mil pares de senten√ßas.\\nEm seguida, cada m√©todo de aumento de dados proposto e implementado neste trabalho √© aplicado sobre o conjunto de senten√ßas\\ndo conjunto padr√£o. Nesta etapa, quando h√° combina√ß√£o de diferentes m√©todos, a entrada de cada m√©todo de aumento de dados √©\\nrestrita √†s senten√ßas do conjunto padr√£o, exceto para o m√©todo de\\nretrotradu√ß√£o, devido a restri√ß√µes de recursos computacionais. No\\ncaso da retrotradu√ß√£o, as senten√ßas sint√©ticas geradas s√£o anexadas\\n√†s senten√ßas aut√™nticas antes da execu√ß√£o dos m√©todos de aumento\\nde dados padr√£o do *pipeline* a fim de otimizar recursos e diminuir o\\ncusto com o uso de GPU.\\nA semente de gera√ß√£o de n√∫meros aleat√≥rios √© fixada em todos\\nos experimentos realizados durante o treinamento do modelo, permitindo a reprodutibilidade dos resultados e uma compara√ß√£o justa\\nentre diferentes treinamentos.\\n\\n\\nPor fim, o desempenho do modelo √© avaliado atrav√©s de um\\nconjunto de avalia√ß√£o cuidadosamente selecionado por linguistas,\\ncontendo 50 senten√ßas para cada grupo relevante no dom√≠nio da\\nLIBRAS. Esses grupos incluem senten√ßas b√°sicas, com n√∫meros\\ncardinais, com palavras hom√¥nimas, relacionadas √† direcionalidade,\\na pessoas famosas, √† intensidade, a lugares e √† nega√ß√£o, permitindo\\numa avalia√ß√£o do desempenho do modelo em diferentes contextos\\nlingu√≠sticos.\\n### **4.4 M√©tricas de Interesse**\\n\\nAvaliar a qualidade das tradu√ß√µes geradas por modelos de NMT √©\\numa tarefa desafiadora devido √† natureza complexa e subjetiva da\\nlinguagem. Diferentes tradu√ß√µes podem ser consideradas aceit√°veis\\npara uma mesma senten√ßa de origem, dependendo de uma variedade\\nde fatores como contexto, estilo e prefer√™ncias individuais.\\nUma das m√©tricas mais amplamente utilizadas para avaliar a\\nqualidade da tradu√ß√£o autom√°tica √© o BLEU ( *Bilingual Evaluation*\\n*Understudy* ) [ 13 ]. O BLEU √© uma m√©trica que varia de 0 a 100 e\\nbusca automatizar e replicar como um humano julgaria a qualidade\\nda tradu√ß√£o. Essa m√©trica √© baseada na compara√ß√£o dos n-gramas\\nda senten√ßa gerada com as tradu√ß√µes de refer√™ncia dispon√≠veis, a\\nfim de calcular a similaridade entre a tradu√ß√£o gerada pelo modelo\\ne a tradu√ß√£o de refer√™ncia.\\nO BLEU4, por exemplo, √© calculado com base em n-gramas de\\nquatro palavras: Isso significa que o modelo √© avaliado com base\\nna precis√£o dos n-gramas de quatro palavras em suas tradu√ß√µes,\\nem compara√ß√£o com as tradu√ß√µes de refer√™ncia. Uma das principais vantagens do BLEU √© sua rapidez de c√°lculo, o que o torna\\numa m√©trica eficiente para avalia√ß√£o autom√°tica. No entanto, √©\\nimportante ressaltar que o BLEU apresenta algumas limita√ß√µes. Por\\nexemplo, n√£o s√£o consideradas as similaridades sem√¢nticas entre\\nas tradu√ß√µes, o que pode levar a pontua√ß√µes imprecisas em alguns\\n\\ncasos.\\n\\nNeste trabalho, a an√°lise dos resultados √© realizada com base no\\ndesempenho do modelo de tradu√ß√£o seguindo a m√©trica BLEU4.\\n### **5 RESULTADOS E DISCUSS√ïES**\\n\\nForam realizadas duas rodadas de experimentos com o objetivo\\nde avaliar o impacto de diferentes m√©todos de aumento de dados\\nno desempenho do modelo de tradu√ß√£o. Na primeira rodada, cada\\nm√©todo foi aplicado em sequ√™ncia sobre o conjunto padr√£o, sem\\nlimitar a quantidade de senten√ßas sint√©ticas geradas. Na segunda\\nrodada, o tamanho do conjunto de treinamento foi restrito a 155\\nmil senten√ßas a fim de equilibrar a propor√ß√£o entre senten√ßas aut√™nticas e sint√©ticas. A Tabela 2 mostra o desempenho do modelo\\nutilizando exclusivamente o corpus aut√™ntico com 65 mil pares de\\nsenten√ßas, sem a aplica√ß√£o de estrat√©gias de aumento de dados. Em\\ncontraste com o corpus padr√£o, que inclui os m√©todos de aumento\\nde dados do *pipeline* de tradu√ß√£o do VLibras mencionados na se√ß√£o\\n4.2, totalizando 106 mil pares de senten√ßas.\\nTodos os m√©todos de aumento de dados apresentaram melhorias\\nem rela√ß√£o ao conjunto padr√£o, conforme observado na Tabela 3. A\\nretrotradu√ß√£o trouxe um ganho geral m√©dio significativo (+15,82\\nBLEU), mesmo com o menor acr√©scimo de dados (totalizando 155 mil\\nsenten√ßas). Tanto a revers√£o com substitui√ß√£o de palavras alinhadas,\\nquanto a substitui√ß√£o por sin√¥nimos baseada em *part-of-speech*\\n\\n\\n322\\n\\n\\n-----\\n\\nWebMedia‚Äô2024, Juiz de Fora, Brazil Silva et al.\\n\\n**Tabela 2: Resultado em BLEU do desempenho do modelo**\\n**utilizando apenas o corpus aut√™ntico, sem aumento de dados.**\\n\\n|Categoria|Pontua√ß√£o BLEU|\\n|---|---|\\n|B√°sicas|17,35|\\n|Cardinais|6,22|\\n|Contexto|4,71|\\n|Direcionalidade|0,00|\\n|Famosos|0,00|\\n|Intensidade|0,00|\\n|Lugares|10,79|\\n|Nega√ß√£o|0,00|\\n|Romanos|3,92|\\n\\n\\n\\n*tagging* demonstraram resultados pr√≥ximos (50,73 e 50,32 pontos\\nde BLEU, respectivamente). Observa-se que o aumento de dados traz\\nconsider√°veis melhorias, analizando sob a perspectiva da m√©trica\\nBLEU, para todos os grupos de valida√ß√£o.\\n\\n**Tabela 3: Resultado em BLEU do desempenho do modelo de**\\n**tradu√ß√£o utilizando as t√©cnicas de aumento de dados selecio-**\\n**nadas.**\\n\\n\\n\\n\\n\\n\\n|Col1|Padr√£o (106 mil)|RT (155 mil)|RV + SA (313 mil)|SS (346 mil)|\\n|---|---|---|---|---|\\n|B√°sicas|42,05|51,66|47,89|52,67|\\n|Cardinais|50,18|64,78|72,86|70,52|\\n|Contexto|24,37|43,92|47,67|38,86|\\n|Direcionais|0,00|18,57|30,28|29,06|\\n|Famosos|27,34|45,07|43,23|40,70|\\n|Intensidade|38,47|49,38|42,5|36,55|\\n|Lugares|44,04|56,68|51,85|55,04|\\n|Nega√ß√£o|44,04|50,84|57,14|62,42|\\n|Romanos|25,86|57,85|63,15|67,12|\\n|M√©dia|32,93|48,75|50,73|50,32|\\n\\n\\n**Legenda: RT (Retrotradu√ß√£o); RV (Revers√£o); SA (Substitui√ß√£o**\\n**alinhada); SS (Substitui√ß√£o por sin√¥nimos)**\\n\\nAo adicionar a retrotradu√ß√£o em conjunto com a combina√ß√£o\\nde revers√£o com substitui√ß√£o alinhada, conforme a Tabela 4, foi\\nobservada uma pequena piora n√£o significativa no resultado geral.\\nPor outro lado, a combina√ß√£o de retrotradu√ß√£o com a substitui√ß√£o\\npor sin√¥nimos resultou em um ganho expressivo de desempenho\\n(+19,25 BLEU em rela√ß√£o ao conjunto padr√£o), possivelmente devido ao aumento substancial na quantidade de senten√ßas resultante\\ndessa combina√ß√£o (totalizando 540 mil senten√ßas). No entanto, ao incluir mais m√©todos de aumento de dados, mesmo que isso aumente\\nainda mais a quantidade de senten√ßas no conjunto de treinamento,\\nn√£o foi observada uma melhoria significativa no desempenho do\\nmodelo. Isso sugere que h√° uma limita√ß√£o na melhoria do desempenho proporcionada pela quantidade de senten√ßas sint√©ticas geradas,\\nespecialmente em fun√ß√£o da quantidade de dados aut√™nticos dispon√≠veis. Esses resultados evidenciam a import√¢ncia de encontrar\\n\\n\\n**Figura 3: M√©dia em BLEU do desempenho das t√©cnicas de**\\n**aumento de dados sem restri√ß√£o no tamanho do conjunto de**\\n**treinamento.**\\n\\num equil√≠brio adequado entre a quantidade de dados aut√™nticos e\\nsint√©ticos no conjunto de treinamento.\\nNa segunda fase dos experimentos, conforme exibido na Tabela 5,\\nonde o tamanho do conjunto de treinamento foi limitado, o m√©todo\\nde retrotradu√ß√£o obteve os melhores resultados. Esse resultado n√£o\\n√© surpreendente, uma vez que a retrotradu√ß√£o √© um dos m√©todos\\nmais estabelecidos e amplamente utilizados para aumento de dados\\n\\nem PLN.\\n\\nAo considerar um cen√°rio com aumento de 25% nos dados para\\ncada m√©todo empregado, foram testadas diversas combina√ß√µes de\\nm√©todos aos pares, conforme visto na Tabela 6. Notadamente, a\\ncombina√ß√£o de retrotradu√ß√£o e substitui√ß√£o por sin√¥nimos com\\n*part-for-speech tagging* demonstrou o melhor desempenho quando\\nh√° a limita√ß√£o do tamanho do conjunto de treinamento, apresentando ganho de 16,5 BLEU. Essa combina√ß√£o j√° havia apresentado\\nbons resultados quando n√£o h√° limita√ß√£o no tamanho do conjunto\\nde treinamento, com 19,25 pontos de BLEU sobre o padr√£o (ver\\nTabela 4), isso mostra que as duas t√©cnicas produzem resultados\\nbons quando usadas em conjunto. Uma poss√≠vel explica√ß√£o para o\\nbom desempenho dessa combina√ß√£o √© o aumento da diversidade do\\nvocabul√°rio proporcionado pela retrotradu√ß√£o. Ao traduzir as senten√ßas de volta para o idioma original, o conjunto de treinamento\\npassa a contar com um vocabul√°rio mais amplo, o que, por sua vez,\\naumenta as op√ß√µes de substitui√ß√£o por sin√¥nimos.\\nDevido √† limita√ß√£o na quantidade de senten√ßas, a efic√°cia do\\nm√©todo de revers√£o foi reduzida quando combinado com outras\\nt√©cnicas. Essa √© uma desvantagem que o experimento traz para\\nesse m√©todo, porque ele √© sugerido como uma tarefa destinada a\\n\\n\\n323\\n\\n\\n-----\\n\\nUma Investiga√ß√£o sobre T√©cnicas de Data Augmentation Aplicadas a Tradu√ß√£o Autom√°tica Portugu√™s-LIBRAS WebMedia‚Äô2024, Juiz de Fora, Brazil\\n\\n**Tabela 4: Desempenho em BLEU de combina√ß√µes de t√©cnicas de aumento de dados sem restri√ß√£o na quantidade de senten√ßas.**\\n\\n\\n|Col1|Padr√£o (106 mil)|RT + RV + SA (467 mil)|RT + SS (540 mil)|RT + RV + SS (695 mil)|Todos (851 mil)|\\n|---|---|---|---|---|---|\\n|B√°sicas|42,05|48,55|48,14|50,76|53,53|\\n|Cardinais|50,18|61,54|67,29|66,79|66,49|\\n|Contexto|24,37|45,68|43,73|39,36|49,25|\\n|Direcionalidade|0,00|27,66|33,60|36,95|32,66|\\n|Famosos|27,34|43,74|48,09|44,83|42,85|\\n|Intensidade|38,47|45,37|39,61|42,29|42,29|\\n|Lugares|44,04|54,45|48,10|52,42|47,12|\\n|Nega√ß√£o|44,04|53,84|65,94|59,17|59,34|\\n|Romanos|25,86|74,17|75,19|76,07|75,21|\\n|M√©dia|32,93|50,55|52,18|52,07|52,08|\\n\\n\\n**Legenda: RT (Retrotradu√ß√£o); RV (Revers√£o); SA (Substitui√ß√£o alinhada); SS (Substitui√ß√£o por sin√¥nimos)**\\n\\n\\n\\n\\n\\n\\n**Tabela 5: Resultado em BLEU de cada t√©cnica utilizada isola-**\\n\\n**damente, com restri√ß√£o de 155 mil senten√ßas no conjunto de**\\n**treinamento.**\\n\\n|Col1|Padr√£o|RT|SS|RV|SA|\\n|---|---|---|---|---|---|\\n|B√°sicas|42,05|51,66|48,74|43,07|48,51|\\n|Cardinais|50,18|64,78|56,29|60,33|60,69|\\n|Contexto|24,37|43,92|35,00|32,80|37,85|\\n|Direcionalidade|0,00|18,57|0|0|22,34|\\n|Famosos|27,34|45,07|47,41|34,36|45,23|\\n|Intensidade|38,47|49,38|38,54|38,65|43,14|\\n|Lugares|44,04|56,68|51,5|47,87|47,73|\\n|Nega√ß√£o|44,04|50,84|58,87|42,95|53,6|\\n|Romanos|25,86|57,85|40,72|31,07|57,25|\\n|M√©dia|32,93|48,75|41,89|36,78|46,26|\\n\\n\\n\\n**Legenda: RT (Retrotradu√ß√£o); RV (Revers√£o); SA(Substitui√ß√£o**\\n**alinhada); SS (Substitui√ß√£o por sin√¥nimos)**\\n\\nfortalecer o *encoder* de forma independente de outras t√©cnicas de\\naumento de dados. Al√©m das combina√ß√µes de pares de m√©todos,\\ntamb√©m foram exploradas outras configura√ß√µes envolvendo tr√™s\\nt√©cnicas distintas.\\nPor fim, foram testadas mais algumas configura√ß√µes, conforme\\nmostrado na Tabela 7. Uma combina√ß√£o de 25% de retrotradu√ß√£o,\\n25% de substitui√ß√£o por sin√¥nimos e 25% de substitui√ß√£o alinhada\\ne uma configura√ß√£o com 1/3 de contribui√ß√£o por cada m√©todo.\\nObserva-se, no entanto, que essas combina√ß√µes n√£o resultaram\\nem melhorias significativas em rela√ß√£o aos resultados obtidos anteriormente. Sendo assim, mesmo com a combina√ß√£o de diversas\\nt√©cnicas de aumento de dados, a quantidade de senten√ßas ainda √©\\num fator limitante para alcan√ßar melhorias de desempenho.\\n\\n\\n**Figura 4: M√©dia em BLEU do desempenho das t√©cnicas de**\\n**aumento de dados com limita√ß√£o de 155 mil senten√ßas no**\\n**conjunto de treinamento.**\\n\\n\\n324\\n\\n\\n-----\\n\\nWebMedia‚Äô2024, Juiz de Fora, Brazil Silva et al.\\n\\n**Tabela 6: Resultado em BLEU do treinamento do modelo de tradu√ß√£o com contribui√ß√£o de 25 mil senten√ßas por m√©todo.**\\n\\n|Col1|Padr√£o|RV + SA|RV + SS|RT + SS|RT + RV|RT + SA|\\n|---|---|---|---|---|---|---|\\n|B√°sicas|42,05|47,29|40,84|48,37|47,86|46,25|\\n|Cardinais|50,18|66,69|57,17|66,33|67,18|64,80|\\n|Contexto|24,37|43,86|33,08|38,86|38,56|43,28|\\n|Direcionalidade|0,00|18,61|17,12|28,12|21,91|25,01|\\n|Famosos|27,34|52,12|41,58|46,67|42,07|46,46|\\n|Intensidade|38,47|40,21|49,63|43,62|44,40|42,06|\\n|Lugares|44,04|48,65|47,97|50,84|52,17|52,65|\\n|Nega√ß√£o|44,04|57,69|48,55|60,05|46,69|50,59|\\n|Romanos|25,86|49,51|34,07|62,09|48,99|60,08|\\n|M√©dia|32,93|47,18|41,11|49,43|45,53|47,90|\\n\\n\\n\\n**Legenda: RT (Retrotradu√ß√£o); RV (Revers√£o); SA (Substitui√ß√£o alinhada); SS (Substitui√ß√£o por sin√¥nimos)**\\n\\n\\n**Tabela 7: Resultado em BLEU para combina√ß√µes envolvendo**\\n**retrotradu√ß√£o, substitui√ß√£o alinhada e substitui√ß√£o por sin√¥-**\\n**nimos.**\\n\\n\\n\\n\\n\\n|Col1|Padr√£o|1 RT + 3 1 SA + 1 SS 3 3|1 RT + 2 1 SA + 1 SS 4 4|\\n|---|---|---|---|\\n|B√°sicas|42,05|54,24|52,80|\\n|Cardinais|50,18|63,44|63,78|\\n|Contexto|24,37|43,20|44,02|\\n|Direcionalidade|0,00|22,35|16,92|\\n|Famosos|27,34|49,94|50,71|\\n|Intensidade|38,47|42,44|33,28|\\n|Lugares|44,04|51,37|51,24|\\n|Nega√ß√£o|44,04|39,94|52,71|\\n|Romanos|25,86|60,28|62,68|\\n|M√©dia|32,93|47,46|47,57|\\n\\n\\n**Legenda: RT (Retrotradu√ß√£o); SA (Substitui√ß√£o alinhada);**\\n**SS (Substitui√ß√£o por sin√¥nimos)**\\n\\n\\n### **6 CONCLUS√ÉO**\\n\\nA gera√ß√£o de dados sint√©ticos para aprimorar modelos de NMT em\\ncen√°rios de poucos recursos ( *low-resource language* ), especialmente\\npara LS como a LIBRAS, √© de suma import√¢ncia para a acessibilidade\\ne inclus√£o de pessoas surdas. Dessa forma, este trabalho explorou\\ndiferentes m√©todos de aumento de dados a fim de identificar quais\\nabordagens podem melhorar o desempenho do tradutor, com base\\nem testes computacionais amparados pela m√©trica BLEU.\\nObservou-se que a t√©cnica amplamente utilizada de retrotradu√ß√£o tamb√©m √© eficaz na tradu√ß√£o de PB para LIBRAS, trazendo um\\nganho de 15,82 pontos de BLEU em rela√ß√£o ao conjunto padr√£o.\\nA combina√ß√£o de retrotradu√ß√£o e substitui√ß√£o por sin√¥nimos com\\n*part-of-speech tagging* trouxe os melhores resultados em ambos os\\ncen√°rios: sem restri√ß√£o no tamanho do conjunto de treinamento\\n(+19,25 BLEU sobre o padr√£o) e tamb√©m quando o conjunto de\\ntreinamento foi limitado a 155 mil senten√ßas (+16,5 BLEU sobre\\no padr√£o). Essas t√©cnicas podem ser utilizadas para aumentar a\\nquantidade de exemplos em conjuntos de senten√ßas que estejam\\n\\n\\nsub-representados no corpus. Destaca-se tamb√©m, diante dos resultados alcan√ßados, a import√¢ncia de manter um equil√≠brio entre a\\nquantidade de dados sint√©ticos gerados em rela√ß√£o √† quantidade de\\ndados aut√™nticos no corpus original.\\nPor fim, enxerga-se que a investiga√ß√£o de t√©cnicas mais custosas,\\npor√©m potencialmente mais eficazes, pode abrir novas possibilidades para aprimorar ainda mais a qualidade da tradu√ß√£o autom√°tica.\\nModelos de linguagem podem produzir texto fluente em uma variedade de contextos lingu√≠sticos, portanto, utilizar esses modelos\\npara gerar dados sint√©ticos √© uma sugest√£o para trabalhos futuros.\\n### **AGRADECIMENTOS**\\n\\nOs autores agradecem √† Secretaria Nacional dos Direitos da Pessoa\\ncom Defici√™ncia do Minist√©rio dos Direitos Humanos e da Cidadania\\npelo apoio financeiro para a realiza√ß√£o desta pesquisa.\\n### **REFER√äNCIAS**\\n\\n[1] T. M. U. Ara√∫jo. 2012. *Uma solu√ß√£o para gera√ß√£o autom√°tica de trilhas em*\\n*l√≠ngua brasileira de sinais em conte√∫dos multim√≠dia* . Tese (Doutorado em Automa√ß√£o e Sistemas). Universidade Federal do Rio Grande do Norte, Natal.\\nhttps://repositorio.ufrn.br/handle/123456789/15190\\n\\n[2] Renan Costa e Diego Ramon Silva e Samuel Moreira e Daniel Faustino Souza\\ne Rostand Edson Costa e Tiago Maritan Ara√∫jo. 2024. Avalia√ß√£o do uso de\\nmodelos de aprendizagem profunda na tradu√ß√£o autom√°tica de l√≠nguas de sinais.\\n*Revista Principia - Divulga√ß√£o Cient√≠fica e Tecnol√≥gica do IFPB* 0, 0 (2024). https:\\n//periodicos.ifpb.edu.br/index.php/principia/article/view/8053\\n\\n[3] Marzieh Fadaee, Arianna Bisazza, and Christof Monz. 2017. Data Augmentation\\nfor Low-Resource Neural Machine Translation. In *Proceedings of the 55th An-*\\n*nual Meeting of the Association for Computational Linguistics (Volume 2: Short*\\n*Papers)*, Regina Barzilay and Min-Yen Kan (Eds.). Association for Computational\\nLinguistics, Vancouver, Canada, 567‚Äì573. https://doi.org/10.18653/v1/P17-2090\\n\\n[4] Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi,\\nTeruko Mitamura, and Eduard Hovy. 2021. A Survey of Data Augmentation\\nApproaches for NLP. In *Findings of the Association for Computational Linguistics:*\\n*ACL-IJCNLP 2021*, Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.).\\nAssociation for Computational Linguistics, Online, 968‚Äì988. https://doi.org/10.\\n18653/v1/2021.findings-acl.84\\n\\n[5] Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor O. K. Li. 2018. Universal\\nNeural Machine Translation for Extremely Low Resource Languages. *CoRR*\\nabs/1802.05368 (2018). arXiv:1802.05368 http://arxiv.org/abs/1802.05368\\n\\n[6] Nathan Hartmann, Erick Fonseca, Christopher Shulby, Marcos Treviso, Jessica\\nRodrigues, and Sandra Aluisio. 2017. Portuguese Word Embeddings: Evaluating\\non Word Analogies and Natural Language Tasks. In *Proceedings of the XI Brazilian*\\n*Symposium in Information and Human Language Technology and Collocated Events*\\n*(STIL 2017)* . Uberlandia, Minas Gerais, Brazil.\\n\\n\\n325\\n\\n\\n-----\\n\\nUma Investiga√ß√£o sobre T√©cnicas de Data Augmentation Aplicadas a Tradu√ß√£o Autom√°tica Portugu√™s-LIBRAS WebMedia‚Äô2024, Juiz de Fora, Brazil\\n\\n\\n\\n[7] Jin Yea Jang, Han-Mu Park, Saim Shin, Suna Shin, Byungcheon Yoon, and Gahgene Gweon. 2022. Automatic Gloss-level Data Augmentation for Sign Language Translation. In *Proceedings of the Thirteenth Language Resources and Eva-*\\n*luation Conference*, Nicoletta Calzolari, Fr√©d√©ric B√©chet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara,\\nBente Maegaard, Joseph Mariani, H√©l√®ne Mazo, Jan Odijk, and Stelios Piperidis\\n(Eds.). European Language Resources Association, Marseille, France, 6808‚Äì6813.\\nhttps://aclanthology.org/2022.lrec-1.734\\n\\n[8] Z. Liang, H. Li, and J. Chai. 2023. Sign Language Translation: A Survey of\\nApproaches and Techniques. *Electronics* 12, 12 (2023). https://doi.org/10.3390/\\nelectronics12122678\\n\\n[9] Manuella Aschoff C. B. Lima, Tiago Maritan U. de Ara√∫jo, Rostand E. O. Costa, and\\nErickson S. Oliveira. 2022. A machine translation mechanism of Brazilian Portuguese to Libras with syntactic-semantic adequacy. *Natural Language Engineering*\\n28, 3 (2022), 271‚Äì294. https://doi.org/10.1017/S1351324920000662\\n\\n[10] Alexandre Magueresse, Vincent Carles, and Evan Heetderks. 2020. Low-resource\\nLanguages: A Review of Past Work and Future Challenges. *CoRR* abs/2006.07264\\n(2020). arXiv:2006.07264 https://arxiv.org/abs/2006.07264\\n\\n[11] Mieradilijiang Maimaiti, Yang Liu, Huanbo Luan, Zegao Pan, and Maosong Sun.\\n2021. Improving Data Augmentation for Low-Resource NMT Guided by POSTagging and Paraphrase Embedding. *ACM Trans. Asian Low-Resour. Lang. Inf.*\\n*Process.* 20, 6, Article 107 (aug 2021), 21 pages. https://doi.org/10.1145/3464427\\n\\n[12] Amit Moryossef, Kayo Yin, Graham Neubig, and Yoav Goldberg. 2021. Data\\nAugmentation for Sign Language Gloss Translation. In *Proceedings of the 1st*\\n*International Workshop on Automatic Translation for Signed and Spoken Languages*\\n*(AT4SSL)*, Dimitar Shterionov (Ed.). Association for Machine Translation in the\\nAmericas, Virtual. https://aclanthology.org/2021.mtsummit-at4ssl.1\\n\\n[13] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu:\\na Method for Automatic Evaluation of Machine Translation. In *Proceedings of*\\n*the 40th Annual Meeting of the Association for Computational Linguistics*, Pierre\\nIsabelle, Eugene Charniak, and Dekang Lin (Eds.). Association for Computational\\nLinguistics, Philadelphia, Pennsylvania, USA, 311‚Äì318. https://doi.org/10.3115/\\n1073083.1073135\\n\\n\\n\\n[14] Alberto Poncelas, Dimitar Shterionov, Andy Way, Gideon Maillette de Buy Wenniger, and Peyman Passban. 2018. Investigating Backtranslation in Neural Machine Translation. In *Proceedings of the 21st Annual Conference of the European*\\n*Association for Machine Translation*, Juan Antonio P√©rez-Ortiz, Felipe S√°nchezMart√≠nez, Miquel Espl√†-Gomis, Maja Popoviƒá, Celia Rico, Andr√© Martins, Joachim Van den Bogaert, and Mikel L. Forcada (Eds.). Alicante, Spain, 269‚Äì278.\\nhttps://aclanthology.org/2018.eamt-main.25\\n\\n[15] Surangika Ranathunga, En-Shiun Annie Lee, Marjana Prifti Skenduli, Ravi\\nShekhar, Mehreen Alam, and Rishemjit Kaur. 2023. Neural Machine Translation for Low-resource Languages: A Survey. *ACM Comput. Surv.* 55, 11, Article\\n229 (feb 2023), 37 pages. https://doi.org/10.1145/3567592\\n\\n[16] V√≠ctor M. S√°nchez-Cartagena, Miquel Espl√†-Gomis, Juan Antonio P√©rez-Ortiz, and\\nFelipe S√°nchez-Mart√≠nez. 2021. Rethinking Data Augmentation for Low-Resource\\nNeural Machine Translation: A Multi-Task Learning Approach. In *Proceedings of*\\n*the 2021 Conference on Empirical Methods in Natural Language Processing*, MarieFrancine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.).\\nAssociation for Computational Linguistics, Online and Punta Cana, Dominican\\nRepublic, 8502‚Äì8516. https://doi.org/10.18653/v1/2021.emnlp-main.669\\n\\n[17] Vin√≠cius Ver√≠ssimo, Cec√≠lia Silva, Vitor Hanael, Caio Moraes, Rostand Costa,\\nTiago Maritan, Manuella Aschoff, and Tha√≠s Gaud√™ncio. 2019. A study on the\\nuse of sequence-to-sequence neural networks for automatic translation of brazilian portuguese to libras. In *Proceedings of the 25th Brazillian Symposium on*\\n*Multimedia and the Web* . 101‚Äì108.\\n\\n[18] Jing Wang and Lina Yang. 2022. Effective Data Augmentation Methods for CCMT\\n2022. In *Machine Translation*, Tong Xiao and Juan Pino (Eds.). Springer Nature\\nSingapore, Singapore, 135‚Äì142.\\n\\n[19] Xinyi Wang, Hieu Pham, Zihang Dai, and Graham Neubig. 2018. SwitchOut:\\nan Efficient Data Augmentation Algorithm for Neural Machine Translation. In\\n*Proceedings of the 2018 Conference on Empirical Methods in Natural Language*\\n*Processing*, Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun‚Äôichi Tsujii\\n(Eds.). Association for Computational Linguistics, Brussels, Belgium, 856‚Äì861.\\nhttps://doi.org/10.18653/v1/D18-1100\\n\\n\\n326\\n\\n\\n-----\\n\\n'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 218
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:24:47.734261Z",
     "start_time": "2025-05-12T23:24:47.730614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from process_text import split_references\n",
    "ref = text.split('**REFER√äNCIAS**')[1].split('###')[0]\n",
    "# ref = text.split('\\n\\n\\n\\n')[1].split('###')[0]\n",
    "ref = clean_abs(ref)\n",
    "ref = split_references(ref)\n",
    "ref"
   ],
   "id": "547d12266d24c8c2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[1] T. M. U. Ara√∫jo. 2012. Uma solu√ß√£o para gera√ß√£o autom√°tica de trilhas em l√≠ngua brasileira de sinais em conte√∫dos multim√≠dia . Tese (Doutorado em Automa√ß√£o e Sistemas). Universidade Federal do Rio Grande do Norte, Natal. https://repositorio.ufrn.br/handle/123456789/15190',\n",
       " '[2] Renan Costa e Diego Ramon Silva e Samuel Moreira e Daniel Faustino Souza e Rostand Edson Costa e Tiago Maritan Ara√∫jo. 2024. Avalia√ß√£o do uso de modelos de aprendizagem profunda na tradu√ß√£o autom√°tica de l√≠nguas de sinais. Revista Principia - Divulga√ß√£o Cient√≠fica e Tecnol√≥gica do IFPB 0, 0 (2024). https: //periodicos.ifpb.edu.br/index.php/principia/article/view/8053',\n",
       " '[3] Marzieh Fadaee, Arianna Bisazza, and Christof Monz. 2017. Data Augmentation for Low-Resource Neural Machine Translation. In Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Regina Barzilay and Min-Yen Kan (Eds.). Association for Computational Linguistics, Vancouver, Canada, 567‚Äì573. https://doi.org/10.18653/v1/P17-2090',\n",
       " '[4] Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi, Teruko Mitamura, and Eduard Hovy. 2021. A Survey of Data Augmentation Approaches for NLP. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, Online, 968‚Äì988. https://doi.org/10. 18653/v1/2021.findings-acl.84',\n",
       " '[5] Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor O. K. Li. 2018. Universal Neural Machine Translation for Extremely Low Resource Languages. CoRR abs/1802.05368 (2018). arXiv:1802.05368 http://arxiv.org/abs/1802.05368',\n",
       " '[6] Nathan Hartmann, Erick Fonseca, Christopher Shulby, Marcos Treviso, Jessica Rodrigues, and Sandra Aluisio. 2017. Portuguese Word Embeddings: Evaluating on Word Analogies and Natural Language Tasks. In Proceedings of the XI Brazilian Symposium in Information and Human Language Technology and Collocated Events (STIL 2017) . Uberlandia, Minas Gerais, Brazil.   325   -----  Uma Investiga√ß√£o sobre T√©cnicas de Data Augmentation Aplicadas a Tradu√ß√£o Autom√°tica Portugu√™s-LIBRAS WebMedia‚Äô2024, Juiz de Fora, Brazil',\n",
       " '[7] Jin Yea Jang, Han-Mu Park, Saim Shin, Suna Shin, Byungcheon Yoon, and Gahgene Gweon. 2022. Automatic Gloss-level Data Augmentation for Sign Language Translation. In Proceedings of the Thirteenth Language Resources and Eva- luation Conference, Nicoletta Calzolari, Fr√©d√©ric B√©chet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, H√©l√®ne Mazo, Jan Odijk, and Stelios Piperidis (Eds.). European Language Resources Association, Marseille, France, 6808‚Äì6813. https://aclanthology.org/2022.lrec-1.734',\n",
       " '[8] Z. Liang, H. Li, and J. Chai. 2023. Sign Language Translation: A Survey of Approaches and Techniques. Electronics 12, 12 (2023). https://doi.org/10.3390/ electronics12122678',\n",
       " '[9] Manuella Aschoff C. B. Lima, Tiago Maritan U. de Ara√∫jo, Rostand E. O. Costa, and Erickson S. Oliveira. 2022. A machine translation mechanism of Brazilian Portuguese to Libras with syntactic-semantic adequacy. Natural Language Engineering 28, 3 (2022), 271‚Äì294. https://doi.org/10.1017/S1351324920000662',\n",
       " '[10] Alexandre Magueresse, Vincent Carles, and Evan Heetderks. 2020. Low-resource Languages: A Review of Past Work and Future Challenges. CoRR abs/2006.07264 (2020). arXiv:2006.07264 https://arxiv.org/abs/2006.07264',\n",
       " '[11] Mieradilijiang Maimaiti, Yang Liu, Huanbo Luan, Zegao Pan, and Maosong Sun. 2021. Improving Data Augmentation for Low-Resource NMT Guided by POSTagging and Paraphrase Embedding. ACM Trans. Asian Low-Resour. Lang. Inf. Process. 20, 6, Article 107 (aug 2021), 21 pages. https://doi.org/10.1145/3464427',\n",
       " '[12] Amit Moryossef, Kayo Yin, Graham Neubig, and Yoav Goldberg. 2021. Data Augmentation for Sign Language Gloss Translation. In Proceedings of the 1st International Workshop on Automatic Translation for Signed and Spoken Languages (AT4SSL), Dimitar Shterionov (Ed.). Association for Machine Translation in the Americas, Virtual. https://aclanthology.org/2021.mtsummit-at4ssl.1',\n",
       " '[13] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Pierre Isabelle, Eugene Charniak, and Dekang Lin (Eds.). Association for Computational Linguistics, Philadelphia, Pennsylvania, USA, 311‚Äì318. https://doi.org/10.3115/ 1073083.1073135',\n",
       " '[14] Alberto Poncelas, Dimitar Shterionov, Andy Way, Gideon Maillette de Buy Wenniger, and Peyman Passban. 2018. Investigating Backtranslation in Neural Machine Translation. In Proceedings of the 21st Annual Conference of the European Association for Machine Translation, Juan Antonio P√©rez-Ortiz, Felipe S√°nchezMart√≠nez, Miquel Espl√†-Gomis, Maja Popoviƒá, Celia Rico, Andr√© Martins, Joachim Van den Bogaert, and Mikel L. Forcada (Eds.). Alicante, Spain, 269‚Äì278. https://aclanthology.org/2018.eamt-main.25',\n",
       " '[15] Surangika Ranathunga, En-Shiun Annie Lee, Marjana Prifti Skenduli, Ravi Shekhar, Mehreen Alam, and Rishemjit Kaur. 2023. Neural Machine Translation for Low-resource Languages: A Survey. ACM Comput. Surv. 55, 11, Article 229 (feb 2023), 37 pages. https://doi.org/10.1145/3567592',\n",
       " '[16] V√≠ctor M. S√°nchez-Cartagena, Miquel Espl√†-Gomis, Juan Antonio P√©rez-Ortiz, and Felipe S√°nchez-Mart√≠nez. 2021. Rethinking Data Augmentation for Low-Resource Neural Machine Translation: A Multi-Task Learning Approach. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, MarieFrancine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 8502‚Äì8516. https://doi.org/10.18653/v1/2021.emnlp-main.669',\n",
       " '[17] Vin√≠cius Ver√≠ssimo, Cec√≠lia Silva, Vitor Hanael, Caio Moraes, Rostand Costa, Tiago Maritan, Manuella Aschoff, and Tha√≠s Gaud√™ncio. 2019. A study on the use of sequence-to-sequence neural networks for automatic translation of brazilian portuguese to libras. In Proceedings of the 25th Brazillian Symposium on Multimedia and the Web . 101‚Äì108.',\n",
       " '[18] Jing Wang and Lina Yang. 2022. Effective Data Augmentation Methods for CCMT 2022. In Machine Translation, Tong Xiao and Juan Pino (Eds.). Springer Nature Singapore, Singapore, 135‚Äì142.',\n",
       " '[19] Xinyi Wang, Hieu Pham, Zihang Dai, and Graham Neubig. 2018. SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun‚Äôichi Tsujii (Eds.). Association for Computational Linguistics, Brussels, Belgium, 856‚Äì861. https://doi.org/10.18653/v1/D18-1100   326   -----']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 219
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:24:53.250624Z",
     "start_time": "2025-05-12T23:24:53.247441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus.loc[38, 'referencias'] = ref\n",
    "corpus.loc[38]['referencias']"
   ],
   "id": "c8a052d8a0e6cd94",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[1] T. M. U. Ara√∫jo. 2012. Uma solu√ß√£o para gera√ß√£o autom√°tica de trilhas em l√≠ngua brasileira de sinais em conte√∫dos multim√≠dia . Tese (Doutorado em Automa√ß√£o e Sistemas). Universidade Federal do Rio Grande do Norte, Natal. https://repositorio.ufrn.br/handle/123456789/15190',\n",
       " '[2] Renan Costa e Diego Ramon Silva e Samuel Moreira e Daniel Faustino Souza e Rostand Edson Costa e Tiago Maritan Ara√∫jo. 2024. Avalia√ß√£o do uso de modelos de aprendizagem profunda na tradu√ß√£o autom√°tica de l√≠nguas de sinais. Revista Principia - Divulga√ß√£o Cient√≠fica e Tecnol√≥gica do IFPB 0, 0 (2024). https: //periodicos.ifpb.edu.br/index.php/principia/article/view/8053',\n",
       " '[3] Marzieh Fadaee, Arianna Bisazza, and Christof Monz. 2017. Data Augmentation for Low-Resource Neural Machine Translation. In Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Regina Barzilay and Min-Yen Kan (Eds.). Association for Computational Linguistics, Vancouver, Canada, 567‚Äì573. https://doi.org/10.18653/v1/P17-2090',\n",
       " '[4] Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi, Teruko Mitamura, and Eduard Hovy. 2021. A Survey of Data Augmentation Approaches for NLP. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, Online, 968‚Äì988. https://doi.org/10. 18653/v1/2021.findings-acl.84',\n",
       " '[5] Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor O. K. Li. 2018. Universal Neural Machine Translation for Extremely Low Resource Languages. CoRR abs/1802.05368 (2018). arXiv:1802.05368 http://arxiv.org/abs/1802.05368',\n",
       " '[6] Nathan Hartmann, Erick Fonseca, Christopher Shulby, Marcos Treviso, Jessica Rodrigues, and Sandra Aluisio. 2017. Portuguese Word Embeddings: Evaluating on Word Analogies and Natural Language Tasks. In Proceedings of the XI Brazilian Symposium in Information and Human Language Technology and Collocated Events (STIL 2017) . Uberlandia, Minas Gerais, Brazil.   325   -----  Uma Investiga√ß√£o sobre T√©cnicas de Data Augmentation Aplicadas a Tradu√ß√£o Autom√°tica Portugu√™s-LIBRAS WebMedia‚Äô2024, Juiz de Fora, Brazil',\n",
       " '[7] Jin Yea Jang, Han-Mu Park, Saim Shin, Suna Shin, Byungcheon Yoon, and Gahgene Gweon. 2022. Automatic Gloss-level Data Augmentation for Sign Language Translation. In Proceedings of the Thirteenth Language Resources and Eva- luation Conference, Nicoletta Calzolari, Fr√©d√©ric B√©chet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, H√©l√®ne Mazo, Jan Odijk, and Stelios Piperidis (Eds.). European Language Resources Association, Marseille, France, 6808‚Äì6813. https://aclanthology.org/2022.lrec-1.734',\n",
       " '[8] Z. Liang, H. Li, and J. Chai. 2023. Sign Language Translation: A Survey of Approaches and Techniques. Electronics 12, 12 (2023). https://doi.org/10.3390/ electronics12122678',\n",
       " '[9] Manuella Aschoff C. B. Lima, Tiago Maritan U. de Ara√∫jo, Rostand E. O. Costa, and Erickson S. Oliveira. 2022. A machine translation mechanism of Brazilian Portuguese to Libras with syntactic-semantic adequacy. Natural Language Engineering 28, 3 (2022), 271‚Äì294. https://doi.org/10.1017/S1351324920000662',\n",
       " '[10] Alexandre Magueresse, Vincent Carles, and Evan Heetderks. 2020. Low-resource Languages: A Review of Past Work and Future Challenges. CoRR abs/2006.07264 (2020). arXiv:2006.07264 https://arxiv.org/abs/2006.07264',\n",
       " '[11] Mieradilijiang Maimaiti, Yang Liu, Huanbo Luan, Zegao Pan, and Maosong Sun. 2021. Improving Data Augmentation for Low-Resource NMT Guided by POSTagging and Paraphrase Embedding. ACM Trans. Asian Low-Resour. Lang. Inf. Process. 20, 6, Article 107 (aug 2021), 21 pages. https://doi.org/10.1145/3464427',\n",
       " '[12] Amit Moryossef, Kayo Yin, Graham Neubig, and Yoav Goldberg. 2021. Data Augmentation for Sign Language Gloss Translation. In Proceedings of the 1st International Workshop on Automatic Translation for Signed and Spoken Languages (AT4SSL), Dimitar Shterionov (Ed.). Association for Machine Translation in the Americas, Virtual. https://aclanthology.org/2021.mtsummit-at4ssl.1',\n",
       " '[13] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Pierre Isabelle, Eugene Charniak, and Dekang Lin (Eds.). Association for Computational Linguistics, Philadelphia, Pennsylvania, USA, 311‚Äì318. https://doi.org/10.3115/ 1073083.1073135',\n",
       " '[14] Alberto Poncelas, Dimitar Shterionov, Andy Way, Gideon Maillette de Buy Wenniger, and Peyman Passban. 2018. Investigating Backtranslation in Neural Machine Translation. In Proceedings of the 21st Annual Conference of the European Association for Machine Translation, Juan Antonio P√©rez-Ortiz, Felipe S√°nchezMart√≠nez, Miquel Espl√†-Gomis, Maja Popoviƒá, Celia Rico, Andr√© Martins, Joachim Van den Bogaert, and Mikel L. Forcada (Eds.). Alicante, Spain, 269‚Äì278. https://aclanthology.org/2018.eamt-main.25',\n",
       " '[15] Surangika Ranathunga, En-Shiun Annie Lee, Marjana Prifti Skenduli, Ravi Shekhar, Mehreen Alam, and Rishemjit Kaur. 2023. Neural Machine Translation for Low-resource Languages: A Survey. ACM Comput. Surv. 55, 11, Article 229 (feb 2023), 37 pages. https://doi.org/10.1145/3567592',\n",
       " '[16] V√≠ctor M. S√°nchez-Cartagena, Miquel Espl√†-Gomis, Juan Antonio P√©rez-Ortiz, and Felipe S√°nchez-Mart√≠nez. 2021. Rethinking Data Augmentation for Low-Resource Neural Machine Translation: A Multi-Task Learning Approach. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, MarieFrancine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 8502‚Äì8516. https://doi.org/10.18653/v1/2021.emnlp-main.669',\n",
       " '[17] Vin√≠cius Ver√≠ssimo, Cec√≠lia Silva, Vitor Hanael, Caio Moraes, Rostand Costa, Tiago Maritan, Manuella Aschoff, and Tha√≠s Gaud√™ncio. 2019. A study on the use of sequence-to-sequence neural networks for automatic translation of brazilian portuguese to libras. In Proceedings of the 25th Brazillian Symposium on Multimedia and the Web . 101‚Äì108.',\n",
       " '[18] Jing Wang and Lina Yang. 2022. Effective Data Augmentation Methods for CCMT 2022. In Machine Translation, Tong Xiao and Juan Pino (Eds.). Springer Nature Singapore, Singapore, 135‚Äì142.',\n",
       " '[19] Xinyi Wang, Hieu Pham, Zihang Dai, and Graham Neubig. 2018. SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun‚Äôichi Tsujii (Eds.). Association for Computational Linguistics, Brussels, Belgium, 856‚Äì861. https://doi.org/10.18653/v1/D18-1100   326   -----']"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 220
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:54:18.735951Z",
     "start_time": "2025-05-12T22:54:18.727002Z"
    }
   },
   "cell_type": "code",
   "source": "corpus.loc[4]",
   "id": "908a1e0ac4df7a74",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "titulo               Tagging Enriched Bank Transactions Using LLM-G...\n",
       "informacoes_url                                                       \n",
       "idioma                                                         english\n",
       "storage_key          ../articles/original/english/985-24767-1-10-20...\n",
       "autores              [Daniel de S. Moraes,  Polyana B. da Costa,  P...\n",
       "data_publicacao                                             11-09-2024\n",
       "resumo               avaliar a qualidade das taxonomias e as tags a...\n",
       "keywords             Large Language Models, Natural Language Proces...\n",
       "referencias          [[1] Rohan Anil, Sebastian Borgeaud, Yonghui W...\n",
       "text                 9/02/2008 √†s 19:20:05 Molho de macarr√£o de coz...\n",
       "artigo_tokenizado    [9/02/2008, √†s, 19:20:05, Molho, de, macarr√£o,...\n",
       "pos_tagger           [[9/02/2008, NUM, NUM], [√†s, ADP, ADP], [19:20...\n",
       "lema                 [9/02/2008, a o, 19:20:05, Molho, de, macarr√£o...\n",
       "Name: 4, dtype: object"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 136
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:42:36.271875Z",
     "start_time": "2025-05-12T23:42:34.244756Z"
    }
   },
   "cell_type": "code",
   "source": "text = pymupdf4llm.to_markdown('../articles/original/english/985-24767-1-10-20240923.pdf')",
   "id": "89f5a1755e218b8a",
   "outputs": [],
   "execution_count": 249
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:46:59.688380Z",
     "start_time": "2025-05-12T23:46:59.684537Z"
    }
   },
   "cell_type": "code",
   "source": "text",
   "id": "7c0c9851ccd0df50",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# **Tagging Enriched Bank Transactions Using LLM-Generated Topic** **Taxonomies**\\n\\n## Daniel de S. Moraes\\n#### TeleM√≠dia Lab - PUC-Rio danielmoraes@telemidia.puc-rio.br\\n## Ivan de J. P. Pinto\\n#### TeleM√≠dia Lab - PUC-Rio ivan@telemidia.puc-rio.br\\n## Matheus A. S. Pinto\\n#### BTG Pactual matheus.adler@btgpactual.com\\n## Gabriela Tourinho\\n#### BTG Pactual gabriela.tourinho@btgpactual.com\\n\\n## Polyana B. da Costa\\n#### TeleM√≠dia Lab - PUC-Rio polyana@telemidia.puc-rio.br\\n## S√©rgio Colcher\\n#### TeleM√≠dia Lab - PUC-Rio colcher@inf.puc-rio.br\\n## Rafael H. Rocha\\n#### BTG Pactual rafael-h.rocha@btgpactual.com\\n## Marcos Rabaioli\\n#### BTG Pactual marcos.rabaioli@btgpactual.com\\n\\n## Pedro T. C. Santos\\n#### TeleM√≠dia Lab - PUC-Rio thiagocutrim@telemidia.puc-rio.br\\n## Antonio J. G. Busson\\n#### BTG Pactual antonio.busson@btgpactual.com\\n## Rennan Gaio\\n#### BTG Pactual rennan.gaio@btgpactual.com\\n## David Favaro\\n#### BTG Pactual david.favaro@btgpactual.com\\n\\n**Taxonomies**\\n\\n\\n**Transaction: Purchase**\\n\\n**at PAG*distr_nona on**\\n**19/02/2008 at 19:20:05**\\n\\n**Italian Cuisine**\\n\\n**Pasta**\\n\\n**Sauce**\\n\\n**Pizza**\\n\\n**Wine**\\n\\n\\n\\n\\n\\n|name|macrocategory|microcategory|\\n|---|---|---|\\n|Nona Ristorante|Food|Restaurant|\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n**Figure 1: Overview of our method. (Note: Merchant and transaction data are fabricated for demonstration only).**\\n\\n### **ABSTRACT**\\n\\nThis work presents an unsupervised method for tagging banking\\nconsumers‚Äô transactions using automatically constructed and expanded topic taxonomies. Initially, we enrich the bank transactions\\nvia web scraping to collect relevant descriptions, which are then\\npreprocessed using NLP techniques to generate candidate terms.\\nTopic taxonomies are created using instruction-based fine-tuned\\nLLMs (Large Language Models). To expand existing taxonomies\\nwith new terms, we use zero-shot prompting to determine where\\nto add new nodes. The resulting taxonomies are used to assign\\ndescriptive tags that characterize the transactions in the retail bank\\ndataset. For evaluation, 12 volunteers completed a two-part form\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia‚Äô2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n¬© 2024 SBC ‚Äì Brazilian Computing Society.\\nISSN 2966-2753\\n\\n\\nassessing the quality of the taxonomies and the tags assigned to\\nmerchants. The evaluation revealed a coherence rate exceeding 90%\\nfor the chosen taxonomies. Additionally, taxonomy expansion using\\nLLMs demonstrated promising results for parent node prediction,\\nwith F1-scores of 89% and 70% for *Food* and *Shopping* taxonomies,\\nrespectively.\\n### **KEYWORDS**\\n\\nLarge Language Models, Natural Language Processing, Web Scrapping, Topic Modeling\\n### **1 INTRODUCTION**\\n\\nMany recent studies have focused on the application of Machine\\nLearning-based methods for the classification and characterization of financial transactions. For example, Vollset et al . [23] and\\nBusson et al . [4] explored an approach to hierarchically classifying\\n\\n\\n267\\n\\n\\n-----\\n\\nWebMedia‚Äô2024, Juiz de Fora, Brazil Moraes et al.\\n\\n\\nfinancial transactions, employing a predefined set of categories/subcategories that describe purchase types and transactions. However,\\nthese methods apply a limited, predefined set of static classes, which\\nrestricts the ability to extend classifications based on user experiences when encountering new, previously undefined categories.\\nIn this context, to expand the possible set of classes/tags to label\\na transaction, we developed an unsupervised method based on\\n*topic taxonomies* . Taxonomies are very useful in the structural and\\nsemantic analyses of topics and textual data. However, creating and\\nmaintaining them is often costly and challenging to scale manually.\\nTherefore, recent works have tackled the automatic creation and\\nexpansion of *topic taxonomies*, in which each node in a hierarchy\\nrepresents a conceptual topic composed of semantically coherent\\n\\nterms.\\n\\nWe present an unsupervised method for automatically constructing and expanding topic taxonomies with instruction-based LLMs\\n(Large Language Models), in a Zero-Shot manner. Candidate terms\\nfor the initial version of the taxonomy are obtained using topic\\nmodeling and keyword extraction techniques. Then we apply LLMs\\nto post-process the resulting terms, create a hierarchy, and add new\\nterms to an existing taxonomy. Since the taxonomies are derived\\nfrom a corpus of unstructured texts describing niches of consuming\\nhabits, we opted to investigate the use of LLMs in our approach.\\nLLMs are often pre-trained on a large corpus of text, allowing them\\nto learn contextual representations that capture the intricacies of\\nhuman language.\\nWe applied our method to a private dataset of transactions of\\na retail bank, enriched with scraped data from food and shopping\\ncompanies, and evaluated the resulting taxonomies quantitatively.\\nThe generated tags of our topic taxonomies are then assigned to the\\nbank transactions characterizing the companies in each transaction,\\nas shown on Figure 1. In total, 58 topic taxonomies were created\\nfor the *Food* category and 6 for the *Shopping* category.\\nA two-step quantitative evaluation was conducted on a subset of the taxonomies. For this evaluation, we selected the topic\\ntaxonomies with the highest number of terms in each category:\\n\"Brazilian Cuisine\" from *Food* and \"Clothing and Accessories\" from\\n*Shopping* . Taxonomies with more terms are most likely to result\\nin a deeper hierarchy, which gives more data for evaluation. We\\nasked 12 volunteers to answer a two-part form, which assessed\\nthe quality of the created taxonomies and the quality of the tags\\nassigned to label transactions. The evaluation showed an average\\ncoherence of tags to transactions above 90%.\\nAs more scraped data from food and shopping companies are\\nadded to the retail bank‚Äôs dataset, the topic taxonomies will need\\nto be updated to include new terms. We used LLMs for this task as\\nwell, employing commercial LLMs like Gemini Pro [ 1 ] and GPT4 [ 14 ], alongside open-source LLM options such as LLaMA-Alpaca\\n(7B) [ 22 ], Phi-2 [1], and Mixtral 8x7B [ 9 ]. We showcase their results in\\nboth taxonomy creation and expansion. For the expansion part, we\\nalso compared our method to existing ones (a BERT-based method\\nand Musubu[ 20 ]) on the SemEval dataset and our generated taxonomies as well. Gemini Pro achieved the best results, with F1scores of 89% and 70% for parent node prediction on the *Food* and\\n*Shopping* taxonomies, respectively.\\n\\n1 https://huggingface.co/microsoft/phi-2\\n\\n\\nThe remainder of the paper is structured as follows: Section 2,\\nreviews the related work, highlighting existing approaches. Section 3 provides the necessary background, laying the foundation\\nfor our methodologies and contextualizing our contributions. Section 4 details the dataset construction process, explaining how we\\nenriched and prepared the data for the taxonomies‚Äô construction. In\\nSection 5, we describe the creation of the taxonomies, outlining the\\nmethods used to generate them. Section 6 discusses the expansion\\nof the taxonomies, demonstrating how they can be dynamically\\nextended to accommodate new categories. Section 7 focuses on the\\nevaluation of these taxonomies, presenting the metrics and results\\nthat validate their accuracy and also the quality of the tags assigned\\nto the transactions. Finally, Section 8 concludes the paper, summarizing our findings, discussing their implications, and suggesting\\ndirections for future research.\\n### **2 RELATED WORK**\\n\\nTaxonomies represent the structure behind a collection of documents, organizing the hierarchical relationships between terms in a\\ntree structure [ 13 ]. They play an essential part in the structural and\\nsemantic analysis of textual data, providing valuable content for\\nmany applications that involve information retrieval and filtering,\\nsuch as web searching, recommendation systems, classification,\\nand question answering.\\nSince creating and maintaining taxonomies is a costly task, often difficult to scale if done manually, methods that automatically\\nconstruct and update them are desirable. Early works on automatic\\ntaxonomy creation focused on building hypernym-hyponym taxonomies, where each pair of terms expresses an ‚Äòis-a‚Äô relationship\\n\\n[ 19 ]. More recent works have tackled the automatic creation of\\nother taxonomies, such as topic taxonomies. In a topic taxonomy,\\neach node represents a conceptual topic composed of semantically\\ncoherent terms.\\n\\nIn this context, Zhang et al . [28] developed TaxoGen, an unsupervised method for constructing topic taxonomies. Taxogen uses\\nthe SkipGram model from an input text corpus to embed all the\\nconcept terms into a latent space that captures their semantics. In\\nthis space, the authors applied a clustering method to construct a\\nhierarchy recursively based on a variation of the spherical K-means\\nalgorithm.\\nAnother work that focuses on topic taxonomies is TaxoCom\\n\\n[ 10 ], a framework for automatic taxonomy expansion. TaxoCom is\\na hierarchical topic discovery framework that recursively expands\\nan initial taxonomy by discovering new sub-topics. It uses locally\\ndiscriminative embeddings and adaptive clustering, resulting in\\na low-dimensional embedding space that effectively encodes the\\ntextual similarity between terms. One main disadvantage of TaxoCom is that it requires a large set of quality phrases in the target\\nlanguage, and curating these phrases can be costly. The quality of\\nthe output taxonomy is highly dependent on those phrases.\\nRegarding the automatic expansion of taxonomies, an important related example is Musubu [ 20 ], a framework for low-resource\\ntaxonomy enrichment that uses a Language Model (LM) as a knowledge base to infer term relationships. For the taxonomy expansion\\npart of out method, we used Musubu as a baseline for comparison.\\n\\n\\n268\\n\\n\\n-----\\n\\nTagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies WebMedia‚Äô2024, Juiz de Fora, Brazil\\n\\n\\nAs to using Large Language Models for taxonomy tasks, Chen\\net al . [6] investigated how LLMs, like GPT-3, perform in taxonomy\\nconstruction tasks. The authors compared two approaches: finetuning, which involves training the LLM on a specific dataset to\\nadapt it for taxonomy tasks, and prompt techniques, where the\\nLLM receives instructions and examples to perform a task without\\nbeing explicitly trained for it. Their findings showed that prompt\\ntechniques such as few-shot learning generally outperform finetuning, particularly with smaller datasets. Based on these findings,\\nwe applied prompting techniques, specifically zero-shot prompting,\\nacross various LLMs to assess their effectiveness in constructing and\\nexpanding taxonomies. Section 7 shows the results of our approach,\\nas well as the results of applying Musubu[20] as baseline.\\n### **3 BACKGROUND**\\n\\nIn this section, we provide a comprehensive background on Large\\nLanguage Models (LLMs), and the concept of Prompt-tuning. These\\nconcepts are essential to understanding the construction and editing\\nof taxonomies utilizing LLMs.\\n### **3.1 Large Language Models**\\n\\nLately, Large Language Models (LLMs) have garnered significant\\nattention for their exceptional performance in various NLP tasks.\\nLLMs, such as GPT-3[ 3 ] and LLAMA[ 22 ], are characterized by their\\nmassive scale, comprising billions of parameters and being trained\\non vast amounts of data. These models are often pre-trained in\\nan unsupervised manner on large corpora of textual data, such as\\nbooks, articles, and web pages, allowing them to learn contextual\\nrepresentations that capture the intricacies of human language.\\nTo use LLMs for specific purposes, a highly effective approach is\\nto fine-tune them on task-specific data. Fine-tuning enables LLMs\\nto adapt to specific domains or tasks with minimal labeled data,\\nsignificantly reducing the need for large annotated datasets. However, in scenarios where labeled data is scarce or difficult to obtain,\\nLLMs can also be used without specific training or additional data,\\nin a Zero-Shot manner [ 21 ]. Given the scale of these models and\\nthe data they are trained on, LLMs embed vast knowledge that enables them to achieve high generalization capabilities and perform\\ntasks in diverse contexts, even without specific training for those\\ntasks[16].\\nIn our experiments, we tested several types of language models,\\nfrom private LLMs (GPT 4 [ 14 ], Gemini Pro [2] ), to open-source LLMs\\n(Llama 2 [ 22 ]), to a Mixture of Experts LLM (Mixtral [ 9 ], and a Small\\nLanguage Model (SLM), Phi 2 [3] .\\n### **3.2 Prompt Engineering**\\n\\nPrompt Engineering is a fundamental technique used to enhance\\nthe performance and adaptability of Large Language Models (LLMs)\\nin specific tasks or domains [ 7 ]. It involves optimizing and crafting\\nprompts to efficiently use language models (LMs) [ 3 ]. This approach\\nallows researchers and practitioners to tailor the behavior and output of LLMs, making them more suitable for targeted applications.\\n\\n2 https://deepmind.google/technologies/gemini/pro/\\n3 https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-ofsmall-language-models/\\n\\n\\nTechniques such as Zero-shot prompting, Few-shot prompting,\\nChain of Thought, ReAct, Self-Consistency etc. have been explored\\nto guide LLMs toward desired responses [ 18, 21, 25 ‚Äì 27 ]. The effectiveness of prompt tuning has been demonstrated in various\\napplications, including question-answering, summarization, and\\ndialogue generation. The choice of prompt greatly influences the\\ngenerated output, and by carefully crafting prompts, researchers\\ncan guide the model‚Äôs responses toward desired behaviors. For example, in language translation, a prompt can specify the source\\nlanguage and desired target language to ensure accurate and fluent translations. In our method, we used the Zero-Shot prompting\\ntechnique.\\n### **3.3 Zero-Shot Prompting**\\n\\nSince LLMs (Large Language Models) are trained on vast amounts\\nof data, they can follow instructions and perform tasks in contexts\\nwhere they were not specifically trained, in a Zero-Shot (ZS) manner.\\nThis prompting style allows the model to adapt, making it versatile.\\nA Zero-Shot (ZS) prompt directly instructs the model to perform\\na task without additional examples or demonstrations to guide\\nthe LLM‚Äôs response, which is why they are also known as task\\ninstructions [21].\\nIn a study by Li [11], the authors highlighted several advantages\\nof using ZS prompts, such as the ability to craft highly interpretable\\nprompts, requiring fewer training data or examples, a more straightforward prompt design process, and a flexible prompt structure.\\nAdditionally, Reynolds and McDonell [17] noted that carefully engineered zero-shot prompts can outperform few-shot prompts in\\ncertain scenarios, as examples can sometimes be interpreted as part\\nof a narrative rather than as a guiding mechanism. This finding also\\ninfluenced our decision to use zero-shot prompting in our method.\\n### **4 DATASET CONSTRUCTION**\\n\\nThis work uses a proprietary dataset consisting of consumer transactions from a retail bank. Each transaction includes only a merchant\\nname indicating the business where that purchase occurred along\\nwith macro and micro categories as illustrated in Figure 1 The macro\\nand micro are originally assigned by [ 4 ] using the information from\\nthe business activities and products.\\nWe focus on two macro-categories from this dataset: *Food* and\\n*Shopping*, selecting the top 50,000 businesses with the highest number of transactions for each category.\\nWith the limited initial information, assigning detailed tags to\\ntransactions is challenging. To address this, we augment the dataset\\nthrough a data enrichment process involving web scraping. Using\\ntools such as Selenium [4] and Beautiful Soup [5], we gathered activity\\ndescriptions for companies in each macro category. For the *Food*\\nmacro category, the search was conducted on specialized platforms\\nfor restaurants and food delivery services. For the *Shopping* macro\\ncategory, we obtained establishment descriptions directly from\\ninternet indexing and search tools.\\nIn the context of enrichment for the *Food* macro category, web\\nscraping was conducted as follows: (1) the centers of all Brazilian\\nstate capitals and the Federal District were used as base locations\\n\\n4 https://www.selenium.dev/about/\\n5 https://readthedocs.org/projects/beautiful-soup-4/downloads/pdf/latest/\\n\\n\\n269\\n\\n\\n-----\\n\\nWebMedia‚Äô2024, Juiz de Fora, Brazil Moraes et al.\\n\\n\\nfor restaurant searching; (2) for each location, restaurants listed on\\nthe first one hundred pages of the platform were extracted. After\\ncompleting these steps, the information was combined with the\\nmerchant database using the merchant‚Äôs name and micro categories.\\nFor the *Shopping* macro category, the description of each merchant was obtained using Google Search Engine [6], selecting descriptions from the first ten search results. The final description\\nconsists of a concatenation of all the obtained descriptions. The\\nsearch queries were constructed using the merchant names combined with their micro categories.\\n### **5 TAXONOMY CONSTRUCTION**\\n\\nTo automatically create topic taxonomies for *Food* and *Shopping*\\nbusinesses, we developed a 3-step method. First, we preprocess the\\ndescriptions in our enriched dataset to retain only the relevant parts\\nof the text. Next, we apply two techniques to select candidate terms\\nfor the topic taxonomies: keyword extraction and topic modeling.\\nIn the post-processing phase, we use large language models\\n(LLMs) to refine the results of each step, filtering out unrelated\\nterms. Finally, we use LLMs again to organize the final terms into\\nhierarchies, forming the topic taxonomies.\\n### **5.1 Preprocessing**\\n\\nWe applied a few NLP techniques to refine the businesses‚Äô descriptions in our dataset. At first, we remove stop words to eliminate\\ncommonly used words that do not carry significant meaning in\\nour contexts. Then, to retain only the most relevant portions of the\\ndescriptions, we employ part-of-speech (POS) tagging to identify\\nand exclude words that belong to specific POS categories. The list\\nof POS tag categories that were removed includes ADV, CCONJ,\\nADP, AUX, CONJ, DET, INTJ, PART, PRON, PUNCT, SYM, SCONJ,\\nADJ, VERB, PROPN. [7]\\n\\nAfter this initial preprocessing step, we run the first iteration\\nof the candidate term selection part to build a filter of generic\\nwords, not to create topic taxonomies yet. For this step, we use the\\nentire corpus of descriptions for each macro category, resulting in\\ntwo corpora ( *Food* and *Shopping* ). For each micro category in the\\nmacro categories‚Äô corpora, we use Keyword Extraction and Topic\\nModeling to gather candidate terms for the filter, combining the\\nresults of both techniques in a list. Then, We use an LLM to remove\\nthe terms it identifies as unrelated to the main topic (each micro\\ncategory) from the list. The prompt that we used for requesting this\\nseparation is illustrated below.\\n\\nprompt= \"Given the terms in the following list: \"+\\n<wordsList> +\". Separate them into two groups. In\\ngroup 1 the terms with no relation to the topic \"+\\n<type> +\". And in group 2 the terms that are related.\"\\n\\n**Listing 1: Prompt for separating candidate terms related to**\\n**the type of establishment**\\n\\nBy using this prompt, we try to ensure that the model‚Äôs response\\nis consistently formatted according to the pattern described in it,\\nfacilitating the processing of the resulting string, although, some\\n\\n6 https://www.google.com\\n7 https://spacy.io/usage/linguistic-features#pos-tagging\\n\\n\\nof the LLMs we tested did not output the response in the requested\\nformat. Once we complete one iteration of this method for each\\nmacro category in our dataset, we add the words of group 2 to the\\ncorresponding list of generic words. We apply the corresponding\\nfilter of generic words for each macro category corpus, resulting in\\nthe final preprocessed corpus.\\n### **5.2 Candidate Terms Selection**\\n\\nFor this part of our method, we use each preprocessed corpus\\nseparately. For the *Food* corpus, we group the descriptions based\\non their micro-categories, creating 58 sub-corpus specific to that\\ndomain. We have six micro categories for the *Shopping* corpus,\\nresulting in 6 specific sub-corpus. The candidate terms selection\\nmethods are applied to each sub-corpus, creating topic taxonomies\\nwhere the main topic is the micro category.\\n\\n*5.2.1* *Keyword Extraction.* The first approach to candidate term\\nselection was to use an unsupervised keyword selection method\\ncalled Yake! [ 5 ]. This method is based on statistical text features\\nextracted from single documents to select the most relevant keywords from that text. It does not require training on a document set\\nand is not dependent on dictionaries, text size, language, domain,\\nor external corpora.\\nYake! allows for the specification of parameters such as the\\nlanguage of the text, the maximum size of the n-grams being sought,\\nand others. In our method, we customized only the language to\\nPortuguese, and the maximum number of keywords sought for each\\nset of descriptions was 30 words.\\nAfter extracting the keywords from each group of descriptions,\\nwe obtained a total set of *ùëÅ* candidate terms. However, these terms\\nare further filtered using an LLM, where we ask it to separate the\\nterms related to the main topic from those unrelated, as explained\\nearlier in subsection 5.1.\\n\\n*5.2.2* *Topic Modeling.* Our second approach to collecting initial\\ntopics and candidate terms was Topic Modeling. We applied the\\nLatent Dirichlet Allocation algorithm [ 2 ], available at the Gensim\\nLibrary [8] .\\nWe construct a dictionary for each macro-category corpus in\\nour macro-categories corpora by extracting unique tokens and\\nbigrams. After a few empirical tests, we set the minimum frequency\\nof a bigram to 20 occurrences. Since some corpora have a minimal\\nnumber of tokens (the micro category \"Greek Cuisine\" from the *Food*\\nmacro category has only five stores marked as such, with a corpus\\nof only 127 tokens), we had to set a reasonably small number so that\\nsmaller corpora could also have a few bigrams. With the resulting\\ndictionary of tokens, the LDA algorithm was applied. Three main\\nparameters are to be defined in an LDA algorithm: number of topics,\\n*alpha*, and *beta* .\\nThe number of topics defines the latent topics to be extracted\\nfrom the corpus. The parameter *alpha* is *a priori* belief in documenttopic distribution, while *beta* is *a priori* belief in topic-word distribution.\\nTo define the number of topics for each micro category corpus,\\nwe tried numbers from 1 to 5, constantly checking which configuration would result in the best average topic coherence for that\\n\\n8 https://pypi.org/project/gensim/\\n\\n\\n270\\n\\n\\n-----\\n\\nTagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies WebMedia‚Äô2024, Juiz de Fora, Brazil\\n\\n\\ncorpus. Small corpora would have 1 or 2 topics, while bigger ones\\nwould have 5. To correctly define the *alpha* and *beta* priors, we\\nwould have to analyze the distribution for each category corpus\\n\\n[ 24 ]. Since this would be rather difficult, we set those priors to be\\nauto-defined by the LDA algorithm, which learns these parameters\\nbased on the corpus. We select the terms with the highest coherence\\nwith the resulting topics. Each topic returns 20 words with their\\ncoherence scores, but we do not use all of them as some have very\\nlow coherence. After testing a few configurations, for each topic,\\nwe select 60% of the terms with the highest coherence within that\\ntopic.\\nWith initial terms for each topic taxonomy, we ask an LLM\\nto separate the ones closely related to the main topic from those\\nunrelated, as mentioned earlier.\\n### **5.3 Hierarchy Construction**\\n\\nOnce we have the post-processed lists of candidate terms obtained\\nby each technique mentioned in subsection 5.2, we merge them and\\nremove repetitions. After the merge, for each macro category, we\\nhave lists of terms for each micro category, representing each topic\\ntaxonomy. However, they do not have any hierarchy level between\\nthe terms configuring the taxonomy.\\nTo tackle this problem, we use an LLM again, this time with a\\nprompt that searches for sub-categories within the terms of a topic\\nto create these hierarchies. The prompt is illustrated below:\\n\\nprompt=\"Create a dictionary by hierarchically arranging the\\n\\nfollowing words:\" + <wordsList> +.\" Use JSON format as\\nthe output such as the following: {\\\\\"key\\\\\": [\\\\\" list\\nof words\\\\\"]}\"\\n\\n**Listing 2: Prompt for creating a hierarchy for each list of**\\n**tags.**\\n\\nWith this prompt, we seek to ensure that the LLM response has a\\nconsistent pattern and facilitates handling the returned string. After\\nthis step, we have a hierarchy of terms in each topic taxonomy in\\nthe *Food* and *Shopping* macro categories.\\n### **5.4 Merchant Tagging**\\n\\nWith the topic taxonomies for both *Food* and *Shopping* macrocategories, we can now assign tags to merchants/establishments.\\nTo do so, we use the descriptions attached to these establishments,\\nand we see which terms from a taxonomy are mentioned in their\\ndescriptions with a reverse index algorithm. We employ the taxonomy whose topic is the same as the establishment‚Äôs micro category,\\nas shown in Figure 2.\\n### **6 TAXONOMY EXPANSION**\\n\\nAnother essential part of our method is the automatic expansion of\\nexisting taxonomies as new terms arrive, derived from additional\\nmerchant scrapped data, as shown in Section 4. In this section, we\\npresent our approach to taxonomy expansion by using instructionbased LLMs.\\n\\nAs new transactions may include new businesses, new terms\\ncan emerge from the descriptions obtained through the scraping\\nprocess. Therefore, we need to update the taxonomies with these\\n\\n\\nnew terms maintaining and enriching the created hierarchies with\\nthe potential new terms.\\nAfter completing the transaction enrichment process, including\\nthe search for business descriptions and the selection of candidate\\nterms, if relevant terms not included in the current hierarchies are\\ndetected, we initiate the expansion process.\\n### **6.1 Prompt engineering instruction for** **taxonomy representation**\\n\\nFirst, we represent our topic taxonomies in a format that can be\\ninterpreted by an LLM. We employed a generic prompt, illustrated\\nbelow, across all tested methods to convert topics into root nodes\\nand their terms into child nodes.\\n\\nChilds of [ROOT]: [CHILD1,CHILD2,CHILD3]\\nChilds of [CHILD1]: [CHILD4,CHILD5]\\n\\nChilds of [CHILD2]: [CHILD6]\\n\\n...\\n\\n**Listing 3: Prompt for representation of taxonomy**\\n### **6.2 Predicting the parent of a node**\\n\\nTo experiment with taxonomies expansion, we used two datasets:\\nour *Food* and *Shopping* topic taxonomies and the taxonomies from\\nSemEval-2015 Task 17 [ 15 ]. Those are low-resource taxonomies,\\nwith thousands of nodes or less, which are appropriate for the current prompt size of LLMs. We used the SemEval dataset to compare\\nthe results with well-established methods for taxonomy expansion,\\nsuch as Musubu [ 20 ]. Similar to their experiments, we hid 20% of\\nthe terms (chosen randomically) in the taxonomies to predict their\\nrespective parent nodes. To verify the parent/root of a new term,\\nwe used the following prompt:\\n\\n**Listin** **g** **4: Prom** **p** **t for searchin** **g** **for a node‚Äôs** **p** **arent**\\n\\nprompt=\"Who is the father of \"+<new_term>+\"?\"\\n\\nIn Table 1, we see the F1-Scores for parent node prediction. Equation 1 showcases how to calculate the F1-Score. TP is the number\\n\\nof true positives, nodes that were correctly assigned as parents of\\nchild nodes. FP is the number of false positives, nodes that were\\nincorrectly assigned as a parent to a child node. FN is the number\\nof false negatives, nodes that should have been assigned as parent\\nnodes but were not.\\n\\n2 ‚àó *ùëáùëÉ*\\n*ùêπ* 1 = (1)\\n2 ‚àó *ùëáùëÉ* + *ùêπùëÉ* + *ùêπùëÅ*\\n\\nFor baseline models, we used Bert and Musubu; for commercial\\nLLMs, Gemini Pro and GPT-4; and for open-source LLMs, LLamaAlpaca(7B), Phi-2, and Mixtral 8x7B. We evaluate them in 4 taxonomies from the SemEval dataset and our taxonomies. For each\\ntaxonomy, the LLMs perform significantly better than Musubu,\\nwith GPT-4 and Gemini Pro having the highest F1-Scores, with the\\nlatter beating the former by a few points. However, the most recent\\nopen-source options (Phi-2 and Mixtral 8x7B) are getting close in\\nperformance.\\n\\n\\n271\\n\\n\\n-----\\n\\nWebMedia‚Äô2024, Juiz de Fora, Brazil Moraes et al.\\n\\n\\n\\n\\n**Tags assigned to establishments from the**\\n\\n***\"Clothing & Accessories\"*** **micro category**\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n**Figure 2: Assigning tags to establishments based on a topic**\\n**taxonomy.**\\n\\nIt is important to note that while SemEval taxonomies have\\nthousands of nodes, ours have only a few hundred, which we can\\nassume is a significant reason for the degrading performance of\\nMusubu and Bert (LMs or LM-based methods). In contrast, the LLMs\\nhave a robust performance in such low-resource settings. This also\\nshows that LLMs have a remarkable understanding of questions\\nand zero-shot performance, generalizing well even for datasets in\\ndifferent languages.\\n### **7 TAXONOMY EVALUATION**\\n\\nTo properly evaluate the topic taxonomies that we created in this\\nwork, we developed a two-step qualitative evaluation of a limited\\npart of the results.\\nIn total, 58 topic taxonomies were created for the *Food* set and\\n6 for the *Shopping* set. For our evaluation, we selected the topic\\ntaxonomies with the highest number of terms in each part (the\\n\"Brazilian Cuisine\" taxonomy for the *Food* part and the \"Clothing\\nand Accessories\" taxonomy for the *Shopping* one). First, we assess\\nthe quality of removing generic terms from each taxonomy, and\\nthen, we evaluate the tags assigned to establishments based on that\\ntaxonomy. We asked 12 volunteers to answer a two-part form.\\n*Part 1 - Accuracy of the terms that were selected as related to the*\\n*topic* : In this part, we evaluate if the LLMs could correctly group\\nthe relevant and non-relevant terms, removing the generic terms.\\nTo do so, we defined a ground truth with the relevant terms as\\n\\n\\ntrue positives and the non-relevant terms as true negatives. Table 3\\nshows the results.\\n\\nGPT-4 was the best model, followed by Gemini Pro, both scoring\\nover 60% accuracy for the Brazilian Cuisine taxonomy and over\\n86% accuracy for the Clothing and Accessories taxonomy. Smaller\\nlanguage models such as Phi 2 and Llama 2 7B performed poorly\\nboth in removing generic terms and in formatting the response\\naccordingly, with Phi 2 being particularly verbose.\\n*Part 2 - Human Evaluation of the Quality of the Tagging Process* :\\nIn this part, the volunteers were asked to examine if the tags assigned to an establishment were appropriate and coherent to that\\nestablishment‚Äôs description. We selected the top 5 establishments\\nwith the highest transactions for each micro category. We asked our\\nevaluators to analyze the tags assigned to describe that establishment and choose the ones that were not appropriate. This way, we\\nhave a coherence ratio for each establishment based on the number\\n\\nof proper tags divided by the total number of tags. We average the\\nresults of our 12 evaluators and present them in Table 2. Figure 2\\nshows the \"Clothing & Accessories\" taxonomy that was evaluated\\nand 2 of the merchants and the tags assigned to them that were\\nincluded in the evaluation.\\n### **8 CONCLUSION**\\n\\nIn this work, we presented an unsupervised method for automatically creating and expanding topic taxonomies using LLMs. We\\nevaluated some of the generated taxonomies and applied them\\nin transaction tagging in a retailer‚Äôs bank dataset. The evaluation\\nshowed promising results, with average coherence scores above 90%\\nfor the two selected taxonomies. The taxonomies‚Äô expansion with\\nGemini Pro also showed exciting results for parent node prediction,\\nwith F1-scores of 89% and 70% for *Food* and *Shopping* taxonomies,\\nrespectively.\\nFor future work on taxonomy construction, we plan to test\\nmore robust term selection methods, such as embedding-based\\napproaches. We also plan on conducting ablation studies to validate whether the keyword extraction and topic modeling parts\\nhelp improve the quality of the taxonomies created, by using a\\nbaseline prompt to ask the LLM to generate child nodes given a\\nparent node. In terms of taxonomy expansion, there are several\\ntasks to explore, ranging from node-level operations to generating\\nentire sub-trees and identifying similar structures. Additionally, we\\nintend to enhance our instruction-tuned LLM for taxonomy tasks\\n\\n\\n|Method|SemEval-2015 Task 17 Chemical Equipment Food Science|Our taxonomies Food Shopping|\\n|---|---|---|\\n|Gemini Pro|0.68 0.80 0.91 0.72|0.89 0.73|\\n|GPT-4|0.65 0.78 0.89 0.70|0.87 0.71|\\n|Mixtral-8x7B|0.59 0.63 0.80 0.57|0.74 0.60|\\n|Phi-2|0.56 0.52 0.68 0.56|0.64 0.54|\\n|LLama 7B|0.51 0.42 0.58 0.46|0.60 0.49|\\n|Musubu|0.35 0.46 0.37 0.42|0.21 0.13|\\n|Bert-Base|0.13 0.14 0.12 0.16|0.11 0.06|\\n\\n\\n**Table 1: F1-score for parent node prediction.**\\n\\n272\\n\\n\\n-----\\n\\nTagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies WebMedia‚Äô2024, Juiz de Fora, Brazil\\n\\n|Col1|Brazilian Cuisine Taxonomy Average Coherence Number of Tags|Col3|Clothing & Accessories Taxonomy Average Coherence Number of Tags|Col5|\\n|---|---|---|---|---|\\n|Merchant 1|92.30%|10|97.11%|8|\\n|Merchant 2|94.23%|8|83.07%|5|\\n|Merchant 3|89.23%|5|94.38%|5|\\n|Merchant 4|87.17%|6|93.84%|5|\\n|Merchant 5|93.40%|7|97.43%|6|\\n\\n\\n\\n**Table 2: Results of evaluating the tags assigned to each merchant/establishment.**\\n\\n\\n|Col1|Brazilian Cuisine|Clothing & Accessories|\\n|---|---|---|\\n|Llama 2 7B|29.54%|52.78%|\\n|Phi 2|40.90|73.68%|\\n|Mixtral 8x7B v0.1|46.93%|70.27%|\\n|Gemini Pro|61.36%|86.11%|\\n|GPT 4|68.08%|86.84%|\\n\\n\\n**Table 3: Accuracy of using each LLM to remove generic words**\\n**from each topic taxonomy.**\\n\\nby fine-tuning or employing more efficient methods such as LoRA\\n\\n[8].\\n### **LIMITATIONS**\\n\\nTo address the limitations of our work, we begin with the taxonomy construction component. In this phase, we relied on topic\\nmodeling and keywords extraction to select candidate terms for our\\ntaxonomies. The LDA algorithm used for topic modeling performs\\nsuboptimally when the base corpus is small. Some of our topics had\\ncorpora with vocabularies of fewer than 100 words, which can result\\nin topics containing irrelevant or incoherent terms. Additionally,\\nwe could have further experimented with the LDA hyperparameters\\nfor each micro-category corpus.\\nRegarding the evaluation of the generated taxonomies, we did not\\nassess topic completeness. Without a ground truth, it is challenging\\nto quantify how comprehensively the terms in a taxonomy cover the\\nmain topic. Furthermore, we evaluated only 2 of the 64 taxonomies\\ngenerated by our method, leaving a substantial portion unexamined.\\nIn the taxonomy expansion experiment, we evaluated only a lowresource setting with fewer than a thousand nodes. Most studies\\nfocus on taxonomies with hundreds of thousands or more nodes.\\n\\nThis presents a challenge for LLMs due to their limited context.\\nAddressing this contextual limitation could benefit from insights\\nfound in other works that tackle similar issues [12].\\n### **ETHICS STATEMENT**\\n\\nIn this work, we ensure the utmost protection of customers and\\nstore sensitive data by exclusively using non-sensitive information\\nin our dataset. Our prompts solely rely on selected words from store\\ndescriptions, thus avoiding any direct usage of personal or sensitive\\ninformation. No customer-specific data or store-sensitive details\\nare integrated into the system, upholding privacy and security as\\ntop priorities.\\n\\n\\nMoreover, we strictly adhere to ethical guidelines during our\\nexperiments involving volunteers, and no personal data is collected\\nfrom them. Our focus lies solely on analyzing the results of our\\nproposed approach. Participants‚Äô anonymity and confidentiality are\\nmaintained throughout the research process, ensuring a responsible\\nand trustworthy approach to data handling.\\n### **ACKNOWLEDGMENTS**\\n\\nThe authors would like to acknowledge BTG Pactual for the partnership and financial support to this research.\\n### **REFERENCES**\\n\\n[1] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,\\nRadu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al . 2023. Gemini:\\na family of highly capable multimodal models.\\n\\n[2] David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation.\\n*Journal of machine Learning research* 3, Jan (2003), 993‚Äì1022.\\n\\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al . 2020. Language models are few-shot learners. *Advances in neural*\\n*information processing systems* 33 (2020), 1877‚Äì1901.\\n\\n[4] Antonio J. G. Busson, Rafael Rocha, Rennan Gaio, Rafael Miceli, Ivan Pereira,\\nDaniel de S. Moraes, S√©rgio Colcher, Alvaro Veiga, Bruno Rizzi, Francisco Evangelista, Leandro Santos, Fellipe Marques, Marcos Rabaioli, Diego Feldberg, Debora\\nMattos, Jo√£o Pasqua, and Diogo Dias. 2023. Hierarchical Classification of Financial Transactions Through Context-Fusion of Transformer-based Embeddings\\nand Taxonomy-aware Attention Layer. In *Anais do II Brazilian Workshop on Arti-*\\n*ficial Intelligence in Finance (BWAIF 2023) (BWAIF 2023)* . Sociedade Brasileira de\\nComputa√ß√£o. https://doi.org/10.5753/bwaif.2023.229322\\n\\n[5] Ricardo Campos, V√≠tor Mangaravite, Arian Pasquali, Al√≠pio Jorge, C√©lia Nunes,\\nand Adam Jatowt. 2020. YAKE! Keyword extraction from single documents using\\nmultiple local features. *Information Sciences* 509 (2020), 257‚Äì289.\\n\\n[6] Boqi Chen, Fandi Yi, and D√°niel Varr√≥. 2023. Prompting or Fine-tuning? A\\nComparative Study of Large Language Models for Taxonomy Construction. In\\n*2023 ACM/IEEE International Conference on Model Driven Engineering Languages*\\n*and Systems Companion (MODELS-C)* . IEEE, 588‚Äì596.\\n\\n[7] Sabit Ekin. 2023. Prompt engineering for ChatGPT: a quick guide to techniques,\\ntips, and best practices. *Authorea Preprints* (2023).\\n\\n[8] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\\nWang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large\\nlanguage models. *arXiv preprint arXiv:2106.09685* (2021).\\n\\n[9] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche\\nSavary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou\\nHanna, Florian Bressand, et al . 2024. Mixtral of experts. *arXiv preprint*\\n*arXiv:2401.04088* (2024).\\n\\n[10] Dongha Lee, Jiaming Shen, SeongKu Kang, Susik Yoon, Jiawei Han, and Hwanjo\\nYu. 2022. TaxoCom: Topic Taxonomy Completion with Hierarchical Discovery of\\nNovel Topic Clusters. In *Proceedings of the ACM Web Conference 2022* . 2819‚Äì2829.\\n\\n[11] Yinheng Li. 2023. A Practical Survey on Zero-Shot Prompt Design for In-Context\\nLearning. In *Proceedings of the 14th International Conference on Recent Advances*\\n*in Natural Language Processing* . 641‚Äì647.\\n\\n[12] Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun\\nMa, and Zhoujun Li. 2023. Unleashing Infinite-Length Input Capacity for Largescale Language Models with Self-Controlled Memory System. *arXiv preprint*\\n*arXiv:2304.13343* (2023).\\n\\n[13] Irina Nikishina, Varvara Logacheva, Alexander Panchenko, and Natalia\\nLoukachevitch. 2020. RUSSE‚Äô2020: Findings of the First Taxonomy Enrichment\\n\\n\\n273\\n\\n\\n-----\\n\\nWebMedia‚Äô2024, Juiz de Fora, Brazil Moraes et al.\\n\\n\\nTask for the Russian language. *arXiv preprint arXiv:2005.11176* (2020).\\n\\n[14] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\\n\\n[15] Octavian Popescu and Carlo Strapparava. 2015. Semeval 2015, task 7: Diachronic\\ntext evaluation. In *Proceedings of the 9th International Workshop on Semantic*\\n*Evaluation (SemEval 2015)* . 870‚Äì878.\\n\\n[16] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of\\ntransfer learning with a unified text-to-text transformer. *The Journal of Machine*\\n*Learning Research* 21, 1 (2020), 5485‚Äì5551.\\n\\n[17] Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language\\nmodels: Beyond the few-shot paradigm. In *Extended abstracts of the 2021 CHI*\\n*conference on human factors in computing systems* . 1‚Äì7.\\n\\n[18] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal,\\nand Aman Chadha. 2024. A systematic survey of prompt engineering in large\\nlanguage models: Techniques and applications. *arXiv preprint arXiv:2402.07927*\\n(2024).\\n\\n[19] Rion Snow, Daniel Jurafsky, and Andrew Ng. 2004. Learning syntactic patterns\\nfor automatic hypernym discovery. *Advances in neural information processing*\\n*systems* 17 (2004).\\n\\n[20] Kunihiro Takeoka, Kosuke Akimoto, and Masafumi Oyamada. 2021. Low-resource\\ntaxonomy enrichment with pretrained language models. In *Proceedings of the*\\n*2021 Conference on Empirical Methods in Natural Language Processing* . 2747‚Äì2758.\\n\\n[21] Adrian Tam. [n. d.]. What Are Zero-Shot Prompting and Few-Shot Prompting. https://machinelearningmastery.com/what-are-zero-shot-prompting-andfew-shot-prompting/. Accessed: 2024-07-02.\\n\\n\\n\\n[22] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\\nLachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal\\nAzhar, et al . 2023. Llama: Open and efficient foundation language models. *arXiv*\\n*preprint arXiv:2302.13971* (2023).\\n\\n[23] Erlend Vollset, Eirik Folkestad, Marius Rise Gallala, and Jon Atle Gulla. 2017.\\nMaking use of external company data to improve the classification of bank transactions. In *Advanced Data Mining and Applications: 13th International Conference,*\\n*ADMA 2017, Singapore, November 5‚Äì6, 2017, Proceedings 13* . Springer, 767‚Äì780.\\n\\n[24] Hanna Wallach, David Mimno, and Andrew McCallum. 2009. Rethinking LDA:\\nWhy priors matter. *Advances in neural information processing systems* 22 (2009).\\n\\n[25] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang,\\nAakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain\\nof thought reasoning in language models. *arXiv preprint arXiv:2203.11171* (2022).\\n\\n[26] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei\\nXia, H Chi, Quoc V Le, and Denny Zhou. [n. d.]. Chain-of-Thought Prompting\\nElicits Reasoning in Large Language Models. ([n. d.]).\\n\\n[27] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,\\nand Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting in Language\\nModels. In *International Conference on Learning Representations (ICLR)* .\\n\\n[28] Chao Zhang, Fangbo Tao, Xiusi Chen, Jiaming Shen, Meng Jiang, Brian Sadler,\\nMichelle Vanni, and Jiawei Han. 2018. TaxoGen: Unsupervised Topic Taxonomy\\nConstruction by Adaptive Term Embedding and Clustering. In *Proceedings of the*\\n*24th ACM SIGKDD International Conference on Knowledge Discovery & Data Min-*\\n*ing* (London, United Kingdom) *(KDD ‚Äô18)* . Association for Computing Machinery,\\nNew York, NY, USA, 2701‚Äì2709. https://doi.org/10.1145/3219819.3220064\\n\\n\\n274\\n\\n\\n-----\\n\\n'"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 253
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:47:48.393112Z",
     "start_time": "2025-05-12T23:47:48.389482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tmp = text.split('\\n### **1')[1].split('**REFERENCES')[0]\n",
    "tmp"
   ],
   "id": "cc8c27c94a4d21b9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' INTRODUCTION**\\n\\nMany recent studies have focused on the application of Machine\\nLearning-based methods for the classification and characterization of financial transactions. For example, Vollset et al . [23] and\\nBusson et al . [4] explored an approach to hierarchically classifying\\n\\n\\n267\\n\\n\\n-----\\n\\nWebMedia‚Äô2024, Juiz de Fora, Brazil Moraes et al.\\n\\n\\nfinancial transactions, employing a predefined set of categories/subcategories that describe purchase types and transactions. However,\\nthese methods apply a limited, predefined set of static classes, which\\nrestricts the ability to extend classifications based on user experiences when encountering new, previously undefined categories.\\nIn this context, to expand the possible set of classes/tags to label\\na transaction, we developed an unsupervised method based on\\n*topic taxonomies* . Taxonomies are very useful in the structural and\\nsemantic analyses of topics and textual data. However, creating and\\nmaintaining them is often costly and challenging to scale manually.\\nTherefore, recent works have tackled the automatic creation and\\nexpansion of *topic taxonomies*, in which each node in a hierarchy\\nrepresents a conceptual topic composed of semantically coherent\\n\\nterms.\\n\\nWe present an unsupervised method for automatically constructing and expanding topic taxonomies with instruction-based LLMs\\n(Large Language Models), in a Zero-Shot manner. Candidate terms\\nfor the initial version of the taxonomy are obtained using topic\\nmodeling and keyword extraction techniques. Then we apply LLMs\\nto post-process the resulting terms, create a hierarchy, and add new\\nterms to an existing taxonomy. Since the taxonomies are derived\\nfrom a corpus of unstructured texts describing niches of consuming\\nhabits, we opted to investigate the use of LLMs in our approach.\\nLLMs are often pre-trained on a large corpus of text, allowing them\\nto learn contextual representations that capture the intricacies of\\nhuman language.\\nWe applied our method to a private dataset of transactions of\\na retail bank, enriched with scraped data from food and shopping\\ncompanies, and evaluated the resulting taxonomies quantitatively.\\nThe generated tags of our topic taxonomies are then assigned to the\\nbank transactions characterizing the companies in each transaction,\\nas shown on Figure 1. In total, 58 topic taxonomies were created\\nfor the *Food* category and 6 for the *Shopping* category.\\nA two-step quantitative evaluation was conducted on a subset of the taxonomies. For this evaluation, we selected the topic\\ntaxonomies with the highest number of terms in each category:\\n\"Brazilian Cuisine\" from *Food* and \"Clothing and Accessories\" from\\n*Shopping* . Taxonomies with more terms are most likely to result\\nin a deeper hierarchy, which gives more data for evaluation. We\\nasked 12 volunteers to answer a two-part form, which assessed\\nthe quality of the created taxonomies and the quality of the tags\\nassigned to label transactions. The evaluation showed an average\\ncoherence of tags to transactions above 90%.\\nAs more scraped data from food and shopping companies are\\nadded to the retail bank‚Äôs dataset, the topic taxonomies will need\\nto be updated to include new terms. We used LLMs for this task as\\nwell, employing commercial LLMs like Gemini Pro [ 1 ] and GPT4 [ 14 ], alongside open-source LLM options such as LLaMA-Alpaca\\n(7B) [ 22 ], Phi-2 [1], and Mixtral 8x7B [ 9 ]. We showcase their results in\\nboth taxonomy creation and expansion. For the expansion part, we\\nalso compared our method to existing ones (a BERT-based method\\nand Musubu[ 20 ]) on the SemEval dataset and our generated taxonomies as well. Gemini Pro achieved the best results, with F1scores of 89% and 70% for parent node prediction on the *Food* and\\n*Shopping* taxonomies, respectively.\\n\\n1 https://huggingface.co/microsoft/phi-2\\n\\n\\nThe remainder of the paper is structured as follows: Section 2,\\nreviews the related work, highlighting existing approaches. Section 3 provides the necessary background, laying the foundation\\nfor our methodologies and contextualizing our contributions. Section 4 details the dataset construction process, explaining how we\\nenriched and prepared the data for the taxonomies‚Äô construction. In\\nSection 5, we describe the creation of the taxonomies, outlining the\\nmethods used to generate them. Section 6 discusses the expansion\\nof the taxonomies, demonstrating how they can be dynamically\\nextended to accommodate new categories. Section 7 focuses on the\\nevaluation of these taxonomies, presenting the metrics and results\\nthat validate their accuracy and also the quality of the tags assigned\\nto the transactions. Finally, Section 8 concludes the paper, summarizing our findings, discussing their implications, and suggesting\\ndirections for future research.\\n### **2 RELATED WORK**\\n\\nTaxonomies represent the structure behind a collection of documents, organizing the hierarchical relationships between terms in a\\ntree structure [ 13 ]. They play an essential part in the structural and\\nsemantic analysis of textual data, providing valuable content for\\nmany applications that involve information retrieval and filtering,\\nsuch as web searching, recommendation systems, classification,\\nand question answering.\\nSince creating and maintaining taxonomies is a costly task, often difficult to scale if done manually, methods that automatically\\nconstruct and update them are desirable. Early works on automatic\\ntaxonomy creation focused on building hypernym-hyponym taxonomies, where each pair of terms expresses an ‚Äòis-a‚Äô relationship\\n\\n[ 19 ]. More recent works have tackled the automatic creation of\\nother taxonomies, such as topic taxonomies. In a topic taxonomy,\\neach node represents a conceptual topic composed of semantically\\ncoherent terms.\\n\\nIn this context, Zhang et al . [28] developed TaxoGen, an unsupervised method for constructing topic taxonomies. Taxogen uses\\nthe SkipGram model from an input text corpus to embed all the\\nconcept terms into a latent space that captures their semantics. In\\nthis space, the authors applied a clustering method to construct a\\nhierarchy recursively based on a variation of the spherical K-means\\nalgorithm.\\nAnother work that focuses on topic taxonomies is TaxoCom\\n\\n[ 10 ], a framework for automatic taxonomy expansion. TaxoCom is\\na hierarchical topic discovery framework that recursively expands\\nan initial taxonomy by discovering new sub-topics. It uses locally\\ndiscriminative embeddings and adaptive clustering, resulting in\\na low-dimensional embedding space that effectively encodes the\\ntextual similarity between terms. One main disadvantage of TaxoCom is that it requires a large set of quality phrases in the target\\nlanguage, and curating these phrases can be costly. The quality of\\nthe output taxonomy is highly dependent on those phrases.\\nRegarding the automatic expansion of taxonomies, an important related example is Musubu [ 20 ], a framework for low-resource\\ntaxonomy enrichment that uses a Language Model (LM) as a knowledge base to infer term relationships. For the taxonomy expansion\\npart of out method, we used Musubu as a baseline for comparison.\\n\\n\\n268\\n\\n\\n-----\\n\\nTagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies WebMedia‚Äô2024, Juiz de Fora, Brazil\\n\\n\\nAs to using Large Language Models for taxonomy tasks, Chen\\net al . [6] investigated how LLMs, like GPT-3, perform in taxonomy\\nconstruction tasks. The authors compared two approaches: finetuning, which involves training the LLM on a specific dataset to\\nadapt it for taxonomy tasks, and prompt techniques, where the\\nLLM receives instructions and examples to perform a task without\\nbeing explicitly trained for it. Their findings showed that prompt\\ntechniques such as few-shot learning generally outperform finetuning, particularly with smaller datasets. Based on these findings,\\nwe applied prompting techniques, specifically zero-shot prompting,\\nacross various LLMs to assess their effectiveness in constructing and\\nexpanding taxonomies. Section 7 shows the results of our approach,\\nas well as the results of applying Musubu[20] as baseline.\\n### **3 BACKGROUND**\\n\\nIn this section, we provide a comprehensive background on Large\\nLanguage Models (LLMs), and the concept of Prompt-tuning. These\\nconcepts are essential to understanding the construction and editing\\nof taxonomies utilizing LLMs.\\n### **3.1 Large Language Models**\\n\\nLately, Large Language Models (LLMs) have garnered significant\\nattention for their exceptional performance in various NLP tasks.\\nLLMs, such as GPT-3[ 3 ] and LLAMA[ 22 ], are characterized by their\\nmassive scale, comprising billions of parameters and being trained\\non vast amounts of data. These models are often pre-trained in\\nan unsupervised manner on large corpora of textual data, such as\\nbooks, articles, and web pages, allowing them to learn contextual\\nrepresentations that capture the intricacies of human language.\\nTo use LLMs for specific purposes, a highly effective approach is\\nto fine-tune them on task-specific data. Fine-tuning enables LLMs\\nto adapt to specific domains or tasks with minimal labeled data,\\nsignificantly reducing the need for large annotated datasets. However, in scenarios where labeled data is scarce or difficult to obtain,\\nLLMs can also be used without specific training or additional data,\\nin a Zero-Shot manner [ 21 ]. Given the scale of these models and\\nthe data they are trained on, LLMs embed vast knowledge that enables them to achieve high generalization capabilities and perform\\ntasks in diverse contexts, even without specific training for those\\ntasks[16].\\nIn our experiments, we tested several types of language models,\\nfrom private LLMs (GPT 4 [ 14 ], Gemini Pro [2] ), to open-source LLMs\\n(Llama 2 [ 22 ]), to a Mixture of Experts LLM (Mixtral [ 9 ], and a Small\\nLanguage Model (SLM), Phi 2 [3] .\\n### **3.2 Prompt Engineering**\\n\\nPrompt Engineering is a fundamental technique used to enhance\\nthe performance and adaptability of Large Language Models (LLMs)\\nin specific tasks or domains [ 7 ]. It involves optimizing and crafting\\nprompts to efficiently use language models (LMs) [ 3 ]. This approach\\nallows researchers and practitioners to tailor the behavior and output of LLMs, making them more suitable for targeted applications.\\n\\n2 https://deepmind.google/technologies/gemini/pro/\\n3 https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-ofsmall-language-models/\\n\\n\\nTechniques such as Zero-shot prompting, Few-shot prompting,\\nChain of Thought, ReAct, Self-Consistency etc. have been explored\\nto guide LLMs toward desired responses [ 18, 21, 25 ‚Äì 27 ]. The effectiveness of prompt tuning has been demonstrated in various\\napplications, including question-answering, summarization, and\\ndialogue generation. The choice of prompt greatly influences the\\ngenerated output, and by carefully crafting prompts, researchers\\ncan guide the model‚Äôs responses toward desired behaviors. For example, in language translation, a prompt can specify the source\\nlanguage and desired target language to ensure accurate and fluent translations. In our method, we used the Zero-Shot prompting\\ntechnique.\\n### **3.3 Zero-Shot Prompting**\\n\\nSince LLMs (Large Language Models) are trained on vast amounts\\nof data, they can follow instructions and perform tasks in contexts\\nwhere they were not specifically trained, in a Zero-Shot (ZS) manner.\\nThis prompting style allows the model to adapt, making it versatile.\\nA Zero-Shot (ZS) prompt directly instructs the model to perform\\na task without additional examples or demonstrations to guide\\nthe LLM‚Äôs response, which is why they are also known as task\\ninstructions [21].\\nIn a study by Li [11], the authors highlighted several advantages\\nof using ZS prompts, such as the ability to craft highly interpretable\\nprompts, requiring fewer training data or examples, a more straightforward prompt design process, and a flexible prompt structure.\\nAdditionally, Reynolds and McDonell [17] noted that carefully engineered zero-shot prompts can outperform few-shot prompts in\\ncertain scenarios, as examples can sometimes be interpreted as part\\nof a narrative rather than as a guiding mechanism. This finding also\\ninfluenced our decision to use zero-shot prompting in our method.\\n### **4 DATASET CONSTRUCTION**\\n\\nThis work uses a proprietary dataset consisting of consumer transactions from a retail bank. Each transaction includes only a merchant\\nname indicating the business where that purchase occurred along\\nwith macro and micro categories as illustrated in Figure 1 The macro\\nand micro are originally assigned by [ 4 ] using the information from\\nthe business activities and products.\\nWe focus on two macro-categories from this dataset: *Food* and\\n*Shopping*, selecting the top 50,000 businesses with the highest number of transactions for each category.\\nWith the limited initial information, assigning detailed tags to\\ntransactions is challenging. To address this, we augment the dataset\\nthrough a data enrichment process involving web scraping. Using\\ntools such as Selenium [4] and Beautiful Soup [5], we gathered activity\\ndescriptions for companies in each macro category. For the *Food*\\nmacro category, the search was conducted on specialized platforms\\nfor restaurants and food delivery services. For the *Shopping* macro\\ncategory, we obtained establishment descriptions directly from\\ninternet indexing and search tools.\\nIn the context of enrichment for the *Food* macro category, web\\nscraping was conducted as follows: (1) the centers of all Brazilian\\nstate capitals and the Federal District were used as base locations\\n\\n4 https://www.selenium.dev/about/\\n5 https://readthedocs.org/projects/beautiful-soup-4/downloads/pdf/latest/\\n\\n\\n269\\n\\n\\n-----\\n\\nWebMedia‚Äô2024, Juiz de Fora, Brazil Moraes et al.\\n\\n\\nfor restaurant searching; (2) for each location, restaurants listed on\\nthe first one hundred pages of the platform were extracted. After\\ncompleting these steps, the information was combined with the\\nmerchant database using the merchant‚Äôs name and micro categories.\\nFor the *Shopping* macro category, the description of each merchant was obtained using Google Search Engine [6], selecting descriptions from the first ten search results. The final description\\nconsists of a concatenation of all the obtained descriptions. The\\nsearch queries were constructed using the merchant names combined with their micro categories.\\n### **5 TAXONOMY CONSTRUCTION**\\n\\nTo automatically create topic taxonomies for *Food* and *Shopping*\\nbusinesses, we developed a 3-step method. First, we preprocess the\\ndescriptions in our enriched dataset to retain only the relevant parts\\nof the text. Next, we apply two techniques to select candidate terms\\nfor the topic taxonomies: keyword extraction and topic modeling.\\nIn the post-processing phase, we use large language models\\n(LLMs) to refine the results of each step, filtering out unrelated\\nterms. Finally, we use LLMs again to organize the final terms into\\nhierarchies, forming the topic taxonomies.\\n### **5.1 Preprocessing**\\n\\nWe applied a few NLP techniques to refine the businesses‚Äô descriptions in our dataset. At first, we remove stop words to eliminate\\ncommonly used words that do not carry significant meaning in\\nour contexts. Then, to retain only the most relevant portions of the\\ndescriptions, we employ part-of-speech (POS) tagging to identify\\nand exclude words that belong to specific POS categories. The list\\nof POS tag categories that were removed includes ADV, CCONJ,\\nADP, AUX, CONJ, DET, INTJ, PART, PRON, PUNCT, SYM, SCONJ,\\nADJ, VERB, PROPN. [7]\\n\\nAfter this initial preprocessing step, we run the first iteration\\nof the candidate term selection part to build a filter of generic\\nwords, not to create topic taxonomies yet. For this step, we use the\\nentire corpus of descriptions for each macro category, resulting in\\ntwo corpora ( *Food* and *Shopping* ). For each micro category in the\\nmacro categories‚Äô corpora, we use Keyword Extraction and Topic\\nModeling to gather candidate terms for the filter, combining the\\nresults of both techniques in a list. Then, We use an LLM to remove\\nthe terms it identifies as unrelated to the main topic (each micro\\ncategory) from the list. The prompt that we used for requesting this\\nseparation is illustrated below.\\n\\nprompt= \"Given the terms in the following list: \"+\\n<wordsList> +\". Separate them into two groups. In\\ngroup 1 the terms with no relation to the topic \"+\\n<type> +\". And in group 2 the terms that are related.\"\\n\\n**Listing 1: Prompt for separating candidate terms related to**\\n**the type of establishment**\\n\\nBy using this prompt, we try to ensure that the model‚Äôs response\\nis consistently formatted according to the pattern described in it,\\nfacilitating the processing of the resulting string, although, some\\n\\n6 https://www.google.com\\n7 https://spacy.io/usage/linguistic-features#pos-tagging\\n\\n\\nof the LLMs we tested did not output the response in the requested\\nformat. Once we complete one iteration of this method for each\\nmacro category in our dataset, we add the words of group 2 to the\\ncorresponding list of generic words. We apply the corresponding\\nfilter of generic words for each macro category corpus, resulting in\\nthe final preprocessed corpus.\\n### **5.2 Candidate Terms Selection**\\n\\nFor this part of our method, we use each preprocessed corpus\\nseparately. For the *Food* corpus, we group the descriptions based\\non their micro-categories, creating 58 sub-corpus specific to that\\ndomain. We have six micro categories for the *Shopping* corpus,\\nresulting in 6 specific sub-corpus. The candidate terms selection\\nmethods are applied to each sub-corpus, creating topic taxonomies\\nwhere the main topic is the micro category.\\n\\n*5.2.1* *Keyword Extraction.* The first approach to candidate term\\nselection was to use an unsupervised keyword selection method\\ncalled Yake! [ 5 ]. This method is based on statistical text features\\nextracted from single documents to select the most relevant keywords from that text. It does not require training on a document set\\nand is not dependent on dictionaries, text size, language, domain,\\nor external corpora.\\nYake! allows for the specification of parameters such as the\\nlanguage of the text, the maximum size of the n-grams being sought,\\nand others. In our method, we customized only the language to\\nPortuguese, and the maximum number of keywords sought for each\\nset of descriptions was 30 words.\\nAfter extracting the keywords from each group of descriptions,\\nwe obtained a total set of *ùëÅ* candidate terms. However, these terms\\nare further filtered using an LLM, where we ask it to separate the\\nterms related to the main topic from those unrelated, as explained\\nearlier in subsection 5.1.\\n\\n*5.2.2* *Topic Modeling.* Our second approach to collecting initial\\ntopics and candidate terms was Topic Modeling. We applied the\\nLatent Dirichlet Allocation algorithm [ 2 ], available at the Gensim\\nLibrary [8] .\\nWe construct a dictionary for each macro-category corpus in\\nour macro-categories corpora by extracting unique tokens and\\nbigrams. After a few empirical tests, we set the minimum frequency\\nof a bigram to 20 occurrences. Since some corpora have a minimal\\nnumber of tokens (the micro category \"Greek Cuisine\" from the *Food*\\nmacro category has only five stores marked as such, with a corpus\\nof only 127 tokens), we had to set a reasonably small number so that\\nsmaller corpora could also have a few bigrams. With the resulting\\ndictionary of tokens, the LDA algorithm was applied. Three main\\nparameters are to be defined in an LDA algorithm: number of topics,\\n*alpha*, and *beta* .\\nThe number of topics defines the latent topics to be extracted\\nfrom the corpus. The parameter *alpha* is *a priori* belief in documenttopic distribution, while *beta* is *a priori* belief in topic-word distribution.\\nTo define the number of topics for each micro category corpus,\\nwe tried numbers from 1 to 5, constantly checking which configuration would result in the best average topic coherence for that\\n\\n8 https://pypi.org/project/gensim/\\n\\n\\n270\\n\\n\\n-----\\n\\nTagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies WebMedia‚Äô2024, Juiz de Fora, Brazil\\n\\n\\ncorpus. Small corpora would have 1 or 2 topics, while bigger ones\\nwould have 5. To correctly define the *alpha* and *beta* priors, we\\nwould have to analyze the distribution for each category corpus\\n\\n[ 24 ]. Since this would be rather difficult, we set those priors to be\\nauto-defined by the LDA algorithm, which learns these parameters\\nbased on the corpus. We select the terms with the highest coherence\\nwith the resulting topics. Each topic returns 20 words with their\\ncoherence scores, but we do not use all of them as some have very\\nlow coherence. After testing a few configurations, for each topic,\\nwe select 60% of the terms with the highest coherence within that\\ntopic.\\nWith initial terms for each topic taxonomy, we ask an LLM\\nto separate the ones closely related to the main topic from those\\nunrelated, as mentioned earlier.\\n### **5.3 Hierarchy Construction**\\n\\nOnce we have the post-processed lists of candidate terms obtained\\nby each technique mentioned in subsection 5.2, we merge them and\\nremove repetitions. After the merge, for each macro category, we\\nhave lists of terms for each micro category, representing each topic\\ntaxonomy. However, they do not have any hierarchy level between\\nthe terms configuring the taxonomy.\\nTo tackle this problem, we use an LLM again, this time with a\\nprompt that searches for sub-categories within the terms of a topic\\nto create these hierarchies. The prompt is illustrated below:\\n\\nprompt=\"Create a dictionary by hierarchically arranging the\\n\\nfollowing words:\" + <wordsList> +.\" Use JSON format as\\nthe output such as the following: {\\\\\"key\\\\\": [\\\\\" list\\nof words\\\\\"]}\"\\n\\n**Listing 2: Prompt for creating a hierarchy for each list of**\\n**tags.**\\n\\nWith this prompt, we seek to ensure that the LLM response has a\\nconsistent pattern and facilitates handling the returned string. After\\nthis step, we have a hierarchy of terms in each topic taxonomy in\\nthe *Food* and *Shopping* macro categories.\\n### **5.4 Merchant Tagging**\\n\\nWith the topic taxonomies for both *Food* and *Shopping* macrocategories, we can now assign tags to merchants/establishments.\\nTo do so, we use the descriptions attached to these establishments,\\nand we see which terms from a taxonomy are mentioned in their\\ndescriptions with a reverse index algorithm. We employ the taxonomy whose topic is the same as the establishment‚Äôs micro category,\\nas shown in Figure 2.\\n### **6 TAXONOMY EXPANSION**\\n\\nAnother essential part of our method is the automatic expansion of\\nexisting taxonomies as new terms arrive, derived from additional\\nmerchant scrapped data, as shown in Section 4. In this section, we\\npresent our approach to taxonomy expansion by using instructionbased LLMs.\\n\\nAs new transactions may include new businesses, new terms\\ncan emerge from the descriptions obtained through the scraping\\nprocess. Therefore, we need to update the taxonomies with these\\n\\n\\nnew terms maintaining and enriching the created hierarchies with\\nthe potential new terms.\\nAfter completing the transaction enrichment process, including\\nthe search for business descriptions and the selection of candidate\\nterms, if relevant terms not included in the current hierarchies are\\ndetected, we initiate the expansion process.\\n### **6.1 Prompt engineering instruction for** **taxonomy representation**\\n\\nFirst, we represent our topic taxonomies in a format that can be\\ninterpreted by an LLM. We employed a generic prompt, illustrated\\nbelow, across all tested methods to convert topics into root nodes\\nand their terms into child nodes.\\n\\nChilds of [ROOT]: [CHILD1,CHILD2,CHILD3]\\nChilds of [CHILD1]: [CHILD4,CHILD5]\\n\\nChilds of [CHILD2]: [CHILD6]\\n\\n...\\n\\n**Listing 3: Prompt for representation of taxonomy**\\n### **6.2 Predicting the parent of a node**\\n\\nTo experiment with taxonomies expansion, we used two datasets:\\nour *Food* and *Shopping* topic taxonomies and the taxonomies from\\nSemEval-2015 Task 17 [ 15 ]. Those are low-resource taxonomies,\\nwith thousands of nodes or less, which are appropriate for the current prompt size of LLMs. We used the SemEval dataset to compare\\nthe results with well-established methods for taxonomy expansion,\\nsuch as Musubu [ 20 ]. Similar to their experiments, we hid 20% of\\nthe terms (chosen randomically) in the taxonomies to predict their\\nrespective parent nodes. To verify the parent/root of a new term,\\nwe used the following prompt:\\n\\n**Listin** **g** **4: Prom** **p** **t for searchin** **g** **for a node‚Äôs** **p** **arent**\\n\\nprompt=\"Who is the father of \"+<new_term>+\"?\"\\n\\nIn Table 1, we see the F1-Scores for parent node prediction. Equation 1 showcases how to calculate the F1-Score. TP is the number\\n\\nof true positives, nodes that were correctly assigned as parents of\\nchild nodes. FP is the number of false positives, nodes that were\\nincorrectly assigned as a parent to a child node. FN is the number\\nof false negatives, nodes that should have been assigned as parent\\nnodes but were not.\\n\\n2 ‚àó *ùëáùëÉ*\\n*ùêπ* 1 = (1)\\n2 ‚àó *ùëáùëÉ* + *ùêπùëÉ* + *ùêπùëÅ*\\n\\nFor baseline models, we used Bert and Musubu; for commercial\\nLLMs, Gemini Pro and GPT-4; and for open-source LLMs, LLamaAlpaca(7B), Phi-2, and Mixtral 8x7B. We evaluate them in 4 taxonomies from the SemEval dataset and our taxonomies. For each\\ntaxonomy, the LLMs perform significantly better than Musubu,\\nwith GPT-4 and Gemini Pro having the highest F1-Scores, with the\\nlatter beating the former by a few points. However, the most recent\\nopen-source options (Phi-2 and Mixtral 8x7B) are getting close in\\nperformance.\\n\\n\\n271\\n\\n\\n-----\\n\\nWebMedia‚Äô2024, Juiz de Fora, Brazil Moraes et al.\\n\\n\\n\\n\\n**Tags assigned to establishments from the**\\n\\n***\"Clothing & Accessories\"*** **micro category**\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n**Figure 2: Assigning tags to establishments based on a topic**\\n**taxonomy.**\\n\\nIt is important to note that while SemEval taxonomies have\\nthousands of nodes, ours have only a few hundred, which we can\\nassume is a significant reason for the degrading performance of\\nMusubu and Bert (LMs or LM-based methods). In contrast, the LLMs\\nhave a robust performance in such low-resource settings. This also\\nshows that LLMs have a remarkable understanding of questions\\nand zero-shot performance, generalizing well even for datasets in\\ndifferent languages.\\n### **7 TAXONOMY EVALUATION**\\n\\nTo properly evaluate the topic taxonomies that we created in this\\nwork, we developed a two-step qualitative evaluation of a limited\\npart of the results.\\nIn total, 58 topic taxonomies were created for the *Food* set and\\n6 for the *Shopping* set. For our evaluation, we selected the topic\\ntaxonomies with the highest number of terms in each part (the\\n\"Brazilian Cuisine\" taxonomy for the *Food* part and the \"Clothing\\nand Accessories\" taxonomy for the *Shopping* one). First, we assess\\nthe quality of removing generic terms from each taxonomy, and\\nthen, we evaluate the tags assigned to establishments based on that\\ntaxonomy. We asked 12 volunteers to answer a two-part form.\\n*Part 1 - Accuracy of the terms that were selected as related to the*\\n*topic* : In this part, we evaluate if the LLMs could correctly group\\nthe relevant and non-relevant terms, removing the generic terms.\\nTo do so, we defined a ground truth with the relevant terms as\\n\\n\\ntrue positives and the non-relevant terms as true negatives. Table 3\\nshows the results.\\n\\nGPT-4 was the best model, followed by Gemini Pro, both scoring\\nover 60% accuracy for the Brazilian Cuisine taxonomy and over\\n86% accuracy for the Clothing and Accessories taxonomy. Smaller\\nlanguage models such as Phi 2 and Llama 2 7B performed poorly\\nboth in removing generic terms and in formatting the response\\naccordingly, with Phi 2 being particularly verbose.\\n*Part 2 - Human Evaluation of the Quality of the Tagging Process* :\\nIn this part, the volunteers were asked to examine if the tags assigned to an establishment were appropriate and coherent to that\\nestablishment‚Äôs description. We selected the top 5 establishments\\nwith the highest transactions for each micro category. We asked our\\nevaluators to analyze the tags assigned to describe that establishment and choose the ones that were not appropriate. This way, we\\nhave a coherence ratio for each establishment based on the number\\n\\nof proper tags divided by the total number of tags. We average the\\nresults of our 12 evaluators and present them in Table 2. Figure 2\\nshows the \"Clothing & Accessories\" taxonomy that was evaluated\\nand 2 of the merchants and the tags assigned to them that were\\nincluded in the evaluation.\\n### **8 CONCLUSION**\\n\\nIn this work, we presented an unsupervised method for automatically creating and expanding topic taxonomies using LLMs. We\\nevaluated some of the generated taxonomies and applied them\\nin transaction tagging in a retailer‚Äôs bank dataset. The evaluation\\nshowed promising results, with average coherence scores above 90%\\nfor the two selected taxonomies. The taxonomies‚Äô expansion with\\nGemini Pro also showed exciting results for parent node prediction,\\nwith F1-scores of 89% and 70% for *Food* and *Shopping* taxonomies,\\nrespectively.\\nFor future work on taxonomy construction, we plan to test\\nmore robust term selection methods, such as embedding-based\\napproaches. We also plan on conducting ablation studies to validate whether the keyword extraction and topic modeling parts\\nhelp improve the quality of the taxonomies created, by using a\\nbaseline prompt to ask the LLM to generate child nodes given a\\nparent node. In terms of taxonomy expansion, there are several\\ntasks to explore, ranging from node-level operations to generating\\nentire sub-trees and identifying similar structures. Additionally, we\\nintend to enhance our instruction-tuned LLM for taxonomy tasks\\n\\n\\n|Method|SemEval-2015 Task 17 Chemical Equipment Food Science|Our taxonomies Food Shopping|\\n|---|---|---|\\n|Gemini Pro|0.68 0.80 0.91 0.72|0.89 0.73|\\n|GPT-4|0.65 0.78 0.89 0.70|0.87 0.71|\\n|Mixtral-8x7B|0.59 0.63 0.80 0.57|0.74 0.60|\\n|Phi-2|0.56 0.52 0.68 0.56|0.64 0.54|\\n|LLama 7B|0.51 0.42 0.58 0.46|0.60 0.49|\\n|Musubu|0.35 0.46 0.37 0.42|0.21 0.13|\\n|Bert-Base|0.13 0.14 0.12 0.16|0.11 0.06|\\n\\n\\n**Table 1: F1-score for parent node prediction.**\\n\\n272\\n\\n\\n-----\\n\\nTagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies WebMedia‚Äô2024, Juiz de Fora, Brazil\\n\\n|Col1|Brazilian Cuisine Taxonomy Average Coherence Number of Tags|Col3|Clothing & Accessories Taxonomy Average Coherence Number of Tags|Col5|\\n|---|---|---|---|---|\\n|Merchant 1|92.30%|10|97.11%|8|\\n|Merchant 2|94.23%|8|83.07%|5|\\n|Merchant 3|89.23%|5|94.38%|5|\\n|Merchant 4|87.17%|6|93.84%|5|\\n|Merchant 5|93.40%|7|97.43%|6|\\n\\n\\n\\n**Table 2: Results of evaluating the tags assigned to each merchant/establishment.**\\n\\n\\n|Col1|Brazilian Cuisine|Clothing & Accessories|\\n|---|---|---|\\n|Llama 2 7B|29.54%|52.78%|\\n|Phi 2|40.90|73.68%|\\n|Mixtral 8x7B v0.1|46.93%|70.27%|\\n|Gemini Pro|61.36%|86.11%|\\n|GPT 4|68.08%|86.84%|\\n\\n\\n**Table 3: Accuracy of using each LLM to remove generic words**\\n**from each topic taxonomy.**\\n\\nby fine-tuning or employing more efficient methods such as LoRA\\n\\n[8].\\n### **LIMITATIONS**\\n\\nTo address the limitations of our work, we begin with the taxonomy construction component. In this phase, we relied on topic\\nmodeling and keywords extraction to select candidate terms for our\\ntaxonomies. The LDA algorithm used for topic modeling performs\\nsuboptimally when the base corpus is small. Some of our topics had\\ncorpora with vocabularies of fewer than 100 words, which can result\\nin topics containing irrelevant or incoherent terms. Additionally,\\nwe could have further experimented with the LDA hyperparameters\\nfor each micro-category corpus.\\nRegarding the evaluation of the generated taxonomies, we did not\\nassess topic completeness. Without a ground truth, it is challenging\\nto quantify how comprehensively the terms in a taxonomy cover the\\nmain topic. Furthermore, we evaluated only 2 of the 64 taxonomies\\ngenerated by our method, leaving a substantial portion unexamined.\\nIn the taxonomy expansion experiment, we evaluated only a lowresource setting with fewer than a thousand nodes. Most studies\\nfocus on taxonomies with hundreds of thousands or more nodes.\\n\\nThis presents a challenge for LLMs due to their limited context.\\nAddressing this contextual limitation could benefit from insights\\nfound in other works that tackle similar issues [12].\\n### **ETHICS STATEMENT**\\n\\nIn this work, we ensure the utmost protection of customers and\\nstore sensitive data by exclusively using non-sensitive information\\nin our dataset. Our prompts solely rely on selected words from store\\ndescriptions, thus avoiding any direct usage of personal or sensitive\\ninformation. No customer-specific data or store-sensitive details\\nare integrated into the system, upholding privacy and security as\\ntop priorities.\\n\\n\\nMoreover, we strictly adhere to ethical guidelines during our\\nexperiments involving volunteers, and no personal data is collected\\nfrom them. Our focus lies solely on analyzing the results of our\\nproposed approach. Participants‚Äô anonymity and confidentiality are\\nmaintained throughout the research process, ensuring a responsible\\nand trustworthy approach to data handling.\\n### **ACKNOWLEDGMENTS**\\n\\nThe authors would like to acknowledge BTG Pactual for the partnership and financial support to this research.\\n### '"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 256
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:48:04.832326Z",
     "start_time": "2025-05-12T23:48:04.828827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tmp = clean_abs(tmp)\n",
    "tmp"
   ],
   "id": "e4175c22a45916db",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INTRODUCTION  Many recent studies have focused on the application of Machine Learning-based methods for the classification and characterization of financial transactions. For example, Vollset et al . [23] and Busson et al . [4] explored an approach to hierarchically classifying   267   -----  WebMedia‚Äô2024, Juiz de Fora, Brazil Moraes et al.   financial transactions, employing a predefined set of categories/subcategories that describe purchase types and transactions. However, these methods apply a limited, predefined set of static classes, which restricts the ability to extend classifications based on user experiences when encountering new, previously undefined categories. In this context, to expand the possible set of classes/tags to label a transaction, we developed an unsupervised method based on topic taxonomies . Taxonomies are very useful in the structural and semantic analyses of topics and textual data. However, creating and maintaining them is often costly and challenging to scale manually. Therefore, recent works have tackled the automatic creation and expansion of topic taxonomies, in which each node in a hierarchy represents a conceptual topic composed of semantically coherent  terms.  We present an unsupervised method for automatically constructing and expanding topic taxonomies with instruction-based LLMs (Large Language Models), in a Zero-Shot manner. Candidate terms for the initial version of the taxonomy are obtained using topic modeling and keyword extraction techniques. Then we apply LLMs to post-process the resulting terms, create a hierarchy, and add new terms to an existing taxonomy. Since the taxonomies are derived from a corpus of unstructured texts describing niches of consuming habits, we opted to investigate the use of LLMs in our approach. LLMs are often pre-trained on a large corpus of text, allowing them to learn contextual representations that capture the intricacies of human language. We applied our method to a private dataset of transactions of a retail bank, enriched with scraped data from food and shopping companies, and evaluated the resulting taxonomies quantitatively. The generated tags of our topic taxonomies are then assigned to the bank transactions characterizing the companies in each transaction, as shown on Figure 1. In total, 58 topic taxonomies were created for the Food category and 6 for the Shopping category. A two-step quantitative evaluation was conducted on a subset of the taxonomies. For this evaluation, we selected the topic taxonomies with the highest number of terms in each category: \"Brazilian Cuisine\" from Food and \"Clothing and Accessories\" from Shopping . Taxonomies with more terms are most likely to result in a deeper hierarchy, which gives more data for evaluation. We asked 12 volunteers to answer a two-part form, which assessed the quality of the created taxonomies and the quality of the tags assigned to label transactions. The evaluation showed an average coherence of tags to transactions above 90%. As more scraped data from food and shopping companies are added to the retail bank‚Äôs dataset, the topic taxonomies will need to be updated to include new terms. We used LLMs for this task as well, employing commercial LLMs like Gemini Pro [ 1 ] and GPT4 [ 14 ], alongside open-source LLM options such as LLaMA-Alpaca (7B) [ 22 ], Phi-2 [1], and Mixtral 8x7B [ 9 ]. We showcase their results in both taxonomy creation and expansion. For the expansion part, we also compared our method to existing ones (a BERT-based method and Musubu[ 20 ]) on the SemEval dataset and our generated taxonomies as well. Gemini Pro achieved the best results, with F1scores of 89% and 70% for parent node prediction on the Food and Shopping taxonomies, respectively.  1 https://huggingface.co/microsoft/phi-2   The remainder of the paper is structured as follows: Section 2, reviews the related work, highlighting existing approaches. Section 3 provides the necessary background, laying the foundation for our methodologies and contextualizing our contributions. Section 4 details the dataset construction process, explaining how we enriched and prepared the data for the taxonomies‚Äô construction. In Section 5, we describe the creation of the taxonomies, outlining the methods used to generate them. Section 6 discusses the expansion of the taxonomies, demonstrating how they can be dynamically extended to accommodate new categories. Section 7 focuses on the evaluation of these taxonomies, presenting the metrics and results that validate their accuracy and also the quality of the tags assigned to the transactions. Finally, Section 8 concludes the paper, summarizing our findings, discussing their implications, and suggesting directions for future research.  2 RELATED WORK  Taxonomies represent the structure behind a collection of documents, organizing the hierarchical relationships between terms in a tree structure [ 13 ]. They play an essential part in the structural and semantic analysis of textual data, providing valuable content for many applications that involve information retrieval and filtering, such as web searching, recommendation systems, classification, and question answering. Since creating and maintaining taxonomies is a costly task, often difficult to scale if done manually, methods that automatically construct and update them are desirable. Early works on automatic taxonomy creation focused on building hypernym-hyponym taxonomies, where each pair of terms expresses an ‚Äòis-a‚Äô relationship  [ 19 ]. More recent works have tackled the automatic creation of other taxonomies, such as topic taxonomies. In a topic taxonomy, each node represents a conceptual topic composed of semantically coherent terms.  In this context, Zhang et al . [28] developed TaxoGen, an unsupervised method for constructing topic taxonomies. Taxogen uses the SkipGram model from an input text corpus to embed all the concept terms into a latent space that captures their semantics. In this space, the authors applied a clustering method to construct a hierarchy recursively based on a variation of the spherical K-means algorithm. Another work that focuses on topic taxonomies is TaxoCom  [ 10 ], a framework for automatic taxonomy expansion. TaxoCom is a hierarchical topic discovery framework that recursively expands an initial taxonomy by discovering new sub-topics. It uses locally discriminative embeddings and adaptive clustering, resulting in a low-dimensional embedding space that effectively encodes the textual similarity between terms. One main disadvantage of TaxoCom is that it requires a large set of quality phrases in the target language, and curating these phrases can be costly. The quality of the output taxonomy is highly dependent on those phrases. Regarding the automatic expansion of taxonomies, an important related example is Musubu [ 20 ], a framework for low-resource taxonomy enrichment that uses a Language Model (LM) as a knowledge base to infer term relationships. For the taxonomy expansion part of out method, we used Musubu as a baseline for comparison.   268   -----  Tagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies WebMedia‚Äô2024, Juiz de Fora, Brazil   As to using Large Language Models for taxonomy tasks, Chen et al . [6] investigated how LLMs, like GPT-3, perform in taxonomy construction tasks. The authors compared two approaches: finetuning, which involves training the LLM on a specific dataset to adapt it for taxonomy tasks, and prompt techniques, where the LLM receives instructions and examples to perform a task without being explicitly trained for it. Their findings showed that prompt techniques such as few-shot learning generally outperform finetuning, particularly with smaller datasets. Based on these findings, we applied prompting techniques, specifically zero-shot prompting, across various LLMs to assess their effectiveness in constructing and expanding taxonomies. Section 7 shows the results of our approach, as well as the results of applying Musubu[20] as baseline.  3 BACKGROUND  In this section, we provide a comprehensive background on Large Language Models (LLMs), and the concept of Prompt-tuning. These concepts are essential to understanding the construction and editing of taxonomies utilizing LLMs.  3.1 Large Language Models  Lately, Large Language Models (LLMs) have garnered significant attention for their exceptional performance in various NLP tasks. LLMs, such as GPT-3[ 3 ] and LLAMA[ 22 ], are characterized by their massive scale, comprising billions of parameters and being trained on vast amounts of data. These models are often pre-trained in an unsupervised manner on large corpora of textual data, such as books, articles, and web pages, allowing them to learn contextual representations that capture the intricacies of human language. To use LLMs for specific purposes, a highly effective approach is to fine-tune them on task-specific data. Fine-tuning enables LLMs to adapt to specific domains or tasks with minimal labeled data, significantly reducing the need for large annotated datasets. However, in scenarios where labeled data is scarce or difficult to obtain, LLMs can also be used without specific training or additional data, in a Zero-Shot manner [ 21 ]. Given the scale of these models and the data they are trained on, LLMs embed vast knowledge that enables them to achieve high generalization capabilities and perform tasks in diverse contexts, even without specific training for those tasks[16]. In our experiments, we tested several types of language models, from private LLMs (GPT 4 [ 14 ], Gemini Pro [2] ), to open-source LLMs (Llama 2 [ 22 ]), to a Mixture of Experts LLM (Mixtral [ 9 ], and a Small Language Model (SLM), Phi 2 [3] .  3.2 Prompt Engineering  Prompt Engineering is a fundamental technique used to enhance the performance and adaptability of Large Language Models (LLMs) in specific tasks or domains [ 7 ]. It involves optimizing and crafting prompts to efficiently use language models (LMs) [ 3 ]. This approach allows researchers and practitioners to tailor the behavior and output of LLMs, making them more suitable for targeted applications.  2 https://deepmind.google/technologies/gemini/pro/ 3 https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-ofsmall-language-models/   Techniques such as Zero-shot prompting, Few-shot prompting, Chain of Thought, ReAct, Self-Consistency etc. have been explored to guide LLMs toward desired responses [ 18, 21, 25 ‚Äì 27 ]. The effectiveness of prompt tuning has been demonstrated in various applications, including question-answering, summarization, and dialogue generation. The choice of prompt greatly influences the generated output, and by carefully crafting prompts, researchers can guide the model‚Äôs responses toward desired behaviors. For example, in language translation, a prompt can specify the source language and desired target language to ensure accurate and fluent translations. In our method, we used the Zero-Shot prompting technique.  3.3 Zero-Shot Prompting  Since LLMs (Large Language Models) are trained on vast amounts of data, they can follow instructions and perform tasks in contexts where they were not specifically trained, in a Zero-Shot (ZS) manner. This prompting style allows the model to adapt, making it versatile. A Zero-Shot (ZS) prompt directly instructs the model to perform a task without additional examples or demonstrations to guide the LLM‚Äôs response, which is why they are also known as task instructions [21]. In a study by Li [11], the authors highlighted several advantages of using ZS prompts, such as the ability to craft highly interpretable prompts, requiring fewer training data or examples, a more straightforward prompt design process, and a flexible prompt structure. Additionally, Reynolds and McDonell [17] noted that carefully engineered zero-shot prompts can outperform few-shot prompts in certain scenarios, as examples can sometimes be interpreted as part of a narrative rather than as a guiding mechanism. This finding also influenced our decision to use zero-shot prompting in our method.  4 DATASET CONSTRUCTION  This work uses a proprietary dataset consisting of consumer transactions from a retail bank. Each transaction includes only a merchant name indicating the business where that purchase occurred along with macro and micro categories as illustrated in Figure 1 The macro and micro are originally assigned by [ 4 ] using the information from the business activities and products. We focus on two macro-categories from this dataset: Food and Shopping, selecting the top 50,000 businesses with the highest number of transactions for each category. With the limited initial information, assigning detailed tags to transactions is challenging. To address this, we augment the dataset through a data enrichment process involving web scraping. Using tools such as Selenium [4] and Beautiful Soup [5], we gathered activity descriptions for companies in each macro category. For the Food macro category, the search was conducted on specialized platforms for restaurants and food delivery services. For the Shopping macro category, we obtained establishment descriptions directly from internet indexing and search tools. In the context of enrichment for the Food macro category, web scraping was conducted as follows: (1) the centers of all Brazilian state capitals and the Federal District were used as base locations  4 https://www.selenium.dev/about/ 5 https://readthedocs.org/projects/beautiful-soup-4/downloads/pdf/latest/   269   -----  WebMedia‚Äô2024, Juiz de Fora, Brazil Moraes et al.   for restaurant searching; (2) for each location, restaurants listed on the first one hundred pages of the platform were extracted. After completing these steps, the information was combined with the merchant database using the merchant‚Äôs name and micro categories. For the Shopping macro category, the description of each merchant was obtained using Google Search Engine [6], selecting descriptions from the first ten search results. The final description consists of a concatenation of all the obtained descriptions. The search queries were constructed using the merchant names combined with their micro categories.  5 TAXONOMY CONSTRUCTION  To automatically create topic taxonomies for Food and Shopping businesses, we developed a 3-step method. First, we preprocess the descriptions in our enriched dataset to retain only the relevant parts of the text. Next, we apply two techniques to select candidate terms for the topic taxonomies: keyword extraction and topic modeling. In the post-processing phase, we use large language models (LLMs) to refine the results of each step, filtering out unrelated terms. Finally, we use LLMs again to organize the final terms into hierarchies, forming the topic taxonomies.  5.1 Preprocessing  We applied a few NLP techniques to refine the businesses‚Äô descriptions in our dataset. At first, we remove stop words to eliminate commonly used words that do not carry significant meaning in our contexts. Then, to retain only the most relevant portions of the descriptions, we employ part-of-speech (POS) tagging to identify and exclude words that belong to specific POS categories. The list of POS tag categories that were removed includes ADV, CCONJ, ADP, AUX, CONJ, DET, INTJ, PART, PRON, PUNCT, SYM, SCONJ, ADJ, VERB, PROPN. [7]  After this initial preprocessing step, we run the first iteration of the candidate term selection part to build a filter of generic words, not to create topic taxonomies yet. For this step, we use the entire corpus of descriptions for each macro category, resulting in two corpora ( Food and Shopping ). For each micro category in the macro categories‚Äô corpora, we use Keyword Extraction and Topic Modeling to gather candidate terms for the filter, combining the results of both techniques in a list. Then, We use an LLM to remove the terms it identifies as unrelated to the main topic (each micro category) from the list. The prompt that we used for requesting this separation is illustrated below.  prompt= \"Given the terms in the following list: \"+ <wordsList> +\". Separate them into two groups. In group 1 the terms with no relation to the topic \"+ <type> +\". And in group 2 the terms that are related.\"  Listing 1: Prompt for separating candidate terms related to the type of establishment  By using this prompt, we try to ensure that the model‚Äôs response is consistently formatted according to the pattern described in it, facilitating the processing of the resulting string, although, some  6 https://www.google.com 7 https://spacy.io/usage/linguistic-features#pos-tagging   of the LLMs we tested did not output the response in the requested format. Once we complete one iteration of this method for each macro category in our dataset, we add the words of group 2 to the corresponding list of generic words. We apply the corresponding filter of generic words for each macro category corpus, resulting in the final preprocessed corpus.  5.2 Candidate Terms Selection  For this part of our method, we use each preprocessed corpus separately. For the Food corpus, we group the descriptions based on their micro-categories, creating 58 sub-corpus specific to that domain. We have six micro categories for the Shopping corpus, resulting in 6 specific sub-corpus. The candidate terms selection methods are applied to each sub-corpus, creating topic taxonomies where the main topic is the micro category.  5.2.1 Keyword Extraction. The first approach to candidate term selection was to use an unsupervised keyword selection method called Yake! [ 5 ]. This method is based on statistical text features extracted from single documents to select the most relevant keywords from that text. It does not require training on a document set and is not dependent on dictionaries, text size, language, domain, or external corpora. Yake! allows for the specification of parameters such as the language of the text, the maximum size of the n-grams being sought, and others. In our method, we customized only the language to Portuguese, and the maximum number of keywords sought for each set of descriptions was 30 words. After extracting the keywords from each group of descriptions, we obtained a total set of ùëÅ candidate terms. However, these terms are further filtered using an LLM, where we ask it to separate the terms related to the main topic from those unrelated, as explained earlier in subsection 5.1.  5.2.2 Topic Modeling. Our second approach to collecting initial topics and candidate terms was Topic Modeling. We applied the Latent Dirichlet Allocation algorithm [ 2 ], available at the Gensim Library [8] . We construct a dictionary for each macro-category corpus in our macro-categories corpora by extracting unique tokens and bigrams. After a few empirical tests, we set the minimum frequency of a bigram to 20 occurrences. Since some corpora have a minimal number of tokens (the micro category \"Greek Cuisine\" from the Food macro category has only five stores marked as such, with a corpus of only 127 tokens), we had to set a reasonably small number so that smaller corpora could also have a few bigrams. With the resulting dictionary of tokens, the LDA algorithm was applied. Three main parameters are to be defined in an LDA algorithm: number of topics, alpha, and beta . The number of topics defines the latent topics to be extracted from the corpus. The parameter alpha is a priori belief in documenttopic distribution, while beta is a priori belief in topic-word distribution. To define the number of topics for each micro category corpus, we tried numbers from 1 to 5, constantly checking which configuration would result in the best average topic coherence for that  8 https://pypi.org/project/gensim/   270   -----  Tagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies WebMedia‚Äô2024, Juiz de Fora, Brazil   corpus. Small corpora would have 1 or 2 topics, while bigger ones would have 5. To correctly define the alpha and beta priors, we would have to analyze the distribution for each category corpus  [ 24 ]. Since this would be rather difficult, we set those priors to be auto-defined by the LDA algorithm, which learns these parameters based on the corpus. We select the terms with the highest coherence with the resulting topics. Each topic returns 20 words with their coherence scores, but we do not use all of them as some have very low coherence. After testing a few configurations, for each topic, we select 60% of the terms with the highest coherence within that topic. With initial terms for each topic taxonomy, we ask an LLM to separate the ones closely related to the main topic from those unrelated, as mentioned earlier.  5.3 Hierarchy Construction  Once we have the post-processed lists of candidate terms obtained by each technique mentioned in subsection 5.2, we merge them and remove repetitions. After the merge, for each macro category, we have lists of terms for each micro category, representing each topic taxonomy. However, they do not have any hierarchy level between the terms configuring the taxonomy. To tackle this problem, we use an LLM again, this time with a prompt that searches for sub-categories within the terms of a topic to create these hierarchies. The prompt is illustrated below:  prompt=\"Create a dictionary by hierarchically arranging the  following words:\" + <wordsList> +.\" Use JSON format as the output such as the following: {\\\\\"key\\\\\": [\\\\\" list of words\\\\\"]}\"  Listing 2: Prompt for creating a hierarchy for each list of tags.  With this prompt, we seek to ensure that the LLM response has a consistent pattern and facilitates handling the returned string. After this step, we have a hierarchy of terms in each topic taxonomy in the Food and Shopping macro categories.  5.4 Merchant Tagging  With the topic taxonomies for both Food and Shopping macrocategories, we can now assign tags to merchants/establishments. To do so, we use the descriptions attached to these establishments, and we see which terms from a taxonomy are mentioned in their descriptions with a reverse index algorithm. We employ the taxonomy whose topic is the same as the establishment‚Äôs micro category, as shown in Figure 2.  6 TAXONOMY EXPANSION  Another essential part of our method is the automatic expansion of existing taxonomies as new terms arrive, derived from additional merchant scrapped data, as shown in Section 4. In this section, we present our approach to taxonomy expansion by using instructionbased LLMs.  As new transactions may include new businesses, new terms can emerge from the descriptions obtained through the scraping process. Therefore, we need to update the taxonomies with these   new terms maintaining and enriching the created hierarchies with the potential new terms. After completing the transaction enrichment process, including the search for business descriptions and the selection of candidate terms, if relevant terms not included in the current hierarchies are detected, we initiate the expansion process.  6.1 Prompt engineering instruction for taxonomy representation  First, we represent our topic taxonomies in a format that can be interpreted by an LLM. We employed a generic prompt, illustrated below, across all tested methods to convert topics into root nodes and their terms into child nodes.  Childs of [ROOT]: [CHILD1,CHILD2,CHILD3] Childs of [CHILD1]: [CHILD4,CHILD5]  Childs of [CHILD2]: [CHILD6]  ...  Listing 3: Prompt for representation of taxonomy  6.2 Predicting the parent of a node  To experiment with taxonomies expansion, we used two datasets: our Food and Shopping topic taxonomies and the taxonomies from SemEval-2015 Task 17 [ 15 ]. Those are low-resource taxonomies, with thousands of nodes or less, which are appropriate for the current prompt size of LLMs. We used the SemEval dataset to compare the results with well-established methods for taxonomy expansion, such as Musubu [ 20 ]. Similar to their experiments, we hid 20% of the terms (chosen randomically) in the taxonomies to predict their respective parent nodes. To verify the parent/root of a new term, we used the following prompt:  Listin g 4: Prom p t for searchin g for a node‚Äôs p arent  prompt=\"Who is the father of \"+<new_term>+\"?\"  In Table 1, we see the F1-Scores for parent node prediction. Equation 1 showcases how to calculate the F1-Score. TP is the number  of true positives, nodes that were correctly assigned as parents of child nodes. FP is the number of false positives, nodes that were incorrectly assigned as a parent to a child node. FN is the number of false negatives, nodes that should have been assigned as parent nodes but were not.  2 ‚àó ùëáùëÉ ùêπ 1 = (1) 2 ‚àó ùëáùëÉ + ùêπùëÉ + ùêπùëÅ  For baseline models, we used Bert and Musubu; for commercial LLMs, Gemini Pro and GPT-4; and for open-source LLMs, LLamaAlpaca(7B), Phi-2, and Mixtral 8x7B. We evaluate them in 4 taxonomies from the SemEval dataset and our taxonomies. For each taxonomy, the LLMs perform significantly better than Musubu, with GPT-4 and Gemini Pro having the highest F1-Scores, with the latter beating the former by a few points. However, the most recent open-source options (Phi-2 and Mixtral 8x7B) are getting close in performance.   271   -----  WebMedia‚Äô2024, Juiz de Fora, Brazil Moraes et al.     Tags assigned to establishments from the  \"Clothing & Accessories\" micro category           Figure 2: Assigning tags to establishments based on a topic taxonomy.  It is important to note that while SemEval taxonomies have thousands of nodes, ours have only a few hundred, which we can assume is a significant reason for the degrading performance of Musubu and Bert (LMs or LM-based methods). In contrast, the LLMs have a robust performance in such low-resource settings. This also shows that LLMs have a remarkable understanding of questions and zero-shot performance, generalizing well even for datasets in different languages.  7 TAXONOMY EVALUATION  To properly evaluate the topic taxonomies that we created in this work, we developed a two-step qualitative evaluation of a limited part of the results. In total, 58 topic taxonomies were created for the Food set and 6 for the Shopping set. For our evaluation, we selected the topic taxonomies with the highest number of terms in each part (the \"Brazilian Cuisine\" taxonomy for the Food part and the \"Clothing and Accessories\" taxonomy for the Shopping one). First, we assess the quality of removing generic terms from each taxonomy, and then, we evaluate the tags assigned to establishments based on that taxonomy. We asked 12 volunteers to answer a two-part form. Part 1 - Accuracy of the terms that were selected as related to the topic : In this part, we evaluate if the LLMs could correctly group the relevant and non-relevant terms, removing the generic terms. To do so, we defined a ground truth with the relevant terms as   true positives and the non-relevant terms as true negatives. Table 3 shows the results.  GPT-4 was the best model, followed by Gemini Pro, both scoring over 60% accuracy for the Brazilian Cuisine taxonomy and over 86% accuracy for the Clothing and Accessories taxonomy. Smaller language models such as Phi 2 and Llama 2 7B performed poorly both in removing generic terms and in formatting the response accordingly, with Phi 2 being particularly verbose. Part 2 - Human Evaluation of the Quality of the Tagging Process : In this part, the volunteers were asked to examine if the tags assigned to an establishment were appropriate and coherent to that establishment‚Äôs description. We selected the top 5 establishments with the highest transactions for each micro category. We asked our evaluators to analyze the tags assigned to describe that establishment and choose the ones that were not appropriate. This way, we have a coherence ratio for each establishment based on the number  of proper tags divided by the total number of tags. We average the results of our 12 evaluators and present them in Table 2. Figure 2 shows the \"Clothing & Accessories\" taxonomy that was evaluated and 2 of the merchants and the tags assigned to them that were included in the evaluation.  8 CONCLUSION  In this work, we presented an unsupervised method for automatically creating and expanding topic taxonomies using LLMs. We evaluated some of the generated taxonomies and applied them in transaction tagging in a retailer‚Äôs bank dataset. The evaluation showed promising results, with average coherence scores above 90% for the two selected taxonomies. The taxonomies‚Äô expansion with Gemini Pro also showed exciting results for parent node prediction, with F1-scores of 89% and 70% for Food and Shopping taxonomies, respectively. For future work on taxonomy construction, we plan to test more robust term selection methods, such as embedding-based approaches. We also plan on conducting ablation studies to validate whether the keyword extraction and topic modeling parts help improve the quality of the taxonomies created, by using a baseline prompt to ask the LLM to generate child nodes given a parent node. In terms of taxonomy expansion, there are several tasks to explore, ranging from node-level operations to generating entire sub-trees and identifying similar structures. Additionally, we intend to enhance our instruction-tuned LLM for taxonomy tasks   |Method|SemEval-2015 Task 17 Chemical Equipment Food Science|Our taxonomies Food Shopping| |---|---|---| |Gemini Pro|0.68 0.80 0.91 0.72|0.89 0.73| |GPT-4|0.65 0.78 0.89 0.70|0.87 0.71| |Mixtral-8x7B|0.59 0.63 0.80 0.57|0.74 0.60| |Phi-2|0.56 0.52 0.68 0.56|0.64 0.54| |LLama 7B|0.51 0.42 0.58 0.46|0.60 0.49| |Musubu|0.35 0.46 0.37 0.42|0.21 0.13| |Bert-Base|0.13 0.14 0.12 0.16|0.11 0.06|   Table 1: F1-score for parent node prediction.  272   -----  Tagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies WebMedia‚Äô2024, Juiz de Fora, Brazil  |Col1|Brazilian Cuisine Taxonomy Average Coherence Number of Tags|Col3|Clothing & Accessories Taxonomy Average Coherence Number of Tags|Col5| |---|---|---|---|---| |Merchant 1|92.30%|10|97.11%|8| |Merchant 2|94.23%|8|83.07%|5| |Merchant 3|89.23%|5|94.38%|5| |Merchant 4|87.17%|6|93.84%|5| |Merchant 5|93.40%|7|97.43%|6|    Table 2: Results of evaluating the tags assigned to each merchant/establishment.   |Col1|Brazilian Cuisine|Clothing & Accessories| |---|---|---| |Llama 2 7B|29.54%|52.78%| |Phi 2|40.90|73.68%| |Mixtral 8x7B v0.1|46.93%|70.27%| |Gemini Pro|61.36%|86.11%| |GPT 4|68.08%|86.84%|   Table 3: Accuracy of using each LLM to remove generic words from each topic taxonomy.  by fine-tuning or employing more efficient methods such as LoRA  [8].  LIMITATIONS  To address the limitations of our work, we begin with the taxonomy construction component. In this phase, we relied on topic modeling and keywords extraction to select candidate terms for our taxonomies. The LDA algorithm used for topic modeling performs suboptimally when the base corpus is small. Some of our topics had corpora with vocabularies of fewer than 100 words, which can result in topics containing irrelevant or incoherent terms. Additionally, we could have further experimented with the LDA hyperparameters for each micro-category corpus. Regarding the evaluation of the generated taxonomies, we did not assess topic completeness. Without a ground truth, it is challenging to quantify how comprehensively the terms in a taxonomy cover the main topic. Furthermore, we evaluated only 2 of the 64 taxonomies generated by our method, leaving a substantial portion unexamined. In the taxonomy expansion experiment, we evaluated only a lowresource setting with fewer than a thousand nodes. Most studies focus on taxonomies with hundreds of thousands or more nodes.  This presents a challenge for LLMs due to their limited context. Addressing this contextual limitation could benefit from insights found in other works that tackle similar issues [12].  ETHICS STATEMENT  In this work, we ensure the utmost protection of customers and store sensitive data by exclusively using non-sensitive information in our dataset. Our prompts solely rely on selected words from store descriptions, thus avoiding any direct usage of personal or sensitive information. No customer-specific data or store-sensitive details are integrated into the system, upholding privacy and security as top priorities.   Moreover, we strictly adhere to ethical guidelines during our experiments involving volunteers, and no personal data is collected from them. Our focus lies solely on analyzing the results of our proposed approach. Participants‚Äô anonymity and confidentiality are maintained throughout the research process, ensuring a responsible and trustworthy approach to data handling.  ACKNOWLEDGMENTS  The authors would like to acknowledge BTG Pactual for the partnership and financial support to this research.'"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 257
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ca23428dcc6b03b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:56:36.390376Z",
     "start_time": "2025-05-12T22:56:36.387840Z"
    }
   },
   "cell_type": "code",
   "source": "corpus.loc[4,'resumo'] = abs",
   "id": "9077ed2473d7aff4",
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:03:46.264150Z",
     "start_time": "2025-05-12T23:03:46.261748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tmp = text.split('**1 INTRODUCTION')[1].split('**ACKNOWLEDGMENTS**')[0]\n",
    "\n",
    "# head, *_ = re.split(pattern, text, maxsplit=1)\n",
    "tmp = clean_abs(tmp)\n",
    "# tmp"
   ],
   "id": "eaf9e49f7b2b1872",
   "outputs": [],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:49:13.618793Z",
     "start_time": "2025-05-12T23:49:13.615869Z"
    }
   },
   "cell_type": "code",
   "source": "corpus.loc[4,'text'] = tmp",
   "id": "de76bab783c5804f",
   "outputs": [],
   "execution_count": 261
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:50:16.860985Z",
     "start_time": "2025-05-12T23:50:16.483524Z"
    }
   },
   "cell_type": "code",
   "source": "corpus",
   "id": "e31abc706b2c1502",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               titulo informacoes_url  \\\n",
       "0   Recognition of Emotions through Facial Geometr...                   \n",
       "1   Enhancing widget recognition for automated And...                   \n",
       "2   Multi-Domain Spatio-Temporal Deformable Fusion...                   \n",
       "3              Finding Fake News Websites in the Wild                   \n",
       "4   Tagging Enriched Bank Transactions Using LLM-G...                   \n",
       "5   Fast ISP Mode Decision for the Versatile Video...                   \n",
       "6   OntoDrug: Enhancing Brazilian Health System In...                   \n",
       "7   E-BELA: Enhanced Embedding-Based Entity Linkin...                   \n",
       "8   Exploring Visual and Multimodal Interaction in...                   \n",
       "9   Elderly Fall Monitoring in Smart Homes Using W...                   \n",
       "10  A Comprehensive View of the Biases of Toxicity...                   \n",
       "11  Interoperability Testing Guide for the Interne...                   \n",
       "12  An Ensemble Approach to Facial Deepfake Detect...                   \n",
       "13  Twitter and the 2022 Brazilian Elections Portr...                   \n",
       "14  Automatic Time-aware Recognition of Brazilian ...                   \n",
       "15  Investigating User's Attentional Focus in Comp...                   \n",
       "16  Acceptance and Usability of Complex Medical Sy...                   \n",
       "17  Through the Eyes of Instagram: Analyzing Image...                   \n",
       "18  Characterization of the Brazilian musical land...                   \n",
       "19  A Machine-Learning-Driven Fast Video-based Poi...                   \n",
       "20  Constructing a KBQA Framework: Design and Impl...                   \n",
       "21  A Domain-Specific Language for Multimedia Serv...                   \n",
       "22  Why Ignore Content? A Guideline for Intrinsic ...                   \n",
       "23  Impacto da Pandemia na Discuss√£o sobre Sa√∫de M...                   \n",
       "24  Um Framework para An√°lise Bidimensional de Dis...                   \n",
       "25  Middleware para Aplica√ß√µes Distribu√≠das de V√≠d...                   \n",
       "26  O Impacto de Estrat√©gias de Embeddings de Graf...                   \n",
       "27  Cuidado Ub√≠quo de Pacientes com Doen√ßas Cr√¥nic...                   \n",
       "28  Jogos Digitais S√©rios usados para o Exerc√≠cio ...                   \n",
       "29  Crian√ßas e Propagandas no TikTok: identificand...                   \n",
       "30  Quando as Avalia√ß√µes Viram Bombas: Explorando ...                   \n",
       "31  Estrat√©gias de Undersampling para Redu√ß√£o de V...                   \n",
       "32  Arquitetura Multicamadas para Coleta e An√°lise...                   \n",
       "33  Um framework de rastreamento corporal para rea...                   \n",
       "34  √önicos, mas n√£o incompar√°veis: abordagens para...                   \n",
       "35  Uma Proposta de Framework para Sistemas de Con...                   \n",
       "36  Uma Abordagem em Etapa de Processamento para R...                   \n",
       "37  An√°lise de sentimentos de conte√∫do compartilha...                   \n",
       "38  Uma Investiga√ß√£o sobre T√©cnicas de Data Augmen...                   \n",
       "39  An√°lise da Percep√ß√£o do Uso de Cigarros Eletr√¥...                   \n",
       "\n",
       "        idioma                                        storage_key  \\\n",
       "0      english  ../articles/original/english/985-24766-1-10-20...   \n",
       "1      english  ../articles/original/english/985-24752-1-10-20...   \n",
       "2      english  ../articles/original/english/985-24762-1-10-20...   \n",
       "3      english  ../articles/original/english/985-24756-1-10-20...   \n",
       "4      english  ../articles/original/english/985-24767-1-10-20...   \n",
       "5      english  ../articles/original/english/985-24755-1-10-20...   \n",
       "6      english  ../articles/original/english/985-24764-1-10-20...   \n",
       "7      english  ../articles/original/english/985-24750-1-10-20...   \n",
       "8      english  ../articles/original/english/985-24754-1-10-20...   \n",
       "9      english  ../articles/original/english/985-24751-1-10-20...   \n",
       "10     english  ../articles/original/english/985-24737-1-10-20...   \n",
       "11     english  ../articles/original/english/985-24758-1-10-20...   \n",
       "12     english  ../articles/original/english/985-24741-1-10-20...   \n",
       "13     english  ../articles/original/english/985-24769-1-10-20...   \n",
       "14     english  ../articles/original/english/985-24745-1-10-20...   \n",
       "15     english  ../articles/original/english/985-24759-1-10-20...   \n",
       "16     english  ../articles/original/english/985-24740-1-10-20...   \n",
       "17     english  ../articles/original/english/985-24768-1-10-20...   \n",
       "18     english  ../articles/original/english/985-24746-1-10-20...   \n",
       "19     english  ../articles/original/english/985-24739-1-10-20...   \n",
       "20     english  ../articles/original/english/985-24747-1-10-20...   \n",
       "21     english  ../articles/original/english/985-24738-1-10-20...   \n",
       "22     english  ../articles/original/english/985-24776-1-10-20...   \n",
       "23  portuguese  ../articles/original/portuguese/985-24757-1-10...   \n",
       "24  portuguese  ../articles/original/portuguese/985-24771-2-10...   \n",
       "25  portuguese  ../articles/original/portuguese/985-24761-1-10...   \n",
       "26  portuguese  ../articles/original/portuguese/985-24763-2-10...   \n",
       "27  portuguese  ../articles/original/portuguese/985-24749-1-10...   \n",
       "28  portuguese  ../articles/original/portuguese/985-24760-1-10...   \n",
       "29  portuguese  ../articles/original/portuguese/985-24748-1-10...   \n",
       "30  portuguese  ../articles/original/portuguese/985-24765-1-10...   \n",
       "31  portuguese  ../articles/original/portuguese/985-24753-2-10...   \n",
       "32  portuguese  ../articles/original/portuguese/985-24744-1-10...   \n",
       "33  portuguese  ../articles/original/portuguese/985-24770-1-10...   \n",
       "34  portuguese  ../articles/original/portuguese/985-24775-1-10...   \n",
       "35  portuguese  ../articles/original/portuguese/985-24774-1-10...   \n",
       "36  portuguese  ../articles/original/portuguese/985-24772-1-10...   \n",
       "37  portuguese  ../articles/original/portuguese/985-24743-1-10...   \n",
       "38  portuguese  ../articles/original/portuguese/985-24773-1-10...   \n",
       "39  portuguese  ../articles/original/portuguese/985-24742-1-10...   \n",
       "\n",
       "                                              autores data_publicacao  \\\n",
       "0   [Alessandra Alaniz Macedo,  Leandro Persona,  ...      11-09-2024   \n",
       "1   [Yadini P√©rez L√≥pez,  La√≠s Dib Albuquerque,  G...      11-09-2024   \n",
       "2   [Garibaldi da Silveira J√∫nior, Gilberto Kreisl...      11-09-2024   \n",
       "3   [Leandro Araujo,  Jo√£o M. M. Couto,  Luiz Feli...      11-09-2024   \n",
       "4   [Daniel de S. Moraes,  Polyana B. da Costa,  P...      11-09-2024   \n",
       "5   [Larissa Ara√∫jo,  Adson Duarte,  Bruno Zatt,  ...      11-09-2024   \n",
       "6   [Nelson Miranda,  Matheus Matos Machado,   Dil...      11-09-2024   \n",
       "7            [√çtalo M. Pereira  Anderson A. Ferreira]      11-09-2024   \n",
       "8   [Paulo Victor Borges,  Daniel de S. Moraes,  J...      11-09-2024   \n",
       "9   [J√∫lia M. P. Moreira,  Raphael W. Bettio,  And...      11-09-2024   \n",
       "10  [Guilherme Andrade,  Luiz Nery,  Fabr√≠cio Bene...      11-09-2024   \n",
       "11  [Karina da Silva Castelo Branco,  Val√©ria Lell...      11-09-2024   \n",
       "12  [Yan Martins B. Gurevitz Cunha,  Jos√© Matheus ...      11-09-2024   \n",
       "13  [Larissa Malagoli,  Giovana Piorino,  Carlos H...      11-09-2024   \n",
       "14  [Lucas de S. Arcanjo,  Lucas F. Coelho,  Silvi...      11-09-2024   \n",
       "15  [Cassiano da Silva Souza,  Milene Selbach Silv...      11-09-2024   \n",
       "16  [F√°bio Ap. C√¢ndido da Silva,  Andr√© Pimenta Fr...      11-09-2024   \n",
       "17  [Jo√£o Francisco Hecksher Olivetti  Philipe de ...      11-09-2024   \n",
       "18  [Filipe A. S. Moura,  Carlos H. G. Ferreira,  ...      11-09-2024   \n",
       "19  [Gustavo Rehbein,  Eduardo Costa,  Guilherme C...      11-09-2024   \n",
       "20  [R√¥mulo Chrispim de Mello,  Jor√£o Gomes Jr.,  ...      11-09-2024   \n",
       "21  [Franklin Jordan Ventura Quico,  Anselmo L. E....      11-09-2024   \n",
       "22  [Pedro R. Pires,  Bruno B. Rizzi,   Tiago A. A...      11-09-2024   \n",
       "23  [Pedro Bento,  Arthur Buzelin,  Yan Aquino,  I...      11-09-2024   \n",
       "24  [Geovana S. Oliveira,  Ot√°vio Ven√¢ncio,  Vin√≠c...      25-09-2024   \n",
       "25  [Otac√≠lio de A. Ramos Neto,  Rafael C. Chaves,...      11-09-2024   \n",
       "26  [Andr√© Levi Zanon,  Leonardo Rocha,   Marcelo ...      04-10-2024   \n",
       "27  [Lucas Pfeiffer Salom√£o Dias,  L.P.S. Dias,  J...      11-09-2024   \n",
       "28  [Katherin Felipa Carhuaz Malpartida  Kamila Ri...      11-09-2024   \n",
       "29  [Ra√≠ssa Gon√ßalves Lopes Carvalho  Humberto Tor...      11-09-2024   \n",
       "30  [Marcus Vinicius Guerra Ribeiro,  Clara Andrad...      11-09-2024   \n",
       "31  [Guilherme Fonseca,  Gabriel Prenassi,  Washin...      04-10-2024   \n",
       "32  [Juan Felipe Souza Oliveira,  Paulo Cesar Salg...      19-09-2024   \n",
       "33  [Elvis Ribeiro,  Alexandre Brand√£o,  Marcelo G...      11-09-2024   \n",
       "34  [Guilherme O. Aguiar,  Juan P. D. Esteves,  Cl...      11-09-2024   \n",
       "35  [Marcelo Rocha,  Jesus Favela,   D√©bora C. Muc...      11-09-2024   \n",
       "36  [Rodrigo Ferrari de Souza  Marcelo Garcia Manz...      11-09-2024   \n",
       "37  [Giovana Piorino,  Vitor Moreira,  Luiz Henriq...      11-09-2024   \n",
       "38  [Marcos Andr√© Bezerra da Silva,  Manuella Asch...      11-09-2024   \n",
       "39  [Aline Dias,  Richardy R. Tanure,  Jussara M. ...      11-09-2024   \n",
       "\n",
       "                                               resumo  \\\n",
       "0   O reconhecimento de emo√ß√£o tem um significado ...   \n",
       "1   O reconhecimento de widgets √© crucial para tes...   \n",
       "2   A compacta√ß√£o de v√≠deo com perda de im√≥vel int...   \n",
       "3   A batalha contra a dissemina√ß√£o de informa√ß√µes...   \n",
       "4   This work presents an unsupervised method for ...   \n",
       "5   O padr√£o vers√°til de codifica√ß√£o de v√≠deo (VVC...   \n",
       "6   Este artigo apresenta ontodrug, uma ontologia ...   \n",
       "7   Entidade vinculando √© o processo de conectar m...   \n",
       "8   Este artigo apresenta duas ferramentas inovado...   \n",
       "9   O progresso constante da tecnologia, especialm...   \n",
       "10  A linguagem √© um aspecto din√¢mico de nossa cul...   \n",
       "11  A Internet das Coisas (IoT) expandiu a Interne...   \n",
       "12  Esfor√ßos substanciais foram dedicados ao desen...   \n",
       "13  A influ√™ncia das redes sociais on -line nas a√ß...   \n",
       "14  A linguagem de sinais brasileiros (Libras) √© u...   \n",
       "15  Manter o foco atencional do usu√°rio se tornou ...   \n",
       "16  A crescente demanda por testes de imagem torno...   \n",
       "17  A comunica√ß√£o multim√≠dia se tornou uma parte e...   \n",
       "18  Na era digital, servi√ßos de streaming como o S...   \n",
       "19  Nos √∫ltimos anos, o conte√∫do da nuvem de ponto...   \n",
       "20  O crescimento exponencial dos dados na Interne...   \n",
       "21  A virtualiza√ß√£o √© uma tecnologia amplamente us...   \n",
       "22  Com o crescimento constante das informa√ß√µes di...   \n",
       "23  The period of social isolation due to COVID-19...   \n",
       "24  As plataformas de m√≠dia social revolucionaram ...   \n",
       "25  Dentro do escopo da ind√∫stria 4.0, a vis√£o com...   \n",
       "26  Explica√ß√µes em sistemas de recomenda√ß√£o s√£o es...   \n",
       "27  As doen√ßas cr√¥nicas est√£o entre as 7 das 10 pr...   \n",
       "28  O pensamento computacional (CT) √© um processo ...   \n",
       "29  Este estudo aborda a identifica√ß√£o da publicid...   \n",
       "30  Este estudo investiga o fen√¥meno do \" * revis√£...   \n",
       "31  Classifica√ß√£o de texto autom√°tica (ATC) em con...   \n",
       "32  Uma arquitetura multicamada foi desenvolvida p...   \n",
       "33  Aplica√ß√µes e jogos multim√≠dia desempenham um p...   \n",
       "34  Entender o comportamento emocional humano √© um...   \n",
       "35  H√° uma lacuna em plataformas rob√≥ticas de c√≥di...   \n",
       "36  Os sistemas de recomenda√ß√£o s√£o projetados par...   \n",
       "37  O uso crescente das m√≠dias sociais e seu impac...   \n",
       "38  A tradu√ß√£o autom√°tica de portugu√™s para Libras...   \n",
       "39  A ascens√£o de plataformas de v√≠deo como o YouT...   \n",
       "\n",
       "                                             keywords  \\\n",
       "0   Multimedia Processing, Affective Computing, Ma...   \n",
       "1   Automated Android Testing, Test Portability, O...   \n",
       "2   Redes neurais profundas, Melhoria de qualidade...   \n",
       "3   Fake News, Misinformation, Credibility, Websit...   \n",
       "4   Large Language Models, Natural Language Proces...   \n",
       "5        VVC, Intra Prediction, ISP, Machine Learning   \n",
       "6   Medication Ontologies, Drug Management, Semant...   \n",
       "7   Natural Language Processing, Entity Linking, L...   \n",
       "8   Authoring, LLMs, NCL, Code Generation, Visual ...   \n",
       "9   fall detection, wearable technologies, acceler...   \n",
       "10  African American English, AAE, Bias, Toxicity,...   \n",
       "11  Interoperability, Internet of Things, Interope...   \n",
       "12  deep fake detection, self-supervised, vision t...   \n",
       "13  2022 Brazilian Elections, Network Modeling, On...   \n",
       "14  Computer Vision, Sign Language Recognition, Ge...   \n",
       "15  attention monitoring, webcam, data analysis, l...   \n",
       "16  usability issues, radiology systems, qualitati...   \n",
       "17  Instagram, alt-text, social media, image class...   \n",
       "18  Music Preferences, Music Genre Networks, Regio...   \n",
       "19  point clouds, machine learning, V-PCC, complex...   \n",
       "20  KBQA, Complex Questions, Entity Recognition, P...   \n",
       "21                  IoMT, IoT, VNF, SFC, DSL, L-PRISM   \n",
       "22  embeddings, intrinsic evaluation, qualitative ...   \n",
       "23  Redes sociais, COVID-19, pandemia, sa√∫de menta...   \n",
       "24  M√≠dias Sociais, Dissemina√ß√£o de Informa√ß√£o, Mo...   \n",
       "25  Middleware, Distribui√ß√£o de V√≠deo, Edge Comput...   \n",
       "26  Sistemas de Recomenda√ß√£o, Explica√ß√µes, Embeddi...   \n",
       "27  Classifica√ß√£o de Comportamento; Doen√ßas Cr√¥nic...   \n",
       "28  Pensamento Computacional, Jogos Digitais S√©rio...   \n",
       "29  Publicidade Infantil, Crian√ßa, TikTok, Rede So...   \n",
       "30  jogos, videogames, Metacritic, review bombing,...   \n",
       "31  Classifica√ß√£o de Texto, Transformers, Undersam...   \n",
       "32  internet das coisas, monitoramento remoto de s...   \n",
       "33  Realidade Virtual, Jogos S√©rios, Multim√≠dia, R...   \n",
       "34  Computa√ß√£o Afetiva, Respostas Emocionais, Mode...   \n",
       "35  Rob√¥s sociais, Framework para controle de rob√¥...   \n",
       "36  Sistemas de Recomenda√ß√£o, Vi√©s de Popularidade...   \n",
       "37  An√°lise de Sentimentos, Comunidades do Reddit,...   \n",
       "38  Tradu√ß√£o Autom√°tica Neural, Aumento de Dados, ...   \n",
       "39  Cigarros Eletr√¥nicos, Aprendizado de M√°quina, ...   \n",
       "\n",
       "                                          referencias  \\\n",
       "0   [[1] Mauricio Alvarez, David Luengo, and Neil ...   \n",
       "1   [[1] A. A. Abdelhamid, S. Alotaibi, and A. Mou...   \n",
       "2   [[1] Aayushi Agarwal, Akshay Agarwal, Sayan Si...   \n",
       "3   [[1] Leandro Ara√∫jo, Luiz Felipe Nery, Isadora...   \n",
       "4   [[1] Rohan Anil, Sebastian Borgeaud, Yonghui W...   \n",
       "5   [[1] James Bergstra and Yoshua Bengio. 2012. R...   \n",
       "6   [[1] Burhan Ud Din Abbasi, Iram Fatima, Hamid ...   \n",
       "7   [[1] Moonis Ali and Savvas Zannettou. 2024. Fr...   \n",
       "8   [[1] NBR ABNT. [n. d.]. Digital Terrestrial Te...   \n",
       "9   [[1] Ibukun Awolusi, Eric Marks, and Matthew H...   \n",
       "10  [[1] Abubakar Abid, Maheen Farooqi, and James ...   \n",
       "11  [[1] Home Assistant. 2024. Awaken your home. h...   \n",
       "12  [[1] Redha Ali, Russell C. Hardie, Barath Nara...   \n",
       "13  [[1] Peiman Barnaghi, Parsa Ghaffari, and John...   \n",
       "14  [[1] Sunusi Bala Abdullahi and Kosin Chamnongt...   \n",
       "15  [[1] and the ACM Digital Library, [2], due to ...   \n",
       "16  [[1] Reza Abbasi, Monireh Sadeqi Jabali, Reza ...   \n",
       "17  [[1] Rakesh Agrawal, Ramakrishnan Srikant, et ...   \n",
       "18  [[1] Moonis Ali and Savvas Zannettou. 2024. Fr...   \n",
       "19  [[1] Gisle Bjontegaard. 2001. Calculation of a...   \n",
       "20  [[1] Shuang Chen, Qian Liu, Zhiwei Yu, Chin-Ye...   \n",
       "21  [[1] ETSI GS NFV-IFA 011. 2023. Network Functi...   \n",
       "22  [[1] Gediminas Adomavicius and Alexander Tuzhi...   \n",
       "23  [[1] Elia Abi-Jaoude, Karline Treurnicht Naylo...   \n",
       "24  [[1] Marcelo MR Araujo, Carlos HG Ferreira, Ju...   \n",
       "25  [[1] Ganesh Ananthanarayanan, Paramvir Bahl, P...   \n",
       "26  [[1] Ali, M., Berrendorf, M., Hoyt, C.T., Verm...   \n",
       "27  [[1] Rodrigo Simon Bavaresco and Jorge Luis Vi...   \n",
       "28  [[1] AAIDD. 2021. Defining Criteria for Intell...   \n",
       "29  [[1] Instituto Alana. 2022. Notifica√ß√£o enviad...   \n",
       "30  [[1] 2022. O que √© ‚Äôwoke‚Äô e por que o termo ge...   \n",
       "31  [[1] Lasse F Wolff Anthony, Benjamin Kanding, ...   \n",
       "32  [[1] Zahra Ahmadi, Mostafa Haghi Kashani, Moha...   \n",
       "33  [[1] Alberto Luiz Aramaki, Rosana Ferreira Sam...   \n",
       "34  [[1] Bert Bakker, Gijs Schumacher, Kevin Arcen...   \n",
       "35  [[1] Michel Albonico, Milica √êorƒëeviƒá, Engel H...   \n",
       "36  [[1] Himan Abdollahpouri, Masoud Mansoury, Rob...   \n",
       "37  [[1] Rafael J. A. Almeida. 2018. LeIA - L√©xico...   \n",
       "38  [[1] T. M. U. Ara√∫jo. 2012. Uma solu√ß√£o para g...   \n",
       "39  [[1] Shishir Adhikari, Akshay Uppal, Robin Mer...   \n",
       "\n",
       "                                                 text  \\\n",
       "0   Introdu√ß√£o O reconhecimento das emo√ß√µes √© uma ...   \n",
       "1   Introdu√ß√£o O teste automatizado de aplicativos...   \n",
       "2   Introdu√ß√£o A compacta√ß√£o de v√≠deo desempenha u...   \n",
       "3   Introdu√ß√£o Nos √∫ltimos tempos, a sociedade enf...   \n",
       "4   INTRODUCTION  Many recent studies have focused...   \n",
       "5   Introdu√ß√£o Os v√≠deos digitais t√™m sido fundame...   \n",
       "6   Introdu√ß√£o O gerenciamento de medicamentos √© u...   \n",
       "7   Introdu√ß√£o O volume crescente de dados publica...   \n",
       "8   Introdu√ß√£o Aplicativos de TV digital (DTV) [26...   \n",
       "9   Introdu√ß√£o ao incentivo √† inova√ß√£o e tecnologi...   \n",
       "10  Introdu√ß√£o nas √∫ltimas d√©cadas, testemunhamos ...   \n",
       "11  A tecnologia de introdu√ß√£o transformou signifi...   \n",
       "12  Introdu√ß√£o Nos √∫ltimos anos, houve um foco cre...   \n",
       "13  Introdu√ß√£o As plataformas de m√≠dia social on -...   \n",
       "14  Introdu√ß√£o O reconhecimento brasileiro de ling...   \n",
       "15  Introdu√ß√£o O avan√ßo das tecnologias de informa...   \n",
       "16  Introdu√ß√£o Os sistemas de informa√ß√£o em sa√∫de ...   \n",
       "17  Introdu√ß√£o A Web se tornou parte integrante da...   \n",
       "18  Introdu√ß√£o Na era digital, os servi√ßos de stre...   \n",
       "19  As nuvens de ponto de introdu√ß√£o podem ser usa...   \n",
       "20  Introdu√ß√£o O grande volume de dados dispon√≠vei...   \n",
       "21  Introdu√ß√£o A virtualiza√ß√£o √© um conceito que a...   \n",
       "22  Introdu√ß√£o Os sistemas de recomenda√ß√£o s√£o fer...   \n",
       "23  INTRODU√á√ÉO Nos √∫ltimos anos, o panorama da com...   \n",
       "24  INTRODU√á√ÉO As plataformas de m√≠dias sociais re...   \n",
       "25  INTRODU√á√ÉO Nos √∫ltimos anos, a import√¢ncia de ...   \n",
       "26  INTRODU√á√ÉO Sistemas de Recomenda√ß√£o (SsR) s√£o ...   \n",
       "27  INTRODU√á√ÉO A Organiza√ß√£o Mundial de Sa√∫de (OMS...   \n",
       "28  INTRODU√á√ÉO O Pensamento Computacional (PC) √© d...   \n",
       "29  INTRODUCTION No cen√°rio contempor√¢neo, as rede...   \n",
       "30  INTRODU√á√ÉO Atualmente, plataformas digitais de...   \n",
       "31  INTRODU√á√ÉO Classifica√ß√£o Autom√°tica de Texto (...   \n",
       "32  INTRODU√á√ÉO O crescimento exponencial dos siste...   \n",
       "33  Um framework de rastreamento corporal para rea...   \n",
       "34  INTRODU√á√ÉO Num contexto em que as emo√ß√µes huma...   \n",
       "35  INTRODU√á√ÉO A ado√ß√£o de novas tecnologias na ed...   \n",
       "36  INTRODU√á√ÉO Os sistemas de recomenda√ß√£o s√£o pro...   \n",
       "37  INTRODU√á√ÉO As redes sociais t√™m rompido barrei...   \n",
       "38  INTRODU√á√ÉO A evolu√ß√£o da tecnologia tem desemp...   \n",
       "39  INTRODU√á√ÉO A ascens√£o de plataformas de v√≠deo,...   \n",
       "\n",
       "                                    artigo_tokenizado  \\\n",
       "0   [Introdu√ß√£o, O, reconhecimento, das, emo√ß√µes, ...   \n",
       "1   [Introdu√ß√£o, O, teste, automatizado, de, aplic...   \n",
       "2   [Introdu√ß√£o, A, compacta√ß√£o, de, v√≠deo, desemp...   \n",
       "3   [Introdu√ß√£o, Nos, √∫ltimos, tempos, ,, a, socie...   \n",
       "4   [9/02/2008, √†s, 19:20:05, Molho, de, macarr√£o,...   \n",
       "5   [Introdu√ß√£o, Os, v√≠deos, digitais, t√™m, sido, ...   \n",
       "6   [Introdu√ß√£o, O, gerenciamento, de, medicamento...   \n",
       "7   [Introdu√ß√£o, O, volume, crescente, de, dados, ...   \n",
       "8   [Introdu√ß√£o, Aplicativos, de, TV, digital, (, ...   \n",
       "9   [Introdu√ß√£o, ao, incentivo, √†, inova√ß√£o, e, te...   \n",
       "10  [Introdu√ß√£o, nas, √∫ltimas, d√©cadas, ,, testemu...   \n",
       "11  [A, tecnologia, de, introdu√ß√£o, transformou, s...   \n",
       "12  [Introdu√ß√£o, Nos, √∫ltimos, anos, ,, houve, um,...   \n",
       "13  [Introdu√ß√£o, As, plataformas, de, m√≠dia, socia...   \n",
       "14  [Introdu√ß√£o, O, reconhecimento, brasileiro, de...   \n",
       "15  [Introdu√ß√£o, O, avan√ßo, das, tecnologias, de, ...   \n",
       "16  [Introdu√ß√£o, Os, sistemas, de, informa√ß√£o, em,...   \n",
       "17  [Introdu√ß√£o, A, Web, se, tornou, parte, integr...   \n",
       "18  [Introdu√ß√£o, Na, era, digital, ,, os, servi√ßos...   \n",
       "19  [As, nuvens, de, ponto, de, introdu√ß√£o, podem,...   \n",
       "20  [Introdu√ß√£o, O, grande, volume, de, dados, dis...   \n",
       "21  [Introdu√ß√£o, A, virtualiza√ß√£o, √©, um, conceito...   \n",
       "22  [Introdu√ß√£o, Os, sistemas, de, recomenda√ß√£o, s...   \n",
       "23  [INTRODU√á√ÉO, Nos, √∫ltimos, anos, ,, o, panoram...   \n",
       "24  [INTRODU√á√ÉO, As, plataformas, de, m√≠dias, soci...   \n",
       "25  [INTRODU√á√ÉO, Nos, √∫ltimos, anos, ,, a, import√¢...   \n",
       "26  [INTRODU√á√ÉO, Sistemas, de, Recomenda√ß√£o, (, Ss...   \n",
       "27  [INTRODU√á√ÉO, A, Organiza√ß√£o, Mundial, de, Sa√∫d...   \n",
       "28  [INTRODU√á√ÉO, O, Pensamento, Computacional, (, ...   \n",
       "29  [INTRODUCTION, No, cen√°rio, contempor√¢neo, ,, ...   \n",
       "30  [INTRODU√á√ÉO, Atualmente, ,, plataformas, digit...   \n",
       "31  [INTRODU√á√ÉO, Classifica√ß√£o, Autom√°tica, de, Te...   \n",
       "32  [INTRODU√á√ÉO, O, crescimento, exponencial, dos,...   \n",
       "33  [Um, framework, de, rastreamento, corporal, pa...   \n",
       "34  [INTRODU√á√ÉO, Num, contexto, em, que, as, emo√ß√µ...   \n",
       "35  [INTRODU√á√ÉO, A, ado√ß√£o, de, novas, tecnologias...   \n",
       "36  [INTRODU√á√ÉO, Os, sistemas, de, recomenda√ß√£o, s...   \n",
       "37  [INTRODU√á√ÉO, As, redes, sociais, t√™m, rompido,...   \n",
       "38  [INTRODU√á√ÉO, A, evolu√ß√£o, da, tecnologia, tem,...   \n",
       "39  [INTRODU√á√ÉO, A, ascens√£o, de, plataformas, de,...   \n",
       "\n",
       "                                           pos_tagger  \\\n",
       "0   [[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [rec...   \n",
       "1   [[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [tes...   \n",
       "2   [[Introdu√ß√£o, NOUN, NOUN], [A, DET, DET], [com...   \n",
       "3   [[Introdu√ß√£o, NOUN, NOUN], [Nos, ADP, ADP], [√∫...   \n",
       "4   [[9/02/2008, NUM, NUM], [√†s, ADP, ADP], [19:20...   \n",
       "5   [[Introdu√ß√£o, NOUN, NOUN], [Os, DET, DET], [v√≠...   \n",
       "6   [[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [ger...   \n",
       "7   [[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [vol...   \n",
       "8   [[Introdu√ß√£o, PROPN, PROPN], [Aplicativos, PRO...   \n",
       "9   [[Introdu√ß√£o, NOUN, NOUN], [ao, ADP, ADP], [in...   \n",
       "10  [[Introdu√ß√£o, NOUN, NOUN], [nas, ADP, ADP], [√∫...   \n",
       "11  [[A, DET, DET], [tecnologia, NOUN, NOUN], [de,...   \n",
       "12  [[Introdu√ß√£o, NOUN, NOUN], [Nos, ADP, ADP], [√∫...   \n",
       "13  [[Introdu√ß√£o, PROPN, PROPN], [As, DET, DET], [...   \n",
       "14  [[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [rec...   \n",
       "15  [[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [ava...   \n",
       "16  [[Introdu√ß√£o, NOUN, NOUN], [Os, DET, DET], [si...   \n",
       "17  [[Introdu√ß√£o, NOUN, NOUN], [A, DET, DET], [Web...   \n",
       "18  [[Introdu√ß√£o, NOUN, NOUN], [Na, ADP, ADP], [er...   \n",
       "19  [[As, DET, DET], [nuvens, NOUN, NOUN], [de, AD...   \n",
       "20  [[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [gra...   \n",
       "21  [[Introdu√ß√£o, NOUN, NOUN], [A, DET, DET], [vir...   \n",
       "22  [[Introdu√ß√£o, NOUN, NOUN], [Os, DET, DET], [si...   \n",
       "23  [[INTRODU√á√ÉO, PROPN, PROPN], [Nos, ADP, ADP], ...   \n",
       "24  [[INTRODU√á√ÉO, PROPN, PROPN], [As, DET, DET], [...   \n",
       "25  [[INTRODU√á√ÉO, PROPN, PROPN], [Nos, ADP, ADP], ...   \n",
       "26  [[INTRODU√á√ÉO, PROPN, PROPN], [Sistemas, PROPN,...   \n",
       "27  [[INTRODU√á√ÉO, PROPN, PROPN], [A, DET, DET], [O...   \n",
       "28  [[INTRODU√á√ÉO, PROPN, PROPN], [O, DET, DET], [P...   \n",
       "29  [[INTRODUCTION, PROPN, PROPN], [No, ADP, ADP],...   \n",
       "30  [[INTRODU√á√ÉO, PROPN, PROPN], [Atualmente, ADV,...   \n",
       "31  [[INTRODU√á√ÉO, PROPN, PROPN], [Classifica√ß√£o, P...   \n",
       "32  [[INTRODU√á√ÉO, PROPN, PROPN], [O, DET, DET], [c...   \n",
       "33  [[Um, DET, DET], [framework, PROPN, PROPN], [d...   \n",
       "34  [[INTRODU√á√ÉO, PROPN, PROPN], [Num, ADP, ADP], ...   \n",
       "35  [[INTRODU√á√ÉO, PROPN, PROPN], [A, DET, DET], [a...   \n",
       "36  [[INTRODU√á√ÉO, PROPN, PROPN], [Os, DET, DET], [...   \n",
       "37  [[INTRODU√á√ÉO, PROPN, PROPN], [As, DET, DET], [...   \n",
       "38  [[INTRODU√á√ÉO, PROPN, PROPN], [A, DET, DET], [e...   \n",
       "39  [[INTRODU√á√ÉO, PROPN, PROPN], [A, DET, DET], [a...   \n",
       "\n",
       "                                                 lema  \n",
       "0   [introdu√ß√£o, o, reconhecimento, de o, emo√ß√£o, ...  \n",
       "1   [introdu√ß√£o, o, teste, automatizado, de, aplic...  \n",
       "2   [introdu√ß√£o, o, compacta√ß√£o, de, v√≠deo, desemp...  \n",
       "3   [introdu√ß√£o, em o, √∫ltimo, tempo, o, sociedade...  \n",
       "4   [9/02/2008, a o, 19:20:05, Molho, de, macarr√£o...  \n",
       "5   [introdu√ß√£o, o, v√≠deo, digital, ter, ser, fund...  \n",
       "6   [introdu√ß√£o, o, gerenciamento, de, medicamento...  \n",
       "7   [introdu√ß√£o, o, volume, crescente, de, dado, p...  \n",
       "8   [Introdu√ß√£o, Aplicativos, de, tv, digital, DTV...  \n",
       "9   [introdu√ß√£o, a o, incentivo, a o, inova√ß√£o, e,...  \n",
       "10  [introdu√ß√£o, em o, √∫ltimo, d√©cada, testemunham...  \n",
       "11  [o, tecnologia, de, introdu√ß√£o, transformar, s...  \n",
       "12  [introdu√ß√£o, em o, √∫ltimo, ano, haver, um, foc...  \n",
       "13  [introdu√ß√£o, o, plataforma, de, m√≠dia, social,...  \n",
       "14  [introdu√ß√£o, o, reconhecimento, brasileiro, de...  \n",
       "15  [introdu√ß√£o, o, avan√ßo, de o, tecnologia, de, ...  \n",
       "16  [introdu√ß√£o, o, sistema, de, informa√ß√£o, em, s...  \n",
       "17  [introdu√ß√£o, o, Web, se, tornar, parte, integr...  \n",
       "18  [introdu√ß√£o, em o, ser, digital, o, servi√ßo, d...  \n",
       "19  [o, nuvem, de, ponto, de, introdu√ß√£o, poder, s...  \n",
       "20  [introdu√ß√£o, o, grande, volume, de, dado, disp...  \n",
       "21  [introdu√ß√£o, o, virtualiza√ß√£o, ser, um, concei...  \n",
       "22  [introdu√ß√£o, o, sistema, de, recomenda√ß√£o, ser...  \n",
       "23  [INTRODU√á√ÉO, em o, √∫ltimo, ano, o, panorama, d...  \n",
       "24  [INTRODU√á√ÉO, o, plataforma, de, m√≠dia, social,...  \n",
       "25  [INTRODU√á√ÉO, em o, √∫ltimo, ano, o, import√¢ncia...  \n",
       "26  [INTRODU√á√ÉO, Sistemas, de, Recomenda√ß√£o, SsR, ...  \n",
       "27  [INTRODU√á√ÉO, o, Organiza√ß√£o, Mundial, de, Sa√∫d...  \n",
       "28  [INTRODU√á√ÉO, o, Pensamento, Computacional, PC,...  \n",
       "29  [INTRODUCTION, em o, cen√°rio, contempor√¢neo, o...  \n",
       "30  [INTRODU√á√ÉO, Atualmente, plataforma, digital, ...  \n",
       "31  [INTRODU√á√ÉO, Classifica√ß√£o, Autom√°tica, de, Te...  \n",
       "32  [INTRODU√á√ÉO, o, crescimento, exponencial, de o...  \n",
       "33  [um, framework, de, rastreamento, corporal, pa...  \n",
       "34  [INTRODU√á√ÉO, em um, contexto, em, que, o, emo√ß...  \n",
       "35  [INTRODU√á√ÉO, o, ado√ß√£o, de, novo, tecnologia, ...  \n",
       "36  [INTRODU√á√ÉO, o, sistema, de, recomenda√ß√£o, ser...  \n",
       "37  [INTRODU√á√ÉO, o, rede, social, ter, rompir, bar...  \n",
       "38  [INTRODU√á√ÉO, o, evolu√ß√£o, de o, tecnologia, te...  \n",
       "39  [INTRODU√á√ÉO, o, ascens√£o, de, plataforma, de, ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titulo</th>\n",
       "      <th>informacoes_url</th>\n",
       "      <th>idioma</th>\n",
       "      <th>storage_key</th>\n",
       "      <th>autores</th>\n",
       "      <th>data_publicacao</th>\n",
       "      <th>resumo</th>\n",
       "      <th>keywords</th>\n",
       "      <th>referencias</th>\n",
       "      <th>text</th>\n",
       "      <th>artigo_tokenizado</th>\n",
       "      <th>pos_tagger</th>\n",
       "      <th>lema</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Recognition of Emotions through Facial Geometr...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24766-1-10-20...</td>\n",
       "      <td>[Alessandra Alaniz Macedo,  Leandro Persona,  ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O reconhecimento de emo√ß√£o tem um significado ...</td>\n",
       "      <td>Multimedia Processing, Affective Computing, Ma...</td>\n",
       "      <td>[[1] Mauricio Alvarez, David Luengo, and Neil ...</td>\n",
       "      <td>Introdu√ß√£o O reconhecimento das emo√ß√µes √© uma ...</td>\n",
       "      <td>[Introdu√ß√£o, O, reconhecimento, das, emo√ß√µes, ...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [rec...</td>\n",
       "      <td>[introdu√ß√£o, o, reconhecimento, de o, emo√ß√£o, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Enhancing widget recognition for automated And...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24752-1-10-20...</td>\n",
       "      <td>[Yadini P√©rez L√≥pez,  La√≠s Dib Albuquerque,  G...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O reconhecimento de widgets √© crucial para tes...</td>\n",
       "      <td>Automated Android Testing, Test Portability, O...</td>\n",
       "      <td>[[1] A. A. Abdelhamid, S. Alotaibi, and A. Mou...</td>\n",
       "      <td>Introdu√ß√£o O teste automatizado de aplicativos...</td>\n",
       "      <td>[Introdu√ß√£o, O, teste, automatizado, de, aplic...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [tes...</td>\n",
       "      <td>[introdu√ß√£o, o, teste, automatizado, de, aplic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Multi-Domain Spatio-Temporal Deformable Fusion...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24762-1-10-20...</td>\n",
       "      <td>[Garibaldi da Silveira J√∫nior, Gilberto Kreisl...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A compacta√ß√£o de v√≠deo com perda de im√≥vel int...</td>\n",
       "      <td>Redes neurais profundas, Melhoria de qualidade...</td>\n",
       "      <td>[[1] Aayushi Agarwal, Akshay Agarwal, Sayan Si...</td>\n",
       "      <td>Introdu√ß√£o A compacta√ß√£o de v√≠deo desempenha u...</td>\n",
       "      <td>[Introdu√ß√£o, A, compacta√ß√£o, de, v√≠deo, desemp...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [A, DET, DET], [com...</td>\n",
       "      <td>[introdu√ß√£o, o, compacta√ß√£o, de, v√≠deo, desemp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Finding Fake News Websites in the Wild</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24756-1-10-20...</td>\n",
       "      <td>[Leandro Araujo,  Jo√£o M. M. Couto,  Luiz Feli...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A batalha contra a dissemina√ß√£o de informa√ß√µes...</td>\n",
       "      <td>Fake News, Misinformation, Credibility, Websit...</td>\n",
       "      <td>[[1] Leandro Ara√∫jo, Luiz Felipe Nery, Isadora...</td>\n",
       "      <td>Introdu√ß√£o Nos √∫ltimos tempos, a sociedade enf...</td>\n",
       "      <td>[Introdu√ß√£o, Nos, √∫ltimos, tempos, ,, a, socie...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [Nos, ADP, ADP], [√∫...</td>\n",
       "      <td>[introdu√ß√£o, em o, √∫ltimo, tempo, o, sociedade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tagging Enriched Bank Transactions Using LLM-G...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24767-1-10-20...</td>\n",
       "      <td>[Daniel de S. Moraes,  Polyana B. da Costa,  P...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>This work presents an unsupervised method for ...</td>\n",
       "      <td>Large Language Models, Natural Language Proces...</td>\n",
       "      <td>[[1] Rohan Anil, Sebastian Borgeaud, Yonghui W...</td>\n",
       "      <td>INTRODUCTION  Many recent studies have focused...</td>\n",
       "      <td>[9/02/2008, √†s, 19:20:05, Molho, de, macarr√£o,...</td>\n",
       "      <td>[[9/02/2008, NUM, NUM], [√†s, ADP, ADP], [19:20...</td>\n",
       "      <td>[9/02/2008, a o, 19:20:05, Molho, de, macarr√£o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fast ISP Mode Decision for the Versatile Video...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24755-1-10-20...</td>\n",
       "      <td>[Larissa Ara√∫jo,  Adson Duarte,  Bruno Zatt,  ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O padr√£o vers√°til de codifica√ß√£o de v√≠deo (VVC...</td>\n",
       "      <td>VVC, Intra Prediction, ISP, Machine Learning</td>\n",
       "      <td>[[1] James Bergstra and Yoshua Bengio. 2012. R...</td>\n",
       "      <td>Introdu√ß√£o Os v√≠deos digitais t√™m sido fundame...</td>\n",
       "      <td>[Introdu√ß√£o, Os, v√≠deos, digitais, t√™m, sido, ...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [Os, DET, DET], [v√≠...</td>\n",
       "      <td>[introdu√ß√£o, o, v√≠deo, digital, ter, ser, fund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OntoDrug: Enhancing Brazilian Health System In...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24764-1-10-20...</td>\n",
       "      <td>[Nelson Miranda,  Matheus Matos Machado,   Dil...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Este artigo apresenta ontodrug, uma ontologia ...</td>\n",
       "      <td>Medication Ontologies, Drug Management, Semant...</td>\n",
       "      <td>[[1] Burhan Ud Din Abbasi, Iram Fatima, Hamid ...</td>\n",
       "      <td>Introdu√ß√£o O gerenciamento de medicamentos √© u...</td>\n",
       "      <td>[Introdu√ß√£o, O, gerenciamento, de, medicamento...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [ger...</td>\n",
       "      <td>[introdu√ß√£o, o, gerenciamento, de, medicamento...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>E-BELA: Enhanced Embedding-Based Entity Linkin...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24750-1-10-20...</td>\n",
       "      <td>[√çtalo M. Pereira  Anderson A. Ferreira]</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Entidade vinculando √© o processo de conectar m...</td>\n",
       "      <td>Natural Language Processing, Entity Linking, L...</td>\n",
       "      <td>[[1] Moonis Ali and Savvas Zannettou. 2024. Fr...</td>\n",
       "      <td>Introdu√ß√£o O volume crescente de dados publica...</td>\n",
       "      <td>[Introdu√ß√£o, O, volume, crescente, de, dados, ...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [vol...</td>\n",
       "      <td>[introdu√ß√£o, o, volume, crescente, de, dado, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Exploring Visual and Multimodal Interaction in...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24754-1-10-20...</td>\n",
       "      <td>[Paulo Victor Borges,  Daniel de S. Moraes,  J...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Este artigo apresenta duas ferramentas inovado...</td>\n",
       "      <td>Authoring, LLMs, NCL, Code Generation, Visual ...</td>\n",
       "      <td>[[1] NBR ABNT. [n. d.]. Digital Terrestrial Te...</td>\n",
       "      <td>Introdu√ß√£o Aplicativos de TV digital (DTV) [26...</td>\n",
       "      <td>[Introdu√ß√£o, Aplicativos, de, TV, digital, (, ...</td>\n",
       "      <td>[[Introdu√ß√£o, PROPN, PROPN], [Aplicativos, PRO...</td>\n",
       "      <td>[Introdu√ß√£o, Aplicativos, de, tv, digital, DTV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Elderly Fall Monitoring in Smart Homes Using W...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24751-1-10-20...</td>\n",
       "      <td>[J√∫lia M. P. Moreira,  Raphael W. Bettio,  And...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O progresso constante da tecnologia, especialm...</td>\n",
       "      <td>fall detection, wearable technologies, acceler...</td>\n",
       "      <td>[[1] Ibukun Awolusi, Eric Marks, and Matthew H...</td>\n",
       "      <td>Introdu√ß√£o ao incentivo √† inova√ß√£o e tecnologi...</td>\n",
       "      <td>[Introdu√ß√£o, ao, incentivo, √†, inova√ß√£o, e, te...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [ao, ADP, ADP], [in...</td>\n",
       "      <td>[introdu√ß√£o, a o, incentivo, a o, inova√ß√£o, e,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A Comprehensive View of the Biases of Toxicity...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24737-1-10-20...</td>\n",
       "      <td>[Guilherme Andrade,  Luiz Nery,  Fabr√≠cio Bene...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A linguagem √© um aspecto din√¢mico de nossa cul...</td>\n",
       "      <td>African American English, AAE, Bias, Toxicity,...</td>\n",
       "      <td>[[1] Abubakar Abid, Maheen Farooqi, and James ...</td>\n",
       "      <td>Introdu√ß√£o nas √∫ltimas d√©cadas, testemunhamos ...</td>\n",
       "      <td>[Introdu√ß√£o, nas, √∫ltimas, d√©cadas, ,, testemu...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [nas, ADP, ADP], [√∫...</td>\n",
       "      <td>[introdu√ß√£o, em o, √∫ltimo, d√©cada, testemunham...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Interoperability Testing Guide for the Interne...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24758-1-10-20...</td>\n",
       "      <td>[Karina da Silva Castelo Branco,  Val√©ria Lell...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A Internet das Coisas (IoT) expandiu a Interne...</td>\n",
       "      <td>Interoperability, Internet of Things, Interope...</td>\n",
       "      <td>[[1] Home Assistant. 2024. Awaken your home. h...</td>\n",
       "      <td>A tecnologia de introdu√ß√£o transformou signifi...</td>\n",
       "      <td>[A, tecnologia, de, introdu√ß√£o, transformou, s...</td>\n",
       "      <td>[[A, DET, DET], [tecnologia, NOUN, NOUN], [de,...</td>\n",
       "      <td>[o, tecnologia, de, introdu√ß√£o, transformar, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>An Ensemble Approach to Facial Deepfake Detect...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24741-1-10-20...</td>\n",
       "      <td>[Yan Martins B. Gurevitz Cunha,  Jos√© Matheus ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Esfor√ßos substanciais foram dedicados ao desen...</td>\n",
       "      <td>deep fake detection, self-supervised, vision t...</td>\n",
       "      <td>[[1] Redha Ali, Russell C. Hardie, Barath Nara...</td>\n",
       "      <td>Introdu√ß√£o Nos √∫ltimos anos, houve um foco cre...</td>\n",
       "      <td>[Introdu√ß√£o, Nos, √∫ltimos, anos, ,, houve, um,...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [Nos, ADP, ADP], [√∫...</td>\n",
       "      <td>[introdu√ß√£o, em o, √∫ltimo, ano, haver, um, foc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Twitter and the 2022 Brazilian Elections Portr...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24769-1-10-20...</td>\n",
       "      <td>[Larissa Malagoli,  Giovana Piorino,  Carlos H...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A influ√™ncia das redes sociais on -line nas a√ß...</td>\n",
       "      <td>2022 Brazilian Elections, Network Modeling, On...</td>\n",
       "      <td>[[1] Peiman Barnaghi, Parsa Ghaffari, and John...</td>\n",
       "      <td>Introdu√ß√£o As plataformas de m√≠dia social on -...</td>\n",
       "      <td>[Introdu√ß√£o, As, plataformas, de, m√≠dia, socia...</td>\n",
       "      <td>[[Introdu√ß√£o, PROPN, PROPN], [As, DET, DET], [...</td>\n",
       "      <td>[introdu√ß√£o, o, plataforma, de, m√≠dia, social,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Automatic Time-aware Recognition of Brazilian ...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24745-1-10-20...</td>\n",
       "      <td>[Lucas de S. Arcanjo,  Lucas F. Coelho,  Silvi...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A linguagem de sinais brasileiros (Libras) √© u...</td>\n",
       "      <td>Computer Vision, Sign Language Recognition, Ge...</td>\n",
       "      <td>[[1] Sunusi Bala Abdullahi and Kosin Chamnongt...</td>\n",
       "      <td>Introdu√ß√£o O reconhecimento brasileiro de ling...</td>\n",
       "      <td>[Introdu√ß√£o, O, reconhecimento, brasileiro, de...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [rec...</td>\n",
       "      <td>[introdu√ß√£o, o, reconhecimento, brasileiro, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Investigating User's Attentional Focus in Comp...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24759-1-10-20...</td>\n",
       "      <td>[Cassiano da Silva Souza,  Milene Selbach Silv...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Manter o foco atencional do usu√°rio se tornou ...</td>\n",
       "      <td>attention monitoring, webcam, data analysis, l...</td>\n",
       "      <td>[[1] and the ACM Digital Library, [2], due to ...</td>\n",
       "      <td>Introdu√ß√£o O avan√ßo das tecnologias de informa...</td>\n",
       "      <td>[Introdu√ß√£o, O, avan√ßo, das, tecnologias, de, ...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [ava...</td>\n",
       "      <td>[introdu√ß√£o, o, avan√ßo, de o, tecnologia, de, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Acceptance and Usability of Complex Medical Sy...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24740-1-10-20...</td>\n",
       "      <td>[F√°bio Ap. C√¢ndido da Silva,  Andr√© Pimenta Fr...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A crescente demanda por testes de imagem torno...</td>\n",
       "      <td>usability issues, radiology systems, qualitati...</td>\n",
       "      <td>[[1] Reza Abbasi, Monireh Sadeqi Jabali, Reza ...</td>\n",
       "      <td>Introdu√ß√£o Os sistemas de informa√ß√£o em sa√∫de ...</td>\n",
       "      <td>[Introdu√ß√£o, Os, sistemas, de, informa√ß√£o, em,...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [Os, DET, DET], [si...</td>\n",
       "      <td>[introdu√ß√£o, o, sistema, de, informa√ß√£o, em, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Through the Eyes of Instagram: Analyzing Image...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24768-1-10-20...</td>\n",
       "      <td>[Jo√£o Francisco Hecksher Olivetti  Philipe de ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A comunica√ß√£o multim√≠dia se tornou uma parte e...</td>\n",
       "      <td>Instagram, alt-text, social media, image class...</td>\n",
       "      <td>[[1] Rakesh Agrawal, Ramakrishnan Srikant, et ...</td>\n",
       "      <td>Introdu√ß√£o A Web se tornou parte integrante da...</td>\n",
       "      <td>[Introdu√ß√£o, A, Web, se, tornou, parte, integr...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [A, DET, DET], [Web...</td>\n",
       "      <td>[introdu√ß√£o, o, Web, se, tornar, parte, integr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Characterization of the Brazilian musical land...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24746-1-10-20...</td>\n",
       "      <td>[Filipe A. S. Moura,  Carlos H. G. Ferreira,  ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Na era digital, servi√ßos de streaming como o S...</td>\n",
       "      <td>Music Preferences, Music Genre Networks, Regio...</td>\n",
       "      <td>[[1] Moonis Ali and Savvas Zannettou. 2024. Fr...</td>\n",
       "      <td>Introdu√ß√£o Na era digital, os servi√ßos de stre...</td>\n",
       "      <td>[Introdu√ß√£o, Na, era, digital, ,, os, servi√ßos...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [Na, ADP, ADP], [er...</td>\n",
       "      <td>[introdu√ß√£o, em o, ser, digital, o, servi√ßo, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>A Machine-Learning-Driven Fast Video-based Poi...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24739-1-10-20...</td>\n",
       "      <td>[Gustavo Rehbein,  Eduardo Costa,  Guilherme C...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Nos √∫ltimos anos, o conte√∫do da nuvem de ponto...</td>\n",
       "      <td>point clouds, machine learning, V-PCC, complex...</td>\n",
       "      <td>[[1] Gisle Bjontegaard. 2001. Calculation of a...</td>\n",
       "      <td>As nuvens de ponto de introdu√ß√£o podem ser usa...</td>\n",
       "      <td>[As, nuvens, de, ponto, de, introdu√ß√£o, podem,...</td>\n",
       "      <td>[[As, DET, DET], [nuvens, NOUN, NOUN], [de, AD...</td>\n",
       "      <td>[o, nuvem, de, ponto, de, introdu√ß√£o, poder, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Constructing a KBQA Framework: Design and Impl...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24747-1-10-20...</td>\n",
       "      <td>[R√¥mulo Chrispim de Mello,  Jor√£o Gomes Jr.,  ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O crescimento exponencial dos dados na Interne...</td>\n",
       "      <td>KBQA, Complex Questions, Entity Recognition, P...</td>\n",
       "      <td>[[1] Shuang Chen, Qian Liu, Zhiwei Yu, Chin-Ye...</td>\n",
       "      <td>Introdu√ß√£o O grande volume de dados dispon√≠vei...</td>\n",
       "      <td>[Introdu√ß√£o, O, grande, volume, de, dados, dis...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [gra...</td>\n",
       "      <td>[introdu√ß√£o, o, grande, volume, de, dado, disp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>A Domain-Specific Language for Multimedia Serv...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24738-1-10-20...</td>\n",
       "      <td>[Franklin Jordan Ventura Quico,  Anselmo L. E....</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A virtualiza√ß√£o √© uma tecnologia amplamente us...</td>\n",
       "      <td>IoMT, IoT, VNF, SFC, DSL, L-PRISM</td>\n",
       "      <td>[[1] ETSI GS NFV-IFA 011. 2023. Network Functi...</td>\n",
       "      <td>Introdu√ß√£o A virtualiza√ß√£o √© um conceito que a...</td>\n",
       "      <td>[Introdu√ß√£o, A, virtualiza√ß√£o, √©, um, conceito...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [A, DET, DET], [vir...</td>\n",
       "      <td>[introdu√ß√£o, o, virtualiza√ß√£o, ser, um, concei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Why Ignore Content? A Guideline for Intrinsic ...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24776-1-10-20...</td>\n",
       "      <td>[Pedro R. Pires,  Bruno B. Rizzi,   Tiago A. A...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Com o crescimento constante das informa√ß√µes di...</td>\n",
       "      <td>embeddings, intrinsic evaluation, qualitative ...</td>\n",
       "      <td>[[1] Gediminas Adomavicius and Alexander Tuzhi...</td>\n",
       "      <td>Introdu√ß√£o Os sistemas de recomenda√ß√£o s√£o fer...</td>\n",
       "      <td>[Introdu√ß√£o, Os, sistemas, de, recomenda√ß√£o, s...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [Os, DET, DET], [si...</td>\n",
       "      <td>[introdu√ß√£o, o, sistema, de, recomenda√ß√£o, ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Impacto da Pandemia na Discuss√£o sobre Sa√∫de M...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24757-1-10...</td>\n",
       "      <td>[Pedro Bento,  Arthur Buzelin,  Yan Aquino,  I...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>The period of social isolation due to COVID-19...</td>\n",
       "      <td>Redes sociais, COVID-19, pandemia, sa√∫de menta...</td>\n",
       "      <td>[[1] Elia Abi-Jaoude, Karline Treurnicht Naylo...</td>\n",
       "      <td>INTRODU√á√ÉO Nos √∫ltimos anos, o panorama da com...</td>\n",
       "      <td>[INTRODU√á√ÉO, Nos, √∫ltimos, anos, ,, o, panoram...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [Nos, ADP, ADP], ...</td>\n",
       "      <td>[INTRODU√á√ÉO, em o, √∫ltimo, ano, o, panorama, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Um Framework para An√°lise Bidimensional de Dis...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24771-2-10...</td>\n",
       "      <td>[Geovana S. Oliveira,  Ot√°vio Ven√¢ncio,  Vin√≠c...</td>\n",
       "      <td>25-09-2024</td>\n",
       "      <td>As plataformas de m√≠dia social revolucionaram ...</td>\n",
       "      <td>M√≠dias Sociais, Dissemina√ß√£o de Informa√ß√£o, Mo...</td>\n",
       "      <td>[[1] Marcelo MR Araujo, Carlos HG Ferreira, Ju...</td>\n",
       "      <td>INTRODU√á√ÉO As plataformas de m√≠dias sociais re...</td>\n",
       "      <td>[INTRODU√á√ÉO, As, plataformas, de, m√≠dias, soci...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [As, DET, DET], [...</td>\n",
       "      <td>[INTRODU√á√ÉO, o, plataforma, de, m√≠dia, social,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Middleware para Aplica√ß√µes Distribu√≠das de V√≠d...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24761-1-10...</td>\n",
       "      <td>[Otac√≠lio de A. Ramos Neto,  Rafael C. Chaves,...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Dentro do escopo da ind√∫stria 4.0, a vis√£o com...</td>\n",
       "      <td>Middleware, Distribui√ß√£o de V√≠deo, Edge Comput...</td>\n",
       "      <td>[[1] Ganesh Ananthanarayanan, Paramvir Bahl, P...</td>\n",
       "      <td>INTRODU√á√ÉO Nos √∫ltimos anos, a import√¢ncia de ...</td>\n",
       "      <td>[INTRODU√á√ÉO, Nos, √∫ltimos, anos, ,, a, import√¢...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [Nos, ADP, ADP], ...</td>\n",
       "      <td>[INTRODU√á√ÉO, em o, √∫ltimo, ano, o, import√¢ncia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>O Impacto de Estrat√©gias de Embeddings de Graf...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24763-2-10...</td>\n",
       "      <td>[Andr√© Levi Zanon,  Leonardo Rocha,   Marcelo ...</td>\n",
       "      <td>04-10-2024</td>\n",
       "      <td>Explica√ß√µes em sistemas de recomenda√ß√£o s√£o es...</td>\n",
       "      <td>Sistemas de Recomenda√ß√£o, Explica√ß√µes, Embeddi...</td>\n",
       "      <td>[[1] Ali, M., Berrendorf, M., Hoyt, C.T., Verm...</td>\n",
       "      <td>INTRODU√á√ÉO Sistemas de Recomenda√ß√£o (SsR) s√£o ...</td>\n",
       "      <td>[INTRODU√á√ÉO, Sistemas, de, Recomenda√ß√£o, (, Ss...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [Sistemas, PROPN,...</td>\n",
       "      <td>[INTRODU√á√ÉO, Sistemas, de, Recomenda√ß√£o, SsR, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Cuidado Ub√≠quo de Pacientes com Doen√ßas Cr√¥nic...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24749-1-10...</td>\n",
       "      <td>[Lucas Pfeiffer Salom√£o Dias,  L.P.S. Dias,  J...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>As doen√ßas cr√¥nicas est√£o entre as 7 das 10 pr...</td>\n",
       "      <td>Classifica√ß√£o de Comportamento; Doen√ßas Cr√¥nic...</td>\n",
       "      <td>[[1] Rodrigo Simon Bavaresco and Jorge Luis Vi...</td>\n",
       "      <td>INTRODU√á√ÉO A Organiza√ß√£o Mundial de Sa√∫de (OMS...</td>\n",
       "      <td>[INTRODU√á√ÉO, A, Organiza√ß√£o, Mundial, de, Sa√∫d...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [A, DET, DET], [O...</td>\n",
       "      <td>[INTRODU√á√ÉO, o, Organiza√ß√£o, Mundial, de, Sa√∫d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Jogos Digitais S√©rios usados para o Exerc√≠cio ...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24760-1-10...</td>\n",
       "      <td>[Katherin Felipa Carhuaz Malpartida  Kamila Ri...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O pensamento computacional (CT) √© um processo ...</td>\n",
       "      <td>Pensamento Computacional, Jogos Digitais S√©rio...</td>\n",
       "      <td>[[1] AAIDD. 2021. Defining Criteria for Intell...</td>\n",
       "      <td>INTRODU√á√ÉO O Pensamento Computacional (PC) √© d...</td>\n",
       "      <td>[INTRODU√á√ÉO, O, Pensamento, Computacional, (, ...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [O, DET, DET], [P...</td>\n",
       "      <td>[INTRODU√á√ÉO, o, Pensamento, Computacional, PC,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Crian√ßas e Propagandas no TikTok: identificand...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24748-1-10...</td>\n",
       "      <td>[Ra√≠ssa Gon√ßalves Lopes Carvalho  Humberto Tor...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Este estudo aborda a identifica√ß√£o da publicid...</td>\n",
       "      <td>Publicidade Infantil, Crian√ßa, TikTok, Rede So...</td>\n",
       "      <td>[[1] Instituto Alana. 2022. Notifica√ß√£o enviad...</td>\n",
       "      <td>INTRODUCTION No cen√°rio contempor√¢neo, as rede...</td>\n",
       "      <td>[INTRODUCTION, No, cen√°rio, contempor√¢neo, ,, ...</td>\n",
       "      <td>[[INTRODUCTION, PROPN, PROPN], [No, ADP, ADP],...</td>\n",
       "      <td>[INTRODUCTION, em o, cen√°rio, contempor√¢neo, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Quando as Avalia√ß√µes Viram Bombas: Explorando ...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24765-1-10...</td>\n",
       "      <td>[Marcus Vinicius Guerra Ribeiro,  Clara Andrad...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Este estudo investiga o fen√¥meno do \" * revis√£...</td>\n",
       "      <td>jogos, videogames, Metacritic, review bombing,...</td>\n",
       "      <td>[[1] 2022. O que √© ‚Äôwoke‚Äô e por que o termo ge...</td>\n",
       "      <td>INTRODU√á√ÉO Atualmente, plataformas digitais de...</td>\n",
       "      <td>[INTRODU√á√ÉO, Atualmente, ,, plataformas, digit...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [Atualmente, ADV,...</td>\n",
       "      <td>[INTRODU√á√ÉO, Atualmente, plataforma, digital, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Estrat√©gias de Undersampling para Redu√ß√£o de V...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24753-2-10...</td>\n",
       "      <td>[Guilherme Fonseca,  Gabriel Prenassi,  Washin...</td>\n",
       "      <td>04-10-2024</td>\n",
       "      <td>Classifica√ß√£o de texto autom√°tica (ATC) em con...</td>\n",
       "      <td>Classifica√ß√£o de Texto, Transformers, Undersam...</td>\n",
       "      <td>[[1] Lasse F Wolff Anthony, Benjamin Kanding, ...</td>\n",
       "      <td>INTRODU√á√ÉO Classifica√ß√£o Autom√°tica de Texto (...</td>\n",
       "      <td>[INTRODU√á√ÉO, Classifica√ß√£o, Autom√°tica, de, Te...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [Classifica√ß√£o, P...</td>\n",
       "      <td>[INTRODU√á√ÉO, Classifica√ß√£o, Autom√°tica, de, Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Arquitetura Multicamadas para Coleta e An√°lise...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24744-1-10...</td>\n",
       "      <td>[Juan Felipe Souza Oliveira,  Paulo Cesar Salg...</td>\n",
       "      <td>19-09-2024</td>\n",
       "      <td>Uma arquitetura multicamada foi desenvolvida p...</td>\n",
       "      <td>internet das coisas, monitoramento remoto de s...</td>\n",
       "      <td>[[1] Zahra Ahmadi, Mostafa Haghi Kashani, Moha...</td>\n",
       "      <td>INTRODU√á√ÉO O crescimento exponencial dos siste...</td>\n",
       "      <td>[INTRODU√á√ÉO, O, crescimento, exponencial, dos,...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [O, DET, DET], [c...</td>\n",
       "      <td>[INTRODU√á√ÉO, o, crescimento, exponencial, de o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Um framework de rastreamento corporal para rea...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24770-1-10...</td>\n",
       "      <td>[Elvis Ribeiro,  Alexandre Brand√£o,  Marcelo G...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Aplica√ß√µes e jogos multim√≠dia desempenham um p...</td>\n",
       "      <td>Realidade Virtual, Jogos S√©rios, Multim√≠dia, R...</td>\n",
       "      <td>[[1] Alberto Luiz Aramaki, Rosana Ferreira Sam...</td>\n",
       "      <td>Um framework de rastreamento corporal para rea...</td>\n",
       "      <td>[Um, framework, de, rastreamento, corporal, pa...</td>\n",
       "      <td>[[Um, DET, DET], [framework, PROPN, PROPN], [d...</td>\n",
       "      <td>[um, framework, de, rastreamento, corporal, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>√önicos, mas n√£o incompar√°veis: abordagens para...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24775-1-10...</td>\n",
       "      <td>[Guilherme O. Aguiar,  Juan P. D. Esteves,  Cl...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Entender o comportamento emocional humano √© um...</td>\n",
       "      <td>Computa√ß√£o Afetiva, Respostas Emocionais, Mode...</td>\n",
       "      <td>[[1] Bert Bakker, Gijs Schumacher, Kevin Arcen...</td>\n",
       "      <td>INTRODU√á√ÉO Num contexto em que as emo√ß√µes huma...</td>\n",
       "      <td>[INTRODU√á√ÉO, Num, contexto, em, que, as, emo√ß√µ...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [Num, ADP, ADP], ...</td>\n",
       "      <td>[INTRODU√á√ÉO, em um, contexto, em, que, o, emo√ß...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Uma Proposta de Framework para Sistemas de Con...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24774-1-10...</td>\n",
       "      <td>[Marcelo Rocha,  Jesus Favela,   D√©bora C. Muc...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>H√° uma lacuna em plataformas rob√≥ticas de c√≥di...</td>\n",
       "      <td>Rob√¥s sociais, Framework para controle de rob√¥...</td>\n",
       "      <td>[[1] Michel Albonico, Milica √êorƒëeviƒá, Engel H...</td>\n",
       "      <td>INTRODU√á√ÉO A ado√ß√£o de novas tecnologias na ed...</td>\n",
       "      <td>[INTRODU√á√ÉO, A, ado√ß√£o, de, novas, tecnologias...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [A, DET, DET], [a...</td>\n",
       "      <td>[INTRODU√á√ÉO, o, ado√ß√£o, de, novo, tecnologia, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Uma Abordagem em Etapa de Processamento para R...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24772-1-10...</td>\n",
       "      <td>[Rodrigo Ferrari de Souza  Marcelo Garcia Manz...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Os sistemas de recomenda√ß√£o s√£o projetados par...</td>\n",
       "      <td>Sistemas de Recomenda√ß√£o, Vi√©s de Popularidade...</td>\n",
       "      <td>[[1] Himan Abdollahpouri, Masoud Mansoury, Rob...</td>\n",
       "      <td>INTRODU√á√ÉO Os sistemas de recomenda√ß√£o s√£o pro...</td>\n",
       "      <td>[INTRODU√á√ÉO, Os, sistemas, de, recomenda√ß√£o, s...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [Os, DET, DET], [...</td>\n",
       "      <td>[INTRODU√á√ÉO, o, sistema, de, recomenda√ß√£o, ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>An√°lise de sentimentos de conte√∫do compartilha...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24743-1-10...</td>\n",
       "      <td>[Giovana Piorino,  Vitor Moreira,  Luiz Henriq...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O uso crescente das m√≠dias sociais e seu impac...</td>\n",
       "      <td>An√°lise de Sentimentos, Comunidades do Reddit,...</td>\n",
       "      <td>[[1] Rafael J. A. Almeida. 2018. LeIA - L√©xico...</td>\n",
       "      <td>INTRODU√á√ÉO As redes sociais t√™m rompido barrei...</td>\n",
       "      <td>[INTRODU√á√ÉO, As, redes, sociais, t√™m, rompido,...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [As, DET, DET], [...</td>\n",
       "      <td>[INTRODU√á√ÉO, o, rede, social, ter, rompir, bar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Uma Investiga√ß√£o sobre T√©cnicas de Data Augmen...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24773-1-10...</td>\n",
       "      <td>[Marcos Andr√© Bezerra da Silva,  Manuella Asch...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A tradu√ß√£o autom√°tica de portugu√™s para Libras...</td>\n",
       "      <td>Tradu√ß√£o Autom√°tica Neural, Aumento de Dados, ...</td>\n",
       "      <td>[[1] T. M. U. Ara√∫jo. 2012. Uma solu√ß√£o para g...</td>\n",
       "      <td>INTRODU√á√ÉO A evolu√ß√£o da tecnologia tem desemp...</td>\n",
       "      <td>[INTRODU√á√ÉO, A, evolu√ß√£o, da, tecnologia, tem,...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [A, DET, DET], [e...</td>\n",
       "      <td>[INTRODU√á√ÉO, o, evolu√ß√£o, de o, tecnologia, te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>An√°lise da Percep√ß√£o do Uso de Cigarros Eletr√¥...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24742-1-10...</td>\n",
       "      <td>[Aline Dias,  Richardy R. Tanure,  Jussara M. ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A ascens√£o de plataformas de v√≠deo como o YouT...</td>\n",
       "      <td>Cigarros Eletr√¥nicos, Aprendizado de M√°quina, ...</td>\n",
       "      <td>[[1] Shishir Adhikari, Akshay Uppal, Robin Mer...</td>\n",
       "      <td>INTRODU√á√ÉO A ascens√£o de plataformas de v√≠deo,...</td>\n",
       "      <td>[INTRODU√á√ÉO, A, ascens√£o, de, plataformas, de,...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [A, DET, DET], [a...</td>\n",
       "      <td>[INTRODU√á√ÉO, o, ascens√£o, de, plataforma, de, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 263
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:53:28.567682Z",
     "start_time": "2025-05-12T23:53:28.394603Z"
    }
   },
   "cell_type": "code",
   "source": "corpus.to_json('corpus.json', orient='records', indent=2)",
   "id": "2a24f3ecc848f8e5",
   "outputs": [],
   "execution_count": 265
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:55:29.164881Z",
     "start_time": "2025-05-12T23:55:29.009482Z"
    }
   },
   "cell_type": "code",
   "source": "corpus = pd.read_json('corpus.json')",
   "id": "e77b5da41583b2ab",
   "outputs": [],
   "execution_count": 266
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:55:33.474202Z",
     "start_time": "2025-05-12T23:55:33.045667Z"
    }
   },
   "cell_type": "code",
   "source": "corpus",
   "id": "737e82d8ae43a725",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               titulo informacoes_url  \\\n",
       "0   Recognition of Emotions through Facial Geometr...                   \n",
       "1   Enhancing widget recognition for automated And...                   \n",
       "2   Multi-Domain Spatio-Temporal Deformable Fusion...                   \n",
       "3              Finding Fake News Websites in the Wild                   \n",
       "4   Tagging Enriched Bank Transactions Using LLM-G...                   \n",
       "5   Fast ISP Mode Decision for the Versatile Video...                   \n",
       "6   OntoDrug: Enhancing Brazilian Health System In...                   \n",
       "7   E-BELA: Enhanced Embedding-Based Entity Linkin...                   \n",
       "8   Exploring Visual and Multimodal Interaction in...                   \n",
       "9   Elderly Fall Monitoring in Smart Homes Using W...                   \n",
       "10  A Comprehensive View of the Biases of Toxicity...                   \n",
       "11  Interoperability Testing Guide for the Interne...                   \n",
       "12  An Ensemble Approach to Facial Deepfake Detect...                   \n",
       "13  Twitter and the 2022 Brazilian Elections Portr...                   \n",
       "14  Automatic Time-aware Recognition of Brazilian ...                   \n",
       "15  Investigating User's Attentional Focus in Comp...                   \n",
       "16  Acceptance and Usability of Complex Medical Sy...                   \n",
       "17  Through the Eyes of Instagram: Analyzing Image...                   \n",
       "18  Characterization of the Brazilian musical land...                   \n",
       "19  A Machine-Learning-Driven Fast Video-based Poi...                   \n",
       "20  Constructing a KBQA Framework: Design and Impl...                   \n",
       "21  A Domain-Specific Language for Multimedia Serv...                   \n",
       "22  Why Ignore Content? A Guideline for Intrinsic ...                   \n",
       "23  Impacto da Pandemia na Discuss√£o sobre Sa√∫de M...                   \n",
       "24  Um Framework para An√°lise Bidimensional de Dis...                   \n",
       "25  Middleware para Aplica√ß√µes Distribu√≠das de V√≠d...                   \n",
       "26  O Impacto de Estrat√©gias de Embeddings de Graf...                   \n",
       "27  Cuidado Ub√≠quo de Pacientes com Doen√ßas Cr√¥nic...                   \n",
       "28  Jogos Digitais S√©rios usados para o Exerc√≠cio ...                   \n",
       "29  Crian√ßas e Propagandas no TikTok: identificand...                   \n",
       "30  Quando as Avalia√ß√µes Viram Bombas: Explorando ...                   \n",
       "31  Estrat√©gias de Undersampling para Redu√ß√£o de V...                   \n",
       "32  Arquitetura Multicamadas para Coleta e An√°lise...                   \n",
       "33  Um framework de rastreamento corporal para rea...                   \n",
       "34  √önicos, mas n√£o incompar√°veis: abordagens para...                   \n",
       "35  Uma Proposta de Framework para Sistemas de Con...                   \n",
       "36  Uma Abordagem em Etapa de Processamento para R...                   \n",
       "37  An√°lise de sentimentos de conte√∫do compartilha...                   \n",
       "38  Uma Investiga√ß√£o sobre T√©cnicas de Data Augmen...                   \n",
       "39  An√°lise da Percep√ß√£o do Uso de Cigarros Eletr√¥...                   \n",
       "\n",
       "        idioma                                        storage_key  \\\n",
       "0      english  ../articles/original/english/985-24766-1-10-20...   \n",
       "1      english  ../articles/original/english/985-24752-1-10-20...   \n",
       "2      english  ../articles/original/english/985-24762-1-10-20...   \n",
       "3      english  ../articles/original/english/985-24756-1-10-20...   \n",
       "4      english  ../articles/original/english/985-24767-1-10-20...   \n",
       "5      english  ../articles/original/english/985-24755-1-10-20...   \n",
       "6      english  ../articles/original/english/985-24764-1-10-20...   \n",
       "7      english  ../articles/original/english/985-24750-1-10-20...   \n",
       "8      english  ../articles/original/english/985-24754-1-10-20...   \n",
       "9      english  ../articles/original/english/985-24751-1-10-20...   \n",
       "10     english  ../articles/original/english/985-24737-1-10-20...   \n",
       "11     english  ../articles/original/english/985-24758-1-10-20...   \n",
       "12     english  ../articles/original/english/985-24741-1-10-20...   \n",
       "13     english  ../articles/original/english/985-24769-1-10-20...   \n",
       "14     english  ../articles/original/english/985-24745-1-10-20...   \n",
       "15     english  ../articles/original/english/985-24759-1-10-20...   \n",
       "16     english  ../articles/original/english/985-24740-1-10-20...   \n",
       "17     english  ../articles/original/english/985-24768-1-10-20...   \n",
       "18     english  ../articles/original/english/985-24746-1-10-20...   \n",
       "19     english  ../articles/original/english/985-24739-1-10-20...   \n",
       "20     english  ../articles/original/english/985-24747-1-10-20...   \n",
       "21     english  ../articles/original/english/985-24738-1-10-20...   \n",
       "22     english  ../articles/original/english/985-24776-1-10-20...   \n",
       "23  portuguese  ../articles/original/portuguese/985-24757-1-10...   \n",
       "24  portuguese  ../articles/original/portuguese/985-24771-2-10...   \n",
       "25  portuguese  ../articles/original/portuguese/985-24761-1-10...   \n",
       "26  portuguese  ../articles/original/portuguese/985-24763-2-10...   \n",
       "27  portuguese  ../articles/original/portuguese/985-24749-1-10...   \n",
       "28  portuguese  ../articles/original/portuguese/985-24760-1-10...   \n",
       "29  portuguese  ../articles/original/portuguese/985-24748-1-10...   \n",
       "30  portuguese  ../articles/original/portuguese/985-24765-1-10...   \n",
       "31  portuguese  ../articles/original/portuguese/985-24753-2-10...   \n",
       "32  portuguese  ../articles/original/portuguese/985-24744-1-10...   \n",
       "33  portuguese  ../articles/original/portuguese/985-24770-1-10...   \n",
       "34  portuguese  ../articles/original/portuguese/985-24775-1-10...   \n",
       "35  portuguese  ../articles/original/portuguese/985-24774-1-10...   \n",
       "36  portuguese  ../articles/original/portuguese/985-24772-1-10...   \n",
       "37  portuguese  ../articles/original/portuguese/985-24743-1-10...   \n",
       "38  portuguese  ../articles/original/portuguese/985-24773-1-10...   \n",
       "39  portuguese  ../articles/original/portuguese/985-24742-1-10...   \n",
       "\n",
       "                                              autores data_publicacao  \\\n",
       "0   [Alessandra Alaniz Macedo,  Leandro Persona,  ...      11-09-2024   \n",
       "1   [Yadini P√©rez L√≥pez,  La√≠s Dib Albuquerque,  G...      11-09-2024   \n",
       "2   [Garibaldi da Silveira J√∫nior, Gilberto Kreisl...      11-09-2024   \n",
       "3   [Leandro Araujo,  Jo√£o M. M. Couto,  Luiz Feli...      11-09-2024   \n",
       "4   [Daniel de S. Moraes,  Polyana B. da Costa,  P...      11-09-2024   \n",
       "5   [Larissa Ara√∫jo,  Adson Duarte,  Bruno Zatt,  ...      11-09-2024   \n",
       "6   [Nelson Miranda,  Matheus Matos Machado,   Dil...      11-09-2024   \n",
       "7            [√çtalo M. Pereira  Anderson A. Ferreira]      11-09-2024   \n",
       "8   [Paulo Victor Borges,  Daniel de S. Moraes,  J...      11-09-2024   \n",
       "9   [J√∫lia M. P. Moreira,  Raphael W. Bettio,  And...      11-09-2024   \n",
       "10  [Guilherme Andrade,  Luiz Nery,  Fabr√≠cio Bene...      11-09-2024   \n",
       "11  [Karina da Silva Castelo Branco,  Val√©ria Lell...      11-09-2024   \n",
       "12  [Yan Martins B. Gurevitz Cunha,  Jos√© Matheus ...      11-09-2024   \n",
       "13  [Larissa Malagoli,  Giovana Piorino,  Carlos H...      11-09-2024   \n",
       "14  [Lucas de S. Arcanjo,  Lucas F. Coelho,  Silvi...      11-09-2024   \n",
       "15  [Cassiano da Silva Souza,  Milene Selbach Silv...      11-09-2024   \n",
       "16  [F√°bio Ap. C√¢ndido da Silva,  Andr√© Pimenta Fr...      11-09-2024   \n",
       "17  [Jo√£o Francisco Hecksher Olivetti  Philipe de ...      11-09-2024   \n",
       "18  [Filipe A. S. Moura,  Carlos H. G. Ferreira,  ...      11-09-2024   \n",
       "19  [Gustavo Rehbein,  Eduardo Costa,  Guilherme C...      11-09-2024   \n",
       "20  [R√¥mulo Chrispim de Mello,  Jor√£o Gomes Jr.,  ...      11-09-2024   \n",
       "21  [Franklin Jordan Ventura Quico,  Anselmo L. E....      11-09-2024   \n",
       "22  [Pedro R. Pires,  Bruno B. Rizzi,   Tiago A. A...      11-09-2024   \n",
       "23  [Pedro Bento,  Arthur Buzelin,  Yan Aquino,  I...      11-09-2024   \n",
       "24  [Geovana S. Oliveira,  Ot√°vio Ven√¢ncio,  Vin√≠c...      25-09-2024   \n",
       "25  [Otac√≠lio de A. Ramos Neto,  Rafael C. Chaves,...      11-09-2024   \n",
       "26  [Andr√© Levi Zanon,  Leonardo Rocha,   Marcelo ...      04-10-2024   \n",
       "27  [Lucas Pfeiffer Salom√£o Dias,  L.P.S. Dias,  J...      11-09-2024   \n",
       "28  [Katherin Felipa Carhuaz Malpartida  Kamila Ri...      11-09-2024   \n",
       "29  [Ra√≠ssa Gon√ßalves Lopes Carvalho  Humberto Tor...      11-09-2024   \n",
       "30  [Marcus Vinicius Guerra Ribeiro,  Clara Andrad...      11-09-2024   \n",
       "31  [Guilherme Fonseca,  Gabriel Prenassi,  Washin...      04-10-2024   \n",
       "32  [Juan Felipe Souza Oliveira,  Paulo Cesar Salg...      19-09-2024   \n",
       "33  [Elvis Ribeiro,  Alexandre Brand√£o,  Marcelo G...      11-09-2024   \n",
       "34  [Guilherme O. Aguiar,  Juan P. D. Esteves,  Cl...      11-09-2024   \n",
       "35  [Marcelo Rocha,  Jesus Favela,   D√©bora C. Muc...      11-09-2024   \n",
       "36  [Rodrigo Ferrari de Souza  Marcelo Garcia Manz...      11-09-2024   \n",
       "37  [Giovana Piorino,  Vitor Moreira,  Luiz Henriq...      11-09-2024   \n",
       "38  [Marcos Andr√© Bezerra da Silva,  Manuella Asch...      11-09-2024   \n",
       "39  [Aline Dias,  Richardy R. Tanure,  Jussara M. ...      11-09-2024   \n",
       "\n",
       "                                               resumo  \\\n",
       "0   O reconhecimento de emo√ß√£o tem um significado ...   \n",
       "1   O reconhecimento de widgets √© crucial para tes...   \n",
       "2   A compacta√ß√£o de v√≠deo com perda de im√≥vel int...   \n",
       "3   A batalha contra a dissemina√ß√£o de informa√ß√µes...   \n",
       "4   This work presents an unsupervised method for ...   \n",
       "5   O padr√£o vers√°til de codifica√ß√£o de v√≠deo (VVC...   \n",
       "6   Este artigo apresenta ontodrug, uma ontologia ...   \n",
       "7   Entidade vinculando √© o processo de conectar m...   \n",
       "8   Este artigo apresenta duas ferramentas inovado...   \n",
       "9   O progresso constante da tecnologia, especialm...   \n",
       "10  A linguagem √© um aspecto din√¢mico de nossa cul...   \n",
       "11  A Internet das Coisas (IoT) expandiu a Interne...   \n",
       "12  Esfor√ßos substanciais foram dedicados ao desen...   \n",
       "13  A influ√™ncia das redes sociais on -line nas a√ß...   \n",
       "14  A linguagem de sinais brasileiros (Libras) √© u...   \n",
       "15  Manter o foco atencional do usu√°rio se tornou ...   \n",
       "16  A crescente demanda por testes de imagem torno...   \n",
       "17  A comunica√ß√£o multim√≠dia se tornou uma parte e...   \n",
       "18  Na era digital, servi√ßos de streaming como o S...   \n",
       "19  Nos √∫ltimos anos, o conte√∫do da nuvem de ponto...   \n",
       "20  O crescimento exponencial dos dados na Interne...   \n",
       "21  A virtualiza√ß√£o √© uma tecnologia amplamente us...   \n",
       "22  Com o crescimento constante das informa√ß√µes di...   \n",
       "23  The period of social isolation due to COVID-19...   \n",
       "24  As plataformas de m√≠dia social revolucionaram ...   \n",
       "25  Dentro do escopo da ind√∫stria 4.0, a vis√£o com...   \n",
       "26  Explica√ß√µes em sistemas de recomenda√ß√£o s√£o es...   \n",
       "27  As doen√ßas cr√¥nicas est√£o entre as 7 das 10 pr...   \n",
       "28  O pensamento computacional (CT) √© um processo ...   \n",
       "29  Este estudo aborda a identifica√ß√£o da publicid...   \n",
       "30  Este estudo investiga o fen√¥meno do \" * revis√£...   \n",
       "31  Classifica√ß√£o de texto autom√°tica (ATC) em con...   \n",
       "32  Uma arquitetura multicamada foi desenvolvida p...   \n",
       "33  Aplica√ß√µes e jogos multim√≠dia desempenham um p...   \n",
       "34  Entender o comportamento emocional humano √© um...   \n",
       "35  H√° uma lacuna em plataformas rob√≥ticas de c√≥di...   \n",
       "36  Os sistemas de recomenda√ß√£o s√£o projetados par...   \n",
       "37  O uso crescente das m√≠dias sociais e seu impac...   \n",
       "38  A tradu√ß√£o autom√°tica de portugu√™s para Libras...   \n",
       "39  A ascens√£o de plataformas de v√≠deo como o YouT...   \n",
       "\n",
       "                                             keywords  \\\n",
       "0   Multimedia Processing, Affective Computing, Ma...   \n",
       "1   Automated Android Testing, Test Portability, O...   \n",
       "2   Redes neurais profundas, Melhoria de qualidade...   \n",
       "3   Fake News, Misinformation, Credibility, Websit...   \n",
       "4   Large Language Models, Natural Language Proces...   \n",
       "5        VVC, Intra Prediction, ISP, Machine Learning   \n",
       "6   Medication Ontologies, Drug Management, Semant...   \n",
       "7   Natural Language Processing, Entity Linking, L...   \n",
       "8   Authoring, LLMs, NCL, Code Generation, Visual ...   \n",
       "9   fall detection, wearable technologies, acceler...   \n",
       "10  African American English, AAE, Bias, Toxicity,...   \n",
       "11  Interoperability, Internet of Things, Interope...   \n",
       "12  deep fake detection, self-supervised, vision t...   \n",
       "13  2022 Brazilian Elections, Network Modeling, On...   \n",
       "14  Computer Vision, Sign Language Recognition, Ge...   \n",
       "15  attention monitoring, webcam, data analysis, l...   \n",
       "16  usability issues, radiology systems, qualitati...   \n",
       "17  Instagram, alt-text, social media, image class...   \n",
       "18  Music Preferences, Music Genre Networks, Regio...   \n",
       "19  point clouds, machine learning, V-PCC, complex...   \n",
       "20  KBQA, Complex Questions, Entity Recognition, P...   \n",
       "21                  IoMT, IoT, VNF, SFC, DSL, L-PRISM   \n",
       "22  embeddings, intrinsic evaluation, qualitative ...   \n",
       "23  Redes sociais, COVID-19, pandemia, sa√∫de menta...   \n",
       "24  M√≠dias Sociais, Dissemina√ß√£o de Informa√ß√£o, Mo...   \n",
       "25  Middleware, Distribui√ß√£o de V√≠deo, Edge Comput...   \n",
       "26  Sistemas de Recomenda√ß√£o, Explica√ß√µes, Embeddi...   \n",
       "27  Classifica√ß√£o de Comportamento; Doen√ßas Cr√¥nic...   \n",
       "28  Pensamento Computacional, Jogos Digitais S√©rio...   \n",
       "29  Publicidade Infantil, Crian√ßa, TikTok, Rede So...   \n",
       "30  jogos, videogames, Metacritic, review bombing,...   \n",
       "31  Classifica√ß√£o de Texto, Transformers, Undersam...   \n",
       "32  internet das coisas, monitoramento remoto de s...   \n",
       "33  Realidade Virtual, Jogos S√©rios, Multim√≠dia, R...   \n",
       "34  Computa√ß√£o Afetiva, Respostas Emocionais, Mode...   \n",
       "35  Rob√¥s sociais, Framework para controle de rob√¥...   \n",
       "36  Sistemas de Recomenda√ß√£o, Vi√©s de Popularidade...   \n",
       "37  An√°lise de Sentimentos, Comunidades do Reddit,...   \n",
       "38  Tradu√ß√£o Autom√°tica Neural, Aumento de Dados, ...   \n",
       "39  Cigarros Eletr√¥nicos, Aprendizado de M√°quina, ...   \n",
       "\n",
       "                                          referencias  \\\n",
       "0   [[1] Mauricio Alvarez, David Luengo, and Neil ...   \n",
       "1   [[1] A. A. Abdelhamid, S. Alotaibi, and A. Mou...   \n",
       "2   [[1] Aayushi Agarwal, Akshay Agarwal, Sayan Si...   \n",
       "3   [[1] Leandro Ara√∫jo, Luiz Felipe Nery, Isadora...   \n",
       "4   [[1] Rohan Anil, Sebastian Borgeaud, Yonghui W...   \n",
       "5   [[1] James Bergstra and Yoshua Bengio. 2012. R...   \n",
       "6   [[1] Burhan Ud Din Abbasi, Iram Fatima, Hamid ...   \n",
       "7   [[1] Moonis Ali and Savvas Zannettou. 2024. Fr...   \n",
       "8   [[1] NBR ABNT. [n. d.]. Digital Terrestrial Te...   \n",
       "9   [[1] Ibukun Awolusi, Eric Marks, and Matthew H...   \n",
       "10  [[1] Abubakar Abid, Maheen Farooqi, and James ...   \n",
       "11  [[1] Home Assistant. 2024. Awaken your home. h...   \n",
       "12  [[1] Redha Ali, Russell C. Hardie, Barath Nara...   \n",
       "13  [[1] Peiman Barnaghi, Parsa Ghaffari, and John...   \n",
       "14  [[1] Sunusi Bala Abdullahi and Kosin Chamnongt...   \n",
       "15  [[1] and the ACM Digital Library, [2], due to ...   \n",
       "16  [[1] Reza Abbasi, Monireh Sadeqi Jabali, Reza ...   \n",
       "17  [[1] Rakesh Agrawal, Ramakrishnan Srikant, et ...   \n",
       "18  [[1] Moonis Ali and Savvas Zannettou. 2024. Fr...   \n",
       "19  [[1] Gisle Bjontegaard. 2001. Calculation of a...   \n",
       "20  [[1] Shuang Chen, Qian Liu, Zhiwei Yu, Chin-Ye...   \n",
       "21  [[1] ETSI GS NFV-IFA 011. 2023. Network Functi...   \n",
       "22  [[1] Gediminas Adomavicius and Alexander Tuzhi...   \n",
       "23  [[1] Elia Abi-Jaoude, Karline Treurnicht Naylo...   \n",
       "24  [[1] Marcelo MR Araujo, Carlos HG Ferreira, Ju...   \n",
       "25  [[1] Ganesh Ananthanarayanan, Paramvir Bahl, P...   \n",
       "26  [[1] Ali, M., Berrendorf, M., Hoyt, C.T., Verm...   \n",
       "27  [[1] Rodrigo Simon Bavaresco and Jorge Luis Vi...   \n",
       "28  [[1] AAIDD. 2021. Defining Criteria for Intell...   \n",
       "29  [[1] Instituto Alana. 2022. Notifica√ß√£o enviad...   \n",
       "30  [[1] 2022. O que √© ‚Äôwoke‚Äô e por que o termo ge...   \n",
       "31  [[1] Lasse F Wolff Anthony, Benjamin Kanding, ...   \n",
       "32  [[1] Zahra Ahmadi, Mostafa Haghi Kashani, Moha...   \n",
       "33  [[1] Alberto Luiz Aramaki, Rosana Ferreira Sam...   \n",
       "34  [[1] Bert Bakker, Gijs Schumacher, Kevin Arcen...   \n",
       "35  [[1] Michel Albonico, Milica √êorƒëeviƒá, Engel H...   \n",
       "36  [[1] Himan Abdollahpouri, Masoud Mansoury, Rob...   \n",
       "37  [[1] Rafael J. A. Almeida. 2018. LeIA - L√©xico...   \n",
       "38  [[1] T. M. U. Ara√∫jo. 2012. Uma solu√ß√£o para g...   \n",
       "39  [[1] Shishir Adhikari, Akshay Uppal, Robin Mer...   \n",
       "\n",
       "                                                 text  \\\n",
       "0   Introdu√ß√£o O reconhecimento das emo√ß√µes √© uma ...   \n",
       "1   Introdu√ß√£o O teste automatizado de aplicativos...   \n",
       "2   Introdu√ß√£o A compacta√ß√£o de v√≠deo desempenha u...   \n",
       "3   Introdu√ß√£o Nos √∫ltimos tempos, a sociedade enf...   \n",
       "4   INTRODUCTION  Many recent studies have focused...   \n",
       "5   Introdu√ß√£o Os v√≠deos digitais t√™m sido fundame...   \n",
       "6   Introdu√ß√£o O gerenciamento de medicamentos √© u...   \n",
       "7   Introdu√ß√£o O volume crescente de dados publica...   \n",
       "8   Introdu√ß√£o Aplicativos de TV digital (DTV) [26...   \n",
       "9   Introdu√ß√£o ao incentivo √† inova√ß√£o e tecnologi...   \n",
       "10  Introdu√ß√£o nas √∫ltimas d√©cadas, testemunhamos ...   \n",
       "11  A tecnologia de introdu√ß√£o transformou signifi...   \n",
       "12  Introdu√ß√£o Nos √∫ltimos anos, houve um foco cre...   \n",
       "13  Introdu√ß√£o As plataformas de m√≠dia social on -...   \n",
       "14  Introdu√ß√£o O reconhecimento brasileiro de ling...   \n",
       "15  Introdu√ß√£o O avan√ßo das tecnologias de informa...   \n",
       "16  Introdu√ß√£o Os sistemas de informa√ß√£o em sa√∫de ...   \n",
       "17  Introdu√ß√£o A Web se tornou parte integrante da...   \n",
       "18  Introdu√ß√£o Na era digital, os servi√ßos de stre...   \n",
       "19  As nuvens de ponto de introdu√ß√£o podem ser usa...   \n",
       "20  Introdu√ß√£o O grande volume de dados dispon√≠vei...   \n",
       "21  Introdu√ß√£o A virtualiza√ß√£o √© um conceito que a...   \n",
       "22  Introdu√ß√£o Os sistemas de recomenda√ß√£o s√£o fer...   \n",
       "23  INTRODU√á√ÉO Nos √∫ltimos anos, o panorama da com...   \n",
       "24  INTRODU√á√ÉO As plataformas de m√≠dias sociais re...   \n",
       "25  INTRODU√á√ÉO Nos √∫ltimos anos, a import√¢ncia de ...   \n",
       "26  INTRODU√á√ÉO Sistemas de Recomenda√ß√£o (SsR) s√£o ...   \n",
       "27  INTRODU√á√ÉO A Organiza√ß√£o Mundial de Sa√∫de (OMS...   \n",
       "28  INTRODU√á√ÉO O Pensamento Computacional (PC) √© d...   \n",
       "29  INTRODUCTION No cen√°rio contempor√¢neo, as rede...   \n",
       "30  INTRODU√á√ÉO Atualmente, plataformas digitais de...   \n",
       "31  INTRODU√á√ÉO Classifica√ß√£o Autom√°tica de Texto (...   \n",
       "32  INTRODU√á√ÉO O crescimento exponencial dos siste...   \n",
       "33  Um framework de rastreamento corporal para rea...   \n",
       "34  INTRODU√á√ÉO Num contexto em que as emo√ß√µes huma...   \n",
       "35  INTRODU√á√ÉO A ado√ß√£o de novas tecnologias na ed...   \n",
       "36  INTRODU√á√ÉO Os sistemas de recomenda√ß√£o s√£o pro...   \n",
       "37  INTRODU√á√ÉO As redes sociais t√™m rompido barrei...   \n",
       "38  INTRODU√á√ÉO A evolu√ß√£o da tecnologia tem desemp...   \n",
       "39  INTRODU√á√ÉO A ascens√£o de plataformas de v√≠deo,...   \n",
       "\n",
       "                                    artigo_tokenizado  \\\n",
       "0   [Introdu√ß√£o, O, reconhecimento, das, emo√ß√µes, ...   \n",
       "1   [Introdu√ß√£o, O, teste, automatizado, de, aplic...   \n",
       "2   [Introdu√ß√£o, A, compacta√ß√£o, de, v√≠deo, desemp...   \n",
       "3   [Introdu√ß√£o, Nos, √∫ltimos, tempos, ,, a, socie...   \n",
       "4   [9/02/2008, √†s, 19:20:05, Molho, de, macarr√£o,...   \n",
       "5   [Introdu√ß√£o, Os, v√≠deos, digitais, t√™m, sido, ...   \n",
       "6   [Introdu√ß√£o, O, gerenciamento, de, medicamento...   \n",
       "7   [Introdu√ß√£o, O, volume, crescente, de, dados, ...   \n",
       "8   [Introdu√ß√£o, Aplicativos, de, TV, digital, (, ...   \n",
       "9   [Introdu√ß√£o, ao, incentivo, √†, inova√ß√£o, e, te...   \n",
       "10  [Introdu√ß√£o, nas, √∫ltimas, d√©cadas, ,, testemu...   \n",
       "11  [A, tecnologia, de, introdu√ß√£o, transformou, s...   \n",
       "12  [Introdu√ß√£o, Nos, √∫ltimos, anos, ,, houve, um,...   \n",
       "13  [Introdu√ß√£o, As, plataformas, de, m√≠dia, socia...   \n",
       "14  [Introdu√ß√£o, O, reconhecimento, brasileiro, de...   \n",
       "15  [Introdu√ß√£o, O, avan√ßo, das, tecnologias, de, ...   \n",
       "16  [Introdu√ß√£o, Os, sistemas, de, informa√ß√£o, em,...   \n",
       "17  [Introdu√ß√£o, A, Web, se, tornou, parte, integr...   \n",
       "18  [Introdu√ß√£o, Na, era, digital, ,, os, servi√ßos...   \n",
       "19  [As, nuvens, de, ponto, de, introdu√ß√£o, podem,...   \n",
       "20  [Introdu√ß√£o, O, grande, volume, de, dados, dis...   \n",
       "21  [Introdu√ß√£o, A, virtualiza√ß√£o, √©, um, conceito...   \n",
       "22  [Introdu√ß√£o, Os, sistemas, de, recomenda√ß√£o, s...   \n",
       "23  [INTRODU√á√ÉO, Nos, √∫ltimos, anos, ,, o, panoram...   \n",
       "24  [INTRODU√á√ÉO, As, plataformas, de, m√≠dias, soci...   \n",
       "25  [INTRODU√á√ÉO, Nos, √∫ltimos, anos, ,, a, import√¢...   \n",
       "26  [INTRODU√á√ÉO, Sistemas, de, Recomenda√ß√£o, (, Ss...   \n",
       "27  [INTRODU√á√ÉO, A, Organiza√ß√£o, Mundial, de, Sa√∫d...   \n",
       "28  [INTRODU√á√ÉO, O, Pensamento, Computacional, (, ...   \n",
       "29  [INTRODUCTION, No, cen√°rio, contempor√¢neo, ,, ...   \n",
       "30  [INTRODU√á√ÉO, Atualmente, ,, plataformas, digit...   \n",
       "31  [INTRODU√á√ÉO, Classifica√ß√£o, Autom√°tica, de, Te...   \n",
       "32  [INTRODU√á√ÉO, O, crescimento, exponencial, dos,...   \n",
       "33  [Um, framework, de, rastreamento, corporal, pa...   \n",
       "34  [INTRODU√á√ÉO, Num, contexto, em, que, as, emo√ß√µ...   \n",
       "35  [INTRODU√á√ÉO, A, ado√ß√£o, de, novas, tecnologias...   \n",
       "36  [INTRODU√á√ÉO, Os, sistemas, de, recomenda√ß√£o, s...   \n",
       "37  [INTRODU√á√ÉO, As, redes, sociais, t√™m, rompido,...   \n",
       "38  [INTRODU√á√ÉO, A, evolu√ß√£o, da, tecnologia, tem,...   \n",
       "39  [INTRODU√á√ÉO, A, ascens√£o, de, plataformas, de,...   \n",
       "\n",
       "                                           pos_tagger  \\\n",
       "0   [[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [rec...   \n",
       "1   [[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [tes...   \n",
       "2   [[Introdu√ß√£o, NOUN, NOUN], [A, DET, DET], [com...   \n",
       "3   [[Introdu√ß√£o, NOUN, NOUN], [Nos, ADP, ADP], [√∫...   \n",
       "4   [[9/02/2008, NUM, NUM], [√†s, ADP, ADP], [19:20...   \n",
       "5   [[Introdu√ß√£o, NOUN, NOUN], [Os, DET, DET], [v√≠...   \n",
       "6   [[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [ger...   \n",
       "7   [[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [vol...   \n",
       "8   [[Introdu√ß√£o, PROPN, PROPN], [Aplicativos, PRO...   \n",
       "9   [[Introdu√ß√£o, NOUN, NOUN], [ao, ADP, ADP], [in...   \n",
       "10  [[Introdu√ß√£o, NOUN, NOUN], [nas, ADP, ADP], [√∫...   \n",
       "11  [[A, DET, DET], [tecnologia, NOUN, NOUN], [de,...   \n",
       "12  [[Introdu√ß√£o, NOUN, NOUN], [Nos, ADP, ADP], [√∫...   \n",
       "13  [[Introdu√ß√£o, PROPN, PROPN], [As, DET, DET], [...   \n",
       "14  [[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [rec...   \n",
       "15  [[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [ava...   \n",
       "16  [[Introdu√ß√£o, NOUN, NOUN], [Os, DET, DET], [si...   \n",
       "17  [[Introdu√ß√£o, NOUN, NOUN], [A, DET, DET], [Web...   \n",
       "18  [[Introdu√ß√£o, NOUN, NOUN], [Na, ADP, ADP], [er...   \n",
       "19  [[As, DET, DET], [nuvens, NOUN, NOUN], [de, AD...   \n",
       "20  [[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [gra...   \n",
       "21  [[Introdu√ß√£o, NOUN, NOUN], [A, DET, DET], [vir...   \n",
       "22  [[Introdu√ß√£o, NOUN, NOUN], [Os, DET, DET], [si...   \n",
       "23  [[INTRODU√á√ÉO, PROPN, PROPN], [Nos, ADP, ADP], ...   \n",
       "24  [[INTRODU√á√ÉO, PROPN, PROPN], [As, DET, DET], [...   \n",
       "25  [[INTRODU√á√ÉO, PROPN, PROPN], [Nos, ADP, ADP], ...   \n",
       "26  [[INTRODU√á√ÉO, PROPN, PROPN], [Sistemas, PROPN,...   \n",
       "27  [[INTRODU√á√ÉO, PROPN, PROPN], [A, DET, DET], [O...   \n",
       "28  [[INTRODU√á√ÉO, PROPN, PROPN], [O, DET, DET], [P...   \n",
       "29  [[INTRODUCTION, PROPN, PROPN], [No, ADP, ADP],...   \n",
       "30  [[INTRODU√á√ÉO, PROPN, PROPN], [Atualmente, ADV,...   \n",
       "31  [[INTRODU√á√ÉO, PROPN, PROPN], [Classifica√ß√£o, P...   \n",
       "32  [[INTRODU√á√ÉO, PROPN, PROPN], [O, DET, DET], [c...   \n",
       "33  [[Um, DET, DET], [framework, PROPN, PROPN], [d...   \n",
       "34  [[INTRODU√á√ÉO, PROPN, PROPN], [Num, ADP, ADP], ...   \n",
       "35  [[INTRODU√á√ÉO, PROPN, PROPN], [A, DET, DET], [a...   \n",
       "36  [[INTRODU√á√ÉO, PROPN, PROPN], [Os, DET, DET], [...   \n",
       "37  [[INTRODU√á√ÉO, PROPN, PROPN], [As, DET, DET], [...   \n",
       "38  [[INTRODU√á√ÉO, PROPN, PROPN], [A, DET, DET], [e...   \n",
       "39  [[INTRODU√á√ÉO, PROPN, PROPN], [A, DET, DET], [a...   \n",
       "\n",
       "                                                 lema  \n",
       "0   [introdu√ß√£o, o, reconhecimento, de o, emo√ß√£o, ...  \n",
       "1   [introdu√ß√£o, o, teste, automatizado, de, aplic...  \n",
       "2   [introdu√ß√£o, o, compacta√ß√£o, de, v√≠deo, desemp...  \n",
       "3   [introdu√ß√£o, em o, √∫ltimo, tempo, o, sociedade...  \n",
       "4   [9/02/2008, a o, 19:20:05, Molho, de, macarr√£o...  \n",
       "5   [introdu√ß√£o, o, v√≠deo, digital, ter, ser, fund...  \n",
       "6   [introdu√ß√£o, o, gerenciamento, de, medicamento...  \n",
       "7   [introdu√ß√£o, o, volume, crescente, de, dado, p...  \n",
       "8   [Introdu√ß√£o, Aplicativos, de, tv, digital, DTV...  \n",
       "9   [introdu√ß√£o, a o, incentivo, a o, inova√ß√£o, e,...  \n",
       "10  [introdu√ß√£o, em o, √∫ltimo, d√©cada, testemunham...  \n",
       "11  [o, tecnologia, de, introdu√ß√£o, transformar, s...  \n",
       "12  [introdu√ß√£o, em o, √∫ltimo, ano, haver, um, foc...  \n",
       "13  [introdu√ß√£o, o, plataforma, de, m√≠dia, social,...  \n",
       "14  [introdu√ß√£o, o, reconhecimento, brasileiro, de...  \n",
       "15  [introdu√ß√£o, o, avan√ßo, de o, tecnologia, de, ...  \n",
       "16  [introdu√ß√£o, o, sistema, de, informa√ß√£o, em, s...  \n",
       "17  [introdu√ß√£o, o, Web, se, tornar, parte, integr...  \n",
       "18  [introdu√ß√£o, em o, ser, digital, o, servi√ßo, d...  \n",
       "19  [o, nuvem, de, ponto, de, introdu√ß√£o, poder, s...  \n",
       "20  [introdu√ß√£o, o, grande, volume, de, dado, disp...  \n",
       "21  [introdu√ß√£o, o, virtualiza√ß√£o, ser, um, concei...  \n",
       "22  [introdu√ß√£o, o, sistema, de, recomenda√ß√£o, ser...  \n",
       "23  [INTRODU√á√ÉO, em o, √∫ltimo, ano, o, panorama, d...  \n",
       "24  [INTRODU√á√ÉO, o, plataforma, de, m√≠dia, social,...  \n",
       "25  [INTRODU√á√ÉO, em o, √∫ltimo, ano, o, import√¢ncia...  \n",
       "26  [INTRODU√á√ÉO, Sistemas, de, Recomenda√ß√£o, SsR, ...  \n",
       "27  [INTRODU√á√ÉO, o, Organiza√ß√£o, Mundial, de, Sa√∫d...  \n",
       "28  [INTRODU√á√ÉO, o, Pensamento, Computacional, PC,...  \n",
       "29  [INTRODUCTION, em o, cen√°rio, contempor√¢neo, o...  \n",
       "30  [INTRODU√á√ÉO, Atualmente, plataforma, digital, ...  \n",
       "31  [INTRODU√á√ÉO, Classifica√ß√£o, Autom√°tica, de, Te...  \n",
       "32  [INTRODU√á√ÉO, o, crescimento, exponencial, de o...  \n",
       "33  [um, framework, de, rastreamento, corporal, pa...  \n",
       "34  [INTRODU√á√ÉO, em um, contexto, em, que, o, emo√ß...  \n",
       "35  [INTRODU√á√ÉO, o, ado√ß√£o, de, novo, tecnologia, ...  \n",
       "36  [INTRODU√á√ÉO, o, sistema, de, recomenda√ß√£o, ser...  \n",
       "37  [INTRODU√á√ÉO, o, rede, social, ter, rompir, bar...  \n",
       "38  [INTRODU√á√ÉO, o, evolu√ß√£o, de o, tecnologia, te...  \n",
       "39  [INTRODU√á√ÉO, o, ascens√£o, de, plataforma, de, ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titulo</th>\n",
       "      <th>informacoes_url</th>\n",
       "      <th>idioma</th>\n",
       "      <th>storage_key</th>\n",
       "      <th>autores</th>\n",
       "      <th>data_publicacao</th>\n",
       "      <th>resumo</th>\n",
       "      <th>keywords</th>\n",
       "      <th>referencias</th>\n",
       "      <th>text</th>\n",
       "      <th>artigo_tokenizado</th>\n",
       "      <th>pos_tagger</th>\n",
       "      <th>lema</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Recognition of Emotions through Facial Geometr...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24766-1-10-20...</td>\n",
       "      <td>[Alessandra Alaniz Macedo,  Leandro Persona,  ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O reconhecimento de emo√ß√£o tem um significado ...</td>\n",
       "      <td>Multimedia Processing, Affective Computing, Ma...</td>\n",
       "      <td>[[1] Mauricio Alvarez, David Luengo, and Neil ...</td>\n",
       "      <td>Introdu√ß√£o O reconhecimento das emo√ß√µes √© uma ...</td>\n",
       "      <td>[Introdu√ß√£o, O, reconhecimento, das, emo√ß√µes, ...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [rec...</td>\n",
       "      <td>[introdu√ß√£o, o, reconhecimento, de o, emo√ß√£o, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Enhancing widget recognition for automated And...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24752-1-10-20...</td>\n",
       "      <td>[Yadini P√©rez L√≥pez,  La√≠s Dib Albuquerque,  G...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O reconhecimento de widgets √© crucial para tes...</td>\n",
       "      <td>Automated Android Testing, Test Portability, O...</td>\n",
       "      <td>[[1] A. A. Abdelhamid, S. Alotaibi, and A. Mou...</td>\n",
       "      <td>Introdu√ß√£o O teste automatizado de aplicativos...</td>\n",
       "      <td>[Introdu√ß√£o, O, teste, automatizado, de, aplic...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [tes...</td>\n",
       "      <td>[introdu√ß√£o, o, teste, automatizado, de, aplic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Multi-Domain Spatio-Temporal Deformable Fusion...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24762-1-10-20...</td>\n",
       "      <td>[Garibaldi da Silveira J√∫nior, Gilberto Kreisl...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A compacta√ß√£o de v√≠deo com perda de im√≥vel int...</td>\n",
       "      <td>Redes neurais profundas, Melhoria de qualidade...</td>\n",
       "      <td>[[1] Aayushi Agarwal, Akshay Agarwal, Sayan Si...</td>\n",
       "      <td>Introdu√ß√£o A compacta√ß√£o de v√≠deo desempenha u...</td>\n",
       "      <td>[Introdu√ß√£o, A, compacta√ß√£o, de, v√≠deo, desemp...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [A, DET, DET], [com...</td>\n",
       "      <td>[introdu√ß√£o, o, compacta√ß√£o, de, v√≠deo, desemp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Finding Fake News Websites in the Wild</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24756-1-10-20...</td>\n",
       "      <td>[Leandro Araujo,  Jo√£o M. M. Couto,  Luiz Feli...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A batalha contra a dissemina√ß√£o de informa√ß√µes...</td>\n",
       "      <td>Fake News, Misinformation, Credibility, Websit...</td>\n",
       "      <td>[[1] Leandro Ara√∫jo, Luiz Felipe Nery, Isadora...</td>\n",
       "      <td>Introdu√ß√£o Nos √∫ltimos tempos, a sociedade enf...</td>\n",
       "      <td>[Introdu√ß√£o, Nos, √∫ltimos, tempos, ,, a, socie...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [Nos, ADP, ADP], [√∫...</td>\n",
       "      <td>[introdu√ß√£o, em o, √∫ltimo, tempo, o, sociedade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tagging Enriched Bank Transactions Using LLM-G...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24767-1-10-20...</td>\n",
       "      <td>[Daniel de S. Moraes,  Polyana B. da Costa,  P...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>This work presents an unsupervised method for ...</td>\n",
       "      <td>Large Language Models, Natural Language Proces...</td>\n",
       "      <td>[[1] Rohan Anil, Sebastian Borgeaud, Yonghui W...</td>\n",
       "      <td>INTRODUCTION  Many recent studies have focused...</td>\n",
       "      <td>[9/02/2008, √†s, 19:20:05, Molho, de, macarr√£o,...</td>\n",
       "      <td>[[9/02/2008, NUM, NUM], [√†s, ADP, ADP], [19:20...</td>\n",
       "      <td>[9/02/2008, a o, 19:20:05, Molho, de, macarr√£o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fast ISP Mode Decision for the Versatile Video...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24755-1-10-20...</td>\n",
       "      <td>[Larissa Ara√∫jo,  Adson Duarte,  Bruno Zatt,  ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O padr√£o vers√°til de codifica√ß√£o de v√≠deo (VVC...</td>\n",
       "      <td>VVC, Intra Prediction, ISP, Machine Learning</td>\n",
       "      <td>[[1] James Bergstra and Yoshua Bengio. 2012. R...</td>\n",
       "      <td>Introdu√ß√£o Os v√≠deos digitais t√™m sido fundame...</td>\n",
       "      <td>[Introdu√ß√£o, Os, v√≠deos, digitais, t√™m, sido, ...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [Os, DET, DET], [v√≠...</td>\n",
       "      <td>[introdu√ß√£o, o, v√≠deo, digital, ter, ser, fund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OntoDrug: Enhancing Brazilian Health System In...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24764-1-10-20...</td>\n",
       "      <td>[Nelson Miranda,  Matheus Matos Machado,   Dil...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Este artigo apresenta ontodrug, uma ontologia ...</td>\n",
       "      <td>Medication Ontologies, Drug Management, Semant...</td>\n",
       "      <td>[[1] Burhan Ud Din Abbasi, Iram Fatima, Hamid ...</td>\n",
       "      <td>Introdu√ß√£o O gerenciamento de medicamentos √© u...</td>\n",
       "      <td>[Introdu√ß√£o, O, gerenciamento, de, medicamento...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [ger...</td>\n",
       "      <td>[introdu√ß√£o, o, gerenciamento, de, medicamento...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>E-BELA: Enhanced Embedding-Based Entity Linkin...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24750-1-10-20...</td>\n",
       "      <td>[√çtalo M. Pereira  Anderson A. Ferreira]</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Entidade vinculando √© o processo de conectar m...</td>\n",
       "      <td>Natural Language Processing, Entity Linking, L...</td>\n",
       "      <td>[[1] Moonis Ali and Savvas Zannettou. 2024. Fr...</td>\n",
       "      <td>Introdu√ß√£o O volume crescente de dados publica...</td>\n",
       "      <td>[Introdu√ß√£o, O, volume, crescente, de, dados, ...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [vol...</td>\n",
       "      <td>[introdu√ß√£o, o, volume, crescente, de, dado, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Exploring Visual and Multimodal Interaction in...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24754-1-10-20...</td>\n",
       "      <td>[Paulo Victor Borges,  Daniel de S. Moraes,  J...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Este artigo apresenta duas ferramentas inovado...</td>\n",
       "      <td>Authoring, LLMs, NCL, Code Generation, Visual ...</td>\n",
       "      <td>[[1] NBR ABNT. [n. d.]. Digital Terrestrial Te...</td>\n",
       "      <td>Introdu√ß√£o Aplicativos de TV digital (DTV) [26...</td>\n",
       "      <td>[Introdu√ß√£o, Aplicativos, de, TV, digital, (, ...</td>\n",
       "      <td>[[Introdu√ß√£o, PROPN, PROPN], [Aplicativos, PRO...</td>\n",
       "      <td>[Introdu√ß√£o, Aplicativos, de, tv, digital, DTV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Elderly Fall Monitoring in Smart Homes Using W...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24751-1-10-20...</td>\n",
       "      <td>[J√∫lia M. P. Moreira,  Raphael W. Bettio,  And...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O progresso constante da tecnologia, especialm...</td>\n",
       "      <td>fall detection, wearable technologies, acceler...</td>\n",
       "      <td>[[1] Ibukun Awolusi, Eric Marks, and Matthew H...</td>\n",
       "      <td>Introdu√ß√£o ao incentivo √† inova√ß√£o e tecnologi...</td>\n",
       "      <td>[Introdu√ß√£o, ao, incentivo, √†, inova√ß√£o, e, te...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [ao, ADP, ADP], [in...</td>\n",
       "      <td>[introdu√ß√£o, a o, incentivo, a o, inova√ß√£o, e,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A Comprehensive View of the Biases of Toxicity...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24737-1-10-20...</td>\n",
       "      <td>[Guilherme Andrade,  Luiz Nery,  Fabr√≠cio Bene...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A linguagem √© um aspecto din√¢mico de nossa cul...</td>\n",
       "      <td>African American English, AAE, Bias, Toxicity,...</td>\n",
       "      <td>[[1] Abubakar Abid, Maheen Farooqi, and James ...</td>\n",
       "      <td>Introdu√ß√£o nas √∫ltimas d√©cadas, testemunhamos ...</td>\n",
       "      <td>[Introdu√ß√£o, nas, √∫ltimas, d√©cadas, ,, testemu...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [nas, ADP, ADP], [√∫...</td>\n",
       "      <td>[introdu√ß√£o, em o, √∫ltimo, d√©cada, testemunham...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Interoperability Testing Guide for the Interne...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24758-1-10-20...</td>\n",
       "      <td>[Karina da Silva Castelo Branco,  Val√©ria Lell...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A Internet das Coisas (IoT) expandiu a Interne...</td>\n",
       "      <td>Interoperability, Internet of Things, Interope...</td>\n",
       "      <td>[[1] Home Assistant. 2024. Awaken your home. h...</td>\n",
       "      <td>A tecnologia de introdu√ß√£o transformou signifi...</td>\n",
       "      <td>[A, tecnologia, de, introdu√ß√£o, transformou, s...</td>\n",
       "      <td>[[A, DET, DET], [tecnologia, NOUN, NOUN], [de,...</td>\n",
       "      <td>[o, tecnologia, de, introdu√ß√£o, transformar, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>An Ensemble Approach to Facial Deepfake Detect...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24741-1-10-20...</td>\n",
       "      <td>[Yan Martins B. Gurevitz Cunha,  Jos√© Matheus ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Esfor√ßos substanciais foram dedicados ao desen...</td>\n",
       "      <td>deep fake detection, self-supervised, vision t...</td>\n",
       "      <td>[[1] Redha Ali, Russell C. Hardie, Barath Nara...</td>\n",
       "      <td>Introdu√ß√£o Nos √∫ltimos anos, houve um foco cre...</td>\n",
       "      <td>[Introdu√ß√£o, Nos, √∫ltimos, anos, ,, houve, um,...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [Nos, ADP, ADP], [√∫...</td>\n",
       "      <td>[introdu√ß√£o, em o, √∫ltimo, ano, haver, um, foc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Twitter and the 2022 Brazilian Elections Portr...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24769-1-10-20...</td>\n",
       "      <td>[Larissa Malagoli,  Giovana Piorino,  Carlos H...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A influ√™ncia das redes sociais on -line nas a√ß...</td>\n",
       "      <td>2022 Brazilian Elections, Network Modeling, On...</td>\n",
       "      <td>[[1] Peiman Barnaghi, Parsa Ghaffari, and John...</td>\n",
       "      <td>Introdu√ß√£o As plataformas de m√≠dia social on -...</td>\n",
       "      <td>[Introdu√ß√£o, As, plataformas, de, m√≠dia, socia...</td>\n",
       "      <td>[[Introdu√ß√£o, PROPN, PROPN], [As, DET, DET], [...</td>\n",
       "      <td>[introdu√ß√£o, o, plataforma, de, m√≠dia, social,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Automatic Time-aware Recognition of Brazilian ...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24745-1-10-20...</td>\n",
       "      <td>[Lucas de S. Arcanjo,  Lucas F. Coelho,  Silvi...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A linguagem de sinais brasileiros (Libras) √© u...</td>\n",
       "      <td>Computer Vision, Sign Language Recognition, Ge...</td>\n",
       "      <td>[[1] Sunusi Bala Abdullahi and Kosin Chamnongt...</td>\n",
       "      <td>Introdu√ß√£o O reconhecimento brasileiro de ling...</td>\n",
       "      <td>[Introdu√ß√£o, O, reconhecimento, brasileiro, de...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [rec...</td>\n",
       "      <td>[introdu√ß√£o, o, reconhecimento, brasileiro, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Investigating User's Attentional Focus in Comp...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24759-1-10-20...</td>\n",
       "      <td>[Cassiano da Silva Souza,  Milene Selbach Silv...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Manter o foco atencional do usu√°rio se tornou ...</td>\n",
       "      <td>attention monitoring, webcam, data analysis, l...</td>\n",
       "      <td>[[1] and the ACM Digital Library, [2], due to ...</td>\n",
       "      <td>Introdu√ß√£o O avan√ßo das tecnologias de informa...</td>\n",
       "      <td>[Introdu√ß√£o, O, avan√ßo, das, tecnologias, de, ...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [ava...</td>\n",
       "      <td>[introdu√ß√£o, o, avan√ßo, de o, tecnologia, de, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Acceptance and Usability of Complex Medical Sy...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24740-1-10-20...</td>\n",
       "      <td>[F√°bio Ap. C√¢ndido da Silva,  Andr√© Pimenta Fr...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A crescente demanda por testes de imagem torno...</td>\n",
       "      <td>usability issues, radiology systems, qualitati...</td>\n",
       "      <td>[[1] Reza Abbasi, Monireh Sadeqi Jabali, Reza ...</td>\n",
       "      <td>Introdu√ß√£o Os sistemas de informa√ß√£o em sa√∫de ...</td>\n",
       "      <td>[Introdu√ß√£o, Os, sistemas, de, informa√ß√£o, em,...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [Os, DET, DET], [si...</td>\n",
       "      <td>[introdu√ß√£o, o, sistema, de, informa√ß√£o, em, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Through the Eyes of Instagram: Analyzing Image...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24768-1-10-20...</td>\n",
       "      <td>[Jo√£o Francisco Hecksher Olivetti  Philipe de ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A comunica√ß√£o multim√≠dia se tornou uma parte e...</td>\n",
       "      <td>Instagram, alt-text, social media, image class...</td>\n",
       "      <td>[[1] Rakesh Agrawal, Ramakrishnan Srikant, et ...</td>\n",
       "      <td>Introdu√ß√£o A Web se tornou parte integrante da...</td>\n",
       "      <td>[Introdu√ß√£o, A, Web, se, tornou, parte, integr...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [A, DET, DET], [Web...</td>\n",
       "      <td>[introdu√ß√£o, o, Web, se, tornar, parte, integr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Characterization of the Brazilian musical land...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24746-1-10-20...</td>\n",
       "      <td>[Filipe A. S. Moura,  Carlos H. G. Ferreira,  ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Na era digital, servi√ßos de streaming como o S...</td>\n",
       "      <td>Music Preferences, Music Genre Networks, Regio...</td>\n",
       "      <td>[[1] Moonis Ali and Savvas Zannettou. 2024. Fr...</td>\n",
       "      <td>Introdu√ß√£o Na era digital, os servi√ßos de stre...</td>\n",
       "      <td>[Introdu√ß√£o, Na, era, digital, ,, os, servi√ßos...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [Na, ADP, ADP], [er...</td>\n",
       "      <td>[introdu√ß√£o, em o, ser, digital, o, servi√ßo, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>A Machine-Learning-Driven Fast Video-based Poi...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24739-1-10-20...</td>\n",
       "      <td>[Gustavo Rehbein,  Eduardo Costa,  Guilherme C...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Nos √∫ltimos anos, o conte√∫do da nuvem de ponto...</td>\n",
       "      <td>point clouds, machine learning, V-PCC, complex...</td>\n",
       "      <td>[[1] Gisle Bjontegaard. 2001. Calculation of a...</td>\n",
       "      <td>As nuvens de ponto de introdu√ß√£o podem ser usa...</td>\n",
       "      <td>[As, nuvens, de, ponto, de, introdu√ß√£o, podem,...</td>\n",
       "      <td>[[As, DET, DET], [nuvens, NOUN, NOUN], [de, AD...</td>\n",
       "      <td>[o, nuvem, de, ponto, de, introdu√ß√£o, poder, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Constructing a KBQA Framework: Design and Impl...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24747-1-10-20...</td>\n",
       "      <td>[R√¥mulo Chrispim de Mello,  Jor√£o Gomes Jr.,  ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O crescimento exponencial dos dados na Interne...</td>\n",
       "      <td>KBQA, Complex Questions, Entity Recognition, P...</td>\n",
       "      <td>[[1] Shuang Chen, Qian Liu, Zhiwei Yu, Chin-Ye...</td>\n",
       "      <td>Introdu√ß√£o O grande volume de dados dispon√≠vei...</td>\n",
       "      <td>[Introdu√ß√£o, O, grande, volume, de, dados, dis...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [O, DET, DET], [gra...</td>\n",
       "      <td>[introdu√ß√£o, o, grande, volume, de, dado, disp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>A Domain-Specific Language for Multimedia Serv...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24738-1-10-20...</td>\n",
       "      <td>[Franklin Jordan Ventura Quico,  Anselmo L. E....</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A virtualiza√ß√£o √© uma tecnologia amplamente us...</td>\n",
       "      <td>IoMT, IoT, VNF, SFC, DSL, L-PRISM</td>\n",
       "      <td>[[1] ETSI GS NFV-IFA 011. 2023. Network Functi...</td>\n",
       "      <td>Introdu√ß√£o A virtualiza√ß√£o √© um conceito que a...</td>\n",
       "      <td>[Introdu√ß√£o, A, virtualiza√ß√£o, √©, um, conceito...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [A, DET, DET], [vir...</td>\n",
       "      <td>[introdu√ß√£o, o, virtualiza√ß√£o, ser, um, concei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Why Ignore Content? A Guideline for Intrinsic ...</td>\n",
       "      <td></td>\n",
       "      <td>english</td>\n",
       "      <td>../articles/original/english/985-24776-1-10-20...</td>\n",
       "      <td>[Pedro R. Pires,  Bruno B. Rizzi,   Tiago A. A...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Com o crescimento constante das informa√ß√µes di...</td>\n",
       "      <td>embeddings, intrinsic evaluation, qualitative ...</td>\n",
       "      <td>[[1] Gediminas Adomavicius and Alexander Tuzhi...</td>\n",
       "      <td>Introdu√ß√£o Os sistemas de recomenda√ß√£o s√£o fer...</td>\n",
       "      <td>[Introdu√ß√£o, Os, sistemas, de, recomenda√ß√£o, s...</td>\n",
       "      <td>[[Introdu√ß√£o, NOUN, NOUN], [Os, DET, DET], [si...</td>\n",
       "      <td>[introdu√ß√£o, o, sistema, de, recomenda√ß√£o, ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Impacto da Pandemia na Discuss√£o sobre Sa√∫de M...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24757-1-10...</td>\n",
       "      <td>[Pedro Bento,  Arthur Buzelin,  Yan Aquino,  I...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>The period of social isolation due to COVID-19...</td>\n",
       "      <td>Redes sociais, COVID-19, pandemia, sa√∫de menta...</td>\n",
       "      <td>[[1] Elia Abi-Jaoude, Karline Treurnicht Naylo...</td>\n",
       "      <td>INTRODU√á√ÉO Nos √∫ltimos anos, o panorama da com...</td>\n",
       "      <td>[INTRODU√á√ÉO, Nos, √∫ltimos, anos, ,, o, panoram...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [Nos, ADP, ADP], ...</td>\n",
       "      <td>[INTRODU√á√ÉO, em o, √∫ltimo, ano, o, panorama, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Um Framework para An√°lise Bidimensional de Dis...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24771-2-10...</td>\n",
       "      <td>[Geovana S. Oliveira,  Ot√°vio Ven√¢ncio,  Vin√≠c...</td>\n",
       "      <td>25-09-2024</td>\n",
       "      <td>As plataformas de m√≠dia social revolucionaram ...</td>\n",
       "      <td>M√≠dias Sociais, Dissemina√ß√£o de Informa√ß√£o, Mo...</td>\n",
       "      <td>[[1] Marcelo MR Araujo, Carlos HG Ferreira, Ju...</td>\n",
       "      <td>INTRODU√á√ÉO As plataformas de m√≠dias sociais re...</td>\n",
       "      <td>[INTRODU√á√ÉO, As, plataformas, de, m√≠dias, soci...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [As, DET, DET], [...</td>\n",
       "      <td>[INTRODU√á√ÉO, o, plataforma, de, m√≠dia, social,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Middleware para Aplica√ß√µes Distribu√≠das de V√≠d...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24761-1-10...</td>\n",
       "      <td>[Otac√≠lio de A. Ramos Neto,  Rafael C. Chaves,...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Dentro do escopo da ind√∫stria 4.0, a vis√£o com...</td>\n",
       "      <td>Middleware, Distribui√ß√£o de V√≠deo, Edge Comput...</td>\n",
       "      <td>[[1] Ganesh Ananthanarayanan, Paramvir Bahl, P...</td>\n",
       "      <td>INTRODU√á√ÉO Nos √∫ltimos anos, a import√¢ncia de ...</td>\n",
       "      <td>[INTRODU√á√ÉO, Nos, √∫ltimos, anos, ,, a, import√¢...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [Nos, ADP, ADP], ...</td>\n",
       "      <td>[INTRODU√á√ÉO, em o, √∫ltimo, ano, o, import√¢ncia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>O Impacto de Estrat√©gias de Embeddings de Graf...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24763-2-10...</td>\n",
       "      <td>[Andr√© Levi Zanon,  Leonardo Rocha,   Marcelo ...</td>\n",
       "      <td>04-10-2024</td>\n",
       "      <td>Explica√ß√µes em sistemas de recomenda√ß√£o s√£o es...</td>\n",
       "      <td>Sistemas de Recomenda√ß√£o, Explica√ß√µes, Embeddi...</td>\n",
       "      <td>[[1] Ali, M., Berrendorf, M., Hoyt, C.T., Verm...</td>\n",
       "      <td>INTRODU√á√ÉO Sistemas de Recomenda√ß√£o (SsR) s√£o ...</td>\n",
       "      <td>[INTRODU√á√ÉO, Sistemas, de, Recomenda√ß√£o, (, Ss...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [Sistemas, PROPN,...</td>\n",
       "      <td>[INTRODU√á√ÉO, Sistemas, de, Recomenda√ß√£o, SsR, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Cuidado Ub√≠quo de Pacientes com Doen√ßas Cr√¥nic...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24749-1-10...</td>\n",
       "      <td>[Lucas Pfeiffer Salom√£o Dias,  L.P.S. Dias,  J...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>As doen√ßas cr√¥nicas est√£o entre as 7 das 10 pr...</td>\n",
       "      <td>Classifica√ß√£o de Comportamento; Doen√ßas Cr√¥nic...</td>\n",
       "      <td>[[1] Rodrigo Simon Bavaresco and Jorge Luis Vi...</td>\n",
       "      <td>INTRODU√á√ÉO A Organiza√ß√£o Mundial de Sa√∫de (OMS...</td>\n",
       "      <td>[INTRODU√á√ÉO, A, Organiza√ß√£o, Mundial, de, Sa√∫d...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [A, DET, DET], [O...</td>\n",
       "      <td>[INTRODU√á√ÉO, o, Organiza√ß√£o, Mundial, de, Sa√∫d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Jogos Digitais S√©rios usados para o Exerc√≠cio ...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24760-1-10...</td>\n",
       "      <td>[Katherin Felipa Carhuaz Malpartida  Kamila Ri...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O pensamento computacional (CT) √© um processo ...</td>\n",
       "      <td>Pensamento Computacional, Jogos Digitais S√©rio...</td>\n",
       "      <td>[[1] AAIDD. 2021. Defining Criteria for Intell...</td>\n",
       "      <td>INTRODU√á√ÉO O Pensamento Computacional (PC) √© d...</td>\n",
       "      <td>[INTRODU√á√ÉO, O, Pensamento, Computacional, (, ...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [O, DET, DET], [P...</td>\n",
       "      <td>[INTRODU√á√ÉO, o, Pensamento, Computacional, PC,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Crian√ßas e Propagandas no TikTok: identificand...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24748-1-10...</td>\n",
       "      <td>[Ra√≠ssa Gon√ßalves Lopes Carvalho  Humberto Tor...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Este estudo aborda a identifica√ß√£o da publicid...</td>\n",
       "      <td>Publicidade Infantil, Crian√ßa, TikTok, Rede So...</td>\n",
       "      <td>[[1] Instituto Alana. 2022. Notifica√ß√£o enviad...</td>\n",
       "      <td>INTRODUCTION No cen√°rio contempor√¢neo, as rede...</td>\n",
       "      <td>[INTRODUCTION, No, cen√°rio, contempor√¢neo, ,, ...</td>\n",
       "      <td>[[INTRODUCTION, PROPN, PROPN], [No, ADP, ADP],...</td>\n",
       "      <td>[INTRODUCTION, em o, cen√°rio, contempor√¢neo, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Quando as Avalia√ß√µes Viram Bombas: Explorando ...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24765-1-10...</td>\n",
       "      <td>[Marcus Vinicius Guerra Ribeiro,  Clara Andrad...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Este estudo investiga o fen√¥meno do \" * revis√£...</td>\n",
       "      <td>jogos, videogames, Metacritic, review bombing,...</td>\n",
       "      <td>[[1] 2022. O que √© ‚Äôwoke‚Äô e por que o termo ge...</td>\n",
       "      <td>INTRODU√á√ÉO Atualmente, plataformas digitais de...</td>\n",
       "      <td>[INTRODU√á√ÉO, Atualmente, ,, plataformas, digit...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [Atualmente, ADV,...</td>\n",
       "      <td>[INTRODU√á√ÉO, Atualmente, plataforma, digital, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Estrat√©gias de Undersampling para Redu√ß√£o de V...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24753-2-10...</td>\n",
       "      <td>[Guilherme Fonseca,  Gabriel Prenassi,  Washin...</td>\n",
       "      <td>04-10-2024</td>\n",
       "      <td>Classifica√ß√£o de texto autom√°tica (ATC) em con...</td>\n",
       "      <td>Classifica√ß√£o de Texto, Transformers, Undersam...</td>\n",
       "      <td>[[1] Lasse F Wolff Anthony, Benjamin Kanding, ...</td>\n",
       "      <td>INTRODU√á√ÉO Classifica√ß√£o Autom√°tica de Texto (...</td>\n",
       "      <td>[INTRODU√á√ÉO, Classifica√ß√£o, Autom√°tica, de, Te...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [Classifica√ß√£o, P...</td>\n",
       "      <td>[INTRODU√á√ÉO, Classifica√ß√£o, Autom√°tica, de, Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Arquitetura Multicamadas para Coleta e An√°lise...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24744-1-10...</td>\n",
       "      <td>[Juan Felipe Souza Oliveira,  Paulo Cesar Salg...</td>\n",
       "      <td>19-09-2024</td>\n",
       "      <td>Uma arquitetura multicamada foi desenvolvida p...</td>\n",
       "      <td>internet das coisas, monitoramento remoto de s...</td>\n",
       "      <td>[[1] Zahra Ahmadi, Mostafa Haghi Kashani, Moha...</td>\n",
       "      <td>INTRODU√á√ÉO O crescimento exponencial dos siste...</td>\n",
       "      <td>[INTRODU√á√ÉO, O, crescimento, exponencial, dos,...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [O, DET, DET], [c...</td>\n",
       "      <td>[INTRODU√á√ÉO, o, crescimento, exponencial, de o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Um framework de rastreamento corporal para rea...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24770-1-10...</td>\n",
       "      <td>[Elvis Ribeiro,  Alexandre Brand√£o,  Marcelo G...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Aplica√ß√µes e jogos multim√≠dia desempenham um p...</td>\n",
       "      <td>Realidade Virtual, Jogos S√©rios, Multim√≠dia, R...</td>\n",
       "      <td>[[1] Alberto Luiz Aramaki, Rosana Ferreira Sam...</td>\n",
       "      <td>Um framework de rastreamento corporal para rea...</td>\n",
       "      <td>[Um, framework, de, rastreamento, corporal, pa...</td>\n",
       "      <td>[[Um, DET, DET], [framework, PROPN, PROPN], [d...</td>\n",
       "      <td>[um, framework, de, rastreamento, corporal, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>√önicos, mas n√£o incompar√°veis: abordagens para...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24775-1-10...</td>\n",
       "      <td>[Guilherme O. Aguiar,  Juan P. D. Esteves,  Cl...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Entender o comportamento emocional humano √© um...</td>\n",
       "      <td>Computa√ß√£o Afetiva, Respostas Emocionais, Mode...</td>\n",
       "      <td>[[1] Bert Bakker, Gijs Schumacher, Kevin Arcen...</td>\n",
       "      <td>INTRODU√á√ÉO Num contexto em que as emo√ß√µes huma...</td>\n",
       "      <td>[INTRODU√á√ÉO, Num, contexto, em, que, as, emo√ß√µ...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [Num, ADP, ADP], ...</td>\n",
       "      <td>[INTRODU√á√ÉO, em um, contexto, em, que, o, emo√ß...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Uma Proposta de Framework para Sistemas de Con...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24774-1-10...</td>\n",
       "      <td>[Marcelo Rocha,  Jesus Favela,   D√©bora C. Muc...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>H√° uma lacuna em plataformas rob√≥ticas de c√≥di...</td>\n",
       "      <td>Rob√¥s sociais, Framework para controle de rob√¥...</td>\n",
       "      <td>[[1] Michel Albonico, Milica √êorƒëeviƒá, Engel H...</td>\n",
       "      <td>INTRODU√á√ÉO A ado√ß√£o de novas tecnologias na ed...</td>\n",
       "      <td>[INTRODU√á√ÉO, A, ado√ß√£o, de, novas, tecnologias...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [A, DET, DET], [a...</td>\n",
       "      <td>[INTRODU√á√ÉO, o, ado√ß√£o, de, novo, tecnologia, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Uma Abordagem em Etapa de Processamento para R...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24772-1-10...</td>\n",
       "      <td>[Rodrigo Ferrari de Souza  Marcelo Garcia Manz...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>Os sistemas de recomenda√ß√£o s√£o projetados par...</td>\n",
       "      <td>Sistemas de Recomenda√ß√£o, Vi√©s de Popularidade...</td>\n",
       "      <td>[[1] Himan Abdollahpouri, Masoud Mansoury, Rob...</td>\n",
       "      <td>INTRODU√á√ÉO Os sistemas de recomenda√ß√£o s√£o pro...</td>\n",
       "      <td>[INTRODU√á√ÉO, Os, sistemas, de, recomenda√ß√£o, s...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [Os, DET, DET], [...</td>\n",
       "      <td>[INTRODU√á√ÉO, o, sistema, de, recomenda√ß√£o, ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>An√°lise de sentimentos de conte√∫do compartilha...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24743-1-10...</td>\n",
       "      <td>[Giovana Piorino,  Vitor Moreira,  Luiz Henriq...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>O uso crescente das m√≠dias sociais e seu impac...</td>\n",
       "      <td>An√°lise de Sentimentos, Comunidades do Reddit,...</td>\n",
       "      <td>[[1] Rafael J. A. Almeida. 2018. LeIA - L√©xico...</td>\n",
       "      <td>INTRODU√á√ÉO As redes sociais t√™m rompido barrei...</td>\n",
       "      <td>[INTRODU√á√ÉO, As, redes, sociais, t√™m, rompido,...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [As, DET, DET], [...</td>\n",
       "      <td>[INTRODU√á√ÉO, o, rede, social, ter, rompir, bar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Uma Investiga√ß√£o sobre T√©cnicas de Data Augmen...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24773-1-10...</td>\n",
       "      <td>[Marcos Andr√© Bezerra da Silva,  Manuella Asch...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A tradu√ß√£o autom√°tica de portugu√™s para Libras...</td>\n",
       "      <td>Tradu√ß√£o Autom√°tica Neural, Aumento de Dados, ...</td>\n",
       "      <td>[[1] T. M. U. Ara√∫jo. 2012. Uma solu√ß√£o para g...</td>\n",
       "      <td>INTRODU√á√ÉO A evolu√ß√£o da tecnologia tem desemp...</td>\n",
       "      <td>[INTRODU√á√ÉO, A, evolu√ß√£o, da, tecnologia, tem,...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [A, DET, DET], [e...</td>\n",
       "      <td>[INTRODU√á√ÉO, o, evolu√ß√£o, de o, tecnologia, te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>An√°lise da Percep√ß√£o do Uso de Cigarros Eletr√¥...</td>\n",
       "      <td></td>\n",
       "      <td>portuguese</td>\n",
       "      <td>../articles/original/portuguese/985-24742-1-10...</td>\n",
       "      <td>[Aline Dias,  Richardy R. Tanure,  Jussara M. ...</td>\n",
       "      <td>11-09-2024</td>\n",
       "      <td>A ascens√£o de plataformas de v√≠deo como o YouT...</td>\n",
       "      <td>Cigarros Eletr√¥nicos, Aprendizado de M√°quina, ...</td>\n",
       "      <td>[[1] Shishir Adhikari, Akshay Uppal, Robin Mer...</td>\n",
       "      <td>INTRODU√á√ÉO A ascens√£o de plataformas de v√≠deo,...</td>\n",
       "      <td>[INTRODU√á√ÉO, A, ascens√£o, de, plataformas, de,...</td>\n",
       "      <td>[[INTRODU√á√ÉO, PROPN, PROPN], [A, DET, DET], [a...</td>\n",
       "      <td>[INTRODU√á√ÉO, o, ascens√£o, de, plataforma, de, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 267
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "aeb37c9d6eae86a1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
