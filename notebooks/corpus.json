[
  {
    "titulo": "A Comprehensive View of the Biases of Toxicity and Sentiment Analysis Methods Towards Utterances with African American English Expressions",
    "informacoes_url": "",
    "idioma": "",
    "storage_key": "985-24737-1-10-20240923.pdf",
    "author": "Guilherme Andrade; Luiz Nery; Fabrício Benevenuto; Flavio Figueiredo and Savvas Zannettou",
    "data_publicacao": "D:20240911115934-03'00'",
    "resumo": "Language is a dynamic aspect of our culture that changes when\nexpressed in different technologies and/or communities. On the Internet, social networks have enabled the diffusion and evolution of\ndifferent dialects, including African American English (AAE). However, this increased usage of different dialects is not without barriers.\nOne particular barrier, the focus of this paper, is on how sentiment\n(Vader, TextBlob, and Flair) and toxicity (Google’s Perspective and\nmodels from the open-source Detoxify) scoring methods present\nbiases towards utterances with AAE expressions. In particular, AI\ntools cannot understand the re-appropriation of the terms, leading\nto false positive scores and biases. Here, we study the bias of Toxicity and Sentiment Analysis models based on experiments performed\non Web-and spoken English datasets.\n###",
    "keywords": "African American English, AAE, Bias, Toxicity, Sentiment",
    "referencias": [
      "[1] Abubakar Abid, Maheen Farooqi, and James Zou. 2021. Large language models\nassociate Muslims with violence. *Nature Machine Intelligence* 3, 6 (2021), 461–463.",
      "[2] CJ Adams. 2018. New York Times: Using AI to host better conversations. https://blog.google/technology/ai/new-york-times-using-ai-host-betterconversations/.",
      "[3] Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, and\nRoland Vollgraf. 2019. FLAIR: An easy-to-use framework for state-of-the-art\nNLP. In *NAACL 2019, 2019 Annual Conference of the North American Chapter of*\n*the Association for Computational Linguistics (Demonstrations)* . 54–59.",
      "[4] Marzieh Babaeianjelodar, Stephen Lorenz, Josh Gordon, Jeanna Matthews, and\nEvan Freitag. 2020. Quantifying gender bias in different corpora. In *Companion*\n*Proceedings of the Web Conference 2020* . 752–759.",
      "[5] Arnetha F Ball. 1992. Cultural preference and the expository writing of AfricanAmerican adolescents. *Written Communication* 9, 4 (1992), 501–532.",
      "[6] Ari Ball-Burack, Michelle Seng Ah Lee, Jennifer Cobbe, and Jatinder Singh. 2021.\nDifferential tweetment: Mitigating racial dialect bias in harmful tweet detection. In *Proceedings of the 2021 ACM Conference on Fairness, Accountability, and*\n*Transparency* . 116–128.",
      "[7] David Bamman, Chris Dyer, and Noah A Smith. 2014. Distributed representations\nof geographically situated language. In *Proceedings of the 52nd Annual Meeting of*\n*the Association for Computational Linguistics (Volume 2: Short Papers)* . 828–834.",
      "[8] John Baugh. 1981. Runnin’down Some Lines: The Language and Culture of Black\nTeenagers.",
      "[9] Su Lin Blodgett, Lisa Green, and Brendan O’Connor. 2016. Demographic dialectal\nvariation in social media: A case study of African-American English. *arXiv*\n*preprint arXiv:1608.08868* (2016).",
      "[10] Su Lin Blodgett, Johnny Wei, and Brendan O’Connor. 2018. Twitter universal\ndependency parsing for African-American and mainstream American English.\nIn *Proceedings of the 56th Annual Meeting of the Association for Computational*\n*Linguistics (Volume 1: Long Papers)* . 1415–1425.",
      "[11] Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T\nKalai. 2016. Man is to computer programmer as woman is to homemaker?\ndebiasing word embeddings. *Advances in neural information processing systems*\n29 (2016).",
      "[12] Zhenpeng Chen, Jie M Zhang, Max Hort, Federica Sarro, and Mark Harman. 2022.\nFairness testing: A comprehensive survey and analysis of trends. *arXiv preprint*\n*arXiv:2207.10223* (2022).",
      "[13] Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017.\nAutomated hate speech detection and the problem of offensive language. In\n*Proceedings of the international AAAI conference on web and social media*, Vol. 11.\n\n\n512–515.",
      "[14] Mark Díaz, Isaac Johnson, Amanda Lazar, Anne Marie Piper, and Darren Gergle.\n2018. Addressing age-related bias in sentiment analysis. In *Proceedings of the*\n*2018 chi conference on human factors in computing systems* . 1–14.",
      "[15] Joey Lee Dillard. 1977. *Lexicon of Black English.* ERIC.",
      "[16] Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. 2018.\nMeasuring and mitigating unintended bias in text classification. In *Proceedings of*\n*the 2018 AAAI/ACM Conference on AI, Ethics, and Society* . 67–73.",
      "[17] Jacob Eisenstein, Brendan O’Connor, Noah A Smith, and Eric P Xing. 2014.\nDiffusion of lexical change in social media. *PloS one* 9, 11 (2014), e113114.",
      "[18] Anjalie Field, Su Lin Blodgett, Zeerak Waseem, and Yulia Tsvetkov. 2021. A\nSurvey of Race, Racism, and Anti-Racism in NLP. In *Proceedings of the 59th*\n*Annual Meeting of the Association for Computational Linguistics and the 11th*\n*International Joint Conference on Natural Language Processing (Volume 1: Long*\n*Papers)* . 1905–1925.",
      "[19] Sarah Florini. 2014. Tweets, Tweeps, and Signifyin’ Communication and Cultural\nPerformance on “Black Twitter”. *Television & New Media* 15, 3 (2014), 223–237.",
      "[20] Patricia Friedrich. 2020. When Englishes go digital. *World Englishes* 39, 1 (2020),\n67–78.",
      "[21] Patricia Friedrich and Eduardo Diniz de Figueiredo. 2016. *The sociolinguistics of*\n*digital Englishes* . Routledge.",
      "[22] David Garcia, Ingmar Weber, and Venkata Rama Kiran Garimella. 2014. Gender\nasymmetries in reality and fiction: The bechdel test of social media. In *Eighth*\n*International AAAI Conference on Weblogs and Social Media* .",
      "[23] Anastasia Giachanou and Fabio Crestani. 2016. Like it or not: A survey of twitter\nsentiment analysis methods. *ACM Computing Surveys (CSUR)* 49, 2 (2016), 1–41.",
      "[24] Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter sentiment classification\nusing distant supervision. *CS224N project report, Stanford* 1, 12 (2009), 2009.",
      "[25] A Gomes, D Antonialli, and T Dias-Oliva. 2019. Drag queens and artificial\nintelligence. Should computers decide what is toxic on the internet. *Internet Lab*\n*blog* (2019).",
      "[26] Hila Gonen and Yoav Goldberg. 2019. Lipstick on a pig: Debiasing methods cover\nup systematic gender biases in word embeddings but do not remove them. *arXiv*\n*preprint arXiv:1903.03862* (2019).",
      "[27] Mark Graham, Bernie Hogan, Ralph K Straumann, and Ahmed Medhat. 2014.\nUneven geographies of user-generated information: Patterns of increasing informational poverty. *Annals of the Association of American Geographers* 104, 4\n(2014), 746–764.",
      "[28] Lisa J Green. 2002. *African American English: a linguistic introduction* . Cambridge\nUniversity Press.",
      "[29] Tommi Gröndahl, Luca Pajola, Mika Juuti, Mauro Conti, and N Asokan. 2018. All\nyou need is\" love\" evading hate speech detection. In *Proceedings of the 11th ACM*\n*workshop on artificial intelligence and security* . 2–12.",
      "[30] Laura Hanu and Unitary team. 2020. Detoxify. Github.\nhttps://github.com/unitaryai/detoxify.",
      "[31] Camille Harris, Matan Halevy, Ayanna Howard, Amy Bruckman, and Diyi Yang.\n2022. Exploring the role of grammar and word choice in bias toward african\namerican english (aae) in hate speech classification. In *2022 ACM Conference on*\n*Fairness, Accountability, and Transparency* . 789–798.",
      "[32] Hossein Hosseini, Sreeram Kannan, Baosen Zhang, and Radha Poovendran. 2017.\nDeceiving google’s perspective api built for detecting toxic comments. *arXiv*\n*preprint arXiv:1702.08138* (2017).",
      "[33] Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews.\nIn *Proceedings of the tenth ACM SIGKDD international conference on Knowledge*\n*discovery and data mining* . 168–177.",
      "[34] Ben Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu\nZhong, and Stephen Denuyl. 2020. Unintended machine learning biases as\nsocial barriers for persons with disabilitiess. *ACM SIGACCESS Accessibility and*\n*Computing* (2020), 1–1.",
      "[35] Clayton Hutto and Eric Gilbert. 2014. Vader: A parsimonious rule-based model\nfor sentiment analysis of social media text. In *Proceedings of the international*\n*AAAI conference on web and social media*, Vol. 8. 216–225.",
      "[36] Sen Jia, Thomas Lansdall-Welfare, and Nello Cristianini. 2015. Measuring gender\nbias in news images. In *Proceedings of the 24th International Conference on World*\n*Wide Web* . 893–898.",
      "[37] Jigsaw. [n. d.]. Perspective API. https://perspectiveapi.com/. Accessed: 2023-0130.",
      "[38] Tyler Kendall and Charlie Farrington. 2021. The Corpus of Regional African\nAmerican Language (Version 2021.07). Eugene, OR: The Online Resources for\nAfrican American Language Project.",
      "[39] Svetlana Kiritchenko and Saif M Mohammad. 2018. Examining Gender and Race\nBias in Two Hundred Sentiment Analysis Systems. *NAACL HLT 2018* (2018), 43.",
      "[40] Animesh Koratana and Kevin Hu. 2018. Toxic speech detection. *URL: https://web.*\n*stanford. edu/class/archive/cs/cs224n/cs224n* 1194 (2018).",
      "[41] Deepak Kumar, Patrick Gage Kelley, Sunny Consolvo, Joshua Mason, Elie\nBursztein, Zakir Durumeric, Kurt Thomas, and Michael Bailey. 2021. Designing\nToxic Content Classification for a Diversity of Perspectives.. In *SOUPS@ USENIX*\n*Security Symposium* . 299–318.\n\n\n9\n\n\n-----\n\nWebMedia’2024, Juiz de Fora, Brazil Resende et al.",
      "[42] Steven Loria. 2018. textblob Documentation. *Release 0.15* 2 (2018).",
      "[43] Patricia Georgiou Marie Pellat. 2018. Perspective Launches In Spanish With\nEl País. https://medium.com/jigsaw/perspective-launches-in-spanish-with-elpa%C3%ADs-dc2385d734b2.",
      "[44] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.\nDistributed representations of words and phrases and their compositionality. In\n*Advances in neural information processing systems* . 3111–3119.",
      "[45] Meena Devii Muralikumar, Yun Shan Yang, and David W McDonald. 2023. A\nHuman-Centered Evaluation of a Toxicity Detection API: Testing Transferability\nand Unpacking Latent Attributes. *ACM Transactions on Social Computing* (2023).",
      "[46] Lisa Nakamura. 2013. *Cybertypes: Race, ethnicity, and identity on the Internet* .\nRoutledge.",
      "[47] Nikolaos Pappas, Georgios Katsimpras, and Efstathios Stamatatos. 2013. Distinguishing the popularity between topics: a system for up-to-date opinion retrieval\nand mining in the web. In *Computational Linguistics and Intelligent Text Process-*\n*ing: 14th International Conference, CICLing 2013, Samos, Greece, March 24-30, 2013,*\n*Proceedings, Part II 14* . Springer, 197–209.",
      "[48] Daniel Borkan Patricia Georgiou, Marie Pellat. 2019. Parlons-en! Perspective and\nTune are now available in French. https://medium.com/jigsaw/perspective-tuneare-now-available-in-french-c4cf1ca198f2.",
      "[49] James W Pennebaker, Martha E Francis, and Roger J Booth. 2001. Linguistic\ninquiry and word count: LIWC 2001. *Mahway: Lawrence Erlbaum Associates* 71,\n2001 (2001), 2001.",
      "[50] Mark A Pitt, Keith Johnson, Elizabeth Hume, Scott Kiesling, and William Raymond. 2005. The Buckeye corpus of conversational speech: Labeling conventions\nand a test of transcriber reliability. *Speech Communication* 45, 1 (2005), 89–95.",
      "[51] Filipe N Ribeiro, Matheus Araújo, Pollyanna Gonçalves, Marcos André Gonçalves,\nand Fabrício Benevenuto. 2016. Sentibench-a benchmark comparison of state-ofthe-practice sentiment analysis methods. *EPJ Data Science* 5, 1 (2016), 1–29.",
      "[52] Max Roser, Hannah Ritchie, and Esteban Ortiz-Ospina. 2015. Internet. *Our World*\n*in Data* (2015). https://ourworldindata.org/internet.",
      "[53] Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, and Noah A Smith. 2019.\nThe risk of racial bias in hate speech detection. In *Proceedings of the 57th annual*\n*meeting of the association for computational linguistics* . 1668–1678.",
      "[54] Geneva Smitherman. 2000. *Black talk: Words and phrases from the hood to the*\n*amen corner* . Houghton Mifflin Harcourt.",
      "[55] Kaikai Song, Ting Yao, Qiang Ling, and Tao Mei. 2018. Boosting image sentiment\nanalysis with visual attention. *Neurocomputing* 312 (2018), 218–228.",
      "[56] Ezekiel Soremekun, Sakshi Udeshi, and Sudipta Chattopadhyay. 2022. Astraea:\nGrammar-based fairness testing. *IEEE Transactions on Software Engineering* 48,\n12 (2022), 5188–5211.",
      "[57] Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly Voll, and Manfred Stede.\n2011. Lexicon-based methods for sentiment analysis. *Computational linguistics*\n37, 2 (2011), 267–307.",
      "[58] Rachael Tatman. 2017. Gender and dialect bias in YouTube’s automatic captions.\nIn *Proceedings of the first ACL workshop on ethics in natural language processing* .\n53–59.",
      "[59] Mike Thelwall. 2014. Heart and soul: Sentiment strength detection in the social\nweb with sentistrength, 2017. *Cyberemotions: Collective emotions in cyberspace*\n(2014).",
      "[60] Pranav Narayanan Venkit and Shomir Wilson. 2021. Identification of bias against\npeople with disabilities in sentiment analysis and toxicity detection models. *arXiv*\n*preprint arXiv:2111.13259* (2021).",
      "[61] Hao Wang, Doğan Can, Abe Kazemzadeh, François Bar, and Shrikanth Narayanan.\n2012. A system for real-time twitter sentiment analysis of 2012 us presidential\nelection cycle. In *Proceedings of the ACL 2012 system demonstrations* . 115–120.",
      "[62] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou.\n2020. Minilm: Deep self-attention distillation for task-agnostic compression of\npre-trained transformers. *Advances in Neural Information Processing Systems* 33\n(2020), 5776–5788.",
      "[63] Maciej Widawski. 2015. *African American slang: A linguistic description* . Cambridge University Press.",
      "[64] Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Jason Kessler, Janyce\nWiebe, Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.\nOpinionFinder: A system for subjectivity analysis. In *Proceedings of HLT/EMNLP*\n*2005 Interactive Demonstrations* . 34–35.",
      "[65] Austin P Wright, Omar Shaikh, Haekyu Park, Will Epperson, Muhammed Ahmed,\nStephane Pinel, Duen Horng Chau, and Diyi Yang. 2021. RECAST: Enabling\nuser recourse and interpretability of toxicity detection models with interactive\nvisualization. *Proceedings of the ACM on Human-Computer Interaction* 5, CSCW1\n(2021), 1–26.",
      "[66] Ali Yadollahi, Ameneh Gholipour Shahraki, and Osmar R Zaiane. 2017. Current\nstate of text sentiment analysis from opinion to emotion mining. *ACM Computing*\n*Surveys (CSUR)* 50, 2 (2017), 1–33.",
      "[67] Min Yang, Qiang Qu, Xiaojun Chen, Chaoxue Guo, Ying Shen, and Kai Lei. 2018.\nFeature-enhanced attention network for target-dependent sentiment classification. *Neurocomputing* 307 (2018), 91–97.\n\n\n10\n\n\n-----"
    ],
    "text": "# **A Comprehensive View of the Biases of Toxicity and Sentiment** **Analysis Methods Towards Utterances with African American** **English Expressions**\n\n## Guilherme Andrade, Luiz Nery, Fabrício Benevenuto, Flavio Figueiredo\n#### UFMG guilherme.hra,luiznery,fabricio,flavio@dcc.ufmg.br\n### **ABSTRACT**\n\nLanguage is a dynamic aspect of our culture that changes when\nexpressed in different technologies and/or communities. On the Internet, social networks have enabled the diffusion and evolution of\ndifferent dialects, including African American English (AAE). However, this increased usage of different dialects is not without barriers.\nOne particular barrier, the focus of this paper, is on how sentiment\n(Vader, TextBlob, and Flair) and toxicity (Google’s Perspective and\nmodels from the open-source Detoxify) scoring methods present\nbiases towards utterances with AAE expressions. In particular, AI\ntools cannot understand the re-appropriation of the terms, leading\nto false positive scores and biases. Here, we study the bias of Toxicity and Sentiment Analysis models based on experiments performed\non Web-and spoken English datasets.\n### **KEYWORDS**\n\nAfrican American English, AAE, Bias, Toxicity, Sentiment\n### **1 INTRODUCTION**\n\nIn recent decades, we have witnessed a substantial rise in Internet\nusage. According to [ 52 ], Internet users increased from approximately 400 million in 2000 to 4.7 billion in 2020. With this increase in\nusage, it is natural that Web applications enable a wide diversity of\nsocial groups to interact among themselves and with other groups.\nSince such applications foster a more open and dynamic form of\nspeech, a natural increase in the written form of dialects that previously were predominantly seen in the spoken form [ 9 ] occurred.\nHowever, such massive amounts of textual data make manual content moderation impracticable. In other words, the heavy usage of\nsocial media has evidenced the urge for automatic moderation tools\nthat measure and moderate improper behavior online. One of the\nmain concerns is the public display of negative/toxic sentiments\nagainst a person or specific group, more drastically when the target\nis a minority group historically marked with discrimination and\nstereotypes. The necessity of dealing with the increasing number\nof deviating content has led many researchers and companies to\nuse AI tools to identify such events [51].\nConcurrently to the increase in Web usage, African-American\nEnglish (AAE) has gone from being seen as a marginalized dialect of\nEnglish to a consolidated vernacular of the language [ 28 ]. Like most\n\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\ndia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\n© 2024 SBC – Brazilian Computing Society.\nISSN 2966-2753\n\n## Savvas Zannettou\n#### TU Delft s.zannettou@tudelft.nl\n\ndialects, the AAE was initially heavily used in spoken form and\nhad the Web as a crucial influence on its emergence in the written\nform [ 9 ]. However, the Web is not only a disseminator of cultural\naspects of our society but also a vehicle where toxicity campaigns\nagainst African Americans are prone to occur. Even though several\nwebsites have well-defined community guidelines, user anonymity\nand lack of unaccountability leave room for misbehavior.\nThe aforementioned rise in AI moderation tools (such as Google’s\nPerspective [ 37 ] and others [ 23, 51, 66 ]) aim to reduce the amount\nof negative or toxic utterances online. Overall, such tools rely on\nMachine Learning (ML) models that help determine proper and improper utterances. Nevertheless, as previous research has discussed,\nautomatic content moderation can backfire and present biases towards minorities [ 11, 34, 53, 60 ]. For instance, a tool for toxicity\nanalysis may present high scores for non-toxic AAE sentences for\nno apparent reason. We show examples of toxicity and sentiment\nanalysis models employed in online text to depict this issue. We\npoint out that it is quite easy to find problematic utterances when\nusing slang terms such as *“n****s”* . In Table 1, we contrast three\npairs of sentences based on their toxicity/negative sentiment levels.\n*Why does the problem arise?* From a linguistic perspective, dialects may inherently manifest behaviors and cultural aspects of\nthe groups in which they were created [ 5, 19 – 21 ]. Terms such as\n“n****r” are problematic for AI models since both the term and its\nvariations have a historical pejorative usage [1] . Nevertheless, this\nsame term was re-appropriated by the black community, so its use\nceased to be considered problematic when used by people inside\nthe black community. Suppose such a fine line between causal\nspeaking and offensive discourse is problematic from a human and\ncomputational perspective. In that case, these interpretations are\nconfounding to automatic content moderation tools. In other words,\ntoxicity/sentiment analysis tools are usually developed using manual rules or supervised ML techniques that employ human-labeled\ndata to extract patterns. The disparate treatment embodied by machine learning models usually replicates discrimination patterns\nhistorically practiced by humans when interacting with processes\nin the real world. Due to biases in this process, a lack of context\nleads models to a concerning scenario where minorities do not\nreceive equal treatment [1, 14, 26, 58].\nThis discussion leads to the research question behind our paper: *Is there a systematic bias on toxicity/sentiment analysis towards*\n*AAE?* To better understand this issue, we present a broad-scale analysis. To do so, we manually curate a dataset of African American\nEnglish Expressions [ 54, 63 ] (these sources were recommended by\n\n1 https://en.wikipedia.org/wiki/Nigga\n\n\n1\n\n\n-----\n\nWebMedia’2024, Juiz de Fora, Brazil Resende et al.\n\n\n**Score** All my friends on\nthe porch and never\nin the house\n\n\nAll my n****s on\nthe porch and neva\nina house\n\n\nYou’re white You’re black I can’t forget Cant fuhgit you\n\nyou\n\n\n\nthe organizers of the to-be-released Oxford Dictionary of African\nAmerican English [2] ). These expressions complement four different\nutterance datasets with some demographic information on race (i.e.,\ninterviews from African American individuals, AAE utterance vs\nnon-AAE utterance labels, and author-supplied labels). *We empha-*\n*size that we cannot state how a speaker identifies regarding her/his*\n*race for some datasets. AAE may also be employed by non-African*\n*Americans. However, we interpret our results using AAE expressions*\n*from our expressions dataset (made available)* .\nThe models we study can be divided into toxicity (Google’s Perspective [ 37 ], Detoxify, and Detoxify Unbiased [ 30 ]), and sentiment\nanalysis (Flair [ 3 ], TextBlob [ 42 ], and Vader [ 35 ]) models, but also\ncan be segmented into machine learning-based (Google’s Perspective, Flair, Detoxify, and Detoxify Unbiased), and lexical, or rule,\nbased (Vader, and TextBlob) models. Our main contributions are:\n\n(1) We present a broad-scale analysis of biases toward utterances\nwith AAE expressions in six out-of-the-shelf models;\n(2) To do so, we focus on unveiling if there is a systematic tendency for AAE utterances to be deemed more toxic or negative sentiment by several models of datasets of different\nnatures (tweets, closed captions, and spoken interviews). To\nreach our results, we manually transcribed a dictionary of\nAAE expressions and used the number of such expressions\nin an utterance as an explanatory feature;\n(3) Other control features include Lexical Analysis [ 49 ], and\ngrammar-based Part of Speech Tagging PoS) labels for words\nin utterances. Overall, we discuss which characteristics of\nthe utterance lead the model to deem it as toxic or of negative\nsentiment. The number of AAE expressions is a recurrent\nstatistically significant feature;\n(4) Using recent language models [ 62 ], we show that in some\ndatasets, even utterances from African-American (AA) speakers that have a similar meaning to those from non-AA speakers, models will, in several cases, score the sentence from\nnon-Whites with more toxic/more negative scores.\n\nBefore continuing, we point out that our work is not the first\nto study the biases of similar models towards minorities [1, 14, 26,\n58 ]. However, we complement prior endeavors with broader-scale\nanalysis. Previous methods focus on a single dataset or model and do\n\n2 https://hutchinscenter.fas.harvard.edu/odaae\n\n\nnot perform the lexical and grammar-based analysis we do here. We\nfurther point out that our goal in this paper is not to pinpoint models\nwith the best accuracy. We focus on showing AAEs and comparing\nif there is a tendency across several models. The datasets where\nwe show this issue range from online texts from Twitter [ 9, 10 ],\nspoken English datasets gathered by linguists [ 38, 50 ], and online\nsingle speaker English from YouTube movie reviews. The YouTube\ndataset (see Section 3) was a manual effort toward gathering data\nwith fewer confounding factors (i.e., single-speaker videos). This\ndataset is made available to the community to improve the current\nand yet-to-come NLP models [3] .\nOur results show that biases are more prominent on online\ndatasets, such as Twitter and YouTube, and less strongly but still\npresent in spoken English interviews. Our research shows that\nusing AAE expressions will likely lead to sentences being deemed\nmore toxic, even when sentences are similar to those with non-AAE\nexpressions. Overall, system developers may use these findings to\ndetermine what model type shall be employed (sentiment analysis\nvs. toxicity scoring) or whether ML vs. lexical-based models are\nmore adequate for their application. More importantly, our findings\nshow that even considering “unbiased models” [ 30 ], ML models still\npresent a bias towards utterances with AAE expressions. Indicating\nthat AAE speakers may still face unwarranted moderation online.\n### **2 BACKGROUND AND RELATED WORK**\n\nThis section presents an overview of the literature. We begin discussing the sociolinguistics of English online, as the increased usage of AAE expressions online is a primary motivation behind our\nwork. Subsequently, we also discuss the motivation behind sentiment analysis tools, available alternatives, their major strengths and\nshortcomings, and how toxicity relates to sentiment analysis. Next,\nwe discuss bias in machine learning methods and how they can\nnegatively influence individuals online and suppress the discourse\nof minority groups. Finally, we focus on those papers most related\nto ours and present a statement on the novelty of our research.\n\n3 https://anonymous.4open.science/r/aae_bias-D396/data/aae_terms_black_talk.\nyaml\n\n\n2\n\n\n-----\n\nA Comprehensive View of Biases Towards Utterances with African American English Expressions WebMedia’2024, Juiz de Fora, Brazil\n\n### **2.1 Sociolinguistics of AAE Online**\n\nAs a research field, sociolinguistics focuses on studying how social context affects the usage and evolution of language. Overall,\nhumans take part in several speech communities throughout their\nlives [ 21 ], and even the same human being may communicate in\ndifferent variations of English depending on the community she/he\nis interacting with. With the rise of the Social Web over the 2000’s\nand 2010’s, the field also focused on how Web communities affect\nlanguage [ 20, 21 ]. In particular, Friedrich and Figueiredo [ 21 ] argue\nthat hundreds of years after the invention of the printing press, the\nwritten usage of English as a language appeared to be “evolving” to\na standard or uniform English. However, with the Web, community\nand individualized language use has increased over recent years.\nAAE is an example of such a case [ 9, 20, 21 ], where the dialect has\nexperienced a rise in usage (particularly online) in recent years.\nOne example of the expansion of AAE in recent years comes\nfrom the movement known as *signifyin’* [ 19 ]. In other words, when\n*signifyin’* one expresses their race via particular dialects, such as\nAAE, on social media. This expression is utilized to resist the oppression present in one’s day lives [ 19, 46 ]. Regarding how AAE\nis spread online, some authors argue that the dialect spreads initially from Web users from large cities to smaller communities in\nwave-like, or viral, patterns [ 17 ]. Frierich and Figueiredo state that:\n“With the Internet, we have witnessed a change in this scenario.\nGender and racial/ethnic activism have become quite strong online\nand have served not only to spread the debates but also to add new\nlayers to them – such as the complex construction of identities in\ncyberspace. And again, we must say, English has been quite present\nin this new picture, mainly because of its lingua franca status.”\nFor several decades, the AAE dialect has also been studied offline [ 63 ]. Over the years, some attempts have been made to catalog\ndifferent expressions from the dialect [ 8, 15, 54 ]. Oxford and Harvard are also organizing a dictionary called the Oxford Dictionary\nof African American English (ODAEE).\nGiven that language constantly evolves, we aimed to collect\na recent corpus of AAE expressions. To do so, we contacted the\norganizers of the ODAAE, asking if they could share the list of\nexpressions used in their dictionary. The organizers kindly denied\nit because ODAEE is still a work in progress. Nevertheless, they\ndid suggest that we use expressions from Smitherman’s Black Talk\nDictionary [ 54 ] as it is a large and somewhat recent corpus. In\nour research, we manually transcribed every expression from this\ndictionary as our AAE expression list.\n### **2.2 Sentiment Analysis and Toxicity Models**\n\nSentiment Analysis identifies sentiments and quantifies their intensity (positive or negative sentiment) in utterances. Current Sentiment Analysis models may be classified into two major categories,\nnamely, machine learning-based (ML models) and lexical-based\napproaches, described below. These methods have been employed\nsince the mid-2000s, and one of the major motivations behind building such approaches was to rate user reviews online.\nML-based Sentiment Analysis models [ 3, 24, 55, 61, 67 ] are built\nover a sample of data points comprising as many examples as possible from positive and negative sentences. Usually, the learning\n\n\nprocedure targets data drawn from a context of interest (e.g., Twitter, Facebook, Marketplaces, etc.), and humans manually label this\ndataset to train models. This family of methods often benefits from\ncomplex word representations and can grasp deeper relationships\nimplied in daily conversations. Lexical-based approaches [ 33, 35,\n49, 57 ], on the other hand, begin by listing seed words considered\nto be representatives of groups of emotions. Once the seed list\nis complete, it is incremented with similar words and synonyms.\nSuch approaches must actively deal with normal word usage that\nmay change the intent/intensity of the sentence, such as negations,\npunctuations, capitalization, etc. Since this approach is based on the\nhuman understanding/application of terms and expressions, their\nperformance on novel datasets may have less statistical variance.\nCompared to lexical-based methods, the ML models rely on the\nvector representation of terms and utterances [ 44 ]. Such vector\nrepresentations are used as inputs to train supervised methods. Collecting high-quality labels to train such models is a difficult-to-reach\npre-requisite (discussed below). On the other hand, lexical-based\napproaches need to explicitly address negations, punctuations, outof-vocabulary occurrences, and more complex relations between\nterms [ 51 ]. To address the gaps left by each family of methods,\nauthors have also proposed hybrid solutions [47, 59, 64].\nMore recently, we have seen a rise in Toxicity Classification\n(compared to Sentiment Analysis) models [ 30, 37, 40 ]. Most, if not\nall, of these approaches are ML-based. Toxic speech is usually considered to be an umbrella term that comprises hate speech, abusive\nlanguage, racism, and so on [ 31, 41 ]. Despite the efforts to address\ntoxic speech, there is not a clear agreement about what it means\nfor a sentence to be toxic. Dixon et al [ 16 ] defines toxicity as rude,\ndisrespectful, or unreasonable language.\nDue to the lack of consensus on toxicity, the inherent ambiguity\nof labeling sentences presents an issue to ML models. The vast\nmajority of datasets use human labelers, which are influenced by\ntheir previous experiences and, most of the time, do not have access\nto the underlying context from which the respective sentence was\ndrawn. This subjectivity and lack of context may cause considerable\nlabeling issues. For example, Kumar et al. [ 41 ] state that people who\nhave suffered harassment in the past are more prone to label random\nsentences from some social networks as toxic than those who did\nnot face such problems. Maybe due to its less restrictive definition\nand to the capacity to encompass many types of harassment, toxicity\nmodels are actively used in practice to moderate discourse in many\nplatforms [2, 43, 48]; however, with some known bias problems.\n### **2.3 Biases Towards Minorities Online**\n\nWe now discuss prior work on the biases of Web datasets and AI\nmodels. Starting from Jia et al. [ 36 ], the authors investigated the\nproportions in which men and women appeared in news articles’\nimages. The authors found that men are considerably more frequently represented than women. Garcia et al. [ 22 ] also described a\nconsistent bias towards men in Twitter content. On Twitter, female\nusers tend to describe more events in which men play important\nroles. Babaeianjelodar et al. [ 4 ] explored the nuances of gender\nbiases over ML models. In all datasets considered, models perform\ndisparately against unprivileged subgroups. Similar findings were\nraised by several other authors considering countries [ 27 ], age [ 14 ],\n\n\n3\n\n\n-----\n\nWebMedia’2024, Juiz de Fora, Brazil Resende et al.\n\n\nreligion [ 1 ], and sexual orientation [ 16 ]. Regarding dialects, Blodgett et al. [ 9 ] studied how language characteristics can change\nconsiderably within the same country. The work focuses on learning distinguishable features between Mainstream American English\n(MAE) and AAE with a geographic context. In [ 25 ], the authors also\npresent another clear differentiation between English focusing on\nDrag queens. Here, the authors find that transgender individuals\nhave a speaking characteristic consistently seen as more toxic.\nAs Bamman et al. [ 7 ] states, language is always situated within\na context. Neglecting this surrounding context leads to disparate\ntreatments. For example, transgender individuals may be speaking\na dialect deemed toxic if used by someone outside of the community.\nHowever, this may be a defense mechanism to cope with tough\nsituations imposed by society [ 25 ]. Similar language signals are\npassive within the black community and the AAE dialect. Studies\nwere already performed to comprehend and measure how much\nML models are biased against AAE speakers [6, 9, 53].\nOverall, we can state that nowadays, it is not hard to find discrimination episodes involving AI models [ 1, 14, 16, 34 ]. For example,\nAbid et al. [ 1 ] interacted with a conversational artificial intelligence\nmodel touching religion-related subjects and noting the inner associations with the topic. Finally, they found a consistent bias associating Muslims with terrorists (in 23% of the test cases) and the Jewish\nwith money (in 5% of the test cases). In the opposite direction, mitigating such biases are also common [ 4, 9, 11, 16, 31 ]. Nevertheless,\nas studied by Gonen et al. [ 26 ], persistent bias may stick with the\nmodel even after active effort has been applied to remove it. We also\nobserve this as we use unbiased versions of recent models. Since\n\nthe complexity of the ML model has increased in the last few years,\nwe could expect the bias to be more elaborate and complex to fight\nagainst. This leads us to the problem of using biased models for\nsensible tasks that may perpetrate harmful behavior.\n### **2.4 Related Work and Research Novelty**\n\nWe now detail prior work that is most related to ours (e.g., evaluating and unveiling biases in similar models) [ 13, 18, 29, 32, 39, 45, 53,\n56, 65 ]. Before doing so, we initially point out that the studying the\nbiases of NLP models towards race is a well-established research\n\ntopic and the survey of Field et al. [ 18 ] presents a recent overview\non this topic. The authors of this survey analyzed 79 different papers on race and NLP systems. Overall, the consensus is that NLP\nstill encodes racial biases (something we also observe) and that race\nis commonly studied as a limited categorical variable. Here, we take\na step towards a broader view of the issues by incorporating in our\nstudy (1) a novel list of AAE expressions, (2) grammatical features\n(PoS), and (3) linguistic features (LIWC) to understand biases.\nOne of the vanguard efforts of looking into the biases of toxic\nscoring APIs (Perspective in particular) was performed by Sap et\nal. [ 53 ]. In contrast, Kiritchenko et al. were among the first to\nstudy the bias of sentiment analysis models [ 39 ]. Starting with the\nformer, the authors discuss how datasets are biased and how models\npropagate such biases. However, unlike our work, the authors only\nstudy Twitter datasets and do not present statistical analysis on\nhow utterance features (grammar, linguistics, and usage of AAE\nexpressions) may explain biases. The authors also only focus on\nPerspective as an out-of-the-shelf model. The former, focused on\n\n\nsentiment analysis, compares over two hundred models from a wellestablished information retrieval sentiment classification challenge.\nHowever, the author does not use real-world datasets as we do and\nfocuses their analysis on sentences of similar meaning but with\nsmall changes in words related to gender, occupation, and race.\nThis approach is similar to the adversarial attacks described next.\nAlthough not focused on measuring biases, the work of Hosseini\net al. [ 32 ], and Gröndahl et al. [ 29 ] both show how small changes\nto a sentence will change model scores. We present a different view\non this finding by incorporating semantic similarity using language\nmodels [ 62 ] and finding that even expressions that are semantically\nclose to one another will differ in scores depending on the number\nof AAE expressions used. Similarly, Davidson et al. [13] discussed\nsome challenges in differentiating hate speech from other offenses.\nThis provides evidence of how language is nuanced, and models\nhave problems with these small nuances. Wright et al. [ 65 ] provides\na tool called RECAST that helps users pinpoint words that need\nchanging in order to reduce the toxicity of a score.\nRegarding human evaluation of models, we refer to the recent\neffort of Muralikumar et al. [ 45 ]. Here, the authors evaluate Perspective and contrast how human scores align with the model. Overall,\nthe score from Perspective is a good predictor (based on a Logistic\nregression) of human labels (“toxic”, “hard to say”, “non-toxic”).\nHowever, agreement is not always present, with the model still\nmaking mistakes. The authors suggest that using a score cut-off of\n0.55, i.e. if the model scores over this value classify the utterance as\ntoxic, will make model outcomes agree with users 50%.\nTesting different definitions of fairness is also an active field of\nresearch [ 12 ], with software tools being developed just for this task\nin NLP models [ 56 ]. We complement these efforts by showing that\nout-of-the-shelf models and API still require further testing.\nOur research differs from previous works by investigating biases in models of different families (ML-based and lexical-based\nmethods) and throughout four datasets representing different contexts (in-person conversations, single-speaker movie reviews, and\npersonal social media posts). Here, we focus on out-of-the-shelve\nmethods and those already applied in real-world forums. Unlike\nprior work, we employ different statistical features of utterances to\nshow how the presence of AAE expressions will lead models to rate\nsentences as more toxic or of negative sentiment. Our statistical\nanalysis also provides insights into what features models explore to\nreach their scores. Finally, the datasets we employed were collected\nto isolate confounding factors. That is: (1) we do not use any dataset\nused to train models or with toxicity/sentiment labels; (2) one of\nour datasets is focused on single-speaker movie reviews, controlling for discourse as a confounder; (3) we also compare models on\nwell-established linguistic datasets on single-person interviews.\n### **3 DATASETS AND PRE-PROCESSING**\n\nWe explore four datasets of different natures to understand the\nextent of biases in toxicity/sentiment analysis models and when\nthey present themselves more strongly. Initially, we use the Twitter\nAAE dataset [ 9, 10 ]. This dataset is interesting as it contains tweets\nclassified as AAE or Mainstream American English (MAE). Tweets\nwere classified using an ML model, and we consider a subset of\ntweets where the model predicted over 80% probability for each\n\n\n4\n\n\n-----\n\nA Comprehensive View of Biases Towards Utterances with African American English Expressions WebMedia’2024, Juiz de Fora, Brazil\n\n\n**Dataset** **Demographic** **# Documents** **# Sentences** **# AAE Expr.** **AAE Expr. per Doc.**\n\n\nYouTube AA Speaker [∗] 150 17828 18308 122.05 (\n\nnon AA Speaker [∗] 484 41464 42729\n\nTwitter AAE Tweet 250 250 372 1.49 (\n\nMAE Teet 250 250 259\n\nCORAAL AA 142 64493 61651 434.16 (\n\n\nAA Speaker [∗] 150 17828 18308 122.05 ( 43%)\n\nnon AA Speaker [∗] 484 41464 42729 85.67\n\nAAE Tweet 250 250 372 1.49 ( 43%)\n\nMAE Teet 250 250 259 1.04\n\n\nCORAAL AA 142 64493 61651 434.16 ( 9%)\n\nBuckeye Caucasian 39 19304 18712 479.79\n**Table 2: Datasets statistics.** [∗] **Indicates that two (agreeing)**\n**authors inferred the demographic. Still, our analysis does**\n**not use it as a variable (we rely on the # of AAE Expr.).**\n\n\nclass. This is a well-established dataset for AAE utterances used by\nother endeavors [ 53 ]. Twitter is one of the major platforms where\none would expect that toxicity and sentiment analysis models could\nmitigate unwanted behavior. On the negative side, as the dataset\ncontains general Tweets, it does not control for confounding factors\nsuch as dialogues, debates, and potentially controversial topics.\nThus, we complement this research with two other datasets.\nOur YouTube dataset comprises subtitles extracted from YouTube\nmovie reviews with a single speaker discoursing about a unique\ntopic per video. The topics are movies from Rotten Tomatoes 100\nBest Movies of All Time. We targeted single-speaker videos to control for any confounding variables that may appear with dialogues.\nAlso, we focus on acclaimed films [4] to control for the possible negative influence of bad content (speakers may still dislike the movies,\nthough). YouTube aims to control both content and dialogue.\nFinally, we explore the linguistic Corpus of Regional African\nAmerican Language (CORAAL) [ 38 ] as representations of spoken\nAfrican-American English. For comparisons, we employ the Buckeye [ 50 ] dataset focused on Caucasian [5] speakers from central Ohio.\nBoth datasets are focused on spoken interviews with transcripts.\nBuckeye was recommended to us by the curators of CORAAL.\nWe now discuss how we identified African-American English\nexpressions (AAE expressions). As stated, we manually transcribed\nthe Black Talk dictionary [ 54 ]. Since AAE first emerged as an oral\nlanguage, the main intent of this dictionary was not to define the\netymological history of terms; instead, it concentrates on the meanings and significance of expressions. The organizers of the not-yetpublished Oxford Dictionary of African American English referred\nus to the Black Talk dictionary as a reliable source.\nThe Black Talk dictionary comprises more than 1800 entries.\nSince some entries are sentences instead of single terms, they may\napply to different pronouns. In such cases, the possible use cases are\nlisted. For example, “BREAK HIM/HER/THEM OFF SOMETHING”\nbecomes three expressions. Our transcription of the entries considers every possible combination presented.\nIn Table 2, we present a summary of our datasets in the number\nof sentences (or utterances), number of words (non-unique), and\nnumber of African-American English expressions present. Over the\nnext few subsections, we now detail each dataset.\n**Twitter:** The Twitter dataset comes from the Twitter AAE [6]\n\nwebsite. To create the dataset, the authors [ 9, 10 ] developed a Latent\nDirichlet Allocation (LDA) based topic model that took into account\nboth the frequency of common terms used in AAE as well as Census\ndata. An initial race estimate is performed based on the location\n\n4 https://www.rottentomatoes.com/top/bestofrt/\n5 https://buckeyecorpus.osu.edu/php/corpusInfo.php\n6 http://slanglab.cs.umass.edu/TwitterAAE/\n\n\nfrom which the account was tweeted. This information is combined\nwith the presence of key terms to derive different latent topics for\nthe corpus. Topics were then explored to label AAE and non-AAE.\nAlthough the authors label over 80 *,* 000 tweets, we focus on a\nsmaller sample of 500 tweets that the authors manually inspected.\nThese tweets were manually labeled with PoS tags to derive an\nAfrican-American English PoS model. According to the authors,\nmore than 18% of the terms used within the African-American\n\ntweets are not in the standard English dictionary. It is also very\ncommon to find words written in their phonological style in AAE e.g. tha (the), iont (I don’t), ova (over), and so on - while the contrary\nwas found to never happen in the Non-AAE tweets.\n**YouTube:** The YouTube dataset is a collection of subtitles from\n\nYouTube movie review videos. A single speaker talks to the audience\nabout a movie production listed among the most relevant movies\never. We considered Rotten Tomatoes’ top 100 best movies of all\ntime ranking due to their prestige among the audience and because\nthey have a higher probability of being well-spoken in a review. For\neach of the top 50 movies from the ranking, two authors manually\nsearched and cataloged as many videos as possible. The authors determined Demographic labels, namely, African-American Women,\nAfrican-American Men, non-African-American Women, and nonAfrican-American Men, in order of appearance when querying the\nmovie name on YouTube. Since YouTube doesn’t naturally disclose\ndemographic information about its users, we had to restrict our\nsearch only to producers who happened to appear on the screen at\nleast once throughout the entire video. The list of movies and the\nrespective YouTube channels is available [7] .\nWhen publishing videos on YouTube, the creators can either\nexplicitly inform their videos’ subtitles or let the YouTube transcription model automatically caption them. Nonetheless, differently from manually informed subtitles, the captioning mechanism,\nby default [8] . Only 21 videos had manual subtitles in our dataset [9] .\nThus, for fair comparisons, we use automatic transcriptions as\nthese are available in **every** video. Finally, it is important to state\nthat transcriptions are not punctuated by YouTube. We use ML\nmodels to correct this behavior on the YouTube dataset and the\n\nCORAAL/Buckeye dataset (below).\nConsidering the observational nature of our study, an extensive\neffort was applied to control the confounding variables’ effect on the\nconclusions. The selection of the most prestigious movies of all time\nwas an attempt to reduce the chance of having negative reviews,\nwhich would comprise higher scores in the toxicity analysis. We\nalso tried to find at least one single-speaker video review for every\nmovie to reduce any sampling bias impact. More importantly, the\nreviews’ first-person nature helps eliminate the possibility of other\npeople’s opinions influencing the argumentative paths.\n*On author inferred demographic variables:* It is worth noticing\nthat identifying race/gender is subjective and prone to errors – we\nonly have our view and not the content producer’s identification.\nThus, we avoid using YouTube demographic variables as input in\n**any** analysis. We employed author-inferred demographics to collect\na diverse (to the extent possible) dataset of utterances. Instead,\n\n7 https://anonymous.4open.science/r/aae_bias-D396/data/youtube_data_description.\n\ncsv\n8 https://support.google.com/youtube/thread/70343381\n9 Results do not change if we look only into these few videos.\n\n\n5\n\n\n-----\n\nWebMedia’2024, Juiz de Fora, Brazil Resende et al.\n\n\nwe rely on the # of AAE Expressions as an explanatory variable.\nNevertheless, we do see an increase in AAE expressions based on\nthe inferred demographic on YouTube (see Table 2).\n**CORAAL and Buckeye:** The Corpus of Regional African American Language [ 38 ] is a long-term corpus developed and maintained\nby the University of Oregon with the support of the National Science\nFoundation. The dataset comprises more than 150 socio-linguistic\ninterviews with African-American English speakers born between\n1891 and 2005. The dataset contains the orthographic transcriptions\nof interviews, together with the person’s age, gender, and city they\nlive in. Thus, each interview from the corpus encompasses many\nsubjects from a given city/community.\nUnlike the YouTube data, the transcriptions here represent the\nentire sentence, accounting for complete punctuation, line-level\nnotes, and even non-linguistic sounds. Beyond that, the data also\ntracks the interviewer’s voice in the dialog. The interviews allow\nthe speakers to talk freely about different topics, an interesting\nfeature that emulates the diversity of daily interactions and mood\nvariations. The dataset aggregates five major sub-corpora from\ndifferent locations in the United States of America, namely, Atlanta (2017), Washington (1968 and 2016), Lower East Side (2009),\nPrinceville (2004), Rochester (2016), and Valdosta (2017).\nThe Buckeye [ 50 ] corpus is an effort started in 1999 and supported by the National Institute on Deafness and Other Communication Disorders and the Office of Research at Ohio State University.\nThe initial goal was to gather approximately 300 *,* 000 words of\nspeech conversation from central Ohio speakers, keeping track of\ntime and phonetic information. To reach that objective, researchers\nselected a group of 40 middle-class Caucasian speakers.\nSimilar to the YouTube dataset, Buckeye sentences are not punctuated. However, instead of automatically generated captions, this\ncorpus employed transcribers who were explicitly instructed not\nto use punctuation within the utterances and not to try to correct\npossible speech “errors” (we segmented sentences ourselves).\n\n*3.0.1* *Sentence Segmentation.* Except for CORAAL, the datasets\ndon’t necessarily follow the correct orthographic rules about punctuation. Considering the other two transcripted corpus (i.e. YouTube\nand Buckeye), we should expect their sentences to be segmented not\naccording to their inherent meaning but to silent intervals (not necessarily long ones) after a continuous pronunciation of words. Such\nsegmentation can drastically misrepresent the sentences’ meanings\nand consequently derive misleading conclusions about the data.\nTo reduce the impacts of incorrect segmentation in later analysis,\nwe employed a machine learning-based segmentation to all corpus,\nexcept the one from Twitter. We believe tweets are self-contained\nmessages where punctuation is not necessarily crucial to the audience’s understanding. Consequently, segmentation is not necessary.\nOn the other hand, we segment the only correctly punctuated corpus, CORAAL. Since we intend to compare the CORAAL dataset\ndirectly against Buckeye’s, we should try to reduce the confounding factors (segmentation). The segmentation task was performed\nusing NVIDIA’s NeMo Toolkit [10] .\n\n*3.0.2* *On the Impact of Swear Words.* We executed two versions of\nour experiments, one considering utterances with swear words and\n\n10 https://github.com/NVIDIA/NeMo\n\n\nanother without. The swear words we considered were taken from\nthe No Swearing project [11], a cooperative effort to help programmers\nremove unwanted language from their applications. At the time of\nwriting, the project listed 363 curse words. Overall, we found no\nstatistical difference in our results nor any significant difference in\nour figures. Here, we present results with swear words.\n\n*3.0.3* *Linguistic Features.* One of the most relevant aspects of our\nanalysis is directly derived from controlling for linguistic and grammatical features from the available utterances. Thus, our research\nfocuses on word classes, or Part-of-Speech (PoS), (e.g., Verb, Noun,\nAdjective, etc.) and language dimensions (e.g., Anger, Hate, etc.).\nThe PoS features consider the function of each word in the sen\ntence. The word *smile* can be considered a verb; however, it can\nalso be considered an adjective when used in certain scenarios, as\nin “ *The smiling baby is really cute* ”. This information can help us understand the sentence’s composition regarding word classes. Only\non TwitterAEE is it that we have manual PoS features. For such\n\na reason and for fair comparisons, we use automatic PoS features\nin all datasets. Thus, to classify the tokens according to their PoS\ncategories, we employ a black-box model [12] . We also point out that\nsimilar results arise when using manual PoS tags on TwitterAAE.\nTo define linguistic features, we used the Linguistic Inquiry and\nWord Count (LIWC) software [ 49 ] in its 2015 release. LIWC is\na research effort that maps words to psychological features (i.e.,\nlanguage dimensions) of speech. A single word may be assigned\nto as many suitable categories as necessary. For example, the word\n*cried* is a 10-categories term (e.g, Affect, Positive Tone, Emotion,\nNegative Emotion, Sad Emotion, Verbs, Past Focus, etc.). Features\nare computed based on the number of occurrences.\n### **4 RESULTS**\n\nWe now present our results. Initially, we compare the model scores\nfor utterances with and without AAE expressions. Next, we determine which factors impact the outcome of different methods. Our\nfinal analysis compares semantically similar sentences.\n### **4.1 Scores Per Usage of AAE Expressions**\n\nFigure 1a compares toxicity/sentiment scores for utterances with\nand without AEE expressions. The figure shows the complementary\ncumulative distributions (CCDF) considering the number of AAE\nexpressions on the utterance. That is, the x-axis of the figure shows\nthe score, whereas the y-axis captures the fraction of sentences for\nthat with scores greater than the one on the x-axis. In this and the\nother CDFs presented in our study, some care must be taken when\ninterpreting results from Textblob and Vader. For these methods,\nnegative values point toward negative sentiment, whereas positive\nvalues point toward positive sentiment. For the other models, 0\ncommonly indicates a not-negative (Vader) sentiment or non-toxic,\nwhereas 1 is a negative sentiment or toxic utterance. This difference comes from sentiment analysis methods commonly measure\n*polarity* from -1 to 1. Unlike Textblob and Flair, Vader returns the\nprobability of a negative, positive, or neutral score (adding up to\none). Here, we focus on the negative probability.\n\n11 https://www.noswearing.com/\n12 https://spacy.io\n\n\n6\n\n\n-----\n\nA Comprehensive View of Biases Towards Utterances with African American English Expressions WebMedia’2024, Juiz de Fora, Brazil\n\n\n|Col1|Col2|Flair|Ma (ML)|ajority of uttr|\n|---|---|---|---|---|\n||||ar eM pa|ojosritiitvye of uttr|\n||Negativ|e Sent.|Positive|Sent.|\n||||||\n||ar eM naejograittiyv eo|f uttr|||\n\n\n1.0 0.5 0.0 0.5 1.0\nx - Polarity\n\n|Col1|Col2|Flair|Ma (ML)|ajority of uttr|\n|---|---|---|---|---|\n||||ar eM pa|ojosritiitvye of uttr|\n||||||\n||Negativ|e Sent.|Positive|Sent.|\n||||||\n||ar eM naejograittiyv eo|f uttr|||\n\n\n\n1.0 0.5 0.0 0.5 1.0\nx - Polarity\n\n|Col1|Col2|Flair|Ma (ML)|ajority of uttr|\n|---|---|---|---|---|\n||||ar eM pa|ojosritiitvye of uttr|\n||||||\n||Negativ|e Sent.|Positive|Sent.|\n||||||\n||ar eM naejograittiyv eo|f uttr|||\n\n\n\n1.0 0.5 0.0 0.5 1.0\nx - Polarity\n\n|0|Col2|Textblob|Ma b (L)|ajority of uttr|\n|---|---|---|---|---|\n|8 6 4 2 0|||ar eM pa|ojosritiitvye of uttr|\n||Negativ|e Sent.|Positive|Sent.|\n||||||\n||ar eM naejograittiyv eo|f uttr|||\n\n|Col1|Col2|Col3|Col4|ar eM na|ejogr.ity of uttr|\n|---|---|---|---|---|---|\n||Bpb|reiaefoss rem en ot0r .e 20||0 A 1 or|AE Expr. more|\n||p|rob.||3 or 5 or|more more|\n|||||||\n|||||||\n\n|Col1|Col2|Col3|ha Mvea|hjoigrihty s ocof ruetstr.|Col6|\n|---|---|---|---|---|---|\n||||CM20uu2tr- 3aolfifk suu|mgagre sette adl .by:||\n|||||||\n|||||||\n\n|Col1|Col2|Col3|ha Mvea|hjoigrihty s ocof ruetstr.|\n|---|---|---|---|---|\n||||CM20uu2tr- 3aolfifk suu|mgagre sette adl .by:|\n||||||\n||||||\n\n|Col1|Col2|Col3|ha Mvea|hjoigrihty s ocof ruetstr.|\n|---|---|---|---|---|\n||||CM20uu2tr- 3aolfifk suu|mgagre sette adl .by:|\n||||||\n||||||\n\n|0|Col2|Textblob|Ma b (L)|ajority of uttr|\n|---|---|---|---|---|\n|8 6 4 2 0|||ar eM pa|ojosritiitvye of uttr|\n||||||\n||Negativ|e Sent.|Positive|Sent.|\n||||||\n||ar eM naejograittiyv eo|f uttr|||\n\n|Col1|Col2|Col3|ar eM na|ejogr.ity of uttr|\n|---|---|---|---|---|\n||B|ias more|||\n||||0 A|AE Expr.|\n||pbp|rere ofos bre .en t0.20|1 or 3 or 5 or|more more more|\n||||7 or|more|\n||||||\n||||||\n\n|Col1|Col2|Col3|ha Mvea|hjoigrihty s ocof ruetstr.|\n|---|---|---|---|---|\n||||Cut-off su|ggested by:|\n||||M20u2r3aliku|mar et al.|\n||||||\n||||||\n\n|Col1|Col2|Col3|ha Mvea|hjoigrihty s ocof ruetstr.|\n|---|---|---|---|---|\n||||Cut-off su|ggested by:|\n||||M20u2r3aliku|mar et al.|\n||||||\n||||||\n\n|Col1|Col2|Col3|ha Mvea|hjoigrihty s ocof ruetstr.|\n|---|---|---|---|---|\n||||Cut-off su|ggested by:|\n||||M20u2r3aliku|mar et al.|\n||||||\n||||||\n\n|0|Col2|Textblob|Ma b (L)|ajority of uttr|\n|---|---|---|---|---|\n|8 6 4 2 0|||ar eM pa|ojosritiitvye of uttr|\n||||||\n||Negativ|e Sent.|Positive|Sent.|\n||||||\n||ar eM naejograittiyv eo|f uttr|||\n\n|Col1|Col2|Col3|ar eM na|ejogr.ity of uttr|\n|---|---|---|---|---|\n||B|ias more|||\n||||0 A|AE Expr.|\n||pbp|rere ofos bre .en t0.20|1 or 3 or 5 or|more more more|\n||||7 or|more|\n||||||\n||||||\n\n|Col1|Col2|Col3|ha Mvea|hjoigrihty s ocof ruetstr.|\n|---|---|---|---|---|\n||||Cut-off su|ggested by:|\n||||M20u2r3aliku|mar et al.|\n||||||\n||||||\n\n|Col1|Col2|Col3|ha Mvea|hjoigrihty s ocof ruetstr.|\n|---|---|---|---|---|\n||||Cut-off su|ggested by:|\n||||M20u2r3aliku|mar et al.|\n||||||\n||||||\n\n|Col1|Col2|Col3|ha Mvea|hjoigrihty s ocof ruetstr.|\n|---|---|---|---|---|\n||||Cut-off su|ggested by:|\n||||M20u2r3aliku|mar et al.|\n||||||\n||||||\n\n\n0.00 0.25 0.50 0.75 1.00\nx - Neg. Probability\n\n\nVader (L)\n\n\n0.00 0.25 0.50 0.75 1.00\nx - Toxic Probability\n\n\nPerspective (ML)\n\n\nDetoxify (ML)\n\n\nDetoxify Unbiased (ML)\n\n\n1.0\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0.0\n\n1.0\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0.0\n\n1.0\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0.0\n\n\n1.0 0.5 0.0 0.5 1.0\nx - Polarity\n\n1.0 0.5 0.0 0.5 1.0\nx - Polarity\n\n1.0 0.5 0.0 0.5 1.0\nx - Polarity\n\n\n\n\n0.00 0.25 0.50 0.75 1.00\nx - Neg. Probability\n\n\n0.00 0.25 0.50 0.75 1.00\nx - Toxic Probability\n\n\n0.00 0.25 0.50 0.75 1.00\nx - Toxic Probability\n\n\n0.00 0.25 0.50 0.75 1.00\nx - Toxic Probability\n\n\n**(a) Twitter**\n\nVader (L)\n\n\nPerspective (ML)\n\n\nDetoxify (ML)\n\n\nDetoxify Unbiased (ML)\n\n\n\n\n0.00 0.25 0.50 0.75 1.00\nx - Neg. Probability\n\n\n0.00 0.25 0.50 0.75 1.00\nx - Toxic Probability\n\n\n0.00 0.25 0.50 0.75 1.00\nx - Toxic Probability\n\n\n0.00 0.25 0.50 0.75 1.00\nx - Toxic Probability\n\n\n**(b) YouTube**\n\nVader (L)\n\n\nPerspective (ML)\n\n\nDetoxify (ML)\n\n\nDetoxify Unbiased (ML)\n\n\n\n\n0.00 0.25 0.50 0.75 1.00\nx - Toxic Probability\n\n\n0.00 0.25 0.50 0.75 1.00\nx - Toxic Probability\n\n\n**(c) CORAAL/Buckeye**\n\n**Figure 1: Score distributions for sentences with and without AAE Expressions**\n\n\nWe can initially see that utterances with AAE expressions receive\nmuch higher scores for Twitter and toxicity models (Perspective,\nDetoxify, and Detoxify Unbiased). There is a clear tendency for\nstatistical dominance – the CDF, or y-value, of utterances with AAE\nexpressions is above the one without AAE expressions regardless of\nx-values. This behavior is attenuated as more AAE expressions are\nconsidered in the utterance (we considered those with at least one,\nthree, five, or seven expressions). For instance, considering Toxicity\nmodels on Twitter, the highest 20% scoring utterances *without* AAE\nexpressions achieve a minimum of 0.25. For utterances *with at least*\n*one* AAE expression, this minimum is around 0.55.\nUsing a Kolmogorov-Smirnov test, we compared whether each\nCDF with AAE expressions differs from those without. Under *𝑝* *<*\n0 *.* 01, Flair did not show this difference on the Twitter dataset when\nconsidering utterances with at least one expression. This is the\n**only** case where we failed to reject the null hypothesis. When we\nconsider lexical models, Textblob and Vader, it appears they present\na mild bias on lower-scoring sentences. To help understand this\nissue, focus on the sentences with scores below 0.5 for Textblob and\n0.20 for Vader. Whether this is an issue will depend on the cut-off\ndevelopers employ (e.g., ±0 *.* 5 appears to mitigate the issue).\nFrom Figures 1b and 1c, we can see the same trend that occurred\non Twitter for the lexical approaches (Textblob and Vader), also occurs on YouTube and CORAAL/Buckeye. This finding likely stems\nfrom the fact that such approaches employ manually curated rules\nthat do not consider AAE, a positive aspect of these approaches. On\nthese datasets, Flair is biased, achieving lower scores (polarity or\ntendency to rate as more negative) for utterances with AAE expressions. The ML models still present biases on YouTube (Figure 1b).\n\n\nHowever, it is important to point out two facts: (1) albeit statistically significant, this bias is negligible on Detoxify for YouTube; (2)\nsuch bias is less present when considering the suggested cut-off\nof 0.55 [ 45 ] for toxicity models. Biases **may not** be an issue when\nusing large cut-off values. To further investigate biases, we now\ncorrelate scores with linguistic features.\n### **4.2 Impact of Grammatical/Linguistic Features**\n\nIn Table 3, we present our Logistic regression results for Twitter.\nFor each tweet, we counted the number of AAE expressions, LIWC\ncategories, and PoS tags as features. Being counts, all of these features have positive values only. To present regression coefficients\non a similar, features were *Min-Max* scaled to the [ 0 *,* 1 ] range before\nthe regression was executed. Models were executed with an intercept variable and no regularization. Models that output polarity\nhad such polarity values re-scaled to [ 0 *,* 1 ] also (by adding one and\ndividing by two). The table presents only the statistically significant\nfeatures ( *𝑝* *<* 0 *.* 05). LIWC features are identified by LIWC_, and PoS\ntags by POS_ . The demographic variable (AAE tweet or non-AAE\ntweet) was used as a Twitter and CORAAL/Buckeye feature.\nSimilar results are found for the other two datasets. For simplicity,\ninstead of presenting the tables with such results, we shall discuss\nthese results throughout the text. We also note similar results when\nusing the manual POS tags for TwitterAAE (omitted for space).\nOur feature of most interest is the AAE_EXPR (the number of AAE\nexpressions on the utterance). The other features act as control\nvariables to ensure that, on some level, such expressions are not\nbeing confounded with other grammatical/linguistic attributes.\n\n\n7\n\n\n-----\n\nWebMedia’2024, Juiz de Fora, Brazil Resende et al.\n\n**Table 3: Logistic Regression coefficients for the Twitter dataset with** *𝑝* *<* 0 *.* 05 **. Each model’s five most relevant coefficients are**\n\n|Features Textblob (L) Flair (ML), < 0, - > 0|Vader (L) Perspective (ML) Detoxify (ML) Detoxify U. (ML), > 0, - < 0|\n|---|---|\n|AAE_EXPR|0.0934 0.2238 0.1779|\n|LIWC_SWEAR LIWC_SEXUAL -0.3354 -1.3448 LIWC_NETSPEAK -2.6379 LIWC_INFORMAL 2.5228 LIWC_NEGATE -0.8331 LIWC_FILLER -2.1683 LIWC_ASSENT LIWC_MALE -0.3237|0.8567 0.9492 1.2792 0.4657 0.5942 0.5609 -0.4496 -0.8121 -0.9239 0.4386 0.7988 0.946 0.2075 0.1855 -0.6053 -0.6377 -0.1599 -0.2614 -0.1985 -0.1505 -0.1968 -0.191|\n|POS_X 0.5655 POS_DET|-0.1636 -0.3857 -0.4255 -0.3066 0.1623 0.3369 0.3268|\n|DEMOGRAPHIC|0.0508 0.1262 0.0762|\n\n**presented in bold, whereas not statistically significant coefficients were omitted. When a coefficient pushes towards a negative**\n\n**sentiment or toxic score, we color it red (** � **) . Positive sentiment and non-toxic score is colored green (** � **) .**\n\n\nFrom the table, we can see that this feature pushes the polarity\nof the Vader model towards having fewer sentiments. The feature is not significant for the other sentiment analysis models.\nNevertheless, this feature is significant for Perspective and Detoxify (but not for Detoxify Unbiased). When we consider YouTube,\nAAE_EXPR is statistically significant for Vader ~~(~~ 0.0617 ), Perspective ~~(~~ 0.2488 ), and – *suprisingly* – Detoxify Unbiased ( 0.1334 ). On\nCORALL/Buckeye, it was significant for Flair ~~(~~ -0.9432, Perspective\n( 0.3291 ), Detoxify ( 0.1754 ), and Detoxify Unbiased ( 0.2126 ).\nFor Twitter and CORAAL/Buckeye, our demographic variable\n( DEMOGRAPHIC ) was used as a categorical feature. This feature was\nstatistically significant for Twitter and not CORAAL/Buckeye. Twitter is the only dataset where the demographic variable was developed to align with AAE utterances. On CORAAL/Buckeye, an\nAfrican-American may not employ AAE, or a Caucasian may employ AAE (in fact, the usage of expressions is comparable in Table 2).\nConsidering the other features, some linguistic features are expected to push models towards negative sentiment or toxic scores\n(this is the case for the feature LIWC_SWEAR in every dataset). Finally, PoS features were only statistically significant for Twitter:\nquantifiers, POS_DET, and the unknown/other tag, POS_X.\n### **4.3 Semantic Comparison**\n\nWe now seek to answer the following question: *How do highly*\n*semantically similar pairs of utterances that achieve diverging scores*\n*differ in their usage of AAE expressions?* Notice that we have two\nconditions here: (1) being similar in meaning and (2) achieving\ndiverging scores. Such pairs of utterances are interesting because\nthey control for confounding factors in semantics.\nWe employ a large language model as our semantic feature extractor [ 62 ]. With this model, utterances are mapped to an embedding vector. We compare these vectors using a cosine similarity\nfor pairs of utterances: -1 indicates entirely dissimilar, 0 indicates\na lack of relationship, and 1 shows completely similar. We deem\ntwo sentences similar when the cosine score is above 0.5 (this is\nless than 0.1% of pairs as discussed below).\nFor each dataset and sentiment/toxicity method of our study,\nwe isolated the top 2.5% and bottom 2.5% scoring utterances. To\nperform a single analysis, we standardized scores. For Perspective\n\n\nand Detoxify, the top 2.5% have a higher chance of being toxic,\nwhereas for Vader, Flair, and Textblob, the top 2.5% have negative\npolarity. Due to memory constraints sampled 100,000 of such pairs\nper dataset and method. Next, we focused only on pairs where one\nof the sentences had at least one AAE expression. If this is not the\ncase, the difference in score certainly is not due to the number of\nAAE expressions employed. When this is the case, the usage of AAE\nexpressions may be the underlying cause. Due to the small sample\nsize, we did not find any pairs on Twitter that met our conditions.\nCombining every method, we found 585,679 unique pairs on\nYouTube with diverging scores (our top 2.5% versus the bottom\n2.5%). Out of these, 568 had a cosine similarity above 0.5%. On\nCORAAL/Buckeye, we found 592,606 unique pairs from our diverging scores filter, with 243 being highly similar. For each setting\n(YouTube or CORAAL/Buckeyey), we computed the number of\npairs where ( *𝑑* *𝑏𝑖𝑎𝑠* ) the most toxic or most negative had more AAE\nexpressions; ( *𝑑* *𝑒𝑞* ) both had the same amount of expressions, and\n( *𝑑* *𝑛𝑜* _ *𝑏𝑖𝑎𝑠* ) the least toxic or most positive had more expressions.\nFor YouTube, we have that: *𝑑* *𝑏𝑖𝑎𝑠* = 272 ( 48% ), *𝑑* *𝑒𝑞* = 87 ( 15% ),\nand *𝑑* *𝑛𝑜* _ *𝑏𝑖𝑎𝑠* = 209 ( 37% ) . Whereas on CORAAL/Buckeye: *𝑑* *𝑏𝑖𝑎𝑠* =\n187 ( 77% ), *𝑑* *𝑒𝑞* = 26 ( 11% ), and *𝑑* *𝑛𝑜* _ *𝑏𝑖𝑎𝑠* = 30 ( 12% ) . Under a Binomial test and *𝑝* *<* 0 *.* 01, in both cases, we find that *𝑑* *𝑏𝑖𝑎𝑠* *> 𝑑* *𝑛𝑜* _ *𝑏𝑖𝑎𝑠* .\nThus, results show statistical evidence of bias [13] .\n### **5 CONCLUSIONS AND LIMITATIONS**\n\nThis paper investigates the biases of sentiment analysis/toxicity\nmethods regarding the usage of AAE expressions. We analyzed the\nperformance of six well-known off-the-shelf methods in light of\nfour different datasets. Our datasets ranged from online texts from\nTwitter to single-speaker closed captions from YouTube and spoken\nEnglish encompassing daily live situations.\nConsidering the latter, or nonexistent, introduction of AAE in\nML datasets, the under-representation of such expressions leads ML\nmodels to present a systemic bias towards AAE. We argue that the\nbiggest problems derive directly from the absence of context in the\nutterances. Since they employ human-crafted rules, lexical-based\n(rule) approaches tend to be less biased than ML models.\n\n13 *𝑑* *𝑒𝑞* is not considered as the counts of AAE expressions usage are certainly not the\nissue in these settings (they are equal)\n\n\n8\n\n\n-----\n\nA Comprehensive View of Biases Towards Utterances with African American English Expressions WebMedia’2024, Juiz de Fora, Brazil\n\n### **ETHICAL CONSIDERATIONS**\n\n**Ethical Concerns:** One of the ethical concerns of our study comes\nfrom using demographic variables related to race **without** groundtruth self-identification labels for speakers. To mitigate this issue,\nwe refrained from using author-inferred demographic variables in\nour study (on the YouTube dataset). CORAAL/Buckeye are wellestablished in linguistics, with CORAAL focusing solely on African\nAmericans. This issue is not present on Twitter, as labels come from\nusing AAE or not (regardless of race). We also point out that our\nmain statistical variable of study is not race. We focus on the usage\nof AAE expressions, where such expressions came from reliable and\nsuggested sources (by the organizers of a well-known dictionary).\n**Unintended Impact:** Readers may interpret our research as\nagainst ML models or automatic utterance scoring tools. We point\nout that this is **not** our statement. Our research advances both re\ncent and large literature on the unintended biases of Lexical/AI/ML\nmodels. We hope our findings will improve how such tools are used;\nmodel advances towards fewer biases or both.\n\n**Researcher Background:** The majority of authors of this study\nare from a region where racial discrimination is still very present\nin the population’s day-to-day lives. Our research aims to foster\nthe ongoing discussion on how AI impacts the lives of different\nhistorically segregated communities.\nIf readers deem any terms or expressions used in this paper\noffensive, we point out that it was not deliberate.\n### **REFERENCES**\n\n[1] Abubakar Abid, Maheen Farooqi, and James Zou. 2021. Large language models\nassociate Muslims with violence. *Nature Machine Intelligence* 3, 6 (2021), 461–463.\n\n[2] CJ Adams. 2018. New York Times: Using AI to host better conversations. https://blog.google/technology/ai/new-york-times-using-ai-host-betterconversations/.\n\n[3] Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, and\nRoland Vollgraf. 2019. FLAIR: An easy-to-use framework for state-of-the-art\nNLP. In *NAACL 2019, 2019 Annual Conference of the North American Chapter of*\n*the Association for Computational Linguistics (Demonstrations)* . 54–59.\n\n[4] Marzieh Babaeianjelodar, Stephen Lorenz, Josh Gordon, Jeanna Matthews, and\nEvan Freitag. 2020. Quantifying gender bias in different corpora. In *Companion*\n*Proceedings of the Web Conference 2020* . 752–759.\n\n[5] Arnetha F Ball. 1992. Cultural preference and the expository writing of AfricanAmerican adolescents. *Written Communication* 9, 4 (1992), 501–532.\n\n[6] Ari Ball-Burack, Michelle Seng Ah Lee, Jennifer Cobbe, and Jatinder Singh. 2021.\nDifferential tweetment: Mitigating racial dialect bias in harmful tweet detection. In *Proceedings of the 2021 ACM Conference on Fairness, Accountability, and*\n*Transparency* . 116–128.\n\n[7] David Bamman, Chris Dyer, and Noah A Smith. 2014. Distributed representations\nof geographically situated language. In *Proceedings of the 52nd Annual Meeting of*\n*the Association for Computational Linguistics (Volume 2: Short Papers)* . 828–834.\n\n[8] John Baugh. 1981. Runnin’down Some Lines: The Language and Culture of Black\nTeenagers.\n\n[9] Su Lin Blodgett, Lisa Green, and Brendan O’Connor. 2016. Demographic dialectal\nvariation in social media: A case study of African-American English. *arXiv*\n*preprint arXiv:1608.08868* (2016).\n\n[10] Su Lin Blodgett, Johnny Wei, and Brendan O’Connor. 2018. Twitter universal\ndependency parsing for African-American and mainstream American English.\nIn *Proceedings of the 56th Annual Meeting of the Association for Computational*\n*Linguistics (Volume 1: Long Papers)* . 1415–1425.\n\n[11] Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T\nKalai. 2016. Man is to computer programmer as woman is to homemaker?\ndebiasing word embeddings. *Advances in neural information processing systems*\n29 (2016).\n\n[12] Zhenpeng Chen, Jie M Zhang, Max Hort, Federica Sarro, and Mark Harman. 2022.\nFairness testing: A comprehensive survey and analysis of trends. *arXiv preprint*\n*arXiv:2207.10223* (2022).\n\n[13] Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017.\nAutomated hate speech detection and the problem of offensive language. In\n*Proceedings of the international AAAI conference on web and social media*, Vol. 11.\n\n\n512–515.\n\n[14] Mark Díaz, Isaac Johnson, Amanda Lazar, Anne Marie Piper, and Darren Gergle.\n2018. Addressing age-related bias in sentiment analysis. In *Proceedings of the*\n*2018 chi conference on human factors in computing systems* . 1–14.\n\n[15] Joey Lee Dillard. 1977. *Lexicon of Black English.* ERIC.\n\n[16] Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. 2018.\nMeasuring and mitigating unintended bias in text classification. In *Proceedings of*\n*the 2018 AAAI/ACM Conference on AI, Ethics, and Society* . 67–73.\n\n[17] Jacob Eisenstein, Brendan O’Connor, Noah A Smith, and Eric P Xing. 2014.\nDiffusion of lexical change in social media. *PloS one* 9, 11 (2014), e113114.\n\n[18] Anjalie Field, Su Lin Blodgett, Zeerak Waseem, and Yulia Tsvetkov. 2021. A\nSurvey of Race, Racism, and Anti-Racism in NLP. In *Proceedings of the 59th*\n*Annual Meeting of the Association for Computational Linguistics and the 11th*\n*International Joint Conference on Natural Language Processing (Volume 1: Long*\n*Papers)* . 1905–1925.\n\n[19] Sarah Florini. 2014. Tweets, Tweeps, and Signifyin’ Communication and Cultural\nPerformance on “Black Twitter”. *Television & New Media* 15, 3 (2014), 223–237.\n\n[20] Patricia Friedrich. 2020. When Englishes go digital. *World Englishes* 39, 1 (2020),\n67–78.\n\n[21] Patricia Friedrich and Eduardo Diniz de Figueiredo. 2016. *The sociolinguistics of*\n*digital Englishes* . Routledge.\n\n[22] David Garcia, Ingmar Weber, and Venkata Rama Kiran Garimella. 2014. Gender\nasymmetries in reality and fiction: The bechdel test of social media. In *Eighth*\n*International AAAI Conference on Weblogs and Social Media* .\n\n[23] Anastasia Giachanou and Fabio Crestani. 2016. Like it or not: A survey of twitter\nsentiment analysis methods. *ACM Computing Surveys (CSUR)* 49, 2 (2016), 1–41.\n\n[24] Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter sentiment classification\nusing distant supervision. *CS224N project report, Stanford* 1, 12 (2009), 2009.\n\n[25] A Gomes, D Antonialli, and T Dias-Oliva. 2019. Drag queens and artificial\nintelligence. Should computers decide what is toxic on the internet. *Internet Lab*\n*blog* (2019).\n\n[26] Hila Gonen and Yoav Goldberg. 2019. Lipstick on a pig: Debiasing methods cover\nup systematic gender biases in word embeddings but do not remove them. *arXiv*\n*preprint arXiv:1903.03862* (2019).\n\n[27] Mark Graham, Bernie Hogan, Ralph K Straumann, and Ahmed Medhat. 2014.\nUneven geographies of user-generated information: Patterns of increasing informational poverty. *Annals of the Association of American Geographers* 104, 4\n(2014), 746–764.\n\n[28] Lisa J Green. 2002. *African American English: a linguistic introduction* . Cambridge\nUniversity Press.\n\n[29] Tommi Gröndahl, Luca Pajola, Mika Juuti, Mauro Conti, and N Asokan. 2018. All\nyou need is\" love\" evading hate speech detection. In *Proceedings of the 11th ACM*\n*workshop on artificial intelligence and security* . 2–12.\n\n[30] Laura Hanu and Unitary team. 2020. Detoxify. Github.\nhttps://github.com/unitaryai/detoxify.\n\n[31] Camille Harris, Matan Halevy, Ayanna Howard, Amy Bruckman, and Diyi Yang.\n2022. Exploring the role of grammar and word choice in bias toward african\namerican english (aae) in hate speech classification. In *2022 ACM Conference on*\n*Fairness, Accountability, and Transparency* . 789–798.\n\n[32] Hossein Hosseini, Sreeram Kannan, Baosen Zhang, and Radha Poovendran. 2017.\nDeceiving google’s perspective api built for detecting toxic comments. *arXiv*\n*preprint arXiv:1702.08138* (2017).\n\n[33] Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews.\nIn *Proceedings of the tenth ACM SIGKDD international conference on Knowledge*\n*discovery and data mining* . 168–177.\n\n[34] Ben Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu\nZhong, and Stephen Denuyl. 2020. Unintended machine learning biases as\nsocial barriers for persons with disabilitiess. *ACM SIGACCESS Accessibility and*\n*Computing* (2020), 1–1.\n\n[35] Clayton Hutto and Eric Gilbert. 2014. Vader: A parsimonious rule-based model\nfor sentiment analysis of social media text. In *Proceedings of the international*\n*AAAI conference on web and social media*, Vol. 8. 216–225.\n\n[36] Sen Jia, Thomas Lansdall-Welfare, and Nello Cristianini. 2015. Measuring gender\nbias in news images. In *Proceedings of the 24th International Conference on World*\n*Wide Web* . 893–898.\n\n[37] Jigsaw. [n. d.]. Perspective API. https://perspectiveapi.com/. Accessed: 2023-0130.\n\n[38] Tyler Kendall and Charlie Farrington. 2021. The Corpus of Regional African\nAmerican Language (Version 2021.07). Eugene, OR: The Online Resources for\nAfrican American Language Project.\n\n[39] Svetlana Kiritchenko and Saif M Mohammad. 2018. Examining Gender and Race\nBias in Two Hundred Sentiment Analysis Systems. *NAACL HLT 2018* (2018), 43.\n\n[40] Animesh Koratana and Kevin Hu. 2018. Toxic speech detection. *URL: https://web.*\n*stanford. edu/class/archive/cs/cs224n/cs224n* 1194 (2018).\n\n[41] Deepak Kumar, Patrick Gage Kelley, Sunny Consolvo, Joshua Mason, Elie\nBursztein, Zakir Durumeric, Kurt Thomas, and Michael Bailey. 2021. Designing\nToxic Content Classification for a Diversity of Perspectives.. In *SOUPS@ USENIX*\n*Security Symposium* . 299–318.\n\n\n9\n\n\n-----\n\nWebMedia’2024, Juiz de Fora, Brazil Resende et al.\n\n\n\n[42] Steven Loria. 2018. textblob Documentation. *Release 0.15* 2 (2018).\n\n[43] Patricia Georgiou Marie Pellat. 2018. Perspective Launches In Spanish With\nEl País. https://medium.com/jigsaw/perspective-launches-in-spanish-with-elpa%C3%ADs-dc2385d734b2.\n\n[44] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.\nDistributed representations of words and phrases and their compositionality. In\n*Advances in neural information processing systems* . 3111–3119.\n\n[45] Meena Devii Muralikumar, Yun Shan Yang, and David W McDonald. 2023. A\nHuman-Centered Evaluation of a Toxicity Detection API: Testing Transferability\nand Unpacking Latent Attributes. *ACM Transactions on Social Computing* (2023).\n\n[46] Lisa Nakamura. 2013. *Cybertypes: Race, ethnicity, and identity on the Internet* .\nRoutledge.\n\n[47] Nikolaos Pappas, Georgios Katsimpras, and Efstathios Stamatatos. 2013. Distinguishing the popularity between topics: a system for up-to-date opinion retrieval\nand mining in the web. In *Computational Linguistics and Intelligent Text Process-*\n*ing: 14th International Conference, CICLing 2013, Samos, Greece, March 24-30, 2013,*\n*Proceedings, Part II 14* . Springer, 197–209.\n\n[48] Daniel Borkan Patricia Georgiou, Marie Pellat. 2019. Parlons-en! Perspective and\nTune are now available in French. https://medium.com/jigsaw/perspective-tuneare-now-available-in-french-c4cf1ca198f2.\n\n[49] James W Pennebaker, Martha E Francis, and Roger J Booth. 2001. Linguistic\ninquiry and word count: LIWC 2001. *Mahway: Lawrence Erlbaum Associates* 71,\n2001 (2001), 2001.\n\n[50] Mark A Pitt, Keith Johnson, Elizabeth Hume, Scott Kiesling, and William Raymond. 2005. The Buckeye corpus of conversational speech: Labeling conventions\nand a test of transcriber reliability. *Speech Communication* 45, 1 (2005), 89–95.\n\n[51] Filipe N Ribeiro, Matheus Araújo, Pollyanna Gonçalves, Marcos André Gonçalves,\nand Fabrício Benevenuto. 2016. Sentibench-a benchmark comparison of state-ofthe-practice sentiment analysis methods. *EPJ Data Science* 5, 1 (2016), 1–29.\n\n[52] Max Roser, Hannah Ritchie, and Esteban Ortiz-Ospina. 2015. Internet. *Our World*\n*in Data* (2015). https://ourworldindata.org/internet.\n\n[53] Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, and Noah A Smith. 2019.\nThe risk of racial bias in hate speech detection. In *Proceedings of the 57th annual*\n*meeting of the association for computational linguistics* . 1668–1678.\n\n[54] Geneva Smitherman. 2000. *Black talk: Words and phrases from the hood to the*\n*amen corner* . Houghton Mifflin Harcourt.\n\n[55] Kaikai Song, Ting Yao, Qiang Ling, and Tao Mei. 2018. Boosting image sentiment\nanalysis with visual attention. *Neurocomputing* 312 (2018), 218–228.\n\n\n\n[56] Ezekiel Soremekun, Sakshi Udeshi, and Sudipta Chattopadhyay. 2022. Astraea:\nGrammar-based fairness testing. *IEEE Transactions on Software Engineering* 48,\n12 (2022), 5188–5211.\n\n[57] Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly Voll, and Manfred Stede.\n2011. Lexicon-based methods for sentiment analysis. *Computational linguistics*\n37, 2 (2011), 267–307.\n\n[58] Rachael Tatman. 2017. Gender and dialect bias in YouTube’s automatic captions.\nIn *Proceedings of the first ACL workshop on ethics in natural language processing* .\n53–59.\n\n[59] Mike Thelwall. 2014. Heart and soul: Sentiment strength detection in the social\nweb with sentistrength, 2017. *Cyberemotions: Collective emotions in cyberspace*\n(2014).\n\n[60] Pranav Narayanan Venkit and Shomir Wilson. 2021. Identification of bias against\npeople with disabilities in sentiment analysis and toxicity detection models. *arXiv*\n*preprint arXiv:2111.13259* (2021).\n\n[61] Hao Wang, Doğan Can, Abe Kazemzadeh, François Bar, and Shrikanth Narayanan.\n2012. A system for real-time twitter sentiment analysis of 2012 us presidential\nelection cycle. In *Proceedings of the ACL 2012 system demonstrations* . 115–120.\n\n[62] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou.\n2020. Minilm: Deep self-attention distillation for task-agnostic compression of\npre-trained transformers. *Advances in Neural Information Processing Systems* 33\n(2020), 5776–5788.\n\n[63] Maciej Widawski. 2015. *African American slang: A linguistic description* . Cambridge University Press.\n\n[64] Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Jason Kessler, Janyce\nWiebe, Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.\nOpinionFinder: A system for subjectivity analysis. In *Proceedings of HLT/EMNLP*\n*2005 Interactive Demonstrations* . 34–35.\n\n[65] Austin P Wright, Omar Shaikh, Haekyu Park, Will Epperson, Muhammed Ahmed,\nStephane Pinel, Duen Horng Chau, and Diyi Yang. 2021. RECAST: Enabling\nuser recourse and interpretability of toxicity detection models with interactive\nvisualization. *Proceedings of the ACM on Human-Computer Interaction* 5, CSCW1\n(2021), 1–26.\n\n[66] Ali Yadollahi, Ameneh Gholipour Shahraki, and Osmar R Zaiane. 2017. Current\nstate of text sentiment analysis from opinion to emotion mining. *ACM Computing*\n*Surveys (CSUR)* 50, 2 (2017), 1–33.\n\n[67] Min Yang, Qiang Qu, Xiaojun Chen, Chaoxue Guo, Ying Shen, and Kai Lei. 2018.\nFeature-enhanced attention network for target-dependent sentiment classification. *Neurocomputing* 307 (2018), 91–97.\n\n\n10\n\n\n-----\n\n",
    "artigo_tokenizado": "",
    "pos_tagger": "",
    "lema": ""
  }
]