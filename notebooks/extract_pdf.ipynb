{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- No arquivo process_pdfs tem toda a lógica. A falta a parte de tokenização, lematização e pos tagger. Isso pode ser feito com o *spacy*.\n",
    "- Além disso, no texto precisa remover alguns simbolos como: * \\n  ## e outros\n",
    "- Resolvi ir limpando o texto em partes devido aos padrões de cada seção do artigo."
   ],
   "id": "f6be318916239fb9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-28T12:39:37.697993Z",
     "start_time": "2025-04-28T12:39:35.618403Z"
    }
   },
   "source": [
    "import json\n",
    "from process_text import process_pdfs"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T12:39:48.767272Z",
     "start_time": "2025-04-28T12:39:37.704219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path_in = '../articles/original/english'\n",
    "path_out = '../corpus'\n",
    "process_pdfs(path_in, path_out)"
   ],
   "id": "e474284f7aed6ec2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ../articles/original/english/985-24769-1-10-20240923.pdf\n",
      "Processing: ../articles/original/english/985-24738-1-10-20240923.pdf\n",
      "Finished processing\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T12:39:48.784781Z",
     "start_time": "2025-04-28T12:39:48.774110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('../corpus/corpus.json', \"rb\") as f:\n",
    "    corpus = json.load(f)"
   ],
   "id": "4b38dbf1afb5c232",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T01:20:36.943432Z",
     "start_time": "2025-04-28T01:20:36.739094Z"
    }
   },
   "cell_type": "code",
   "source": "corpus",
   "id": "6b64bab290d63f85",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'titulo': 'Twitter and the 2022 Brazilian Elections Portrait: A Network and Content-Driven Analysis',\n",
       "  'informacoes_url': '',\n",
       "  'idioma': 'english',\n",
       "  'storage_key': '../articles/original/english/985-24769-1-10-20240923.pdf',\n",
       "  'author': 'Larissa Malagoli; Giovana Piorino; Carlos H. G. Ferreira; and Ana Paula Couto da Silva',\n",
       "  'data_publicacao': '11-09-2024',\n",
       "  'resumo': 'The influence of online social networks on people’s actions and beliefs has grown significantly over the past decade, impacting everyday life. This is especially evident in Brazil, where these platforms have been instrumental in disseminating political content rapidly and widely. In this work, we aim to understand how the political debate surrounding the Brazilian elections of 2022 on Twitter unfolds through different levels of user engagement. We provide a content analysis that unveils the main topics discussed by different users, regardless of the strength of their interactions. Our results enrich the understanding of how online discussions evolved on social media during this important event in the recent history of democracy in Brazil. ###',\n",
       "  'keywords': '2022 Brazilian Elections, Network Modeling, Online Discussions, Twitter, Natural Language Processing',\n",
       "  'referencias': ['[1] Peiman Barnaghi, Parsa Ghaffari, and John G. Breslin. 2016. Opinion Mining and\\nSentiment Polarity on Twitter and Correlation between Events and Sentiment.\\nIn *2016 IEEE Second International Conference on Big Data Computing Service and*\\n*Applications (BigDataService)* . 52–57. https://doi.org/10.1109/BigDataService.\\n2016.36',\n",
       "   '[2] Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. 2008. Fast unfolding of comm. in large networks. *Journal of Statisti-*\\n*cal Mechanics: Theory and Experiment* 2008, 10 (oct 2008), P10008. https:\\n//doi.org/10.1088/1742-5468/2008/10/P10008',\n",
       "   '[3] Josemar Caetano, Samuel Guimarães, Marcelo M. R. Araújo, Márcio Silva, Júlio\\nC. S. Reis, Ana P. C. Silva, Fabrício Benevenuto, and Jussara M. Almeida. 2022.\\nCharacterizing Early Electoral Advertisements on Twitter: A Brazilian Case Study.\\nIn *Social Informatics*, Frank Hopfgartner, Kokil Jaidka, Philipp Mayr, Joemon Jose,\\nand Jan Breitsohl (Eds.). Springer International Publishing, Cham, 257–272.',\n",
       "   '[4] André Cristiani, Douglas Lieira, and Heloisa Camargo. 2020. A Sentiment Analysis of Brazilian Elections Tweets. In *Anais do VIII Symposium on Knowledge*\\n*Discovery, Mining and Learning* (Evento Online). SBC, Porto Alegre, RS, Brasil,\\n153–160. https://doi.org/10.5753/kdmile.2020.11971',\n",
       "   '[5] Jose Martins da Rosa, Renan Saldanha Linhares, Carlos Henrique Gomes Ferreira,\\nGabriel P. Nobre, Fabricio Murai, and Jussara M. Almeida. 2022. Uncovering\\nDiscussion Groups on Claims of Election Fraud from Twitter. In *Proc. of Social*\\n*Informatics: 13th International Conference* . https://doi.org/10.1007/978-3-03119097-1_20',\n",
       "   '[6] Roman Egger and Joanne Yu. 2022. A Topic Modeling Comparison Between LDA,\\nNMF, Top2Vec, and BERTopic to Demystify Twitter Posts. *Frontiers in Sociology*\\n7 (05 2022). https://doi.org/10.3389/fsoc.2022.886498',\n",
       "   '[7] Anna Fang and Zina Ben-Miled. 2017. Does bad news spread faster?. In *2017*\\n*International Conference on Computing, Networking and Communications (ICNC)* .\\n793–797. https://doi.org/10.1109/ICCNC.2017.7876232',\n",
       "   '[8] Juliana Feitosa, Luiz De Camargo, Eloisa Bonatti, Giovanna Simioni, and José\\nBrega. 2023. eXplainable Artificial Intelligence in sentiment analysis of posts\\nabout Covid-19 vaccination on Twitter. In *Proceedings of the 29th Brazilian Sym-*\\n*posium on Multimedia and the Web* (Ribeirão Preto/SP). SBC, Porto Alegre, RS,\\nBrasil, 65–72. https://sol.sbc.org.br/index.php/webmedia/article/view/25867',\n",
       "   '[9] Emilio Ferrara, Herbert Chang, Emily Chen, Goran Muric, and Jaimin Patel. 2020.\\nCharacterizing social media manipulation in the 2020 U.S. presidential election.\\n*First Monday* (2020). https://doi.org/10.5210/fm.v25i11.11431',\n",
       "   '[10] Carlos HG Ferreira, Fabricio Murai, Ana PC Silva, Jussara M Almeida, Martino\\nTrevisan, Luca Vassio, Marco Mellia, and Idilio Drago. 2021. On the dynamics of\\npolitical discussions on instagram: A network perspective. *Online Social Networks*\\n*and Media* 25 (2021), 100155.',\n",
       "   '[11] E Fonseca, L Santos, Marcelo Criscuolo, and S Aluisio. 2016. ASSIN: Avaliacao\\nde similaridade semantica e inferencia textual. In *Computational Processing of the*\\n*Portuguese Language-12th International Conference, Tomar, Portugal* . 13–15.',\n",
       "   '[12] Carlos Henrique Gomes Ferreira, Fabricio Murai, Ana Paula Couto da Silva,\\nJussara Marques de Almeida, Martino Trevisan, Luca Vassio, Idilio Drago, and\\nMarco Mellia. 2020. Unveiling community dynamics on instagram political\\nnetwork. In *Proceedings of the 12th ACM Conference on Web Science* . 231–240.',\n",
       "   '[13] Carlos Henrique Gomes Ferreira, Fabricio Murai, Ana P. C. Silva, Martino Trevisan, Luca Vassio, Idilio Drago, Marco Mellia, and Jussara M. Almeida. 2022. On\\nnetwork backbone extraction for modeling online collective behavior. *PLOS ONE*\\n17, 9 (09 2022), 1–36. https://doi.org/10.1371/journal.pone.0274218',\n",
       "   '[14] Maarten Grootendorst. 2022. BERTopic: Neural topic modeling with a class-based\\nTF-IDF procedure. *arXiv preprint arXiv:2203.05794* (2022).',\n",
       "   '[15] Joshua Guberman, Carol E. Schmitz, and Libby Hemphill. 2016. Quantifying\\nToxicity and Verbal Violence on Twitter. In *Proceedings of the 19th ACM Conference*\\n*on Computer Supported Cooperative Work and Social Computing, CSCW 2015, San*\\n*Francisco, CA, USA, February 27 - March 2, 2016, Companion Volume*, Darren\\nGergle, Meredith Ringel Morris, Pernille Bjørn, and Joseph A. Konstan (Eds.).\\nACM, 277–280. https://doi.org/10.1145/2818052.2869107',\n",
       "   '[16] Ruben Interian and Francisco A Rodrigues. 2023. Group polarization, influence,\\nand domination in online interaction networks: a case study of the 2022 Brazilian\\nelections. *Journal of Physics: Complexity* 4, 3 (Sept. 2023), 035008. https://doi.\\norg/10.1088/2632-072x/acf6a4',\n",
       "   '[17] Andressa Kappaun and Jonice Oliveira. 2023. Análise sobre Viés de Gênero no\\nYoutube: Um Estudo sobre as Eleições Presidenciais de 2018 e 2022. In *Anais do XII*\\n*Brazilian Workshop on Social Network Analysis and Mining* (João Pessoa/PB). SBC,\\nPorto Alegre, RS, Brasil, 127–138. https://doi.org/10.5753/brasnam.2023.230625',\n",
       "   '[18] Renan S. Linhares, José M. Rosa, Carlos H. G. Ferreira, Fabricio Murai, Gabriel\\nNobre, and Jussara Almeida. 2022. Uncovering Coordinated Communities on\\nTwitter During the 2020 U.S. Election. In *Proc. of ASONAM* .',\n",
       "   '[19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A\\nRobustly Optimized BERT Pretraining Approach. *CoRR* abs/1907.11692 (2019).\\narXiv:1907.11692 http://arxiv.org/abs/1907.11692',\n",
       "   '[20] Caio Machado, Beatriz Kira, Vidya Narayanan, Bence Kollanyi, and Philip Howard.\\n2019. A Study of Misinformation in WhatsApp Groups with a Focus on the\\nBrazilian Presidential Elections.. In *Companion Proceedings of The 2019 World Wide*\\n*Web Conference* (San Francisco, USA) *(WWW ’19)* . Association for Computing\\nMachinery, New York, NY, USA, 1013–1019. https://doi.org/10.1145/3308560.\\n3316738',\n",
       "   '[21] Larissa G. Malagoli, Julia Stancioli, Carlos H. G. Ferreira, Marisa Vasconcelos,\\nAna Paula Couto da Silva, and Jussara M. Almeida. 2021. A Look into COVID19 Vaccination Debate on Twitter. In *Proceedings of the 13th ACM Web Science*\\n*Conference 2021* (Virtual Event, United Kingdom) *(WebSci ’21)* . Association for\\nComputing Machinery, New York, NY, USA, 225–233. https://doi.org/10.1145/\\n3447535.3462498',\n",
       "   '[22] Philip May. 2021. Machine translated multilingual STS benchmark dataset. https:\\n//github.com/PhilipMay/stsb-multi-mt',\n",
       "   '[23] Carlos Navarrete, Mariana Macedo, Rachael Colley, Jingling Zhang, Nicole Ferrada, Maria Eduarda Mello, Rodrigo Lira, Carmelo Bastos-Filho, Umberto Grandi,\\nJerome Lang, and César A. Hidalgo. 2023. Understanding Political Divisiveness\\nusing Online Participation data from the 2022 French and Brazilian Presidential\\nElections. arXiv:2211.04577 [cs.CY]',\n",
       "   '[24] M. E. J. Newman and M. Girvan. 2004. Finding and evaluating community\\nstructure in networks. *Phys. Rev. E* 69 (Feb 2004), 026113. Issue 2. https://doi.\\norg/10.1103/PhysRevE.69.026113',\n",
       "   '[25] Gabriel Nobre, Carlos Ferreira, and Jussara Almeida. 2020. Beyond Groups:\\nUncovering Dynamic Communities on the WhatsApp Network of Information\\nDissemination. In *SocInfo’ 2020* .',\n",
       "   '[26] Gabriel Peres Nobre, Carlos H.G. Ferreira, and Jussara M. Almeida. 2022. A\\nHierarchical Network-Oriented Analysis of User Participation in Misinformation\\nSpread on WhatsApp. *Information Processing and Management* 59, 1 (jan 2022),\\n21 pages. https://doi.org/10.1016/j.ipm.2021.102757',\n",
       "   '[27] Beatriz Paiva, Beatriz Barbosa, Ana Silva, and Mirella Moro. 2023. O debate do\\nfeminismo no Twitter: Um estudo de caso das eleições brasileiras de 2022. In\\n*Anais do XII Brazilian Workshop on Social Network Analysis and Mining* (João\\nPessoa/PB). SBC, Porto Alegre, RS, Brasil, 103–114. https://doi.org/10.5753/\\nbrasnam.2023.230537',\n",
       "   '[28] Livy Real, Erick Fonseca, and Hugo Goncalo Oliveira. 2020. The assin 2 shared\\ntask: a quick overview. In *International Conference on Computational Processing*\\n*of the Portuguese Language* . Springer, 406–412.',\n",
       "   '[29] Julio Reis, Philipe Melo, Fabiano Belém, Fabricio Murai, Jussara Almeida, and\\nFabricio Benevenuto. 2023. Helping Fact-Checkers Identify Fake News Stories\\nShared through Images on WhatsApp. In *Proceedings of the 29th Brazilian Sym-*\\n*posium on Multimedia and the Web* (Ribeirão Preto/SP). SBC, Porto Alegre, RS,\\nBrasil, 159–167. https://sol.sbc.org.br/index.php/webmedia/article/view/25877',\n",
       "   '[30] Gonzalo A. Ruz, Pablo A. Henríquez, and Aldo Mascareño. 2020. Sentiment\\nanalysis of Twitter data during critical events through Bayesian Net. class. *Future*\\n*Gen. Computer Systems* 106 (2020), 92–104. https://doi.org/10.1016/j.future.2020.\\n01.005',\n",
       "   '[31] Maria Santana, Juliana Lima, Andreiwid Correa, and Kellyton Brito. 2023. Engajamento no TikTok dos candidatos às eleições Brasileiras de 2022 – Resultados Iniciais. In *Anais do XII Brazilian Workshop on Social Network Analysis*\\n*and Mining* (João Pessoa/PB). SBC, Porto Alegre, RS, Brasil, 151–162. https:\\n//doi.org/10.5753/brasnam.2023.230641',\n",
       "   '[32] Daiana Santos and Lilian Berton. 2023. Analysis of Twitter users’ sentiments\\nabout the first round 2022 presidential election in Brazil. In *Anais do XX Encontro*\\n*Nacional de Inteligência Artificial e Computacional* (Belo Horizonte/MG). SBC,\\nPorto Alegre, RS, Brasil, 880–893. https://doi.org/10.5753/eniac.2023.234511',\n",
       "   '[33] M. Ángeles Serrano, Marián Boguñá, and Alessandro Vespignani. 2009. Extracting\\nthe multiscale backbone of complex weighted networks. *Proceedings of the*\\n*National Academy of Sciences* 106, 16 (April 2009), 6483–6488. https://doi.org/10.\\n1073/pnas.0808904106',\n",
       "   '[34] Sarah Silva and Elaine Faria. 2023. Análise de sentimentos expressos no Twitter\\nem relação aos candidatos da eleição presidencial de 2022. In *Anais do XII Brazilian*\\n*Workshop on Social Network Analysis and Mining* (João Pessoa/PB). SBC, Porto\\nAlegre, RS, Brasil, 79–90. https://doi.org/10.5753/brasnam.2023.229992',\n",
       "   '[35] Fábio Souza, Rodrigo Nogueira, and Roberto Lotufo. 2020. BERTimbau: pretrained\\nBERT models for Brazilian Portuguese. In *9th Brazilian Conference on Intelligent*\\n*Systems, BRACIS, Rio Grande do Sul, Brazil, October 20-23 (to appear)* .',\n",
       "   '[36] Yla Tausczik and James Pennebaker. 2010. The psychological meaning of words:\\nLIWC and computerized text analysis methods. *Journal of language and social*\\n*psychology* 29, 1 (2010), 24–54.',\n",
       "   '[37] Otavio R. Venâncio, Carlos H. G. Ferreira, Jussara M. Almeida, and Ana Paula\\nC. da Silva. 2024. Unraveling User Coordination on Telegram: A Comprehensive\\nAnalysis of Political Mobilization during the 2022 Brazilian Presidential Election.\\n*Proceedings of the International AAAI Conference on Web and Social Media* 18, 1\\n(May 2024), 1545–1556. https://doi.org/10.1609/icwsm.v18i1.31408',\n",
       "   '[38] Shlomo Yitzhaki. 1979. Relative deprivation and the Gini coefficient. *The quarterly*\\n*journal of economics* (1979), 321–324.\\n\\n\\n291\\n\\n\\n-----'],\n",
       "  'text': '# **Twitter and the 2022 Brazilian Elections Portrait:** **A Network and Content-Driven Analysis**\\n\\n## Larissa Malagoli\\n#### larissagomes@dcc.ufmg.br Universidade Federal de Minas Gerais\\n## Carlos H. G. Ferreira\\n#### chgferreira@ufop.edu.br Universidade Federal de Ouro Preto\\n### **ABSTRACT**\\n\\nThe influence of online social networks on people’s actions and\\nbeliefs has grown significantly over the past decade, impacting\\neveryday life. This is especially evident in Brazil, where these platforms have been instrumental in disseminating political content\\nrapidly and widely. In this work, we aim to understand how the political debate surrounding the Brazilian elections of 2022 on Twitter\\nunfolds through different levels of user engagement. We provide a\\ncontent analysis that unveils the main topics discussed by different\\nusers, regardless of the strength of their interactions. Our results\\nenrich the understanding of how online discussions evolved on\\nsocial media during this important event in the recent history of\\ndemocracy in Brazil.\\n### **KEYWORDS**\\n\\n2022 Brazilian Elections, Network Modeling, Online Discussions,\\nTwitter, Natural Language Processing\\n### **1 INTRODUCTION**\\n\\nOnline Social Media Platforms (OSMPs) have served as one of\\nthe main stages for the organization and development of many\\nsignificant social movements worldwide, from health [ 8, 21 ] to\\npolitics [ 9, 34 ]. Platforms such as (former) Twitter [1] [ 30 ], WhatsApp\\n\\n[ 25, 26, 29 ] and Telegram [ 37 ], have been the focus of many works\\nthat aimed to analyze how information is disseminated in such\\nplatforms and its implications to societies.\\nThis scenario is no different in Brazil, where OSMPs have been\\nmainly used for political debates. Several studies in the literature\\n\\n[ 3, 4, 10, 12, 16, 17, 20, 27, 29, 31, 34, 37 ] characterized the Brazilian political debates across a variety of platforms. In contrast to\\nprevious work that focused on Twitter, we here provide a more\\ncomprehensive analysis of the Brazilian political landscape of 2022\\nby using network modeling and backbone extraction methods to\\nshow how the political debate unfolds through different levels of\\nuser engagement and diffusion. To accomplish this, we crawled\\nTwitter to collect about 741K shared tweets covering the days of\\nthe two election. We then model the information spread during\\n\\n1 Twitter has been recently rebranded as X. Yet, we maintain the reference to the old\\nplatform´s name as our study relies on features commonly associated with it.\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n© 2024 SBC – Brazilian Computing Society.\\nISSN 2966-2753\\n\\n## Giovana Piorino\\n#### giovana.piorino@dcc.ufmg.br Universidade Federal de Minas Gerais\\n## Ana Paula Couto da Silva\\n#### ana.coutosilva@dcc.ufmg.br Universidade Federal de Minas Gerais\\n\\neach round using a media-centric network that connects users who\\nshared the same content (overall network) and employ backbone\\nextraction methods to identify the group of users who frequently\\nshare the same pieces of information [ 18, 33 ]. These users are placed\\nin the core of the media-centric network, thereby driving the election debate on Twitter. Finally, we analyzed the text shared by the\\nusers from three different (yet complementary) perspectives: topic\\nextraction, sentiment analysis, and psycholinguistic analysis.\\nOur main findings are as follows: (i) The identification of backbone networks reveals a set of users engaged through stronger\\ninteractions. This structured network may prompt these users to\\nact more cohesively in promoting specific content, thereby influencing discourse on a particular topic. (ii) The topics diffused by\\nusers in backbones differ from those shared by the overall network. Interestingly, the most shared topics in these backbones, not\\npresent in the overall network, are more aligned with supporting\\nthe candidate Luis Inácio Lula da Silva and celebrating his victory.\\n(iii) There is a small set of users who shared a significant amount\\nof information across both election rounds, with a higher potential\\nto reach a larger audience due to their above-average number of\\nfollowers in our dataset.\\n### **2 RELATED WORK**\\n\\nOur work is not the first to examine the Brazilian political scenario on OSMPs. The authors in [ 25, 26 ] analyzed the messages\\nexchanged on WhatsApp during the 2018 Brazilian general elections. Similar to our work, the authors built a network based on\\nusers that shared the same content messages in one or more groups.\\nThey also employed backbone extraction techniques to search for\\nstrongly connected user communities to evidence possible coordination actions in sharing specific content. In the same direction,\\nMachado *et al.* [ 20 ] explored misinformation in WhatsApp content\\nduring the same election period. The main results showed that viral\\ncontent in WhatsApp groups was mainly based on hate speech\\nand fake news. In [ 3 ], the authors characterized the tweets of preelection advertisement for the 2016, 2018 and 2020 elections. By\\napplying psycholinguistic and sentiment analysis techniques, the\\nresults showed that early advertisements are usually negative or\\nneutral, with the neutral sentiment growing over time, and there is\\na pattern in the use of hashtags and links, along with mentions to\\n\\nentities.\\nFocusing specifically on the 2022 Brazilian elections, the study of\\nVenâncio et al . [37] applied the methodology of backbone extraction\\n\\n\\n283\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Larissa Malagoli, Giovana Piorino, Carlos H. G. Ferreira, and Ana Paula Couto da Silva\\n\\n\\nto search for evidences of coordination in the information dissemi\\nnation on Telegram. This study not only highlighted the growing\\ninfluence of messaging apps on political mobilization, but also contributed to the understanding of digital communication strategies in\\nmodern electoral contexts. The authors in [ 23 ] collected data from\\nan online experiment where participants built personalized government programs by combining policies proposed by the candidates\\nof the 2022 French and Brazilian presidential elections, identifying\\npolarizing proposals. Santana *et al.* presented a study that analyzed\\nthe use and engagement of the TikTok profiles of the two leading\\ncandidates: Lula and Bolsonaro [ 31 ]. The authors in [ 17 ] applied\\nsentiment analysis in order to identify gender bias on comments\\non YouTube in the 2018 and 2022 elections.\\n\\nIn the context of Twitter, Silva and Faria [ 34 ] analyzed the sentiments expressed by Twitter users regarding the presidential candidates, with the aim of verifying whether the candidates’ performance is related to their popularity on social media. The authors in\\n\\n[ 32 ] analyzed the opinions of Brazilians about the candidates using\\nmachine learning techniques. The work in [ 4 ] investigated how\\nsentiment analysis was a prominent factor in interpreting the possible relationship between the opinions of social media users and the\\nfinal result of the 2018 elections in Brazil. Paiva *et al.* focused on\\nunderstanding how some feminist causes were addressed during\\nthe elections [ 27 ]. Finally, the authors in [ 16 ] developed a methodology, which is a similar approach used in our work, to discover\\nthe contribution of specific groups to network polarization.\\nThe investigation we offer here complements the aforementioned\\nstudies and greatly builds on their findings by offering a broader\\nset of analysis on a large Twitter dataset. We rely on backbone extraction methods to show how the political debate unfolds through\\ndifferent levels of users’ engagement. Our results enrich the understanding of how online discussions evolved on social media during\\nthis important event in the recent history of democracy in Brazil.\\n### **3 METHODOLOGY**\\n\\nThis section describes our methodology, including data collection,\\nmodeling and analysis.\\n### **3.1 Dataset**\\n\\nWe collect Portuguese-language tweets shared during the two\\nrounds of 2022 Brazilian general elections, occurred on October\\n2 *[𝑛𝑑]*, 2022 and October 30 *[𝑡ℎ]*, 2022.\\nThe collection was done using the Twitter API Search. [2] We built\\na list of keywords that include terms such as the official election\\nkeyword used by the Twitter and the most important presidential\\ncandidates. Specifically, we consider the following list of keywords:\\n*Eleições2022* [3] *, Lula, Bolsonaro, Ciro, SimoneTebet* . [4]\\n\\nTable 1 show the total number of unique tweets and retweets\\nfor each keyword, after filtering tweets with at least two users\\nin the dataset that retweeted that message. Furthermore, tweets\\nwith the same ID that contain multiple keywords were counted\\nonly once. We analyzed approximately 741K shared tweets across\\n\\n2 https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/apireference/get-search-tweets\\n3 Elections2022\\n4 These candidates accounted for over 98% of all votes.\\n\\n\\nthe two election rounds. As expected, in our dataset the official\\nTwitter general election keyword ( *Eleições2022* ) is the most shared\\none, followed by the keywords with the name of the two main\\npresidential candidates.\\n\\n**Table 1: First and Second Round Datasets.**\\n\\n**Keywords** **First Round** **Second Round**\\n**# Unique** **# Unique**\\n**# Retweets** **# Retweets**\\n**Tweets** **Tweets**\\n\\n*Elei* *ç* *ões2022* 2,145 186,594 875 153,525\\n*Bolsonaro* 3,771 154,165 2,716 112,840\\n\\n*Lula* 3,094 100,066 2,930 96,546\\n\\n*Ciro* 1,705 44,185 445 11,995\\n\\n*SimoneTebet* 4 64 4 197\\n\\n**Total** 8,774 409,956 5,835 331,175\\n\\nTo provide a first overview of our data, we look into the contents\\nof the collected retweets. We do so by showing in Figure 1 the word\\nclouds with the top 100 most frequent words (in numbers of the\\nretweets) during the first and second election rounds. In the first\\nround (Figure 1.a) we note that elections related words are predominant, such as *vote* and *president* . Interestingly, the term electronic\\nvoting machine is also one of the most used terms, probably due to\\nthe suspicion about its credibility raised by the supporters of Jair\\nBolsonaro candidate. [5] During the second round word cloud (Figure\\n1.b), we observe the presence of words celebrating the victory of\\nthe Luis Inácio Lula da Silva, such as *victory*, *lulapresidente2022*,\\n*democracy* . Finally, it is worth noting the presence of the word\\n*Northeast* in both election rounds, which is a Brazilian region in\\nwhich Lula has many supporters. [6]\\n### **3.2 Network Modeling**\\n\\nTo investigate the 2022 election debate on Twitter, we employed a\\nnetwork model, known as *media-centric* network, which connects\\nusers who shared similar content [ 5, 18, 25, 26 ]. By analyzing the\\nproperties of such media-centric networks, we are able to determine\\nwhich actors contribute the most to content propagation.\\nWe built two graphs, where each graph represents a mediacentric network capturing the user sharing patterns during each\\nelection round. In each graph *𝐺* ( *𝑉, 𝐸* ), a node *𝑣* ∈ *𝑉* corresponds\\nto a user who retweeted a tweet, and an undirected edge *𝑒* =( *𝑣* *𝑖*, *𝑣* *𝑗* )\\nis included in *𝐸* if the users corresponding to *𝑣* *𝑖* and *𝑣* *𝑗* shared the\\nsame tweet (exactly the same textual content) at least once. The\\nweight of *𝑒* is the number of tweets both users shared in common.\\nThe total amount of retweets shared across each network is the\\n\\nsum of all edge weights.\\n### **3.3** **Key Users Identification**\\n\\nIn addition to providing a general overview of content dissemination on Twitter, the media-centric network model allows us to\\nidentify the group of users who frequently share the same pieces of\\n\\n5 https://www.nytimes.com/2022/09/29/world/americas/election-bolsonaro-brazilfraud.html?smid=url-share\\n6 https://www.cnnbrasil.com.br/politica/nordeste-e-a-unicaregiao-em-que-lula-obteve-mais-votos-que-bolsonaro-confira/\\nhttps://www.theguardian.com/world/2022/nov/01/brazil-election-how-lula-won-therunoff-from-sao-paulo-to-the-north-east\\n\\n\\n284\\n\\n\\n-----\\n\\nTwitter and the 2022 Brazilian Elections Portrait: A Network and Content-Driven Analysis WebMedia’2024, Juiz de Fora, Brazil\\n\\n(a) First Round. (b) Second Round.\\n\\n**Figure 1: Retweets’ word clouds.**\\n\\n\\ninformation, thereby driving the election debate on Twitter. Here,\\nwe denote these users as *key users* .\\nWe next focus on finding pairs of users whose sharing patterns\\nare non-random (strong). In other words, we filter out noisy or\\nsporadic edges, revealing only pairs of users whose shared tweets\\ndeviate disproportionately from the expected number of content\\nshares. To that end, we used the DF+NB network backbone extraction method from the literature to filter out weaker edges, thus\\nretaining only stronger edges [ 18 ]. DF+NB method combines the\\nDisparity Filter method [ 25, 33 ] with the concept of Neighborhood\\nOverlap. Specifically, DF considers as reference model for a user\\nsharing content independently of the others a uniform distribution\\nof the edge weights incident to the corresponding node. Thus, an\\nedge ( *𝑣* *𝑖* *, 𝑣* *𝑗* ) is retained in the backbone if its weight greatly deviates\\n(from a statistical point of view) from this reference model for both\\n*𝑣* *𝑖* and *𝑣* *𝑗* . This method effectively highlights edges that demonstrate\\nconsistent and repeated behavior between pairs of users. With the\\nNeighborhood Overlap filter, DF+NB goes further by removing peripheral and bridge connections, focusing on edges between users\\nwith common neighbors who also share similar patterns of content\\ndissemination. DF+NB showed to be more effective in scenarios\\nwith high levels of noise, as data collected from Twitter [18].\\nTo parametrize the DF+NB method, we set *𝛼* = 0 *.* 05, which represents the evidence of the existence of users whose shared same\\n\\ncontent [ 13 ]. This parameter is the p-value used to test against the\\nassumption of uniform distribution for independent behavior. For\\nthe filter based on the neighborhood overlap metric, we assume the\\nthreshold given by the 95 *[𝑡ℎ]* percentile of the neighborhood overlap\\ndistribution.\\n\\nAfter extracting the backbone of each graph, we applied the\\nwidely used Louvain community detection algorithm [ 2 ] to identify\\nand analyze patterns of user groupings and their organization in\\neach backbone. The goal of the Louvain algorithm is to maximize\\ncommunity modularity, which is a key metric representing the\\ndensity of connections within communities compared to a hypothetical random network. Modularity values range from -0.5 to +1,\\nwith higher scores (above 0.3) indicating well-defined community\\nstructures [24].\\n### **3.4 Content Analysis**\\n\\nBesides identifying the key users who shared a high volume of\\nsimilar information during the two election rounds, we are also\\ninterested in characterizing what they were talking about. In other\\n\\n\\nwords, this study focuses on retweets to analyze content dissemination and user behavior in the electoral context. To achieve this,\\nwe analyzed the text shared by them from three different (yet complementary) perspectives: topic extraction, sentiment analysis, and\\npsycholinguistic analysis.\\n\\n*3.4.1* *Topic Extraction.* To identify the debated topics, we applied\\nthe BERTopic model [ 14 ], which was proved to be one of the best\\nmodels for the analysis of short-text data [6].\\nThe process of BERTopic begins with converting a collection\\nof retweets into vector representations using the BERTimbau, a\\nPortuguese language pre-trained model as a base for the transformer in order to improve the performance [ 11, 22, 28, 35 ]. Subsequently, the dimensionality of these vectors is reduced using the\\n*Uniform Manifold Approximation and Projection for Dimension Re-*\\n*duction* (UMAP) technique, enhancing the efficiency of subsequent\\nclustering processes. Following the dimensionality reduction, the\\n*Hierarchical Density-Based Spatial Clustering of Applications with*\\n*Noise* (HDBSCAN) algorithm groups these low-dimensional vector\\nrepresentations into clusters based on semantic similarities. These\\nclusters (or documents) are then analyzed using the *Class-based*\\n*Term Frequency-Inverse Document Frequency* (c-TF-IDF) technique\\nto identify distinctive words for each cluster, thereby defining the\\ntopics associated with each group of retweets. Here, the *Maxi-*\\n*mal Marginal Relevance* (MMR) parameter was applied in order to\\nachieve better diversification between the topics’ keywords. This\\nparameter limits the number of duplicated words among the topics,\\ncomparing word embeddings with topic embedding. It also leads\\nto a less occurrence of synonyms among the topics, making them\\nmore concise and avoiding redundancy.\\nFor the parameterization, we followed the recommendations\\nin the BERTopic documentation to find a balanced compromise\\nbetween the number of topics and the size of the dataset. [7] As a\\nresult, we obtain the following parameterization: the number of\\nneighbors and the component parameters required by UMAP were\\nset to 10 and 5, respectively. The minimum topic size was set to\\n5, which controls the minimum number of unique retweets on a\\ntopic. The minimum number of words required to visualize topics\\ncontents was set to 20, in order to inform broader content about\\nthe topics’ subjects and their relation to the elections. Finally, MMR\\nwas adjusted to 0.6 (on a scale of 0 to 1).\\n\\n7 https://maartengr.github.io/BERTopic/\\n\\n\\n285\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Larissa Malagoli, Giovana Piorino, Carlos H. G. Ferreira, and Ana Paula Couto da Silva\\n\\n**Table 2: Characterization of the topology of the networks and backbones.**\\n\\n**# Comm.**\\n**Network Date** **# Nodes** **# Edges** **Tweets Retweets** **Avg.** **Avg.** **# Comm.** **With >** **Modul.**\\n**(%)** **(%)** **Degree** **[Density]** **Clustering** **[# Components Size Giant]** **Comp.** **10 Users**\\n\\nComplete 1st Round 71,585 192,539,317 8,774 409,956 5,379.32 0.075 0.6955 13 71,559 25 9 0.38\\nDF+NB 1st Round 5,192 (7.25%) 137,165 (0.0712%) 944 67,521 52.84 0.010 0.4261 433 3,658 457 18 0.42\\nComplete 2nd Round 60,288 152,603,351 5,835 331,175 5,062.48 0.084 0.6958 14 60,258 20 7 0.35\\nDF+NB 2nd Round 3,704 ( 6.14% ) 33,480 ( 0.0219% ) 556 33,637 18.08 0.005 0.4682 308 2,799 335 20 0.68\\n\\n\\n*3.4.2* *Psycholinguistic Analysis.* We delve deeper into the content\\nanalysis by understanding the psycholinguistic properties of the\\nshared text. We rely on the Linguistic Inquiry and Word Count\\n(LIWC) lexicon [ 36 ] to categorize words in the text in linguistic\\nstyle, affective and cognitive attributes. We then compute the average frequency of the attributes over the retweets. In our data, we\\nidentify all 64 attributes, out of the available in LIWC’s Portuguese\\ndictionary.\\nWe then identify attributes that characterize the discourse on\\nour data. We rank the attributes according to their capacity to\\ndiscriminate the retweets, estimated by the Gini Coefficient [ 38 ]\\nand we use the top-10 to create heatmaps that can better highlight\\nattributes associated with our dataset.\\n\\nThe heatmap cells in a column indicate the relative deviation of\\nthe given attribute for the given keyword from the other keywords.\\nIn other words, each column (attribute) is normalized following the\\nz-score – i.e., *𝑧* = ( *𝑥* − *𝑚𝑒𝑎𝑛* )/ *𝑠𝑡𝑑* . Thus, each value gets subtracted\\nfrom the average of the column, then divided by the standard deviation of the column. Locations are color-coded red (resp. blue)\\nwhen the attribute is more (resp. less) present than the average.\\n\\n*3.4.3* *Sentiment Analysis.* Finally, we study the sentiments expressed by individuals debating the elections on Twitter. For this\\npurpose, we used the XLM-RoBERTa [8] ( *Cross Lingual Language*\\n*Model - Robustly Optimized BERT-Pretraining Approach* ) model,\\nwhich is available on the Hugging Face library. This model is a\\nfine-tuned version of RoBERTa [ 19 ], trained on a Twitter database\\ncontaining 198 million multilingual tweets, with Portuguese being\\nthe second most frequently occurring language in these tweets. The\\nXLM-RoBERTa model returns the probabilities of a particular tweet\\nbeing classified as positive, negative, or neutral. In our analysis,\\nwe classify the sentiment of a tweet as the class with the highest\\nprobability assigned by the model.\\n### **4 RESULTS**\\n\\nThis section presents our results and their findings.\\n### **4.1 Topological Analysis**\\n\\nTable 2 shows the analysis of the network topologies for the two\\nevents of interest, revealing distinct structures between the complete networks and their respective backbones. DF+NB significantly\\nreduces the weak links in both graphs, leading to reductions in the\\nnumber of nodes and edges. Considering the first round, the complete network consists of 71,585 nodes and 192,539,317 edges with\\nan average degree of 5,379.32. In contrast, applying the DF+NB\\nmethod reduces this network to 5,192 nodes and 137,165 edges. In\\n\\n8 https://huggingface.co/docs/transformers/model_doc/xlm-roberta\\n\\n\\nthe second round, instead, the complete network consists of 60,288\\nnodes and 152,603,351 edges with an average degree of 5,062.48.\\nApplying the DF+NB method reduces this network to 3,704 nodes\\nand 33,480 edges. Both backbones highlight the users who are most\\nactive in sharing content: they retweeted, approximately, 16% and\\n10% of the produced tweets.\\nOur results also show the increase in the modularity metric,\\nespecially regarding the backbones. This suggests a highly connected and structured community networks, showing the potential\\nof DF+NB in filtering out noise in the data. Moreover, this increase\\nin modularity reveals the growing complexity of the interaction network across the two election rounds, which is further highlighted\\nby the rise in the number of larger communities in the resulting\\ntopologies.\\n*Takeaway.* The identification of backbone networks reveals a set\\nof users engaged through stronger interactions. This structured network may prompt these users to act more cohesively in promoting\\nspecific content, thereby influencing the discourse.\\n### **4.2 Content Analysis**\\n\\nHere, we split our content analysis into two types of dissemination:\\n*widespread dissemination*, which examines the complete mediacentric networks of the two election rounds, and *key users’ dissemi-*\\n*nation*, where we analyze only the retweets shared by the users in\\nthe backbones. While the former provides a broader overview of\\nthe debate surrounding the theme we are interested in, the latter\\nallows us to delve deeper into the core of the discussion, filtering\\nout the weak interactions that may obscure the main topics and\\nconcerns regarding the Brazilian elections.\\n\\n*4.2.1* *Widespread Dissemination.* We first look at the disseminated\\ntopics by all Twitter users in our dataset. This analysis was performed using the BERTopic model for the tweets disseminated\\nover the two election rounds. Initially, 192 topics were identified\\nthrough the application of BERTopic. However, in order to focus\\nour analysis on the most influential discussions, we prioritize the\\n20 most popular discussed topics, in number of retweets, being able\\nto shed light on the predominant themes of the Twitter users. Table\\n3 provides a comprehensive overview of the final topics, including\\nthe most discriminating words and a brief description of each topic.\\nTopics 1, 2, 3, 7, 11, 13, 14 and 18 are mainly related to Lula’s\\nvictory. Topics 1 and 18 highlight the Brazilian regions in which\\nLula was the candidate winner as well as the fact that the United\\nStates President, Joe Biden, was a one of the first to internationally\\nrecognize and congratulate Lula’s victory. [10] To illustrate, the most\\n\\n10 https://www.whitehouse.gov/briefing-room/statementsreleases/2022/10/30/statement-by-president-joe-biden-congratulating-luiz-inaciolula-da-silva-as-president-of-brazil/\\n\\n\\n286\\n\\n\\n-----\\n\\nTwitter and the 2022 Brazilian Elections Portrait: A Network and Content-Driven Analysis WebMedia’2024, Juiz de Fora, Brazil\\n\\n**Table 3: Top discussion topics found on Twitter.**\\n\\n**ID** **#** **#** **Most Discriminative Words** **Description**\\n\\n**Tweets** **Retweets**\\n\\n1 587 48978 inácio, silva, luiz, president, biden, elected, vic- Discusses President Lula’s victory in the election in the second round\\ntory, new, brazil, luis and the possibility of his upcoming victory during the first round. Cites\\nJoe Biden, president of the United States, who was one of the first international fi g ures to reco g nize Lula’s election.\\n2 473 30496 elections2022, turn, turned, lulanofirstturn13, Regarding the turnaround in votes that Lula had, when the votes from\\nnortheast, elections2022, lulapresident1, elec- the northeast began to be counted.\\ntions2022, turnaround, lulinha\\n\\n3 76 28892 supporters, celebrating, turnaround, victory, Refers to Lula’s victory and the voters’ celebration.\\np art y, celebrate, streets, brasília, a g ainst, p etista\\n\\n4 167 24892 way, third, fault, simone, chance, second, have, Mentions the discourse of a third way of opposition to Lula and Bolsonaro,\\nciro, y ou, voted q uotin g candidates Ciro Gomes and Simone Tebet.\\n5 97 22882 history, times, time, re-election, since, 1st, pres- Topic that debates about the possibility of reelection of Bolsonaro and\\nident, re-elect, term, succeeds the fact of him bein g the first Brazilian p resident to not be re-elected.\\n6 145 18158 mourning, thousand, pandemic, 700, covid, Issues and fatalities that occurred during Bolsonaro’s administration in\\ndeaths, dead, p eo p le, lost, durin g the COVID-19 p andemic p eriod.\\n7 293 14738 over, nightmare, goodbye, won, bye, lulapresi- Electoral opponents of the Bolsonaro government celebrating the election\\ndent2022, end, well, above, finall y results.\\n8 65 13181 deputy, federal, paulo, ferreira, voted, mg, niko- Mentions the State’s Elections for House of Representatives and Senate.\\nlas, elected, p araná, senator\\n\\n9 42 12755 elected, federal, woman, first, paulo, historic, Comments on the electoral victory of women for the position of consenate, all, damares, a g ainst g resswomen.\\n10 25 11790 stupid, general, voting, others, vote, for him, Criticizes voters for their decision to vote on polemic candidates from\\nenou g h, re g ions, minas, ri g ht far ri g ht, includin g Bolsonaro.\\n11 124 11604 thank you, thank you, congratulations, god, People celebrating and thanking the Brazilian democracy regime with\\ng ood, democrac y, countr y, all, sir, above Lula’s election.\\n12 60 11202 lost, neymar, fall, equal, lose, falling, stick, cup, Mentions terms related to the World Cup, which took place close to the\\nthis, in this election p eriod.\\n13 40 11181 lo, lulapresident2022, let’s go, turn, big, lulapres- Talks about Lula’s victory and his first speech.\\nident1, victor y, moment, luiz, listen\\n\\n14 53 11140 urgent, missing, only, less, missing, thousand, Refers to the first round when Lula led with 48.43% of the votes and\\nvictor y, lulaonFirstRound13, elections2022, g ive almost was elected and the victor y of Lula in second round.\\n15 51 10758 was, fraud, winning, won, good, right, talking, Debates about the turnaround, with some users using the discourse of\\nu p, the, turned electoral fraud.\\n16 168 10030 zema, minas, nikolas, strange, general, some- Discussion of the voting outcomes for the state of Minas Gerais, debating\\nthing, mg, winning, wrong, vote on how the senate and governor votes were for far right candidates, but\\nthe most voted for p resident in the re g ion was Lula.\\n17 477 9902 voted, simone, blank, null, asshole, voted, ciro, Critics on null votes and about votes for the third and fourth place candig et, y ou, dick dates of the p residential election.\\n18 53 9704 states, leads, northeast, all, general, leading, mi- Comments on the regions of Brazil that Lula was leading the dispute.\\nnas, bahia, re g ion, mato\\n\\n19 263 9454 street, you, are, any, stay, what, someone, any, A topic with common used words in tweets in Portuguese, commenting\\ntweet, p eo p le the event.\\n20 41 9367 first, woman, elected, federal, PT, paulo, new, Discusses the first trans women elected for different Brazilian states as\\nsomethin g, su pp ort, s p con g resswomen.\\n\\n\\nretweet content in topic 18 (4,419 retweets) is: *Lula leads in all*\\n*states of the Northeast* . Furthermore, discussions also celebrate *the*\\n*victory of democracy*, reflecting disapproval of the previous Brazilian\\npresident’s governance.\\nSome topics reveal the main themes that attracted attention\\nduring the elections. For instance, topic 15 focuses on a recurring\\ntheme consistently explored by far-right supporters: the possibility\\nof election fraud. Topics 4 and 17 underscore the spread of discussions surrounding the concept of the *third way*, an alternative\\nproposed by certain individuals aiming to circumvent the polarization between the two leading candidates, Lula and Bolsonaro.\\n\\n\\nThis approach encourages voters to contemplate voting for candidates such as Simone Tebet and Ciro Gomes [11] . Topic 6 reflects the\\ncontroversial decisions taken during the period of the COVID-19\\npandemic by Jair Bolsonaro’s government. [12]\\n\\nBesides the discussion around the presidential candidates, our\\ndata also highlights retweets about the states’ government elections\\nas well as the deputy elections. Topics 8 and 16 focus on the discussion around candidates from Minas Gerais state. In particular, topic\\n\\n11 https://www.lemonde.fr/en/international/article/2022/09/28/brazil-election-thirdway-candidates-gain-little-ground-against-lula-and-bolsonaro_5998463_4.html\\n12 https://www.kcl.ac.uk/covid-19-in-brazil-how-jair-bolsonaro-created-a-calamity\\n\\n\\n287\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Larissa Malagoli, Giovana Piorino, Carlos H. G. Ferreira, and Ana Paula Couto da Silva\\n\\n**Table 4: Top 20 discussion topics found on Twitter for backbones.**\\n\\n**ID** **#** **#** **Most Discriminative Words** **Description (New Topics Only)**\\n\\n**Tweets** **Retweets**\\n\\n1 587 8005 inácio, silva, luiz, president, biden, elected, victor y, new, brazil, luis\\n2 473 5865 elections2022, turn, turned, lulanofirstturn13,\\nnortheast, elections2022, lulapresident1, elections2022, turnaround, lulinha\\n\\n3 76 3893 supporters, celebrating, turnaround, victory,\\np art y, celebrate, streets, brasília, a g ainst, p etista\\n4 79 3684 difference, falls, million, less, fell, thousand, 46, Highlights the difference between Lula’s and Bolsonaro’s votes\\nonl y, elections2022, votes\\n\\n5 53 3112 urgent, missing, only, less, missing, thousand,\\nvictor y, lulaonFirstRound13, elections2022, g ive\\n6 97 2987 history, times, time, re-election, since, 1st, president, re-elect, term, succeeds\\n7 42 2732 elected, federal, woman, first, paulo, historic,\\nsenate, all, damares, a g ainst\\n8 56 2418 must, minutes, prf, 19, 10, next, in this, night, Discuss the projections by major news agencies, which estimate that Lula\\ng lobo, cam p ai g n would sur p ass Bolsonaro in votes\\n9 41 2179 first, woman, elected, federal, PT, paulo, new,\\nsomethin g, su pp ort, s p\\n10 177 2126 2nd, datafolha, 1st, round, second, presidential, DataFolha survey indicating a high likelihood of second-round runoffs\\nelections2022, g overnor, will, need for the p residential race\\n11 53 1976 states, leads, northeast, all, general, leading, minas, bahia, re g ion, mato\\n12 167 1874 way, third, fault, simone, chance, second, have,\\nciro, y ou, voted\\n13 65 1838 deputy, federal, paulo, ferreira, voted, mg, nikolas, elected, p araná, senator\\n14 45 1776 advantage, over, continues, determined, 47, 90, After the majority of voting machine results were cleared, Lula was\\nmillion, ballots, leadershi p, almost leadin g the race, s p arkin g wides p read discussion amon g voters\\n15 83 1667 northeast, arriving, always, north, pará, elec- Tweets celebrating the Northeast region votes were being counted, which\\ntions2022, bahia, re g ion si g nificantl y im p acted the votin g results in favor of Lula\\n16 69 1652 health, want, education, people, good, freedom, Concerns about education and health issues\\nlife, live, g overn, because\\n\\n\\n17 40 1565 lo, lulapresident2022, let’s go, turn, big, lulapresident1, victor y, moment, luiz, listen\\n18 64 1526 turn, delicious, lulinha, lulapresident, lulanomelhorturno13, calm, elections2022, god, turned,\\nnortheast\\n\\n\\nTweets with the use of \"He who laughs last, laughs best\" to comment on\\nLula’s victory in the election results.\\n\\n\\n19 293 1454 over, nightmare, goodbye, won, bye, lulapresident2022, end, well, above, finall y\\n20 37 1443 amazons, pandemic, during, vote, for him, Controversial outcomes arose from the presidential election in the state\\nshame, seems, man y, leadin g, leads of Amazonas, due to Bolsonaro’s actions durin g the COVID-19 crisis [9]\\n\\n\\n16 raised questions about the apparent contradictions in the voting\\npatterns, where state voters gave victory to Lula as president while\\nsimultaneously voting predominantly for far-right candidates for\\nthe state government, deputies, and senators. Moreover, topics 9\\nand 20 are related with the increase of the number of elected female\\n\\ncandidates in 2022 elections.\\n\\nFigure 2 shows the percentage of retweets per topic in each round.\\nAs expected, some topics were more prominent in one election\\nround than in the other, due to the nature of the discussions. For\\ninstance, the elections for the chamber of deputies and senators\\n(topics 8, 10, 16 and 18), as well as Lula’s victory (topics 1, 3, 11\\nand 13), were more prominent in specific rounds. However, some\\n\\n\\ntopics were broadly discussed almost equally in both rounds (topics\\n2, 15, 19 and 20). Among these topics we highlight the topic 15,\\nwhich was related to possible frauds in the election, a theme highly\\nexplored mainly by the far-right voters, through the dissemination\\nof fake news and misinformation about the matter. [13]\\n\\nWe now turn our attention to the psycholinguistic analysis of the\\ndebate. Figure 3 shows the results. In the first round, we emphasize\\nthe notable presence of words related to *home, money, assent, see*\\n*and sexual* . Retweets containing words associated with *home* often\\ndepict individuals describing their voting experiences (leaving their\\n\\n13 https://www.nytimes.com/interactive/2022/10/25/world/americas/brazilbolsonaro-misinformation.html\\n\\n\\n288\\n\\n\\n-----\\n\\nTwitter and the 2022 Brazilian Elections Portrait: A Network and Content-Driven Analysis WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\n100\\n\\n\\n\\n80\\n\\n60\\n\\n40\\n\\n20\\n\\n0\\n\\n|Col1|Col2|\\n|---|---|\\n|||\\n\\n\\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\\n\\nTopics\\n\\n**Figure 2: Percentage of normalized retweets per topic.**\\n\\nhomes to go vote) [14] or discussing family-related matters such as\\nlosses due to the COVID-19 pandemic. [15] Money related words\\nare mainly related to economical concerns. In the second round,\\nretweets frequently use words regarding *death, friend, religion* and\\n*positive emotion* . Interestingly, religion (moral and religious concerns) was actually a theme highly emphasized by Bolsonaro’s\\ncampaign. [16] Positive emotions were probably expressed by the\\nLula’s supporters due to his victory. Death, instead, was closely\\nrelated to the retweets regarding the COVID-19 pandemic and the\\nway Bolsonaro’s government deals with it.\\n\\n0 *.* 50 0 *.* 75 1 *.* 00 1 *.* 25 1 *.* 50\\n\\nAssent\\n\\nDeath\\n\\nFriend\\n\\nHome\\n\\nMoney\\nNonfluencies\\nPositive Emotion\\n\\nSee\\n\\nSexual\\n\\nReligion\\n\\nFirst Round Second Round\\n\\n**Figure 3: Top-10 LIWC attributes (Complete networks).**\\n\\nFinally, we focus on the sentiment analysis. Table 5 shows the\\noverall sentiment distribution of the retweets. Negative sentiments\\ndominate across both election rounds. However, the percentage\\nof positive retweets increases by 2.2 times in the second round,\\ncorroborated by the increase in positive emotion-related words in\\nthe retweets (see Figure 3).\\nWe go further in our analysis by presenting the sentiment breakdown by topic. Figure 4 summarizes the *contrastive score*, calculated\\nas the difference between the fraction of positive and negative\\nretweets. Across both rounds, negative sentiment predominates\\n\\n14 \"They abused the public sector, lied, threatened believers and employees, attempted\\na coup, used the police to stop voters on their way to vote. It didn’t help. “Good\\nevening, President Lula! - popular resistance won.” Read and enjoy @Maufalavigna\\n#DomingoDetremuraSDV\"\\n15 \"The route from home to the polling place passes through my work and the UBS\\nwhere I took the 4th dose (the one in the photo). On the way, all I could think about\\nwas the 9 patients I lost. In my mother’s desperation for me to get vaccinated... While\\nBolsonaro was riding a jet-ski. #Eleicao2022\"\\n16 https://edition.cnn.com/2022/10/29/americas/brazil-elections-gun-religion-intllatam/index.html\\n\\n\\n**Figure 4: Contrasting sentiment score (complete networks).**\\n\\noverall. Topics 3 and 14, regarding Lula’s victory, were exceptions\\nto this trend. The analysis captures an important insight regarding\\nthe polarized nature of political debate, particularly on platforms\\nlike Twitter [1, 7, 15].\\n\\n**Table 5: Sentiment distribution (complete networks).**\\n\\n**Date** **Ne** **g** **ative** **(** **%** **)** **Positive** **(** **%** **)** **Neutral** **(** **%** **)**\\n1st Round 243,098 (60 *.* 1%) 17,791 (4 *.* 4%) 143,333 (35 *.* 5%)\\n2nd Round 171,128 ( 53 *.* 3% ) 31,863 ( 9 *.* 9% ) 118,325 ( 36 *.* 8% )\\n\\n*4.2.2* *Key Users’ Dissemination.* We now turn our attention to the\\ncontent diffused by the *key users*, whose belong to the DF+NB\\nbackbones. Their interactions extend beyond random occurrences,\\nbeing in the core of the discussion across the analyzed Twitter\\nnetworks.\\n\\nTable 4 lists the top 20 topics retweeted by these users. Twelve of\\nthese topics are the same as those shared by all users in the complete\\nnetworks, though they may appear in a different order. Analyzing\\nthe complementary set of topics, we observe that these topics are\\nmore related to Lula’s performance in the election, as well as to\\nsome issues usually raised by the opposition to Bolsonaro, such as\\nconcerns about previous government actions towards education,\\nhealth, and the COVID-19 crisis. We also note that the majority of\\nretweets in the top 20 for the backbones are related to discussing\\nor celebrating Lula’s victory.\\nOur data unveils interesting changes in the psycholinguistic\\nattributes of the content shared by users in the extracted backbones.\\nFigure 5 shows these results. Words related to *family* prevail in\\nthe content shared by these users, mainly in the first round. To\\nbetter understanding what users shared in this topic, we manually\\nanalyzed our data. These messages mainly mentioned Bolsonaro’s\\nfamily, which is strongly involved in politics and has been at the\\ncenter of several controversial situations reported by the media, as\\nwell as family conflicts due to the political polarization, which was\\na remarkable characteristic of 2022 Brazilian elections. Regarding\\nthe second election round, retweets with *religion* related words\\nattracted more attention of these users.\\n\\n**Table 6: Sentiment distribution (key users).**\\n\\n**Date** **Ne** **g** **ative** **(** **%** **)** **Positive** **(** **%** **)** **Neutral** **(** **%** **)**\\n1st Round 29,815 (44 *.* 16%) 2,238 (3 *.* 31%) 35,468 (52 *.* 53%)\\n2nd Round 15,545 ( 46 *.* 21% ) 4,399 ( 13 *.* 08% ) 13,693 ( 40 *.* 71% )\\n\\nLastly, results on the sentiment analysis are shown in Figure\\n6 and Table 6. Key users tend to balance the shared content with\\nnegative and neutral sentiments in both rounds. Unlike users from\\nthe complete networks, key users shared more tweets with neutral\\n\\n\\nFirst Round\\n\\nSecond Round\\n\\n\\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\\n\\nTopics\\n\\n\\n1.0\\n\\n0.5\\n\\n0.0\\n\\n0.5\\n\\n1.0\\n\\n\\n289\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Larissa Malagoli, Giovana Piorino, Carlos H. G. Ferreira, and Ana Paula Couto da Silva\\n\\n\\n0 *.* 50 0 *.* 75 1 *.* 00 1 *.* 25 1 *.* 50\\n\\nAssent\\n\\nCertain\\n\\nDeath\\n\\nFamily\\nHumans\\nNonfluencies\\nPositive Emotion\\n\\nSee\\n\\nWork\\n\\nReligion\\n\\nFirst Round Second Round\\n\\n**Figure 5: Top-10 LIWC attributes (key users).**\\n\\nsentiments. Moreover, retweets shared in the second round tend to\\nbe more positive than those shared by the overall users. Specifically,\\nwe highlight topic 14, which pertains to Lula’s leadership in the\\nsecond round. These retweets are highly positive, suggesting strong\\nsupport from these users for the possibility of Lula’s victory.\\n*Takeaway.* The topics diffused by the users in backbones (core)\\ndiffer from the ones shared by the overall network (with peripheral\\nusers as well). Interestingly, the most shared topics in backbones\\nthat are not on the overall networks are more aligned with supporting Lula and celebrating his victory. Sentiments towards the\\ncontent are more positive in the second round, mainly considering the topic in which Lula’s victory is discussed (topic 14). The\\npercentage of negative retweets is smaller in the two rounds.\\n\\n\\nFirst Round\\n\\nSecond Round\\n\\n\\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\\n\\nTopics\\n\\n\\n1.0\\n\\n0.5\\n\\n0.0\\n\\n0.5\\n\\n1.0\\n\\n\\n**Figure 6: Contrasting sentiment score (key users).**\\n### **4.3 Persistence Analysis**\\n\\nWe next focus on understanding if the key users persist over time.\\nSpecifically, we consider the users in DF+NB and analyze the dynamics of these users over the two election rounds.\\n\\nA total of 624 users, representing 7% of users on the DF+NB\\nbackbones, shared information across the two rounds. This small\\npercentage of users demonstrated significant activity, retweeting\\nalmost 22% of the messages shared across the events of interest\\nfor the backbones. All persistent users are non-verified users [17]\\n\\nand they have, in general, more followers than the users in the\\ncomplete network. [18] Due to space constrains, we focus our final\\ndiscussion on the main topics persistent users boost the most within\\nTwitter networks. The most shared topics by the persistent users are\\nalmost the same that those shared by the key users, except by one\\ntopic with the following most discriminative words: *bahia, millions,*\\n*cleared, still, people, day, voting, missing, voted*, which focuses on\\nLula’s leadership in the second round, with a significant margin of\\nvotes in the state of Bahia.\\n\\n17 Our data were collected previous the introduction of plans to buy the blue ticks.\\n18 We omitted the probability distribution due space constraints.\\n\\n\\n*Takeaway.* Although the set of persistent users is small (7%), our\\nresults suggest that they play an important role in content diffusion\\nby the key users, accounting for 22% of the retweets. The topics\\nthey share are almost identical to those shared by the key users, and\\ntheir potential to reach a larger audience is higher than that of users\\nin the complete networks, as they typically have more followers.\\n### **5 DISCUSSION**\\n\\nThe influence of online social networks on people’s actions and\\nbeliefs has grown significantly over the past decade, impacting\\neveryday life. Regarding politics, these platforms provide citizens\\nwith a way to voice their opinions and connect with other voters\\nthrough content dissemination. In this work, we characterized the\\ndebate surrounding the Brazilian elections of 2022 on Twitter by\\nexploring the shared content among users’ interactions. We conducted our analysis over two types of interactions: the weak ones,\\nwhich tend to be randomly made by the involved users (mainly on a\\n*noisy* network, such as Twitter), and the strong ones, which are consistently made by a set of core users. The core users were identified\\nthrough backbone extraction techniques found in the literature.\\nOur findings show that the topics, explored by the overall users\\nin both rounds, reflect individuals’ opinions on election results,\\ncontroversies surrounding the previous government, and Brazilian\\npolitical events in general. Among the most shared topics by key\\nusers, they largely retained the most widely disseminated topics,\\nthough they diverged slightly by focusing on disseminating more\\ntopics that supported Lula’s victory or discussed matters against\\nprevious government decisions, particularly related to the pandemic period and government affairs such as health and education.\\nPersistent users, who actively engaged in the debate in both rounds,\\nare users with considerable numbers of followers and unverified\\naccounts. The main disseminated topics by them were almost the\\nsame as the ones shared by the key users, underscoring this group as\\na representation of the core network with significant responsibility\\nfor the topics disseminated.\\nThe prevalence of psycholinguistic attributes associated with\\n*home* (in the first round network) and *family* (in the key and persistent users group) was prominent in the initial round. However,\\nin the second round, there was an increase in the use of words\\nrelated to *death, positive emotion* and *religion* . These attributes align\\nwith expectations regarding topics heavily discussed during these\\nelections, such as speeches focusing on religion and family and\\ntheir impact on the general voters, as well as the dissemination\\nof information and opinions on deaths during the pandemic and\\nLula’s electoral victory. Negative sentiment prevailed in the debate,\\ncharacterized by contentious issues and high polarization, but the\\ndissemination by key users showed more balance, with a significant\\npercentage of neutral sentiment as well.\\nThe main limitation of our analysis is due to the way we collected\\nour data. By utilizing a select set of keywords, chosen based on our\\nperceived representativeness as the event of interest, we were only\\nable to capture a partial view of the overall Twitter debate. Despite\\nthis limitation, our work enriches the understanding of how the\\n2022 Brazilian elections were discussed on Twitter.\\n\\n**Acknowledgements** The research leading to these results has\\nbeen funded by CNPQ, CAPES and FAPEMIG.\\n\\n\\n290\\n\\n\\n-----\\n\\nTwitter and the 2022 Brazilian Elections Portrait: A Network and Content-Driven Analysis WebMedia’2024, Juiz de Fora, Brazil\\n\\n### **REFERENCES**\\n\\n[1] Peiman Barnaghi, Parsa Ghaffari, and John G. Breslin. 2016. Opinion Mining and\\nSentiment Polarity on Twitter and Correlation between Events and Sentiment.\\nIn *2016 IEEE Second International Conference on Big Data Computing Service and*\\n*Applications (BigDataService)* . 52–57. https://doi.org/10.1109/BigDataService.\\n2016.36\\n\\n[2] Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. 2008. Fast unfolding of comm. in large networks. *Journal of Statisti-*\\n*cal Mechanics: Theory and Experiment* 2008, 10 (oct 2008), P10008. https:\\n//doi.org/10.1088/1742-5468/2008/10/P10008\\n\\n[3] Josemar Caetano, Samuel Guimarães, Marcelo M. R. Araújo, Márcio Silva, Júlio\\nC. S. Reis, Ana P. C. Silva, Fabrício Benevenuto, and Jussara M. Almeida. 2022.\\nCharacterizing Early Electoral Advertisements on Twitter: A Brazilian Case Study.\\nIn *Social Informatics*, Frank Hopfgartner, Kokil Jaidka, Philipp Mayr, Joemon Jose,\\nand Jan Breitsohl (Eds.). Springer International Publishing, Cham, 257–272.\\n\\n[4] André Cristiani, Douglas Lieira, and Heloisa Camargo. 2020. A Sentiment Analysis of Brazilian Elections Tweets. In *Anais do VIII Symposium on Knowledge*\\n*Discovery, Mining and Learning* (Evento Online). SBC, Porto Alegre, RS, Brasil,\\n153–160. https://doi.org/10.5753/kdmile.2020.11971\\n\\n[5] Jose Martins da Rosa, Renan Saldanha Linhares, Carlos Henrique Gomes Ferreira,\\nGabriel P. Nobre, Fabricio Murai, and Jussara M. Almeida. 2022. Uncovering\\nDiscussion Groups on Claims of Election Fraud from Twitter. In *Proc. of Social*\\n*Informatics: 13th International Conference* . https://doi.org/10.1007/978-3-03119097-1_20\\n\\n[6] Roman Egger and Joanne Yu. 2022. A Topic Modeling Comparison Between LDA,\\nNMF, Top2Vec, and BERTopic to Demystify Twitter Posts. *Frontiers in Sociology*\\n7 (05 2022). https://doi.org/10.3389/fsoc.2022.886498\\n\\n[7] Anna Fang and Zina Ben-Miled. 2017. Does bad news spread faster?. In *2017*\\n*International Conference on Computing, Networking and Communications (ICNC)* .\\n793–797. https://doi.org/10.1109/ICCNC.2017.7876232\\n\\n[8] Juliana Feitosa, Luiz De Camargo, Eloisa Bonatti, Giovanna Simioni, and José\\nBrega. 2023. eXplainable Artificial Intelligence in sentiment analysis of posts\\nabout Covid-19 vaccination on Twitter. In *Proceedings of the 29th Brazilian Sym-*\\n*posium on Multimedia and the Web* (Ribeirão Preto/SP). SBC, Porto Alegre, RS,\\nBrasil, 65–72. https://sol.sbc.org.br/index.php/webmedia/article/view/25867\\n\\n[9] Emilio Ferrara, Herbert Chang, Emily Chen, Goran Muric, and Jaimin Patel. 2020.\\nCharacterizing social media manipulation in the 2020 U.S. presidential election.\\n*First Monday* (2020). https://doi.org/10.5210/fm.v25i11.11431\\n\\n[10] Carlos HG Ferreira, Fabricio Murai, Ana PC Silva, Jussara M Almeida, Martino\\nTrevisan, Luca Vassio, Marco Mellia, and Idilio Drago. 2021. On the dynamics of\\npolitical discussions on instagram: A network perspective. *Online Social Networks*\\n*and Media* 25 (2021), 100155.\\n\\n[11] E Fonseca, L Santos, Marcelo Criscuolo, and S Aluisio. 2016. ASSIN: Avaliacao\\nde similaridade semantica e inferencia textual. In *Computational Processing of the*\\n*Portuguese Language-12th International Conference, Tomar, Portugal* . 13–15.\\n\\n[12] Carlos Henrique Gomes Ferreira, Fabricio Murai, Ana Paula Couto da Silva,\\nJussara Marques de Almeida, Martino Trevisan, Luca Vassio, Idilio Drago, and\\nMarco Mellia. 2020. Unveiling community dynamics on instagram political\\nnetwork. In *Proceedings of the 12th ACM Conference on Web Science* . 231–240.\\n\\n[13] Carlos Henrique Gomes Ferreira, Fabricio Murai, Ana P. C. Silva, Martino Trevisan, Luca Vassio, Idilio Drago, Marco Mellia, and Jussara M. Almeida. 2022. On\\nnetwork backbone extraction for modeling online collective behavior. *PLOS ONE*\\n17, 9 (09 2022), 1–36. https://doi.org/10.1371/journal.pone.0274218\\n\\n[14] Maarten Grootendorst. 2022. BERTopic: Neural topic modeling with a class-based\\nTF-IDF procedure. *arXiv preprint arXiv:2203.05794* (2022).\\n\\n[15] Joshua Guberman, Carol E. Schmitz, and Libby Hemphill. 2016. Quantifying\\nToxicity and Verbal Violence on Twitter. In *Proceedings of the 19th ACM Conference*\\n*on Computer Supported Cooperative Work and Social Computing, CSCW 2015, San*\\n*Francisco, CA, USA, February 27 - March 2, 2016, Companion Volume*, Darren\\nGergle, Meredith Ringel Morris, Pernille Bjørn, and Joseph A. Konstan (Eds.).\\nACM, 277–280. https://doi.org/10.1145/2818052.2869107\\n\\n[16] Ruben Interian and Francisco A Rodrigues. 2023. Group polarization, influence,\\nand domination in online interaction networks: a case study of the 2022 Brazilian\\nelections. *Journal of Physics: Complexity* 4, 3 (Sept. 2023), 035008. https://doi.\\norg/10.1088/2632-072x/acf6a4\\n\\n[17] Andressa Kappaun and Jonice Oliveira. 2023. Análise sobre Viés de Gênero no\\nYoutube: Um Estudo sobre as Eleições Presidenciais de 2018 e 2022. In *Anais do XII*\\n*Brazilian Workshop on Social Network Analysis and Mining* (João Pessoa/PB). SBC,\\nPorto Alegre, RS, Brasil, 127–138. https://doi.org/10.5753/brasnam.2023.230625\\n\\n[18] Renan S. Linhares, José M. Rosa, Carlos H. G. Ferreira, Fabricio Murai, Gabriel\\nNobre, and Jussara Almeida. 2022. Uncovering Coordinated Communities on\\nTwitter During the 2020 U.S. Election. In *Proc. of ASONAM* .\\n\\n[19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A\\nRobustly Optimized BERT Pretraining Approach. *CoRR* abs/1907.11692 (2019).\\narXiv:1907.11692 http://arxiv.org/abs/1907.11692\\n\\n\\n\\n[20] Caio Machado, Beatriz Kira, Vidya Narayanan, Bence Kollanyi, and Philip Howard.\\n2019. A Study of Misinformation in WhatsApp Groups with a Focus on the\\nBrazilian Presidential Elections.. In *Companion Proceedings of The 2019 World Wide*\\n*Web Conference* (San Francisco, USA) *(WWW ’19)* . Association for Computing\\nMachinery, New York, NY, USA, 1013–1019. https://doi.org/10.1145/3308560.\\n3316738\\n\\n[21] Larissa G. Malagoli, Julia Stancioli, Carlos H. G. Ferreira, Marisa Vasconcelos,\\nAna Paula Couto da Silva, and Jussara M. Almeida. 2021. A Look into COVID19 Vaccination Debate on Twitter. In *Proceedings of the 13th ACM Web Science*\\n*Conference 2021* (Virtual Event, United Kingdom) *(WebSci ’21)* . Association for\\nComputing Machinery, New York, NY, USA, 225–233. https://doi.org/10.1145/\\n3447535.3462498\\n\\n[22] Philip May. 2021. Machine translated multilingual STS benchmark dataset. https:\\n//github.com/PhilipMay/stsb-multi-mt\\n\\n[23] Carlos Navarrete, Mariana Macedo, Rachael Colley, Jingling Zhang, Nicole Ferrada, Maria Eduarda Mello, Rodrigo Lira, Carmelo Bastos-Filho, Umberto Grandi,\\nJerome Lang, and César A. Hidalgo. 2023. Understanding Political Divisiveness\\nusing Online Participation data from the 2022 French and Brazilian Presidential\\nElections. arXiv:2211.04577 [cs.CY]\\n\\n[24] M. E. J. Newman and M. Girvan. 2004. Finding and evaluating community\\nstructure in networks. *Phys. Rev. E* 69 (Feb 2004), 026113. Issue 2. https://doi.\\norg/10.1103/PhysRevE.69.026113\\n\\n[25] Gabriel Nobre, Carlos Ferreira, and Jussara Almeida. 2020. Beyond Groups:\\nUncovering Dynamic Communities on the WhatsApp Network of Information\\nDissemination. In *SocInfo’ 2020* .\\n\\n[26] Gabriel Peres Nobre, Carlos H.G. Ferreira, and Jussara M. Almeida. 2022. A\\nHierarchical Network-Oriented Analysis of User Participation in Misinformation\\nSpread on WhatsApp. *Information Processing and Management* 59, 1 (jan 2022),\\n21 pages. https://doi.org/10.1016/j.ipm.2021.102757\\n\\n[27] Beatriz Paiva, Beatriz Barbosa, Ana Silva, and Mirella Moro. 2023. O debate do\\nfeminismo no Twitter: Um estudo de caso das eleições brasileiras de 2022. In\\n*Anais do XII Brazilian Workshop on Social Network Analysis and Mining* (João\\nPessoa/PB). SBC, Porto Alegre, RS, Brasil, 103–114. https://doi.org/10.5753/\\nbrasnam.2023.230537\\n\\n[28] Livy Real, Erick Fonseca, and Hugo Goncalo Oliveira. 2020. The assin 2 shared\\ntask: a quick overview. In *International Conference on Computational Processing*\\n*of the Portuguese Language* . Springer, 406–412.\\n\\n[29] Julio Reis, Philipe Melo, Fabiano Belém, Fabricio Murai, Jussara Almeida, and\\nFabricio Benevenuto. 2023. Helping Fact-Checkers Identify Fake News Stories\\nShared through Images on WhatsApp. In *Proceedings of the 29th Brazilian Sym-*\\n*posium on Multimedia and the Web* (Ribeirão Preto/SP). SBC, Porto Alegre, RS,\\nBrasil, 159–167. https://sol.sbc.org.br/index.php/webmedia/article/view/25877\\n\\n[30] Gonzalo A. Ruz, Pablo A. Henríquez, and Aldo Mascareño. 2020. Sentiment\\nanalysis of Twitter data during critical events through Bayesian Net. class. *Future*\\n*Gen. Computer Systems* 106 (2020), 92–104. https://doi.org/10.1016/j.future.2020.\\n01.005\\n\\n[31] Maria Santana, Juliana Lima, Andreiwid Correa, and Kellyton Brito. 2023. Engajamento no TikTok dos candidatos às eleições Brasileiras de 2022 – Resultados Iniciais. In *Anais do XII Brazilian Workshop on Social Network Analysis*\\n*and Mining* (João Pessoa/PB). SBC, Porto Alegre, RS, Brasil, 151–162. https:\\n//doi.org/10.5753/brasnam.2023.230641\\n\\n[32] Daiana Santos and Lilian Berton. 2023. Analysis of Twitter users’ sentiments\\nabout the first round 2022 presidential election in Brazil. In *Anais do XX Encontro*\\n*Nacional de Inteligência Artificial e Computacional* (Belo Horizonte/MG). SBC,\\nPorto Alegre, RS, Brasil, 880–893. https://doi.org/10.5753/eniac.2023.234511\\n\\n[33] M. Ángeles Serrano, Marián Boguñá, and Alessandro Vespignani. 2009. Extracting\\nthe multiscale backbone of complex weighted networks. *Proceedings of the*\\n*National Academy of Sciences* 106, 16 (April 2009), 6483–6488. https://doi.org/10.\\n1073/pnas.0808904106\\n\\n[34] Sarah Silva and Elaine Faria. 2023. Análise de sentimentos expressos no Twitter\\nem relação aos candidatos da eleição presidencial de 2022. In *Anais do XII Brazilian*\\n*Workshop on Social Network Analysis and Mining* (João Pessoa/PB). SBC, Porto\\nAlegre, RS, Brasil, 79–90. https://doi.org/10.5753/brasnam.2023.229992\\n\\n[35] Fábio Souza, Rodrigo Nogueira, and Roberto Lotufo. 2020. BERTimbau: pretrained\\nBERT models for Brazilian Portuguese. In *9th Brazilian Conference on Intelligent*\\n*Systems, BRACIS, Rio Grande do Sul, Brazil, October 20-23 (to appear)* .\\n\\n[36] Yla Tausczik and James Pennebaker. 2010. The psychological meaning of words:\\nLIWC and computerized text analysis methods. *Journal of language and social*\\n*psychology* 29, 1 (2010), 24–54.\\n\\n[37] Otavio R. Venâncio, Carlos H. G. Ferreira, Jussara M. Almeida, and Ana Paula\\nC. da Silva. 2024. Unraveling User Coordination on Telegram: A Comprehensive\\nAnalysis of Political Mobilization during the 2022 Brazilian Presidential Election.\\n*Proceedings of the International AAAI Conference on Web and Social Media* 18, 1\\n(May 2024), 1545–1556. https://doi.org/10.1609/icwsm.v18i1.31408\\n\\n[38] Shlomo Yitzhaki. 1979. Relative deprivation and the Gini coefficient. *The quarterly*\\n*journal of economics* (1979), 321–324.\\n\\n\\n291\\n\\n\\n-----\\n\\n',\n",
       "  'artigo_tokenizado': ['#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Twitter',\n",
       "   'and',\n",
       "   'the',\n",
       "   '2022',\n",
       "   'Brazilian',\n",
       "   'Elections',\n",
       "   'Portrait',\n",
       "   ':*',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   'A',\n",
       "   'Network',\n",
       "   'and',\n",
       "   'Content',\n",
       "   '-',\n",
       "   'Driven',\n",
       "   'Analysis',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Larissa',\n",
       "   'Malagoli',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'larissagomes@dcc.ufmg.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Carlos',\n",
       "   'H.',\n",
       "   'G.',\n",
       "   'Ferreira',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'chgferreira@ufop.edu.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Ouro',\n",
       "   'Preto',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'ABSTRACT',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'The',\n",
       "   'influence',\n",
       "   'of',\n",
       "   'online',\n",
       "   'social',\n",
       "   'networks',\n",
       "   'on',\n",
       "   'people',\n",
       "   '’s',\n",
       "   'actions',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'beliefs',\n",
       "   'has',\n",
       "   'grown',\n",
       "   'significantly',\n",
       "   'over',\n",
       "   'the',\n",
       "   'past',\n",
       "   'decade',\n",
       "   ',',\n",
       "   'impacting',\n",
       "   '\\n',\n",
       "   'everyday',\n",
       "   'life',\n",
       "   '.',\n",
       "   'This',\n",
       "   'is',\n",
       "   'especially',\n",
       "   'evident',\n",
       "   'in',\n",
       "   'Brazil',\n",
       "   ',',\n",
       "   'where',\n",
       "   'these',\n",
       "   'platforms',\n",
       "   'have',\n",
       "   'been',\n",
       "   'instrumental',\n",
       "   'in',\n",
       "   'disseminating',\n",
       "   'political',\n",
       "   'content',\n",
       "   '\\n',\n",
       "   'rapidly',\n",
       "   'and',\n",
       "   'widely',\n",
       "   '.',\n",
       "   'In',\n",
       "   'this',\n",
       "   'work',\n",
       "   ',',\n",
       "   'we',\n",
       "   'aim',\n",
       "   'to',\n",
       "   'understand',\n",
       "   'how',\n",
       "   'the',\n",
       "   'political',\n",
       "   'debate',\n",
       "   'surrounding',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'elections',\n",
       "   'of',\n",
       "   '2022',\n",
       "   'on',\n",
       "   'Twitter',\n",
       "   '\\n',\n",
       "   'unfolds',\n",
       "   'through',\n",
       "   'different',\n",
       "   'levels',\n",
       "   'of',\n",
       "   'user',\n",
       "   'engagement',\n",
       "   '.',\n",
       "   'We',\n",
       "   'provide',\n",
       "   'a',\n",
       "   '\\n',\n",
       "   'content',\n",
       "   'analysis',\n",
       "   'that',\n",
       "   'unveils',\n",
       "   'the',\n",
       "   'main',\n",
       "   'topics',\n",
       "   'discussed',\n",
       "   'by',\n",
       "   'different',\n",
       "   '\\n',\n",
       "   'users',\n",
       "   ',',\n",
       "   'regardless',\n",
       "   'of',\n",
       "   'the',\n",
       "   'strength',\n",
       "   'of',\n",
       "   'their',\n",
       "   'interactions',\n",
       "   '.',\n",
       "   'Our',\n",
       "   'results',\n",
       "   '\\n',\n",
       "   'enrich',\n",
       "   'the',\n",
       "   'understanding',\n",
       "   'of',\n",
       "   'how',\n",
       "   'online',\n",
       "   'discussions',\n",
       "   'evolved',\n",
       "   'on',\n",
       "   '\\n',\n",
       "   'social',\n",
       "   'media',\n",
       "   'during',\n",
       "   'this',\n",
       "   'important',\n",
       "   'event',\n",
       "   'in',\n",
       "   'the',\n",
       "   'recent',\n",
       "   'history',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'democracy',\n",
       "   'in',\n",
       "   'Brazil',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'KEYWORDS',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '2022',\n",
       "   'Brazilian',\n",
       "   'Elections',\n",
       "   ',',\n",
       "   'Network',\n",
       "   'Modeling',\n",
       "   ',',\n",
       "   'Online',\n",
       "   'Discussions',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'Twitter',\n",
       "   ',',\n",
       "   'Natural',\n",
       "   'Language',\n",
       "   'Processing',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   '1',\n",
       "   'INTRODUCTION',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Online',\n",
       "   'Social',\n",
       "   'Media',\n",
       "   'Platforms',\n",
       "   '(',\n",
       "   'OSMPs',\n",
       "   ')',\n",
       "   'have',\n",
       "   'served',\n",
       "   'as',\n",
       "   'one',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'main',\n",
       "   'stages',\n",
       "   'for',\n",
       "   'the',\n",
       "   'organization',\n",
       "   'and',\n",
       "   'development',\n",
       "   'of',\n",
       "   'many',\n",
       "   '\\n',\n",
       "   'significant',\n",
       "   'social',\n",
       "   'movements',\n",
       "   'worldwide',\n",
       "   ',',\n",
       "   'from',\n",
       "   'health',\n",
       "   '[',\n",
       "   '8',\n",
       "   ',',\n",
       "   '21',\n",
       "   ']',\n",
       "   'to',\n",
       "   '\\n',\n",
       "   'politics',\n",
       "   '[',\n",
       "   '9',\n",
       "   ',',\n",
       "   '34',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Platforms',\n",
       "   'such',\n",
       "   'as',\n",
       "   '(',\n",
       "   'former',\n",
       "   ')',\n",
       "   'Twitter',\n",
       "   '[',\n",
       "   '1',\n",
       "   ']',\n",
       "   '[',\n",
       "   '30',\n",
       "   ']',\n",
       "   ',',\n",
       "   'WhatsApp',\n",
       "   '\\n\\n',\n",
       "   '[',\n",
       "   '25',\n",
       "   ',',\n",
       "   '26',\n",
       "   ',',\n",
       "   '29',\n",
       "   ']',\n",
       "   'and',\n",
       "   'Telegram',\n",
       "   '[',\n",
       "   '37',\n",
       "   ']',\n",
       "   ',',\n",
       "   'have',\n",
       "   'been',\n",
       "   'the',\n",
       "   'focus',\n",
       "   'of',\n",
       "   'many',\n",
       "   'works',\n",
       "   '\\n',\n",
       "   'that',\n",
       "   'aimed',\n",
       "   'to',\n",
       "   'analyze',\n",
       "   'how',\n",
       "   'information',\n",
       "   'is',\n",
       "   'disseminated',\n",
       "   'in',\n",
       "   'such',\n",
       "   '\\n',\n",
       "   'platforms',\n",
       "   'and',\n",
       "   'its',\n",
       "   'implications',\n",
       "   'to',\n",
       "   'societies',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'This',\n",
       "   'scenario',\n",
       "   'is',\n",
       "   'no',\n",
       "   'different',\n",
       "   'in',\n",
       "   'Brazil',\n",
       "   ',',\n",
       "   'where',\n",
       "   'OSMPs',\n",
       "   'have',\n",
       "   'been',\n",
       "   '\\n',\n",
       "   'mainly',\n",
       "   'used',\n",
       "   'for',\n",
       "   'political',\n",
       "   'debates',\n",
       "   '.',\n",
       "   'Several',\n",
       "   'studies',\n",
       "   'in',\n",
       "   'the',\n",
       "   'literature',\n",
       "   '\\n\\n',\n",
       "   '[',\n",
       "   '3',\n",
       "   ',',\n",
       "   '4',\n",
       "   ',',\n",
       "   '10',\n",
       "   ',',\n",
       "   '12',\n",
       "   ',',\n",
       "   '16',\n",
       "   ',',\n",
       "   '17',\n",
       "   ',',\n",
       "   '20',\n",
       "   ',',\n",
       "   '27',\n",
       "   ',',\n",
       "   '29',\n",
       "   ',',\n",
       "   '31',\n",
       "   ',',\n",
       "   '34',\n",
       "   ',',\n",
       "   '37',\n",
       "   ']',\n",
       "   'characterized',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'political',\n",
       "   'debates',\n",
       "   'across',\n",
       "   'a',\n",
       "   'variety',\n",
       "   'of',\n",
       "   'platforms',\n",
       "   '.',\n",
       "   'In',\n",
       "   'contrast',\n",
       "   'to',\n",
       "   '\\n',\n",
       "   'previous',\n",
       "   'work',\n",
       "   'that',\n",
       "   'focused',\n",
       "   'on',\n",
       "   'Twitter',\n",
       "   ',',\n",
       "   'we',\n",
       "   'here',\n",
       "   'provide',\n",
       "   'a',\n",
       "   'more',\n",
       "   '\\n',\n",
       "   'comprehensive',\n",
       "   'analysis',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'political',\n",
       "   'landscape',\n",
       "   'of',\n",
       "   '2022',\n",
       "   '\\n',\n",
       "   'by',\n",
       "   'using',\n",
       "   'network',\n",
       "   'modeling',\n",
       "   'and',\n",
       "   'backbone',\n",
       "   'extraction',\n",
       "   'methods',\n",
       "   'to',\n",
       "   '\\n',\n",
       "   'show',\n",
       "   'how',\n",
       "   'the',\n",
       "   'political',\n",
       "   'debate',\n",
       "   'unfolds',\n",
       "   'through',\n",
       "   'different',\n",
       "   'levels',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'user',\n",
       "   'engagement',\n",
       "   'and',\n",
       "   'diffusion',\n",
       "   '.',\n",
       "   'To',\n",
       "   'accomplish',\n",
       "   'this',\n",
       "   ',',\n",
       "   'we',\n",
       "   'crawled',\n",
       "   '\\n',\n",
       "   'Twitter',\n",
       "   'to',\n",
       "   'collect',\n",
       "   'about',\n",
       "   '741',\n",
       "   'K',\n",
       "   'shared',\n",
       "   'tweets',\n",
       "   'covering',\n",
       "   'the',\n",
       "   'days',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'two',\n",
       "   'election',\n",
       "   '.',\n",
       "   'We',\n",
       "   'then',\n",
       "   'model',\n",
       "   'the',\n",
       "   'information',\n",
       "   'spread',\n",
       "   'during',\n",
       "   '\\n\\n',\n",
       "   '1',\n",
       "   'Twitter',\n",
       "   'has',\n",
       "   'been',\n",
       "   'recently',\n",
       "   'rebranded',\n",
       "   'as',\n",
       "   'X.',\n",
       "   'Yet',\n",
       "   ',',\n",
       "   'we',\n",
       "   'maintain',\n",
       "   'the',\n",
       "   'reference',\n",
       "   'to',\n",
       "   'the',\n",
       "   'old',\n",
       "   '\\n',\n",
       "   'platform´s',\n",
       "   'name',\n",
       "   'as',\n",
       "   'our',\n",
       "   'study',\n",
       "   'relies',\n",
       "   'on',\n",
       "   'features',\n",
       "   'commonly',\n",
       "   'associated',\n",
       "   'with',\n",
       "   'it',\n",
       "   '.',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   ':',\n",
       "   'Proceedings',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Web',\n",
       "   '(',\n",
       "   'WebMe',\n",
       "   '\\n',\n",
       "   'dia’2024',\n",
       "   ')',\n",
       "   '.',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '.',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   ':',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   ',',\n",
       "   '2024',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'SBC',\n",
       "   '–',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '-',\n",
       "   '2753',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Giovana',\n",
       "   'Piorino',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'giovana.piorino@dcc.ufmg.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Ana',\n",
       "   'Paula',\n",
       "   'Couto',\n",
       "   'da',\n",
       "   'Silva',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'ana.coutosilva@dcc.ufmg.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   '\\n\\n',\n",
       "   'each',\n",
       "   'round',\n",
       "   'using',\n",
       "   'a',\n",
       "   'media',\n",
       "   '-',\n",
       "   'centric',\n",
       "   'network',\n",
       "   'that',\n",
       "   'connects',\n",
       "   'users',\n",
       "   'who',\n",
       "   '\\n',\n",
       "   'shared',\n",
       "   'the',\n",
       "   'same',\n",
       "   'content',\n",
       "   '(',\n",
       "   'overall',\n",
       "   'network',\n",
       "   ')',\n",
       "   'and',\n",
       "   'employ',\n",
       "   'backbone',\n",
       "   '\\n',\n",
       "   'extraction',\n",
       "   'methods',\n",
       "   'to',\n",
       "   'identify',\n",
       "   'the',\n",
       "   'group',\n",
       "   'of',\n",
       "   'users',\n",
       "   'who',\n",
       "   'frequently',\n",
       "   '\\n',\n",
       "   'share',\n",
       "   'the',\n",
       "   'same',\n",
       "   'pieces',\n",
       "   'of',\n",
       "   'information',\n",
       "   '[',\n",
       "   '18',\n",
       "   ',',\n",
       "   '33',\n",
       "   ']',\n",
       "   '.',\n",
       "   'These',\n",
       "   'users',\n",
       "   'are',\n",
       "   'placed',\n",
       "   '\\n',\n",
       "   'in',\n",
       "   'the',\n",
       "   'core',\n",
       "   'of',\n",
       "   'the',\n",
       "   'media',\n",
       "   '-',\n",
       "   'centric',\n",
       "   'network',\n",
       "   ',',\n",
       "   'thereby',\n",
       "   'driving',\n",
       "   'the',\n",
       "   'election',\n",
       "   'debate',\n",
       "   'on',\n",
       "   'Twitter',\n",
       "   '.',\n",
       "   'Finally',\n",
       "   ',',\n",
       "   'we',\n",
       "   'analyzed',\n",
       "   'the',\n",
       "   'text',\n",
       "   'shared',\n",
       "   'by',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'users',\n",
       "   'from',\n",
       "   'three',\n",
       "   'different',\n",
       "   '(',\n",
       "   'yet',\n",
       "   'complementary',\n",
       "   ')',\n",
       "   'perspectives',\n",
       "   ':',\n",
       "   'topic',\n",
       "   '\\n',\n",
       "   'extraction',\n",
       "   ',',\n",
       "   'sentiment',\n",
       "   'analysis',\n",
       "   ',',\n",
       "   'and',\n",
       "   'psycholinguistic',\n",
       "   'analysis',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Our',\n",
       "   'main',\n",
       "   'findings',\n",
       "   'are',\n",
       "   'as',\n",
       "   'follows',\n",
       "   ':',\n",
       "   '(',\n",
       "   'i',\n",
       "   ')',\n",
       "   'The',\n",
       "   'identification',\n",
       "   'of',\n",
       "   'backbone',\n",
       "   'networks',\n",
       "   'reveals',\n",
       "   'a',\n",
       "   'set',\n",
       "   'of',\n",
       "   'users',\n",
       "   'engaged',\n",
       "   'through',\n",
       "   'stronger',\n",
       "   '\\n',\n",
       "   'interactions',\n",
       "   '.',\n",
       "   'This',\n",
       "   'structured',\n",
       "   'network',\n",
       "   'may',\n",
       "   'prompt',\n",
       "   'these',\n",
       "   'users',\n",
       "   'to',\n",
       "   '\\n',\n",
       "   'act',\n",
       "   'more',\n",
       "   'cohesively',\n",
       "   'in',\n",
       "   'promoting',\n",
       "   'specific',\n",
       "   'content',\n",
       "   ',',\n",
       "   'thereby',\n",
       "   'influencing',\n",
       "   'discourse',\n",
       "   'on',\n",
       "   'a',\n",
       "   'particular',\n",
       "   'topic',\n",
       "   '.',\n",
       "   '(',\n",
       "   'ii',\n",
       "   ')',\n",
       "   'The',\n",
       "   'topics',\n",
       "   'diffused',\n",
       "   'by',\n",
       "   '\\n',\n",
       "   'users',\n",
       "   'in',\n",
       "   'backbones',\n",
       "   'differ',\n",
       "   'from',\n",
       "   'those',\n",
       "   'shared',\n",
       "   'by',\n",
       "   'the',\n",
       "   'overall',\n",
       "   'network',\n",
       "   '.',\n",
       "   'Interestingly',\n",
       "   ',',\n",
       "   'the',\n",
       "   'most',\n",
       "   'shared',\n",
       "   'topics',\n",
       "   'in',\n",
       "   'these',\n",
       "   'backbones',\n",
       "   ',',\n",
       "   'not',\n",
       "   '\\n',\n",
       "   'present',\n",
       "   'in',\n",
       "   'the',\n",
       "   'overall',\n",
       "   'network',\n",
       "   ',',\n",
       "   'are',\n",
       "   'more',\n",
       "   'aligned',\n",
       "   'with',\n",
       "   'supporting',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'candidate',\n",
       "   'Luis',\n",
       "   'Inácio',\n",
       "   'Lula',\n",
       "   'da',\n",
       "   'Silva',\n",
       "   'and',\n",
       "   'celebrating',\n",
       "   'his',\n",
       "   'victory',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '(',\n",
       "   'iii',\n",
       "   ')',\n",
       "   'There',\n",
       "   'is',\n",
       "   'a',\n",
       "   'small',\n",
       "   'set',\n",
       "   'of',\n",
       "   'users',\n",
       "   'who',\n",
       "   'shared',\n",
       "   'a',\n",
       "   'significant',\n",
       "   'amount',\n",
       "   '\\n',\n",
       "   'of',\n",
       "   'information',\n",
       "   'across',\n",
       "   'both',\n",
       "   'election',\n",
       "   'rounds',\n",
       "   ',',\n",
       "   'with',\n",
       "   'a',\n",
       "   'higher',\n",
       "   'potential',\n",
       "   '\\n',\n",
       "   'to',\n",
       "   'reach',\n",
       "   'a',\n",
       "   'larger',\n",
       "   'audience',\n",
       "   'due',\n",
       "   'to',\n",
       "   'their',\n",
       "   'above',\n",
       "   '-',\n",
       "   'average',\n",
       "   'number',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'followers',\n",
       "   'in',\n",
       "   'our',\n",
       "   'dataset',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   '2',\n",
       "   'RELATED',\n",
       "   'WORK',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Our',\n",
       "   'work',\n",
       "   'is',\n",
       "   'not',\n",
       "   'the',\n",
       "   'first',\n",
       "   'to',\n",
       "   'examine',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'political',\n",
       "   'scenario',\n",
       "   'on',\n",
       "   'OSMPs',\n",
       "   '.',\n",
       "   'The',\n",
       "   'authors',\n",
       "   'in',\n",
       "   '[',\n",
       "   '25',\n",
       "   ',',\n",
       "   '26',\n",
       "   ']',\n",
       "   'analyzed',\n",
       "   'the',\n",
       "   'messages',\n",
       "   '\\n',\n",
       "   'exchanged',\n",
       "   'on',\n",
       "   'WhatsApp',\n",
       "   'during',\n",
       "   'the',\n",
       "   '2018',\n",
       "   'Brazilian',\n",
       "   'general',\n",
       "   'elections',\n",
       "   '.',\n",
       "   'Similar',\n",
       "   'to',\n",
       "   'our',\n",
       "   'work',\n",
       "   ',',\n",
       "   'the',\n",
       "   'authors',\n",
       "   'built',\n",
       "   'a',\n",
       "   'network',\n",
       "   'based',\n",
       "   'on',\n",
       "   '\\n',\n",
       "   'users',\n",
       "   'that',\n",
       "   'shared',\n",
       "   'the',\n",
       "   'same',\n",
       "   'content',\n",
       "   'messages',\n",
       "   'in',\n",
       "   'one',\n",
       "   'or',\n",
       "   'more',\n",
       "   'groups',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'They',\n",
       "   'also',\n",
       "   'employed',\n",
       "   'backbone',\n",
       "   'extraction',\n",
       "   'techniques',\n",
       "   'to',\n",
       "   'search',\n",
       "   'for',\n",
       "   '\\n',\n",
       "   'strongly',\n",
       "   'connected',\n",
       "   'user',\n",
       "   'communities',\n",
       "   'to',\n",
       "   'evidence',\n",
       "   'possible',\n",
       "   'coordination',\n",
       "   'actions',\n",
       "   'in',\n",
       "   'sharing',\n",
       "   'specific',\n",
       "   'content',\n",
       "   '.',\n",
       "   'In',\n",
       "   'the',\n",
       "   'same',\n",
       "   'direction',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'Machado',\n",
       "   '*',\n",
       "   'et',\n",
       "   'al',\n",
       "   '.',\n",
       "   '*',\n",
       "   '[',\n",
       "   '20',\n",
       "   ']',\n",
       "   'explored',\n",
       "   'misinformation',\n",
       "   'in',\n",
       "   'WhatsApp',\n",
       "   'content',\n",
       "   '\\n',\n",
       "   'during',\n",
       "   'the',\n",
       "   'same',\n",
       "   'election',\n",
       "   'period',\n",
       "   '.',\n",
       "   'The',\n",
       "   'main',\n",
       "   'results',\n",
       "   'showed',\n",
       "   'that',\n",
       "   'viral',\n",
       "   '\\n',\n",
       "   'content',\n",
       "   'in',\n",
       "   'WhatsApp',\n",
       "   'groups',\n",
       "   'was',\n",
       "   'mainly',\n",
       "   'based',\n",
       "   'on',\n",
       "   'hate',\n",
       "   'speech',\n",
       "   '\\n',\n",
       "   ...],\n",
       "  'pos_tagger': '',\n",
       "  'lema': ['Twitter',\n",
       "   'and',\n",
       "   'the',\n",
       "   '2022',\n",
       "   'Brazilian',\n",
       "   'Elections',\n",
       "   'Portrait',\n",
       "   'a',\n",
       "   'Network',\n",
       "   'and',\n",
       "   'Content',\n",
       "   'Driven',\n",
       "   'Analysis',\n",
       "   'Larissa',\n",
       "   'Malagoli',\n",
       "   'larissagomes@dcc.ufmg.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'Carlos',\n",
       "   'H.',\n",
       "   'G.',\n",
       "   'Ferreira',\n",
       "   'chgferreira@ufop.edu.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Ouro',\n",
       "   'Preto',\n",
       "   'ABSTRACT',\n",
       "   'the',\n",
       "   'influence',\n",
       "   'of',\n",
       "   'online',\n",
       "   'social',\n",
       "   'network',\n",
       "   'on',\n",
       "   'people',\n",
       "   '’s',\n",
       "   'action',\n",
       "   'and',\n",
       "   'belief',\n",
       "   'have',\n",
       "   'grow',\n",
       "   'significantly',\n",
       "   'over',\n",
       "   'the',\n",
       "   'past',\n",
       "   'decade',\n",
       "   'impact',\n",
       "   'everyday',\n",
       "   'life',\n",
       "   'this',\n",
       "   'be',\n",
       "   'especially',\n",
       "   'evident',\n",
       "   'in',\n",
       "   'Brazil',\n",
       "   'where',\n",
       "   'these',\n",
       "   'platform',\n",
       "   'have',\n",
       "   'be',\n",
       "   'instrumental',\n",
       "   'in',\n",
       "   'disseminate',\n",
       "   'political',\n",
       "   'content',\n",
       "   'rapidly',\n",
       "   'and',\n",
       "   'widely',\n",
       "   'in',\n",
       "   'this',\n",
       "   'work',\n",
       "   'we',\n",
       "   'aim',\n",
       "   'to',\n",
       "   'understand',\n",
       "   'how',\n",
       "   'the',\n",
       "   'political',\n",
       "   'debate',\n",
       "   'surround',\n",
       "   'the',\n",
       "   'brazilian',\n",
       "   'election',\n",
       "   'of',\n",
       "   '2022',\n",
       "   'on',\n",
       "   'Twitter',\n",
       "   'unfold',\n",
       "   'through',\n",
       "   'different',\n",
       "   'level',\n",
       "   'of',\n",
       "   'user',\n",
       "   'engagement',\n",
       "   'we',\n",
       "   'provide',\n",
       "   'a',\n",
       "   'content',\n",
       "   'analysis',\n",
       "   'that',\n",
       "   'unveil',\n",
       "   'the',\n",
       "   'main',\n",
       "   'topic',\n",
       "   'discuss',\n",
       "   'by',\n",
       "   'different',\n",
       "   'user',\n",
       "   'regardless',\n",
       "   'of',\n",
       "   'the',\n",
       "   'strength',\n",
       "   'of',\n",
       "   'their',\n",
       "   'interaction',\n",
       "   'our',\n",
       "   'result',\n",
       "   'enrich',\n",
       "   'the',\n",
       "   'understanding',\n",
       "   'of',\n",
       "   'how',\n",
       "   'online',\n",
       "   'discussion',\n",
       "   'evolve',\n",
       "   'on',\n",
       "   'social',\n",
       "   'medium',\n",
       "   'during',\n",
       "   'this',\n",
       "   'important',\n",
       "   'event',\n",
       "   'in',\n",
       "   'the',\n",
       "   'recent',\n",
       "   'history',\n",
       "   'of',\n",
       "   'democracy',\n",
       "   'in',\n",
       "   'Brazil',\n",
       "   'keyword',\n",
       "   '2022',\n",
       "   'brazilian',\n",
       "   'Elections',\n",
       "   'Network',\n",
       "   'Modeling',\n",
       "   'Online',\n",
       "   'Discussions',\n",
       "   'Twitter',\n",
       "   'Natural',\n",
       "   'Language',\n",
       "   'processing',\n",
       "   '1',\n",
       "   'introduction',\n",
       "   'Online',\n",
       "   'Social',\n",
       "   'Media',\n",
       "   'Platforms',\n",
       "   'osmp',\n",
       "   'have',\n",
       "   'serve',\n",
       "   'as',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'main',\n",
       "   'stage',\n",
       "   'for',\n",
       "   'the',\n",
       "   'organization',\n",
       "   'and',\n",
       "   'development',\n",
       "   'of',\n",
       "   'many',\n",
       "   'significant',\n",
       "   'social',\n",
       "   'movement',\n",
       "   'worldwide',\n",
       "   'from',\n",
       "   'health',\n",
       "   '8',\n",
       "   '21',\n",
       "   'to',\n",
       "   'politic',\n",
       "   '9',\n",
       "   '34',\n",
       "   'platform',\n",
       "   'such',\n",
       "   'as',\n",
       "   'former',\n",
       "   'Twitter',\n",
       "   '1',\n",
       "   '30',\n",
       "   'WhatsApp',\n",
       "   '25',\n",
       "   '26',\n",
       "   '29',\n",
       "   'and',\n",
       "   'Telegram',\n",
       "   '37',\n",
       "   'have',\n",
       "   'be',\n",
       "   'the',\n",
       "   'focus',\n",
       "   'of',\n",
       "   'many',\n",
       "   'work',\n",
       "   'that',\n",
       "   'aim',\n",
       "   'to',\n",
       "   'analyze',\n",
       "   'how',\n",
       "   'information',\n",
       "   'be',\n",
       "   'disseminate',\n",
       "   'in',\n",
       "   'such',\n",
       "   'platform',\n",
       "   'and',\n",
       "   'its',\n",
       "   'implication',\n",
       "   'to',\n",
       "   'society',\n",
       "   'this',\n",
       "   'scenario',\n",
       "   'be',\n",
       "   'no',\n",
       "   'different',\n",
       "   'in',\n",
       "   'Brazil',\n",
       "   'where',\n",
       "   'osmp',\n",
       "   'have',\n",
       "   'be',\n",
       "   'mainly',\n",
       "   'use',\n",
       "   'for',\n",
       "   'political',\n",
       "   'debate',\n",
       "   'several',\n",
       "   'study',\n",
       "   'in',\n",
       "   'the',\n",
       "   'literature',\n",
       "   '3',\n",
       "   '4',\n",
       "   '10',\n",
       "   '12',\n",
       "   '16',\n",
       "   '17',\n",
       "   '20',\n",
       "   '27',\n",
       "   '29',\n",
       "   '31',\n",
       "   '34',\n",
       "   '37',\n",
       "   'characterize',\n",
       "   'the',\n",
       "   'brazilian',\n",
       "   'political',\n",
       "   'debate',\n",
       "   'across',\n",
       "   'a',\n",
       "   'variety',\n",
       "   'of',\n",
       "   'platform',\n",
       "   'in',\n",
       "   'contrast',\n",
       "   'to',\n",
       "   'previous',\n",
       "   'work',\n",
       "   'that',\n",
       "   'focus',\n",
       "   'on',\n",
       "   'Twitter',\n",
       "   'we',\n",
       "   'here',\n",
       "   'provide',\n",
       "   'a',\n",
       "   'more',\n",
       "   'comprehensive',\n",
       "   'analysis',\n",
       "   'of',\n",
       "   'the',\n",
       "   'brazilian',\n",
       "   'political',\n",
       "   'landscape',\n",
       "   'of',\n",
       "   '2022',\n",
       "   'by',\n",
       "   'use',\n",
       "   'network',\n",
       "   'modeling',\n",
       "   'and',\n",
       "   'backbone',\n",
       "   'extraction',\n",
       "   'method',\n",
       "   'to',\n",
       "   'show',\n",
       "   'how',\n",
       "   'the',\n",
       "   'political',\n",
       "   'debate',\n",
       "   'unfold',\n",
       "   'through',\n",
       "   'different',\n",
       "   'level',\n",
       "   'of',\n",
       "   'user',\n",
       "   'engagement',\n",
       "   'and',\n",
       "   'diffusion',\n",
       "   'to',\n",
       "   'accomplish',\n",
       "   'this',\n",
       "   'we',\n",
       "   'crawl',\n",
       "   'Twitter',\n",
       "   'to',\n",
       "   'collect',\n",
       "   'about',\n",
       "   '741',\n",
       "   'k',\n",
       "   'share',\n",
       "   'tweet',\n",
       "   'cover',\n",
       "   'the',\n",
       "   'day',\n",
       "   'of',\n",
       "   'the',\n",
       "   'two',\n",
       "   'election',\n",
       "   'we',\n",
       "   'then',\n",
       "   'model',\n",
       "   'the',\n",
       "   'information',\n",
       "   'spread',\n",
       "   'during',\n",
       "   '1',\n",
       "   'Twitter',\n",
       "   'have',\n",
       "   'be',\n",
       "   'recently',\n",
       "   'rebrande',\n",
       "   'as',\n",
       "   'X.',\n",
       "   'Yet',\n",
       "   'we',\n",
       "   'maintain',\n",
       "   'the',\n",
       "   'reference',\n",
       "   'to',\n",
       "   'the',\n",
       "   'old',\n",
       "   'platform´s',\n",
       "   'name',\n",
       "   'as',\n",
       "   'our',\n",
       "   'study',\n",
       "   'rely',\n",
       "   'on',\n",
       "   'feature',\n",
       "   'commonly',\n",
       "   'associate',\n",
       "   'with',\n",
       "   'it',\n",
       "   'in',\n",
       "   'proceeding',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'web',\n",
       "   'WebMe',\n",
       "   'dia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   '2024',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'sbc',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '2753',\n",
       "   'Giovana',\n",
       "   'Piorino',\n",
       "   'giovana.piorino@dcc.ufmg.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'Ana',\n",
       "   'Paula',\n",
       "   'Couto',\n",
       "   'da',\n",
       "   'Silva',\n",
       "   'ana.coutosilva@dcc.ufmg.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'each',\n",
       "   'round',\n",
       "   'use',\n",
       "   'a',\n",
       "   'media',\n",
       "   'centric',\n",
       "   'network',\n",
       "   'that',\n",
       "   'connect',\n",
       "   'user',\n",
       "   'who',\n",
       "   'share',\n",
       "   'the',\n",
       "   'same',\n",
       "   'content',\n",
       "   'overall',\n",
       "   'network',\n",
       "   'and',\n",
       "   'employ',\n",
       "   'backbone',\n",
       "   'extraction',\n",
       "   'method',\n",
       "   'to',\n",
       "   'identify',\n",
       "   'the',\n",
       "   'group',\n",
       "   'of',\n",
       "   'user',\n",
       "   'who',\n",
       "   'frequently',\n",
       "   'share',\n",
       "   'the',\n",
       "   'same',\n",
       "   'piece',\n",
       "   'of',\n",
       "   'information',\n",
       "   '18',\n",
       "   '33',\n",
       "   'these',\n",
       "   'user',\n",
       "   'be',\n",
       "   'place',\n",
       "   'in',\n",
       "   'the',\n",
       "   'core',\n",
       "   'of',\n",
       "   'the',\n",
       "   'media',\n",
       "   'centric',\n",
       "   'network',\n",
       "   'thereby',\n",
       "   'drive',\n",
       "   'the',\n",
       "   'election',\n",
       "   'debate',\n",
       "   'on',\n",
       "   'Twitter',\n",
       "   'finally',\n",
       "   'we',\n",
       "   'analyze',\n",
       "   'the',\n",
       "   'text',\n",
       "   'share',\n",
       "   'by',\n",
       "   'the',\n",
       "   'user',\n",
       "   'from',\n",
       "   'three',\n",
       "   'different',\n",
       "   'yet',\n",
       "   'complementary',\n",
       "   'perspective',\n",
       "   'topic',\n",
       "   'extraction',\n",
       "   'sentiment',\n",
       "   'analysis',\n",
       "   'and',\n",
       "   'psycholinguistic',\n",
       "   'analysis',\n",
       "   'our',\n",
       "   'main',\n",
       "   'finding',\n",
       "   'be',\n",
       "   'as',\n",
       "   'follow',\n",
       "   'i',\n",
       "   'the',\n",
       "   'identification',\n",
       "   'of',\n",
       "   'backbone',\n",
       "   'network',\n",
       "   'reveal',\n",
       "   'a',\n",
       "   'set',\n",
       "   'of',\n",
       "   'user',\n",
       "   'engage',\n",
       "   'through',\n",
       "   'strong',\n",
       "   'interaction',\n",
       "   'this',\n",
       "   'structured',\n",
       "   'network',\n",
       "   'may',\n",
       "   'prompt',\n",
       "   'these',\n",
       "   'user',\n",
       "   'to',\n",
       "   'act',\n",
       "   'more',\n",
       "   'cohesively',\n",
       "   'in',\n",
       "   'promote',\n",
       "   'specific',\n",
       "   'content',\n",
       "   'thereby',\n",
       "   'influence',\n",
       "   'discourse',\n",
       "   'on',\n",
       "   'a',\n",
       "   'particular',\n",
       "   'topic',\n",
       "   'ii',\n",
       "   'the',\n",
       "   'topic',\n",
       "   'diffuse',\n",
       "   'by',\n",
       "   'user',\n",
       "   'in',\n",
       "   'backbone',\n",
       "   'differ',\n",
       "   'from',\n",
       "   'those',\n",
       "   'share',\n",
       "   'by',\n",
       "   'the',\n",
       "   'overall',\n",
       "   'network',\n",
       "   'interestingly',\n",
       "   'the',\n",
       "   'most',\n",
       "   'shared',\n",
       "   'topic',\n",
       "   'in',\n",
       "   'these',\n",
       "   'backbone',\n",
       "   'not',\n",
       "   'present',\n",
       "   'in',\n",
       "   'the',\n",
       "   'overall',\n",
       "   'network',\n",
       "   'be',\n",
       "   'more',\n",
       "   'align',\n",
       "   'with',\n",
       "   'support',\n",
       "   'the',\n",
       "   'candidate',\n",
       "   'Luis',\n",
       "   'Inácio',\n",
       "   'Lula',\n",
       "   'da',\n",
       "   'Silva',\n",
       "   'and',\n",
       "   'celebrate',\n",
       "   'his',\n",
       "   'victory',\n",
       "   'iii',\n",
       "   'there',\n",
       "   'be',\n",
       "   'a',\n",
       "   'small',\n",
       "   'set',\n",
       "   'of',\n",
       "   'user',\n",
       "   'who',\n",
       "   'share',\n",
       "   'a',\n",
       "   'significant',\n",
       "   'amount',\n",
       "   'of',\n",
       "   'information',\n",
       "   'across',\n",
       "   'both',\n",
       "   'election',\n",
       "   'round',\n",
       "   'with',\n",
       "   'a',\n",
       "   'high',\n",
       "   'potential',\n",
       "   'to',\n",
       "   'reach',\n",
       "   'a',\n",
       "   'large',\n",
       "   'audience',\n",
       "   'due',\n",
       "   'to',\n",
       "   'their',\n",
       "   'above',\n",
       "   'average',\n",
       "   'number',\n",
       "   'of',\n",
       "   'follower',\n",
       "   'in',\n",
       "   'our',\n",
       "   'dataset',\n",
       "   '2',\n",
       "   'relate',\n",
       "   'work',\n",
       "   'our',\n",
       "   'work',\n",
       "   'be',\n",
       "   'not',\n",
       "   'the',\n",
       "   'first',\n",
       "   'to',\n",
       "   'examine',\n",
       "   'the',\n",
       "   'brazilian',\n",
       "   'political',\n",
       "   'scenario',\n",
       "   'on',\n",
       "   'osmps',\n",
       "   'the',\n",
       "   'author',\n",
       "   'in',\n",
       "   '25',\n",
       "   '26',\n",
       "   'analyze',\n",
       "   'the',\n",
       "   'message',\n",
       "   'exchange',\n",
       "   'on',\n",
       "   'WhatsApp',\n",
       "   'during',\n",
       "   'the',\n",
       "   '2018',\n",
       "   'brazilian',\n",
       "   'general',\n",
       "   'election',\n",
       "   'similar',\n",
       "   'to',\n",
       "   'our',\n",
       "   'work',\n",
       "   'the',\n",
       "   'author',\n",
       "   'build',\n",
       "   'a',\n",
       "   'network',\n",
       "   'base',\n",
       "   'on',\n",
       "   'user',\n",
       "   'that',\n",
       "   'share',\n",
       "   'the',\n",
       "   'same',\n",
       "   'content',\n",
       "   'message',\n",
       "   'in',\n",
       "   'one',\n",
       "   'or',\n",
       "   'more',\n",
       "   'group',\n",
       "   'they',\n",
       "   'also',\n",
       "   'employ',\n",
       "   'backbone',\n",
       "   'extraction',\n",
       "   'technique',\n",
       "   'to',\n",
       "   'search',\n",
       "   'for',\n",
       "   'strongly',\n",
       "   'connect',\n",
       "   'user',\n",
       "   'community',\n",
       "   'to',\n",
       "   'evidence',\n",
       "   'possible',\n",
       "   'coordination',\n",
       "   'action',\n",
       "   'in',\n",
       "   'share',\n",
       "   'specific',\n",
       "   'content',\n",
       "   'in',\n",
       "   'the',\n",
       "   'same',\n",
       "   'direction',\n",
       "   'Machado',\n",
       "   'et',\n",
       "   'al',\n",
       "   '20',\n",
       "   'explore',\n",
       "   'misinformation',\n",
       "   'in',\n",
       "   'WhatsApp',\n",
       "   'content',\n",
       "   'during',\n",
       "   'the',\n",
       "   'same',\n",
       "   'election',\n",
       "   'period',\n",
       "   'the',\n",
       "   'main',\n",
       "   'result',\n",
       "   'show',\n",
       "   'that',\n",
       "   'viral',\n",
       "   'content',\n",
       "   'in',\n",
       "   'WhatsApp',\n",
       "   'group',\n",
       "   'be',\n",
       "   'mainly',\n",
       "   'base',\n",
       "   'on',\n",
       "   'hate',\n",
       "   'speech',\n",
       "   'and',\n",
       "   'fake',\n",
       "   'news',\n",
       "   'in',\n",
       "   '3',\n",
       "   'the',\n",
       "   'author',\n",
       "   'characterize',\n",
       "   'the',\n",
       "   'tweet',\n",
       "   'of',\n",
       "   'preelection',\n",
       "   'advertisement',\n",
       "   'for',\n",
       "   'the',\n",
       "   '2016',\n",
       "   '2018',\n",
       "   'and',\n",
       "   '2020',\n",
       "   'election',\n",
       "   'by',\n",
       "   'apply',\n",
       "   'psycholinguistic',\n",
       "   'and',\n",
       "   'sentiment',\n",
       "   'analysis',\n",
       "   'technique',\n",
       "   'the',\n",
       "   'result',\n",
       "   'show',\n",
       "   'that',\n",
       "   'early',\n",
       "   'advertisement',\n",
       "   'be',\n",
       "   'usually',\n",
       "   'negative',\n",
       "   'or',\n",
       "   'neutral',\n",
       "   'with',\n",
       "   'the',\n",
       "   'neutral',\n",
       "   'sentiment',\n",
       "   'grow',\n",
       "   'over',\n",
       "   'time',\n",
       "   'and',\n",
       "   'there',\n",
       "   'be',\n",
       "   'a',\n",
       "   'pattern',\n",
       "   'in',\n",
       "   'the',\n",
       "   'use',\n",
       "   'of',\n",
       "   'hashtag',\n",
       "   'and',\n",
       "   'link',\n",
       "   'along',\n",
       "   'with',\n",
       "   'mention',\n",
       "   'to',\n",
       "   'entity',\n",
       "   'focus',\n",
       "   'specifically',\n",
       "   'on',\n",
       "   'the',\n",
       "   '2022',\n",
       "   'brazilian',\n",
       "   'election',\n",
       "   'the',\n",
       "   'study',\n",
       "   'of',\n",
       "   'Venâncio',\n",
       "   'et',\n",
       "   'al',\n",
       "   '37',\n",
       "   'apply',\n",
       "   'the',\n",
       "   'methodology',\n",
       "   'of',\n",
       "   'backbone',\n",
       "   'extraction',\n",
       "   '283',\n",
       "   'WebMedia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Larissa',\n",
       "   'Malagoli',\n",
       "   'Giovana',\n",
       "   'Piorino',\n",
       "   'Carlos',\n",
       "   'H.',\n",
       "   'G.',\n",
       "   'Ferreira',\n",
       "   'and',\n",
       "   'Ana',\n",
       "   'Paula',\n",
       "   'Couto',\n",
       "   'da',\n",
       "   'Silva',\n",
       "   'to',\n",
       "   'search',\n",
       "   'for',\n",
       "   'evidence',\n",
       "   'of',\n",
       "   'coordination',\n",
       "   'in',\n",
       "   'the',\n",
       "   'information',\n",
       "   'dissemi',\n",
       "   'nation',\n",
       "   'on',\n",
       "   'Telegram',\n",
       "   'this',\n",
       "   'study',\n",
       "   'not',\n",
       "   'only',\n",
       "   'highlight',\n",
       "   'the',\n",
       "   'grow',\n",
       "   'influence',\n",
       "   'of',\n",
       "   'message',\n",
       "   'app',\n",
       "   'on',\n",
       "   'political',\n",
       "   'mobilization',\n",
       "   'but',\n",
       "   'also',\n",
       "   'contribute',\n",
       "   'to',\n",
       "   'the',\n",
       "   'understanding',\n",
       "   'of',\n",
       "   'digital',\n",
       "   'communication',\n",
       "   'strategy',\n",
       "   'in',\n",
       "   'modern',\n",
       "   'electoral',\n",
       "   'contexts',\n",
       "   'the',\n",
       "   'author',\n",
       "   'in',\n",
       "   '23',\n",
       "   'collect',\n",
       "   'datum',\n",
       "   'from',\n",
       "   'an',\n",
       "   'online',\n",
       "   'experiment',\n",
       "   'where',\n",
       "   'participant',\n",
       "   'build',\n",
       "   'personalized',\n",
       "   'government',\n",
       "   'program',\n",
       "   'by',\n",
       "   'combine',\n",
       "   'policy',\n",
       "   'propose',\n",
       "   'by',\n",
       "   'the',\n",
       "   'candidate',\n",
       "   'of',\n",
       "   'the',\n",
       "   '2022',\n",
       "   'french',\n",
       "   'and',\n",
       "   'brazilian',\n",
       "   'presidential',\n",
       "   'election',\n",
       "   'identify',\n",
       "   'polarize',\n",
       "   'proposal',\n",
       "   'Santana',\n",
       "   'et',\n",
       "   'al',\n",
       "   'present',\n",
       "   'a',\n",
       "   'study',\n",
       "   'that',\n",
       "   'analyze',\n",
       "   'the',\n",
       "   'use',\n",
       "   'and',\n",
       "   'engagement',\n",
       "   'of',\n",
       "   'the',\n",
       "   'tiktok',\n",
       "   'profile',\n",
       "   'of',\n",
       "   'the',\n",
       "   'two',\n",
       "   'lead',\n",
       "   'candidate',\n",
       "   'Lula',\n",
       "   'and',\n",
       "   'Bolsonaro',\n",
       "   '31',\n",
       "   'the',\n",
       "   'author',\n",
       "   'in',\n",
       "   '17',\n",
       "   'apply',\n",
       "   'sentiment',\n",
       "   'analysis',\n",
       "   'in',\n",
       "   'order',\n",
       "   'to',\n",
       "   'identify',\n",
       "   'gender',\n",
       "   'bias',\n",
       "   'on',\n",
       "   'comment',\n",
       "   'on',\n",
       "   'YouTube',\n",
       "   'in',\n",
       "   'the',\n",
       "   '2018',\n",
       "   'and',\n",
       "   '2022',\n",
       "   'election',\n",
       "   'in',\n",
       "   'the',\n",
       "   'context',\n",
       "   'of',\n",
       "   'Twitter',\n",
       "   'Silva',\n",
       "   'and',\n",
       "   'Faria',\n",
       "   '34',\n",
       "   'analyze',\n",
       "   'the',\n",
       "   'sentiment',\n",
       "   'express',\n",
       "   'by',\n",
       "   'Twitter',\n",
       "   'user',\n",
       "   'regard',\n",
       "   'the',\n",
       "   'presidential',\n",
       "   'candidate',\n",
       "   'with',\n",
       "   'the',\n",
       "   'aim',\n",
       "   'of',\n",
       "   'verify',\n",
       "   'whether',\n",
       "   'the',\n",
       "   'candidate',\n",
       "   'performance',\n",
       "   'be',\n",
       "   'relate',\n",
       "   'to',\n",
       "   ...]},\n",
       " {'titulo': 'A Domain-Specific Language for Multimedia Service Function Chains based on Virtualization of Sensors',\n",
       "  'informacoes_url': '',\n",
       "  'idioma': 'english',\n",
       "  'storage_key': '../articles/original/english/985-24738-1-10-20240923.pdf',\n",
       "  'author': 'Franklin Jordan Ventura Quico; Anselmo L. E. Battisti; Débora Muchaluat-Saade; and Flavia C. Delicato',\n",
       "  'data_publicacao': '11-09-2024',\n",
       "  'resumo': 'Virtualization is a widely used technology that can abstract the complexity of heterogeneous environments, such as the Internet of Things (IoT) and multimedia systems. Multimedia sensors are an important data source in the Internet of Things (IoT), which brings the Internet of Media Things (IoMT) paradigm. Based on virtualization and IoMT, the concept of a multimedia Virtual Network Function (multimedia VNF) has been adopted to denote the virtualized representation of devices and also software components that process multimedia streams. In many scenarios, multiple processes must be applied to multimedia streams in a predefined sequence, thus creating the concept of multimedia Service Function Chain (multimedia SFC). Few efforts have been made in the literature to create a description language to support the definition of multimedia SFCs. In order to fill this gap, we propose a Domain Specific Language (DSL) called L-PRISM. This DSL can be used as a conceptual base for developers to implement and virtualize multimedia applications using multimedia VNFs. We also present a Proof of Concept (PoC) that uses L-PRISM to run multimedia SFCs. Our DSL and PoC were evaluated by software developers, and the results show that adopting L-PRISM facilitates the definition and deployment of multimedia SFCs based on multimedia VNFs. ###',\n",
       "  'keywords': 'IoMT, IoT, VNF, SFC, DSL, L-PRISM',\n",
       "  'referencias': ['[1] ETSI GS NFV-IFA 011. 2023. Network Functions Virtualisation (NFV) Release\\n4; Management and Orchestration; VNF Descriptor and Packaging Specification. https://docbox.etsi.org/ISG/NFV/open/Publications_pdf/Specs-Reports/\\nNFV-IFA%20011v4.5.1%20-%20GS%20-%20VNF%20Packaging%20Spec.pdf\\n\\n\\n18\\n\\n\\n-----\\n\\nA Domain-Specific Language for Multimedia Service Function Chains based on Virtualization of Sensors WebMedia’2024, Juiz de Fora, Brazil',\n",
       "   '[2] ETSI GS NFV-IFA 014. 2021. Network Functions Virtualisation (NFV) Release 4;\\nManagement and Orchestration; Network Service Templates Specification. https:\\n//docbox.etsi.org/ISG/NFV/open/Publications_pdf/Specs-Reports/NFV-IFA%\\n20014v4.2.1%20-%20GS%20-%20Network%20Service%20Templates%20Spec.pdf',\n",
       "   '[3] Anselmo Luiz Éden Battisti, Débora Christina Muchaluat-Saade, and Flávia C.\\nDelicato. 2020. V-PRISM: An Edge-Based IoT Architecture to Virtualize Multimedia Sensors. In *2020 IEEE 6th World Forum on Internet of Things (WF-IoT)* . 1–6.\\nhttps://doi.org/10.1109/WF-IoT48130.2020.9221199',\n",
       "   '[4] Anselmo Luiz Éden Battisti, Debora Christina Muchaluat-Saade, and Flavia C\\nDélicato. 2021. Enabling Internet of Media Things with edge-based virtual\\nmultimedia sensors. *IEEE Access* 9 (2021), 59255–59269.',\n",
       "   '[5] Deval Bhamare, Raj Jain, Mohammed Samaka, and Aiman Erbad. 2016. A survey\\non service function chaining. *Journal of Network and Computer Applications* 75\\n(Nov. 2016), 138–155. https://doi.org/10.1016/j.jnca.2016.09.001',\n",
       "   '[6] Martin Björklund. 2010. YANG - A Data Modeling Language for the Network\\nConfiguration Protocol (NETCONF). RFC 6020. https://doi.org/10.17487/RFC6020',\n",
       "   '[7] Alan F Blackwell and Thomas RG Green. 2000. A Cognitive Dimensions questionnaire optimised for users. In *PPIG*, Vol. 13. Citeseer.',\n",
       "   '[8] José Castillo-Lema, Augusto Venâncio Neto, Flávio de Oliveira, and Sergio\\nTakeo Kofuji. 2019. Mininet-NFV: Evolving Mininet with OASIS TOSCA NVF\\nprofiles Towards Reproducible NFV Prototyping. In *2019 IEEE Conference on*\\n*Network Softwarization (NetSoft)* . 506–512. https://doi.org/10.1109/NETSOFT.\\n2019.8806686',\n",
       "   '[9] Mario Di Mauro, Giovanni Galatro, Maurizio Longo, Fabio Postiglione, and Marco\\nTambasco. 2021. Comparative Performability Assessment of SFCs: The Case of\\nContainerized IP Multimedia Subsystem. *IEEE Transactions on Network and Service*\\n*Management* 18, 1 (2021), 258–272. https://doi.org/10.1109/TNSM.2020.3044232',\n",
       "   '[10] GS NFV-SOL 001 ETSI. 2022. Network Functions Virtualisation (NFV) Release\\n4; Protocols and Data Models; NFV descriptors based on TOSCA specification. https://www.etsi.org/deliver/etsi_gs/NFV-SOL/001_099/001/04.03.01_\\n60/gs_NFV-SOL001v040301p.pdf',\n",
       "   '[11] A.H. Ghorab, A. Kusedghi, M. A. Nourian, and A. Akbari. 2020. Joint VNF Load\\nBalancing and Service Auto-Scaling in NFV with Multimedia Case Study. In *2020*\\n*25th International Computer Conference, Computer Society of Iran (CSICC)* . 1–7.\\nhttps://doi.org/10.1109/CSICC49403.2020.9050122',\n",
       "   '[12] Heiko Koziolek. 2008. Goal, question, metric. In *Dependability metrics* . Springer,\\n39–42.',\n",
       "   '[13] Rensis Likert. 1932. A technique for the measurement of attitudes. *Archives of*\\n*psychology* (1932).',\n",
       "   '[14] Yuyi Mao, Changsheng You, Jun Zhang, Kaibin Huang, and Khaled B. Letaief.\\n2017. A Survey on Mobile Edge Computing: The Communication Perspective.\\n*IEEE Comm. Surveys and Tutorials* 19, 4 (2017), 2322–2358. https://doi.org/10.\\n1109/COMST.2017.2745201 arXiv:1701.01090',\n",
       "   '[15] Marjan Mernik, Jan Heering, and Anthony M Sloane. 2005. When and how to\\ndevelop domain-specific languages. *ACM computing surveys (CSUR)* 37, 4 (2005),\\n316–344.',\n",
       "   '[16] Rashid Mijumbi, Joan Serrat, Juan Luis Gorricho, Niels Bouten, Filip De Turck,\\nand Raouf Boutaba. 2016. Network function virtualization: State-of-the-art and\\nresearch challenges. *IEEE Communications Surveys and Tutorials* 18, 1 (2016),\\n236–262. https://doi.org/10.1109/COMST.2015.2477041 arXiv:1509.07675',\n",
       "   '[17] Ali Nauman, Yazdan Ahmad Qadri, Muhammad Amjad, Yousaf Bin Zikria,\\nMuhammad Khalil Afzal, and Sung Won Kim. 2020. Multimedia Internet of\\nThings: A comprehensive survey. *Ieee Access* 8 (2020), 8202–8250.',\n",
       "   '[18] Eman Negm, Soha Makady, and Akram Salah. 2019. Survey on domain specific\\nlanguages implementation aspects. *International Journal of Advanced Computer*\\n*Science and Applications* 10, 11 (2019).',\n",
       "   '[19] Guto Leoni Santos, Diego de Freitas Bezerra, Élisson da Silva Rocha, Leylane\\nFerreira, André Luis Cavalcanti Moreira, Glauco Estácio Gonçalves, Maria Valéria\\nMarquezini, Ákos Recse, Amardeep Mehta, Judith Kelner, Djamel Sadok, and\\nPatricia Takako Endo. 2022. Service Function Chain Placement in Distributed\\nScenarios: A Systematic Review. *Journal of Network and Systems Management*\\n30, 1 (2022), 1–39. https://doi.org/10.1007/s10922-021-09626-4',\n",
       "   '[20] Jürgen Schönwälder, Martin Björklund, and Phil Shafer. 2010. Network configuration management using NETCONF and YANG. *IEEE communications magazine*\\n48, 9 (2010), 166–173.',\n",
       "   '[21] Priyanka Surendran et al . 2012. Technology acceptance model: A survey of\\nliterature. *International Journal of Business and Social Research* 2, 4 (2012), 175–\\n178.',\n",
       "   '[22] OASIS TOSCA. 2017. TOSCA Simple Profile for Network Functions Virtualization\\n(NFV) Version 1.0, Committee Specification Draft 04. https://docs.oasis-open.\\norg/tosca/tosca-nfv/v1.0/csd04/tosca-nfv-v1.0-csd04.html',\n",
       "   '[23] YAML. [n. d.]. YAML Ain’t Markup Language (YAML™) version 1.2. https:\\n//yaml.org/spec/1.2/spec.html. Accessed: Jun 29, 2024.',\n",
       "   '[24] Bo Yi, Xingwei Wang, Keqin Li, Sajal k. Das, and Min Huang. 2018. A comprehensive survey of Network Function Virtualization. *Computer Networks* 133 (2018),\\n212–262. https://doi.org/10.1016/j.comnet.2018.01.021\\n\\n\\n19\\n\\n\\n-----'],\n",
       "  'text': '# **A Domain-Specific Language for Multimedia Service Function** **Chains based on Virtualization of Sensors**\\n\\n## Franklin Jordan Ventura Quico\\n#### fventuraq@midiacom.uff.br Fluminense Federal University Niterói, RJ, Brazil\\n## Débora Muchaluat-Saade\\n#### debora@midiacom.uff.br Fluminense Federal University Niterói, RJ, Brazil\\n### **ABSTRACT**\\n\\nVirtualization is a widely used technology that can abstract the\\ncomplexity of heterogeneous environments, such as the Internet\\nof Things (IoT) and multimedia systems. Multimedia sensors are\\nan important data source in the Internet of Things (IoT), which\\nbrings the Internet of Media Things (IoMT) paradigm. Based on virtualization and IoMT, the concept of a multimedia Virtual Network\\nFunction (multimedia VNF) has been adopted to denote the virtualized representation of devices and also software components that\\nprocess multimedia streams. In many scenarios, multiple processes\\nmust be applied to multimedia streams in a predefined sequence,\\nthus creating the concept of multimedia Service Function Chain\\n(multimedia SFC). Few efforts have been made in the literature to\\ncreate a description language to support the definition of multimedia SFCs. In order to fill this gap, we propose a Domain Specific\\nLanguage (DSL) called L-PRISM. This DSL can be used as a conceptual base for developers to implement and virtualize multimedia\\napplications using multimedia VNFs. We also present a Proof of\\nConcept (PoC) that uses L-PRISM to run multimedia SFCs. Our\\nDSL and PoC were evaluated by software developers, and the results show that adopting L-PRISM facilitates the definition and\\ndeployment of multimedia SFCs based on multimedia VNFs.\\n### **KEYWORDS**\\n\\nIoMT, IoT, VNF, SFC, DSL, L-PRISM\\n### **1 INTRODUCTION**\\n\\nVirtualization is a concept that has paved the way for numerous\\nresearch studies in Cloud Computing, 5G, and IoT [ 14, 24 ]. This\\ntechnology not only reduces the costs of implementing and managing infrastructures, but also serves as a fundamental pillar in\\nparadigms such as Network Function Virtualization (NFV) [ 16, 19 ].\\nThe adoption of NFV and its Virtual Network Function (VNF) components has proved to be effective in reducing the consumption\\nof computational and network resources. Furthermore, virtualization increases the speed and efficiency of resource provisioning\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n© 2024 SBC – Brazilian Computing Society.\\nISSN 2966-2753\\n\\n## Anselmo L. E. Battisti\\n#### anselmo@midiacom.uff.br Fluminense Federal University Niterói, RJ, Brazil\\n## Flavia C. Delicato\\n#### fdelicato@ic.uff.br Fluminense Federal University Niterói, RJ, Brazil\\n\\nin cloud and edge environments, facilitating the management and\\norchestration of these resources [24].\\nMany investigations combine NFV technology with multimedia\\nstream processing. In [ 4 ], the authors explore the concept of virtual\\ndevices that abstract the heterogeneity of multimedia sensors and\\nintroduce virtual multimedia sensors (VMS), which we will refer to\\nas multimedia VNFs in our work. These VMSs enable multimedia\\n\\nstream processing using lightweight virtualization. Furthermore,\\nthe authors demonstrated the feasibility of using chained VMSs to\\ncreate complex multimedia stream processing pipelines. However,\\nthis characteristic was not fully explored in that work. Meanwhile,\\nin [ 11 ], the authors investigated the joint load balancing and autoscaling of multimedia VNFs in edge or cloud environments. In\\nthis context, virtualization concepts like NFV have been actively\\nintegrated into different studies to process multimedia streams.\\nThe Service Function Chain (SFC) is a sequence of sorted VNFs\\nassociated with a Service Level Agreement (SLA) [ 5 ]. By chaining\\nmultimedia functions, it is possible to create complex multimedia\\nstream processing pipelines. When the VNFs in an SFC are tailored\\nfor multimedia processing, they are referred to as multimedia SFCs.\\nDespite the potential of multimedia SFCs in various fields [ 9 ],\\nwe face the challenge of implementing and managing them easily,\\nsecurely, and efficiently. The components used to create multimedia\\nSFCs are typically developed by different entities using multiple\\ntechnologies [ 14 ]. Thus, to create a multimedia SFC, users must have\\na moderate level of knowledge of these technologies. Moreover, the\\ninteroperability of these components sometimes is often limited.\\nTo address these challenges, it is necessary to provide a layer\\nthat encapsulates the technical details of multimedia SFCs. There\\nare different alternatives for this, among which Domain-Specific\\nLanguages (DSLs) stand out [ 18 ]. Unlike other approaches, such as\\nmetamodels or conventional programming languages, DSLs offer\\ngreater flexibility and simplicity.\\nDSLs are language specifications designed to describe the characteristics, syntax, and behavior of systems in specific fields [ 15 ].\\nThese languages enable intuitive integration and management by\\nproviding an abstraction layer encapsulating technical details. This\\nallows users to focus on business logic and specific functionalities. In addition, DSLs facilitate the integration of new or external\\ncomponents more efficiently and less technically.\\nAlthough there have been significant advances in the literature\\nto facilitate the definition and deployment of NFV services, such as\\n\\n\\n11\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Ventura Quico et al.\\n\\n\\nthe TOSCA-NFV metamodel [ 22 ] or ETSI GS NFV-SOL [ 10 ], there\\nis still a considerable gap specifically regarding multimedia SFCs.\\nThe need to address multimedia SFCs separately arises from their\\nunique nature and challenges. Some of these challenges are (i) managing large-volume and high-speed data flows, (ii) synchronizing\\nmultiple formats and media types, and (iii) meeting requirements\\nfor real-time processing, which are not as critical in traditional\\nNFVs [ 17 ]. Furthermore, multimedia SFCs often require greater\\nflexibility in terms of scalability and adaptation of dynamic changes\\nin resource demand. Therefore, although existing models, such as\\nTOSCA-NFV, provide means to describe SFCs in general, the unique\\ncharacteristics of multimedia SFCs demand a specialized approach\\nthat directly addresses their specific requirements and inherent\\nchallenges.\\nIn this work, our aim is to fill this gap by proposing a DSL\\ncalled Language for Programming IoT Sensors for Multimedia (LPRISM), which serves as a conceptual foundation for describing\\nmultimedia SFCs, particularly in the emerging field of the IoMT.\\nThe decision to develop L-PRISM, instead of extending existing\\nsolutions like TOSCA-NFV, is driven by the unique demands of\\nmultimedia SFCs, such as the inherent complexity in their definition\\nand configuration, the need for flexibility in their components and\\nadaptability to specific environments.\\nOur DSL includes a data model that provides detailed structures\\nfor specifying the critical properties of multimedia SFCs. This model\\nenables the specification of the type, formats, and resolutions of\\ndata streams that multimedia VNFs can process and send, but it\\nalso considers scenarios where a multimedia VNF can handle its\\n\\nstreams in various formats and qualities. This aspect is crucial, as\\nthe computational and network requirements associated with a\\nmultimedia VNF are variable and depend on the characteristics of\\nthe multimedia stream to be processed.\\nThis information is vital for developers of multimedia SFCs, as it\\nprovides the necessary details to understand and utilize multimedia\\nVNFs. In turn, this will enable developers to specify their multimedia\\nSFCs efficiently. Additionally, at the application level, our data\\nmodel can facilitate the effective orchestration and management of\\nmultimedia SFCs, optimizing their lifecycle.\\nFurthermore, having a dedicated DSL presents several advantages, such as customization, facilitated implementation, and maintenance of its various components. The main contributions of this\\nwork are:\\n\\n  - L-PRISM makes it possible to describe a multimedia SFC\\nbased on a sequence of multimedia VNFs.\\n\\n  - L-PRISM defines the necessary structures for detailed logging of multimedia VNFs.\\n\\n  - L-PRISM makes using multimedia VNFs developed by third\\nparties easier, so developers of multimedia VNF-based solutions do not need to have advanced knowledge about the\\ntechnologies or tools used for developing the components\\nof a multimedia SFC.\\n\\nThe remainder of this paper is organized as follows. Section 2\\ndescribes the background and related work. Sections 3 and 4 present\\nL-PRISM and its metamodel. Section 5 describes our PoC. In Section\\n\\n6, we present an evaluation of our work. Section 7 brings our main\\nconclusions and future work.\\n\\n### **2 BACKGROUND AND RELATED WORK**\\n\\nTraditional sensors, such as thermostats and presence detectors,\\ntypically produce discrete data in a readable format. However, multimedia sensors produce continuous, complex and large-volume\\ndata, which require high processing rates, massive storage, wide\\nbandwidth, low latency, and high energy consumption [ 17 ]. These\\ncharacteristics make their operation different from that of traditional sensors. One strategy to handle this complexity is to decouple\\nphysical sensors from multimedia stream consumers [4].\\nDifferent languages, metamodels, and DSLs have been developed,\\nwhich have helped develop systems and architectures, such as,\\nYANG [20], TOSCA-NFV [22] and ETSI GS NFV-SOL [10].\\nYANG [ 20 ] is a data modeling language used to describe network\\nconfigurations and telecommunication services. YANG can be used\\nfor NETCONF-based operations, including configuration, data state,\\nremote procedure calls (RPCs), and notifications. YANG models can\\nbe translated into XML syntax, allowing applications that use XML\\nparsers to operate and manipulate YANG models [ 6 ]. However,\\nYANG does not have features required to describe multimedia SFCs.\\nA specific data model that works with virtualization technologies is TOSCA-NFV [ 22 ], which allows describing, deploying, and\\nmanaging applications and services based on NFV. TOSCA-NFV describes in detail the components of its solutions and the interactions\\nand dependencies between them. TOSCA is a modeling language\\nthat uses the template concept to describe the components of a\\nworkload using a topology template, and their relationships using\\na relationship template [22]. It is crucial to note that TOSCA-NFV\\nfocuses primarily on traditional virtualization systems and does\\nnot align well with lightweight virtualization platforms, such as\\nDocker and Kubernetes.\\n\\nMininet-NFV [ 8 ] presents an advanced framework for NFV orchestration, facilitating the implementation and operation of VNFbased network services. This framework is built upon Mininet, a\\ntool widely recognized for its capability in agile experimentation\\nwith networks, SDN and NFV, and extensively used for prototyping,\\ntesting, and implementing NFV solutions. Additionally, MininetNFV supports parameterized TOSCA-NFV templates, using virtual\\nlink descriptors to define detailed and flexible network configurations.\\n\\nETSI GS NFV-SOL 001 [ 10 ] adapts TOSCA-NFV to meet the\\nspecification requirements of ETSI GS NFV IFA 011 [ 1 ] and 014 [ 2 ]\\nfor Virtualized Network Function Descriptors (VNFDs), Network\\nService Descriptors (NSDs), and Physical Network Function Descriptors (PNFDs). Like TOSCA-NFV, it specifies requirements for\\nthe management and orchestration of VNFs.\\nAlthough very related to our proposal, neither TOSCA-NFV nor\\nETSI NFV-SOL specifically addresses the requirements of multimedia applications and systems, particularly concerning multimedia\\nSFCs and their components. Therefore, while our work is based on\\nthe principles of TOSCA-NFV, it stands out by addressing the necessary aspects for the definition, orchestration, and management of\\nmultimedia SFCs implemented on lightweight virtualization platforms.\\n\\nTable 1 compares related languages, templates, and data models\\nthat support the definition of SFCs. The last row of the table presents\\nL-PRISM. The meaning of each column in the table are as follows:\\n\\n\\n12\\n\\n\\n-----\\n\\nA Domain-Specific Language for Multimedia Service Function Chains based on Virtualization of Sensors WebMedia’2024, Juiz de Fora, Brazil\\n\\n  - **IoMT:** Focus on Internet of Media Things applications.\\n\\n  - **Edge Computing:** Tailored for edge computing environ\\nments.\\n\\n  - **Light Virtualization:** Support for lightweight virtualization\\nplatforms.\\n\\n\\n**Table 1: Comparison between related work.**\\n\\n\\n|Related Work|IoMT|Edge Computing|Light Virtualization|\\n|---|---|---|---|\\n|YANG [20]|-|-|-|\\n|TOSCA-NFV [22]|-|✓|-|\\n|Mininet-NFV [8]|-|✓|✓|\\n|ETSI NFV-SOL [10]|-|✓|✓|\\n|L-PRISM|✓|✓|✓|\\n\\n\\nOur previous work [ 3, 4 ] proposed an architecture, which allows the execution and management of multimedia VNFs based\\non lightweight virtualization in an edge-cloud environment. However, at that time, we did not explore multimedia SFCs or provided\\na domain-specific language to describe multimedia SFCs, which\\nwould make the creation and maintenance of multimedia SFCs easier. In this work, we fill in this gap by simplifying the definition of\\nmultimedia SFCs through the development of a DSL.\\n### **3 L-PRISM**\\n\\nThis section presents L-PRISM, a DSL that facilitates the description of multimedia SFCs based on multimedia VNFs. L-PRISM is\\n\\nmainly based on the architecture proposed in [ 3 ] and the TOSCANFV metamodel [ 22 ]. L-PRISM was developed following the stages\\nproposed by Negm et al. [18], detailed in the following sections.\\n### **3.1 Domain Analysis**\\n\\nAn analysis of multimedia applications in virtualized environments\\nwas carried out. It allowed us to understand and abstract the main\\ncharacteristics that influence the operation of these types of application. The characteristics represent configurable aspects such as\\nthe allocation of computing and network resources, communication\\nrequirements, etc.\\nIn addition, an analysis of the interaction between multimedia\\napplications was performed. These interactions can be modeled as\\ndirected graphs, representing complex multimedia applications. We\\ndescribe these complex applications as *multimedia SFCs* and their\\ncomponents as *multimedia VNFs* .\\n### **3.2 L-PRISM Design**\\n\\nL-PRISM was created following the concepts presented in [ 4 ] as a\\nreference. Thus, the design of L-PRISM considers the implementation of *multimedia SFCs* in distributed environments composed of\\nedge and cloud nodes. In addition, L-PRISM was coined to operate\\non lightweight virtualization platforms, such as Docker and Kubernetes. Figure 1 shows an example of a multimedia SFC. The main\\ncomponents of a multimedia SFC are:\\n\\n\\n**Figure 1: Example of a multimedia SFC.**\\n\\n  - *Virtual Device (VD)* is the virtual representation of a physical\\nmultimedia device.\\n\\n  - *Multimedia VNF / VMS* is a function that process the multimedia streams. Unlike VDs, instances of multimedia VNFs\\nare created exclusively for each multimedia SFC.\\n\\n  - *Virtual Links (VL)* provide the communication between (i)\\nVDs and multimedia VNFs, (ii) different multimedia VNFs\\nand (iii) multimedia VNFs and endpoints (user applications).\\n\\nL-PRISM is a flexible language that can be used in compiled\\nand interpreted scenarios. In both cases, the user describes the\\nmultimedia SFC using our proposed language. In a compilationbased scenario, the compiler will create a low-level code that will be\\nprocessed to execute the multimedia SFCs. In an interpreted-based\\nscenario, which is the case of our PoC, an interpreter runs over the\\ncode and creates/instantiates the multimedia SFCs.\\nThe L-PRISM language is based on YAML to provide readability,\\nflexibility, cross-platform support, integration with existing tools,\\nand a fast learning curve [23]. These features make L-PRISM intuitive and easy to use, and in turn, YAML is tailored to our specific\\nneeds and helps to make our DSL easily accepted by developers.\\nL-PRISM follows a model-driven approach. Figure 2 presents\\nthe L-PRISM metamodel, defining its components, attributes, and\\nthe relationships between them. For example, the *swImage* (Image\\nof the multimedia VNF) structure in the metamodel of our DSL\\nallows for detailed description of the type, formats, and resolutions\\nof multimedia streams that a multimedia VNF can process and emit.\\nThis information at the application level can be stored in a database,\\nwhich can then be accessed by developers of multimedia SFCs. This\\nwill enable them to understand the characteristics of multimedia\\nVNFs generally and efficiently specify each component of their\\nmultimedia SFCs.\\nFor an extensive overview of L-PRISM, visit our website [1] .\\n### **4 THE L-PRISM METAMODEL**\\n\\nThis section summarizes the L-PRISM metamodel. Table 2 presents\\nthe attributes of a *𝑐ℎ𝑎𝑖𝑛𝑀𝑜𝑑𝑒𝑙*, which describes a multimedia SFC.\\nIt should be noted that attributes ending with an (*) are mandatory.\\nTable 3 describes the attributes of the *𝑑𝑒𝑣𝑖𝑐𝑒* component, which is\\na virtual representation of a physical device (camera or microphone).\\nOne advantage of virtualized devices is that a single multimedia\\nstream captured from a camera/microphone can be reused multiple\\ntimes and sent to different destinations.\\n\\n1 https://fventuraq.github.io/lprism.html\\n\\n\\n13\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Ventura Quico et al.\\n\\n**Figure 2: Class diagram of the L-PRISM metamodel.**\\n\\n\\n**Table 2: Attributes of a** ***chainModel*** **in L-PRISM**\\n\\n\\n\\n\\n\\n**Table 4: Attributes of multimedia VNF (** ***vms*** **) in L-PRISM**\\n\\n\\n\\n|Name|Type|Description|\\n|---|---|---|\\n|chain- Name*|String|Name of the multimedia SFC.|\\n|chain- Description|String|Multimedia SFC description.|\\n|devices*|map|List of virtual devices (𝑑𝑒𝑣𝑖𝑐𝑒) that com- pose the multimedia SFC.|\\n|vmss*|map|List of multimedia VNFs (𝑣𝑚𝑠) that com- pose the multimedia SFC.|\\n|virtual- Links*|map|List of 𝑣𝑖𝑟𝑡𝑢𝑎𝑙𝐿𝑖𝑛𝑘𝑠 between device and multimedia VNF (vms).|\\n\\n\\n\\n\\n**Table 3: Attributes of** ***device*** **in L-PRISM**\\n\\n|Name|Type|Description|\\n|---|---|---|\\n|device- Name*|String|Unique name of a virtual device that works as an ID in a virtualLink.|\\n|deviceId*|String|Identifies a virtualized . This index 𝑑𝑒𝑣𝑖𝑐𝑒 helps any element of the multimedia SFC to subscribe to a . When a re- 𝑑𝑒𝑣𝑖𝑐𝑒 𝑑𝑒𝑣𝑖𝑐𝑒 ceives a subscription, it replicates its stream and sends it to the new subscriber. It should be noted that virtualized devices are not cre- ated in the multimedia SFC. They are initial- ized separately and can be part of different multimedia SFCs.|\\n\\n\\n\\nTable 4 presents the attributes of a *𝑣𝑚𝑠* (multimedia VNF) component. A multimedia VNF within a multimedia SFC is used to\\nprocess one or multiple multimedia streams and generate one or\\nmore output results. The result of a multimedia VNF can be sent to\\none or more destinations, including an end-user application.\\n\\n\\n\\n\\n\\n|Name|Type|Description|\\n|---|---|---|\\n|vmsName*|String|The name of a multimedia VNF must be unique in a multimedia SFC. This data type works as an in a . 𝐼𝐷 𝑣𝑖𝑟𝑡𝑢𝑎𝑙𝐿𝑖𝑛𝑘|\\n|vms- Description|String|Multimedia VNF description.|\\n|vmsType*|lPrism.- artifacts.- vms.- swImage|The container images used for cre- ating the multimedia VNF.|\\n|startup- Parameters|lPrism.- datatype.- vms.- configurable- Properties|Data type defined initially by TOSCA-NFV; it describes the ini- tial configuration properties of the multimedia VNF.|\\n|host*|lPrism.- nodes.- vms.host|network node that will host the multimedia VNF. The list of avail- able nodes must be shared with the developer of the multimedia SFC.|\\n|virtual- Compute|lPrism.- capabilities.- vms.- virtual- Compute|Computational properties assigned to the multimedia VNF.|\\n\\n\\nTables 5 and 6 present the *source* and *destination* element attributes, respectively. The interconnections between two elements\\n(virtual device or VNF) of a multimedia SFC are described by *vir-*\\n*tualLink* . This element provides information on two other components, the source point ( *source* ) from which the stream is sent\\nand the destination point ( *destination* ) where the stream will be\\nreceived.\\n\\n\\n14\\n\\n\\n-----\\n\\nA Domain-Specific Language for Multimedia Service Function Chains based on Virtualization of Sensors WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\n**Table 5: Attributes of** ***virtualLink-source*** **in L-PRISM.**\\n\\n\\n\\n\\n\\n\\n\\n\\n|Name|Type|Description|\\n|---|---|---|\\n|source- Name*|String|source of a multimedia stream. This name must be identical to that con- figured in a deviceName or a vm- sName.|\\n|source- Type*|String|type of the source node of the virtual link, which can be or . 𝑣𝑚𝑠 𝑑𝑒𝑣𝑖𝑐𝑒|\\n|output- Type*|String|Describes the type of multimedia stream to be sent. Multimedia appli- cations usually process one type of stream but may produce a different output type.|\\n|format- Type|String|The multimedia stream type such as MP4, AVI, MPEG.|\\n|network- Bandwidth|lPrism.data- type.vms.- network- Bandwidth|Bandwidth assigned to the connec- tion. Within a host, it is not neces- sary to specify this attribute, as con- nections are efficiently managed in- ternally. However, for connections between different hosts on the net- work, it is vital to properly establish and adjust the bandwidth according to the characteristics of the multime- dia stream.|\\n\\n\\n**Table 6: Attributes of** ***virtualLink-destination*** **in L-PRISM.**\\n\\n\\n\\n\\n\\n\\n|Name|Type|Description|\\n|---|---|---|\\n|destina- tionName*|String|Which component of the multimedia SFC will receive the stream. This data must be identical to the one config- ured in a of one of the 𝑣𝑚𝑠𝑁𝑎𝑚𝑒 𝑣𝑚𝑠 used in the same multimedia SFC.|\\n|Address|lPrism.- datatype.- vms.- addressData|Complex data type initially defined by TOSCA-NFV and adapted in L- PRISM, which describes a network address.|\\n|Port*|lPrism.- datatype.- vms.- listPorts|Describes the port where the multi- media stream will be received.|\\n|inputType*|String|Describes the type of multimedia stream that the destination of the con- nection accepts.|\\n\\n\\nSource Code 1 gives a simple example of a multimedia SFC described using L-PRISM. Figure 3 shows the visual representation\\nof that SFC. It is composed by one *device*, one *vms* and two *virtu-*\\n*alLinks* . It has one video source (VD 2) that is handled by a VNF\\n(Multimedia VNF 1) that sends the video stream to the final application (192.168.0.101 Port 1002). The next section presents our\\nPoC.\\n\\n\\n**Source Code 1: A multimedia SFC described using L-PRISM.**\\n\\nchainModel :\\n\\nchainName : multimedia SFC t e s t\\n\\nc h a i n D e s c r i p t i o n : Video Flux\\nd e v i c e s : # L i s t of devices, in t h i s example 1 device\\n\\n−\\n\\ndeviceName : VD 2\\n\\nd e v i c e I d : 638 e 7 0 b 1 0 a1 fb d 0 0 2 6 fc c a f4\\n\\nvmss : # L i s t of multimedia VNFs, in t h i s example 1 VNF\\n\\n−\\n\\nvmsName : Multimedia VNF 1\\n\\nvmsDescription : Multimedia VNF 1 d e s c r i p t i o n\\nvmsType : 638 e70b10a1fbd0026fccae8\\nhost : 1 9 2 . 1 6 8 . 0 . 1 1 7 # IP\\n\\nvirtualCompute :\\n\\nvirtualMemory :\\n\\ns i z e : 1024\\n\\nvirtualCPU :\\n\\nnumCpu : 1\\nv i r t u a l l i n k s :\\n\\n−# v i r t u a l L i n k 1 ( \"VD 2\" −> \" Multimedia VNF 1 \" )\\n\\nsource :\\n\\nsourceName : VD 2\\n\\nsourceType : device\\noutputType : video\\nd e s t i n a t i o n :\\n\\ndestinationName : Multimedia VNF 1\\n\\naddress : # d e f i n e f o r o r c h e s t r a t o r\\n\\nport : 5000 # port of multimedia VNF\\ninputType : video\\n−# v i r t u a l L i n k 2 ( f i n a l connection )\\n\\nsource :\\n\\nsourceName : Multimedia VNF 1\\n\\nsourceType : vms\\noutputType : video\\nd e s t i n a t i o n :\\n\\ndestinationName : 1 9 2 . 1 6 8 . 0 . 1 0 1\\n\\naddress : 1 9 2 . 1 6 8 . 0 . 1 0 1\\n\\nport : 10002\\ninputType : video\\n\\n**Figure 3: Multimedia Chain example.**\\n### **5 ALFA 2.0: PROOF OF CONCEPT (POC)**\\n\\nThis section describes the integration of L-PRISM into a platform\\nnamed ALFA [ 3 ] [2] . ALFA uses containers to run VMSs and virtual\\ndevices thus allowing language independence. Our PoC named\\nALFA 2.0 is released under the MIT License. The source code is\\navailable [3] .\\n### **5.1 Database**\\n\\nALFA uses a non-relational database (mongoDB) to store information about VMSs, Virtual Devices, Edge Nodes, VMS types, and\\nDevice types. To extend its functionality, we added the L-PRISM\\ncomponents, such as *chain model*, to the database. The chain model\\n\\n2 Source code available at https://github.com/midiacom/alfa\\n3 Source code of alfa 2.0 https://github.com/fventuraq/alfa\\n\\n\\n15\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Ventura Quico et al.\\n\\n\\nstores information about each created multimedia SFC, such as\\nwhich VMSs are used within a multimedia SFC, which devices are\\nused, and the connections between them.\\nIn addition to these changes, some attributes were modified and\\nadded to the already defined components, such as VMS, VD, Edge\\nNode, VMS types, and Device type. These new attributes helps\\nALFA 2.0 to support the L-PRISM specification. With these changes,\\nit is possible to create, and manage multimedia SFCs defined using\\nL-PRISM inside ALFA 2.0.\\n### **5.2 API**\\n\\nThe ALFA implementation already has an API that is used to manage the entities responsible for executing the VMSs. This API did\\nnot support multimedia SFC, thus a new endpoint was added to the\\nALFA 2.0 API. This new endpoint receives and processes a YAML\\nfile that contains the multimedia SFC described using L-PRISM.\\nTo manage multimedia SFCs, multiple tasks must be performed.\\nThe new endpoint executes commands using the Docker API and\\nthe ALFA API to create VMSs. After that, the virtual devices are\\nlinked to the VMSs, and interconnections between VMSs are cre\\nated.\\n### **5.3 Web Client**\\n\\nALFA API allows different software to interact with the entity of\\nthe platform. The ALFA implementation already has a Web Client\\nto manage the VMSs. Thus, we extended the original web-based\\nclient by adding a new component that allows the upload of a file\\nwith the multimedia SFC description using the L-PRISM language.\\nFigure 4 shows the web client interface used to send the file with\\nthe multimedia SFC description. The Web client presents the VMSs,\\nVirtual Devices, and nodes running in the environment. The figure\\nalso shows the result of the multimedia stream processed by the\\ncreated multimedia SFC.\\n\\n**Figure 4: Web interface for executing L-PRISM files in ALFA**\\n**2.0.**\\n### **6 EVALUATION**\\n\\nThis section describes the L-PRISM evaluation. The experiment\\nwas carried out with computer science students. We asked them\\nto create multimedia SFCs with L-PRISM and also using a manual\\ndeployment.\\n\\n### **6.1 Goal Question Metric**\\n\\nThe experiment aims to evaluate L-PRISM during the development\\nof multimedia SFCs. The Goal Question Metric (GQM) methodology\\nwas adopted in the process [12]. Table 7 presents our goals.\\n\\n**Table 7: Evaluation goals.**\\n\\n|Goal|Description|Perspective|\\n|---|---|---|\\n|G1|Analyze the application engineering process with and without L-PRISM to evaluate the efficiency and productivity of developing multimedia SFC.|Efficiency and Productivity|\\n|G2|Evaluate the comprehensibility of L- PRISM to analyze if variables, attributes, and structures are understandable for the participants.|Usability|\\n\\n|#Q|Description|Metrics|\\n|---|---|---|\\n|Q1|Is the application engineering process us- ing L-PRISM effective in terms of time for developing multimedia SFC, compared to the traditional approach (ALFA)?|M1 - De- velopment effort|\\n|Q2|Does the developer claim that using L- PRISM makes it easier to understand the functional and non-functional require- ments of the multimedia SFC?|M2 - Un- derstanding of require- ments|\\n|Q3|Does the developer claim that using L- PRISM helps create multimedia SFC?|M3 - Per- ceived ease of use|\\n|Q4|Does the developer claim that L-PRISM is useful to create multimedia SFC?|M4 - Per- ceived utility|\\n|Q5|Does the developer claim that using L- PRISM makes it easier to reuse the multi- media SFC created with L-PRISM to create new multimedia SFC?|M5 - Per- ceived reuse|\\n|Q6|Is the process of modifying multimedia SFC faster with L-PRISM compared with the traditional method (ALFA)?|M6 - Reuse effort|\\n\\n\\nThe questions for *𝐺* 1 were adapted from the Technology Acceptance Model (TAM) [ 21 ]. The goal was to quantitatively evaluate\\nthe multimedia SFC development process using L-PRISM and the\\ntraditional method. Metrics *𝑀* 2 *, 𝑀* 3 *, 𝑀* 4 and *𝑀* 5 were measured\\nusing the Likert scale (1 - strongly disagree to 5 - strongly agree)\\n\\n[ 13 ], and for metrics *𝑀* 1 and *𝑀* 6, the time in minutes required to\\nperform each task was requested.\\nFor goal *𝐺* 2, seven questions ( *𝑄* 1 − *𝑄* 7 ) related to Cognitive Dimensions of Notations (CDN) [ 7 ] were used. This approach helped\\nus evaluate L-PRISM from a usability point of view. Table 9 presents\\n\\n\\n\\n\\nFor *𝐺* 1, six questions related to efficiency and productivity in\\nthe process of developing multimedia SFCs were raised. Table 8\\npresents the G1 questions and the metrics related to them.\\n\\n**Table 8: Questions and metrics for goal** ***G1*** **.**\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n16\\n\\n\\n-----\\n\\nA Domain-Specific Language for Multimedia Service Function Chains based on Virtualization of Sensors WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\nthe proposed questions for *𝐺* 2 and the respective metrics. Those\\nquestions should be answered using the Likert scale, for *𝑄* 3 and *𝑄* 4\\n(1 - strongly disagree to 5 - strongly agree) and for *Q1, Q2, Q5, Q6*\\nand *𝑄* 7 (1 - very difficult to 5 - very easy) [13].\\n\\n**Table 9: Questions and metrics for goal** ***G2*** **.**\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|#Q|Description|Metrics|\\n|---|---|---|\\n|Q1|How easy is it to visualize or find the dif- ferent elements and attributes of L-PRISM when creating or changing a multimedia SFC?|M1 - Visibility|\\n|Q2|How easy is modifying a multimedia SFC with L-PRISM?|M2 - Viscosity|\\n|Q3|Is the L-PRISM language too verbose to specify a multimedia SFC?|M3 - Diffuse- ness|\\n|Q4|How well do the L-PRISM elements and attributes represent a multimedia SFC?|M4 - Closeness of Mapping|\\n|Q5|How easy is it to understand the struc- tures and data types of L-PRISM?|M5 - Role Ex- pressiveness|\\n|Q6|There are structures and data types in L- PRISM that can be closely related, and changes in one can affect the other. Are these dependencies easy to be seen?|M6 - Hidden dependencies|\\n|Q7|Does L-PRISM generally seem easy or dif- ficult to understand (for example, when changing different elements of a multime- dia SFC)?|M7 - Hard mental opera- tions|\\n\\n### **6.2 Procedure**\\n\\n\\n\\n*6.2.2* *𝑇* 2 *.* This task compares a raw multimedia stream with the\\nsame processed multimedia stream. To perform this task, it will\\nbe necessary to use *VMS_merge* that will receive the multimedia\\nstream processed by *VMS_gray* and subscribe to *VD_color_bar* to\\nreceive the original stream, *VMS _gray* will need to subscribe to\\n*VD_color_bar* in order to process this multimedia stream and send\\nit to *VMS_merge* . Finally, *VMS_merge* will publish the result to a\\nnetwork point ( *𝐼𝑃* and *𝑃𝑂𝑅𝑇* ).\\n\\n*6.2.3* *𝑇* 3 *.* This task consists of adding the *VD_video_ball* multimedia stream to *𝑇* 2. It is necessary to create a new *VMS_merge*\\nthat will receive the result of *𝑇* 2 through one of its ports, and the\\nother port will subscribe to *VD_video_ball* . Finally, it will publish\\nits result in a given network point ( *𝐼𝑃* and *𝑃𝑂𝑅𝑇* ). The visual result\\nof *𝑇* 3 is presented in the bottom-right part of Figure 4, while Figure\\n5 illustrates the corresponding multimedia SFC.\\n\\n*6.2.4* *𝑇* 4 *.* This task consists of replicating *𝑇* 3, changing the final\\ndestination port to where the final stream is sent. The idea is to be\\nable to visualize two identical multimedia SFCs, created in *𝑇* 3 and\\n*𝑇* 4, running at the same time.\\n\\n**Figure 5: Model of task 3.**\\n### **6.3 Participants**\\n\\nIn order to carry on user experiments, the consent of each participant was requested through a Free and Informed Consent Form\\n(FICF) and all collected data was anonymized. All responses were\\ncollected through a Google Forms questionnaire. No personal or\\nsensible data was collected from participants, except their gender\\nand age. Computer science students were invited to participate.\\nFifteen participants aged 20-39 years accepted our invitation. They\\nwere 14 men and 1 woman. Their academic degree was: seven\\nwere undergraduate, three were graduate, and five post-graduate\\nstudents. We asked their level of experience on XML, JSON and\\nYAML (from 1 (no experience) to 5 (a lot of experience)). The median answers were 4 for XML, 5 for JSON, and 3 for YAML. We\\nbuilt a testbed with edge nodes in our lab to run the experiments.\\nEight participants did it physically in the lab and seven participants\\ndid it remotely. All of them were observed all the time during the\\nexperiment, even the remote ones.\\n### **6.4 Analysis of Results**\\n\\nThe results of our evaluation are presented in Figure 6. Our evaluation focuses on determining whether L-PRISM is efficient, productive, and easy to use.\\nQuestions *𝑄* 2 to *𝑄* 5 of G1 are related to the productivity of\\nL-PRISM. Figure 6(a) shows that the results of these questions\\n\\n\\nFor the evaluation of L-PRISM, four tasks ( *𝑇* 1 − *𝑇* 4 ) were proposed,\\nwhich had to be carried out using our L-PRISM proposal and a\\ntraditional method using our previous implementation with manual\\nconfiguration through the web client interface. For the tasks to be\\ncompleted, additional material was prepared and is available at the\\nL-PRISM website [4] . The site contains information about L-PRISM\\nand different examples of how to implement multimedia SFCs with\\nL-PRISM and the traditional method used for these tests.\\n\\nThe types of components available for this experiment were\\ntwo virtual devices. The first device produces a video of a white\\nsphere bouncing on a black box, which we called *VD_video_ball*,\\nand the second device produces a video of colored bars, which we\\ncalled *VD_color_bar* . Three types of multimedia VNFs were also\\nmade available, namely (i) a multimedia VNF that transforms a\\ncolor video to grayscale called *VMS_gray*, (ii) a multimedia VNF\\nthat only transfers UDP-UDP data and does not apply any changes\\nto the original stream, called *VMS_udp*, and (iii) a multimedia VNF\\nthat merges video-type media streams, grabbing two multimedia\\nstreams and transforming them into one, called *VMS_merge* . The\\ntasks are detailed as follows.\\n\\n*6.2.1* *𝑇* 1 *.* This task is to develop a simple multimedia SFC, where\\n*VMS_gray* will need to subscribe to the *VD_color_bar* to process\\nthe multimedia stream and publish the result to a point within the\\nnetwork ( *𝐼𝑃* and *𝑃𝑂𝑅𝑇* ).\\n\\n4 https://fventuraq.github.io/lprism.html\\n\\n\\n17\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Ventura Quico et al.\\n\\n\\nare positive and the responses expected to validate L-PRISM were\\npositive. With this, we can conclude that the *𝐺* 1 goal was achieved.\\nQuestions *𝑄* 1 and *𝑄* 6 are related to efficiency. To answer *𝑄* 1,\\nFigure 6(b) shows the average time in minutes it took for participants to complete each task with both methods (L-PRISM and\\nTraditional), with a confidence interval of 90%. It can be seen that\\nTasks 1, 2, and 3 have similar times, although the learning curve to\\nlearn a new language is much greater than the use of an intuitive\\ninterface, L-PRISM is equivalent to the traditional method for new\\ndevelopers of multimedia SFCs.\\nIn the case of Task 4, L-PRISM has proven to be superior to the\\ntraditional method. This is because developers came to understand\\nL-PRISM better with the previous tasks, and most developers perceived that they could reuse the code from Task 3 for Task 4. This\\ntask also answers *𝑄* 6 of *𝐺* 1, which focuses on evaluating if L-PRISM\\nallows the reuse of an already created multimedia SFC. As it can be\\nseen in Figure 6(b), using L-PRISM, participants completed Task 4\\nin an average time 77.8% shorter than with the traditional method,\\ndue to L-PRISM code reuse.\\n\\nThe answers to the G2 questions related to the cognitive dimensions are presented in Figure 6(c). It should be noted that, except\\nfor *𝑄* 3 about language diffuseness (verbosity), where a negative\\nresponse was expected, all the others had positive feedback from\\nthe subjects. As it can be seen, the responses for Q3 are somewhat\\nambiguous. A subsequent consultation was made with the participants asking about this result and the conclusion was that the\\nquestion was not very clear for them, hence the disparity in the\\n\\nresponses.\\nAs mentioned above, the objectives of our evaluation were to\\ndemonstrate that L-PRISM is efficient, productive, and easy to use.\\nMoreover, based on the results obtained, we can conclude that goals\\n*𝐺* 1 and *𝐺* 2 were achieved.\\n### **7 CONCLUSION**\\n\\nThis work presented L-PRISM, a DSL to specify multimedia SFCs\\nbased on multimedia VNFs. We also extended the ALFA platform\\nwith the ability to execute SFCs described with L-PRISM. This extension is available in our GitHub [5] repository, where the installation\\nguide and all necessary resources for installing, executing, and\\ntesting our work are provided.\\nWe evaluated L-PRISM, and the results confirmed that our proposal language is efficient, productive, and easy to use. One observation we made was that the learning curve was shorter for the\\ntraditional model, as it focused on learning to use a specific application. In contrast, the learning curve for L-PRISM largely depended\\non the user’s prior experience with programming languages; those\\nwith more experience adapted to our language more quickly and,\\ntherefore, completed tasks faster.\\nFinally, the practical applicability of L-PRISM will mainly depend on multimedia VNFs, as these are the core components of\\nmultimedia SFCs and are responsible for processing multimedia\\nstreams. L-PRISM can be used to deploy applications such as virtual/augmented reality, live streaming, surveillance systems, and\\n\\nmore.\\n\\n5 Source code of alfa 2.0 https://github.com/fventuraq/alfa\\n\\n\\n(a) Participant Responses for Questions Q2-Q5 of G1.\\n\\n(b) Time needed per task in L-PRISM and Traditional methods.\\n\\n(c) Participant Responses for G2 Questions.\\n\\n**Figure 6: Evaluation results.**\\n\\nAs future work, we intend to develop a graphical tool to create/edit SFCs designed with L-PRISM, offering an intuitive user\\ninterface to visualize and modify their topology. We will also propose the integration of resource allocation and scaling algorithms\\ninto ALFA 2.0.\\n### **ACKNOWLEDGMENT**\\n\\nThis work was supported by CAPES, CAPES-Print, CNPq, FAPERJ\\nand INCT-ICoNIoT.\\n### **REFERENCES**\\n\\n[1] ETSI GS NFV-IFA 011. 2023. Network Functions Virtualisation (NFV) Release\\n4; Management and Orchestration; VNF Descriptor and Packaging Specification. https://docbox.etsi.org/ISG/NFV/open/Publications_pdf/Specs-Reports/\\nNFV-IFA%20011v4.5.1%20-%20GS%20-%20VNF%20Packaging%20Spec.pdf\\n\\n\\n18\\n\\n\\n-----\\n\\nA Domain-Specific Language for Multimedia Service Function Chains based on Virtualization of Sensors WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\n\\n[2] ETSI GS NFV-IFA 014. 2021. Network Functions Virtualisation (NFV) Release 4;\\nManagement and Orchestration; Network Service Templates Specification. https:\\n//docbox.etsi.org/ISG/NFV/open/Publications_pdf/Specs-Reports/NFV-IFA%\\n20014v4.2.1%20-%20GS%20-%20Network%20Service%20Templates%20Spec.pdf\\n\\n[3] Anselmo Luiz Éden Battisti, Débora Christina Muchaluat-Saade, and Flávia C.\\nDelicato. 2020. V-PRISM: An Edge-Based IoT Architecture to Virtualize Multimedia Sensors. In *2020 IEEE 6th World Forum on Internet of Things (WF-IoT)* . 1–6.\\nhttps://doi.org/10.1109/WF-IoT48130.2020.9221199\\n\\n[4] Anselmo Luiz Éden Battisti, Debora Christina Muchaluat-Saade, and Flavia C\\nDélicato. 2021. Enabling Internet of Media Things with edge-based virtual\\nmultimedia sensors. *IEEE Access* 9 (2021), 59255–59269.\\n\\n[5] Deval Bhamare, Raj Jain, Mohammed Samaka, and Aiman Erbad. 2016. A survey\\non service function chaining. *Journal of Network and Computer Applications* 75\\n(Nov. 2016), 138–155. https://doi.org/10.1016/j.jnca.2016.09.001\\n\\n[6] Martin Björklund. 2010. YANG - A Data Modeling Language for the Network\\nConfiguration Protocol (NETCONF). RFC 6020. https://doi.org/10.17487/RFC6020\\n\\n[7] Alan F Blackwell and Thomas RG Green. 2000. A Cognitive Dimensions questionnaire optimised for users. In *PPIG*, Vol. 13. Citeseer.\\n\\n[8] José Castillo-Lema, Augusto Venâncio Neto, Flávio de Oliveira, and Sergio\\nTakeo Kofuji. 2019. Mininet-NFV: Evolving Mininet with OASIS TOSCA NVF\\nprofiles Towards Reproducible NFV Prototyping. In *2019 IEEE Conference on*\\n*Network Softwarization (NetSoft)* . 506–512. https://doi.org/10.1109/NETSOFT.\\n2019.8806686\\n\\n[9] Mario Di Mauro, Giovanni Galatro, Maurizio Longo, Fabio Postiglione, and Marco\\nTambasco. 2021. Comparative Performability Assessment of SFCs: The Case of\\nContainerized IP Multimedia Subsystem. *IEEE Transactions on Network and Service*\\n*Management* 18, 1 (2021), 258–272. https://doi.org/10.1109/TNSM.2020.3044232\\n\\n[10] GS NFV-SOL 001 ETSI. 2022. Network Functions Virtualisation (NFV) Release\\n4; Protocols and Data Models; NFV descriptors based on TOSCA specification. https://www.etsi.org/deliver/etsi_gs/NFV-SOL/001_099/001/04.03.01_\\n60/gs_NFV-SOL001v040301p.pdf\\n\\n[11] A.H. Ghorab, A. Kusedghi, M. A. Nourian, and A. Akbari. 2020. Joint VNF Load\\nBalancing and Service Auto-Scaling in NFV with Multimedia Case Study. In *2020*\\n*25th International Computer Conference, Computer Society of Iran (CSICC)* . 1–7.\\nhttps://doi.org/10.1109/CSICC49403.2020.9050122\\n\\n[12] Heiko Koziolek. 2008. Goal, question, metric. In *Dependability metrics* . Springer,\\n39–42.\\n\\n[13] Rensis Likert. 1932. A technique for the measurement of attitudes. *Archives of*\\n*psychology* (1932).\\n\\n\\n\\n[14] Yuyi Mao, Changsheng You, Jun Zhang, Kaibin Huang, and Khaled B. Letaief.\\n2017. A Survey on Mobile Edge Computing: The Communication Perspective.\\n*IEEE Comm. Surveys and Tutorials* 19, 4 (2017), 2322–2358. https://doi.org/10.\\n1109/COMST.2017.2745201 arXiv:1701.01090\\n\\n[15] Marjan Mernik, Jan Heering, and Anthony M Sloane. 2005. When and how to\\ndevelop domain-specific languages. *ACM computing surveys (CSUR)* 37, 4 (2005),\\n316–344.\\n\\n[16] Rashid Mijumbi, Joan Serrat, Juan Luis Gorricho, Niels Bouten, Filip De Turck,\\nand Raouf Boutaba. 2016. Network function virtualization: State-of-the-art and\\nresearch challenges. *IEEE Communications Surveys and Tutorials* 18, 1 (2016),\\n236–262. https://doi.org/10.1109/COMST.2015.2477041 arXiv:1509.07675\\n\\n[17] Ali Nauman, Yazdan Ahmad Qadri, Muhammad Amjad, Yousaf Bin Zikria,\\nMuhammad Khalil Afzal, and Sung Won Kim. 2020. Multimedia Internet of\\nThings: A comprehensive survey. *Ieee Access* 8 (2020), 8202–8250.\\n\\n[18] Eman Negm, Soha Makady, and Akram Salah. 2019. Survey on domain specific\\nlanguages implementation aspects. *International Journal of Advanced Computer*\\n*Science and Applications* 10, 11 (2019).\\n\\n[19] Guto Leoni Santos, Diego de Freitas Bezerra, Élisson da Silva Rocha, Leylane\\nFerreira, André Luis Cavalcanti Moreira, Glauco Estácio Gonçalves, Maria Valéria\\nMarquezini, Ákos Recse, Amardeep Mehta, Judith Kelner, Djamel Sadok, and\\nPatricia Takako Endo. 2022. Service Function Chain Placement in Distributed\\nScenarios: A Systematic Review. *Journal of Network and Systems Management*\\n30, 1 (2022), 1–39. https://doi.org/10.1007/s10922-021-09626-4\\n\\n[20] Jürgen Schönwälder, Martin Björklund, and Phil Shafer. 2010. Network configuration management using NETCONF and YANG. *IEEE communications magazine*\\n48, 9 (2010), 166–173.\\n\\n[21] Priyanka Surendran et al . 2012. Technology acceptance model: A survey of\\nliterature. *International Journal of Business and Social Research* 2, 4 (2012), 175–\\n178.\\n\\n[22] OASIS TOSCA. 2017. TOSCA Simple Profile for Network Functions Virtualization\\n(NFV) Version 1.0, Committee Specification Draft 04. https://docs.oasis-open.\\norg/tosca/tosca-nfv/v1.0/csd04/tosca-nfv-v1.0-csd04.html\\n\\n[23] YAML. [n. d.]. YAML Ain’t Markup Language (YAML™) version 1.2. https:\\n//yaml.org/spec/1.2/spec.html. Accessed: Jun 29, 2024.\\n\\n[24] Bo Yi, Xingwei Wang, Keqin Li, Sajal k. Das, and Min Huang. 2018. A comprehensive survey of Network Function Virtualization. *Computer Networks* 133 (2018),\\n212–262. https://doi.org/10.1016/j.comnet.2018.01.021\\n\\n\\n19\\n\\n\\n-----\\n\\n',\n",
       "  'artigo_tokenizado': ['#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'A',\n",
       "   'Domain',\n",
       "   '-',\n",
       "   'Specific',\n",
       "   'Language',\n",
       "   'for',\n",
       "   'Multimedia',\n",
       "   'Service',\n",
       "   'Function',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Chains',\n",
       "   'based',\n",
       "   'on',\n",
       "   'Virtualization',\n",
       "   'of',\n",
       "   'Sensors',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Franklin',\n",
       "   'Jordan',\n",
       "   'Ventura',\n",
       "   'Quico',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'fventuraq@midiacom.uff.br',\n",
       "   'Fluminense',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'Niterói',\n",
       "   ',',\n",
       "   'RJ',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Débora',\n",
       "   'Muchaluat',\n",
       "   '-',\n",
       "   'Saade',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'debora@midiacom.uff.br',\n",
       "   'Fluminense',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'Niterói',\n",
       "   ',',\n",
       "   'RJ',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'ABSTRACT',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Virtualization',\n",
       "   'is',\n",
       "   'a',\n",
       "   'widely',\n",
       "   'used',\n",
       "   'technology',\n",
       "   'that',\n",
       "   'can',\n",
       "   'abstract',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'complexity',\n",
       "   'of',\n",
       "   'heterogeneous',\n",
       "   'environments',\n",
       "   ',',\n",
       "   'such',\n",
       "   'as',\n",
       "   'the',\n",
       "   'Internet',\n",
       "   '\\n',\n",
       "   'of',\n",
       "   'Things',\n",
       "   '(',\n",
       "   'IoT',\n",
       "   ')',\n",
       "   'and',\n",
       "   'multimedia',\n",
       "   'systems',\n",
       "   '.',\n",
       "   'Multimedia',\n",
       "   'sensors',\n",
       "   'are',\n",
       "   '\\n',\n",
       "   'an',\n",
       "   'important',\n",
       "   'data',\n",
       "   'source',\n",
       "   'in',\n",
       "   'the',\n",
       "   'Internet',\n",
       "   'of',\n",
       "   'Things',\n",
       "   '(',\n",
       "   'IoT',\n",
       "   ')',\n",
       "   ',',\n",
       "   'which',\n",
       "   '\\n',\n",
       "   'brings',\n",
       "   'the',\n",
       "   'Internet',\n",
       "   'of',\n",
       "   'Media',\n",
       "   'Things',\n",
       "   '(',\n",
       "   'IoMT',\n",
       "   ')',\n",
       "   'paradigm',\n",
       "   '.',\n",
       "   'Based',\n",
       "   'on',\n",
       "   'virtualization',\n",
       "   'and',\n",
       "   'IoMT',\n",
       "   ',',\n",
       "   'the',\n",
       "   'concept',\n",
       "   'of',\n",
       "   'a',\n",
       "   'multimedia',\n",
       "   'Virtual',\n",
       "   'Network',\n",
       "   '\\n',\n",
       "   'Function',\n",
       "   '(',\n",
       "   'multimedia',\n",
       "   'VNF',\n",
       "   ')',\n",
       "   'has',\n",
       "   'been',\n",
       "   'adopted',\n",
       "   'to',\n",
       "   'denote',\n",
       "   'the',\n",
       "   'virtualized',\n",
       "   'representation',\n",
       "   'of',\n",
       "   'devices',\n",
       "   'and',\n",
       "   'also',\n",
       "   'software',\n",
       "   'components',\n",
       "   'that',\n",
       "   '\\n',\n",
       "   'process',\n",
       "   'multimedia',\n",
       "   'streams',\n",
       "   '.',\n",
       "   'In',\n",
       "   'many',\n",
       "   'scenarios',\n",
       "   ',',\n",
       "   'multiple',\n",
       "   'processes',\n",
       "   '\\n',\n",
       "   'must',\n",
       "   'be',\n",
       "   'applied',\n",
       "   'to',\n",
       "   'multimedia',\n",
       "   'streams',\n",
       "   'in',\n",
       "   'a',\n",
       "   'predefined',\n",
       "   'sequence',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'thus',\n",
       "   'creating',\n",
       "   'the',\n",
       "   'concept',\n",
       "   'of',\n",
       "   'multimedia',\n",
       "   'Service',\n",
       "   'Function',\n",
       "   'Chain',\n",
       "   '\\n',\n",
       "   '(',\n",
       "   'multimedia',\n",
       "   'SFC',\n",
       "   ')',\n",
       "   '.',\n",
       "   'Few',\n",
       "   'efforts',\n",
       "   'have',\n",
       "   'been',\n",
       "   'made',\n",
       "   'in',\n",
       "   'the',\n",
       "   'literature',\n",
       "   'to',\n",
       "   '\\n',\n",
       "   'create',\n",
       "   'a',\n",
       "   'description',\n",
       "   'language',\n",
       "   'to',\n",
       "   'support',\n",
       "   'the',\n",
       "   'definition',\n",
       "   'of',\n",
       "   'multimedia',\n",
       "   'SFCs',\n",
       "   '.',\n",
       "   'In',\n",
       "   'order',\n",
       "   'to',\n",
       "   'fill',\n",
       "   'this',\n",
       "   'gap',\n",
       "   ',',\n",
       "   'we',\n",
       "   'propose',\n",
       "   'a',\n",
       "   'Domain',\n",
       "   'Specific',\n",
       "   '\\n',\n",
       "   'Language',\n",
       "   '(',\n",
       "   'DSL',\n",
       "   ')',\n",
       "   'called',\n",
       "   'L',\n",
       "   '-',\n",
       "   'PRISM',\n",
       "   '.',\n",
       "   'This',\n",
       "   'DSL',\n",
       "   'can',\n",
       "   'be',\n",
       "   'used',\n",
       "   'as',\n",
       "   'a',\n",
       "   'conceptual',\n",
       "   'base',\n",
       "   'for',\n",
       "   'developers',\n",
       "   'to',\n",
       "   'implement',\n",
       "   'and',\n",
       "   'virtualize',\n",
       "   'multimedia',\n",
       "   '\\n',\n",
       "   'applications',\n",
       "   'using',\n",
       "   'multimedia',\n",
       "   'VNFs',\n",
       "   '.',\n",
       "   'We',\n",
       "   'also',\n",
       "   'present',\n",
       "   'a',\n",
       "   'Proof',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'Concept',\n",
       "   '(',\n",
       "   'PoC',\n",
       "   ')',\n",
       "   'that',\n",
       "   'uses',\n",
       "   'L',\n",
       "   '-',\n",
       "   'PRISM',\n",
       "   'to',\n",
       "   'run',\n",
       "   'multimedia',\n",
       "   'SFCs',\n",
       "   '.',\n",
       "   'Our',\n",
       "   '\\n',\n",
       "   'DSL',\n",
       "   'and',\n",
       "   'PoC',\n",
       "   'were',\n",
       "   'evaluated',\n",
       "   'by',\n",
       "   'software',\n",
       "   'developers',\n",
       "   ',',\n",
       "   'and',\n",
       "   'the',\n",
       "   'results',\n",
       "   'show',\n",
       "   'that',\n",
       "   'adopting',\n",
       "   'L',\n",
       "   '-',\n",
       "   'PRISM',\n",
       "   'facilitates',\n",
       "   'the',\n",
       "   'definition',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'deployment',\n",
       "   'of',\n",
       "   'multimedia',\n",
       "   'SFCs',\n",
       "   'based',\n",
       "   'on',\n",
       "   'multimedia',\n",
       "   'VNFs',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'KEYWORDS',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'IoMT',\n",
       "   ',',\n",
       "   'IoT',\n",
       "   ',',\n",
       "   'VNF',\n",
       "   ',',\n",
       "   'SFC',\n",
       "   ',',\n",
       "   'DSL',\n",
       "   ',',\n",
       "   'L',\n",
       "   '-',\n",
       "   'PRISM',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   '1',\n",
       "   'INTRODUCTION',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Virtualization',\n",
       "   'is',\n",
       "   'a',\n",
       "   'concept',\n",
       "   'that',\n",
       "   'has',\n",
       "   'paved',\n",
       "   'the',\n",
       "   'way',\n",
       "   'for',\n",
       "   'numerous',\n",
       "   '\\n',\n",
       "   'research',\n",
       "   'studies',\n",
       "   'in',\n",
       "   'Cloud',\n",
       "   'Computing',\n",
       "   ',',\n",
       "   '5',\n",
       "   'G',\n",
       "   ',',\n",
       "   'and',\n",
       "   'IoT',\n",
       "   '[',\n",
       "   '14',\n",
       "   ',',\n",
       "   '24',\n",
       "   ']',\n",
       "   '.',\n",
       "   'This',\n",
       "   '\\n',\n",
       "   'technology',\n",
       "   'not',\n",
       "   'only',\n",
       "   'reduces',\n",
       "   'the',\n",
       "   'costs',\n",
       "   'of',\n",
       "   'implementing',\n",
       "   'and',\n",
       "   'managing',\n",
       "   'infrastructures',\n",
       "   ',',\n",
       "   'but',\n",
       "   'also',\n",
       "   'serves',\n",
       "   'as',\n",
       "   'a',\n",
       "   'fundamental',\n",
       "   'pillar',\n",
       "   'in',\n",
       "   '\\n',\n",
       "   'paradigms',\n",
       "   'such',\n",
       "   'as',\n",
       "   'Network',\n",
       "   'Function',\n",
       "   'Virtualization',\n",
       "   '(',\n",
       "   'NFV',\n",
       "   ')',\n",
       "   '[',\n",
       "   '16',\n",
       "   ',',\n",
       "   '19',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'The',\n",
       "   'adoption',\n",
       "   'of',\n",
       "   'NFV',\n",
       "   'and',\n",
       "   'its',\n",
       "   'Virtual',\n",
       "   'Network',\n",
       "   'Function',\n",
       "   '(',\n",
       "   'VNF',\n",
       "   ')',\n",
       "   'components',\n",
       "   'has',\n",
       "   'proved',\n",
       "   'to',\n",
       "   'be',\n",
       "   'effective',\n",
       "   'in',\n",
       "   'reducing',\n",
       "   'the',\n",
       "   'consumption',\n",
       "   '\\n',\n",
       "   'of',\n",
       "   'computational',\n",
       "   'and',\n",
       "   'network',\n",
       "   'resources',\n",
       "   '.',\n",
       "   'Furthermore',\n",
       "   ',',\n",
       "   'virtualization',\n",
       "   'increases',\n",
       "   'the',\n",
       "   'speed',\n",
       "   'and',\n",
       "   'efficiency',\n",
       "   'of',\n",
       "   'resource',\n",
       "   'provisioning',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   ':',\n",
       "   'Proceedings',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Web',\n",
       "   '(',\n",
       "   'WebMe',\n",
       "   '\\n',\n",
       "   'dia’2024',\n",
       "   ')',\n",
       "   '.',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '.',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   ':',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   ',',\n",
       "   '2024',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'SBC',\n",
       "   '–',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '-',\n",
       "   '2753',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Anselmo',\n",
       "   'L.',\n",
       "   'E.',\n",
       "   'Battisti',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'anselmo@midiacom.uff.br',\n",
       "   'Fluminense',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'Niterói',\n",
       "   ',',\n",
       "   'RJ',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Flavia',\n",
       "   'C.',\n",
       "   'Delicato',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'fdelicato@ic.uff.br',\n",
       "   'Fluminense',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'Niterói',\n",
       "   ',',\n",
       "   'RJ',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '\\n\\n',\n",
       "   'in',\n",
       "   'cloud',\n",
       "   'and',\n",
       "   'edge',\n",
       "   'environments',\n",
       "   ',',\n",
       "   'facilitating',\n",
       "   'the',\n",
       "   'management',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'orchestration',\n",
       "   'of',\n",
       "   'these',\n",
       "   'resources',\n",
       "   '[',\n",
       "   '24',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Many',\n",
       "   'investigations',\n",
       "   'combine',\n",
       "   'NFV',\n",
       "   'technology',\n",
       "   'with',\n",
       "   'multimedia',\n",
       "   '\\n',\n",
       "   'stream',\n",
       "   'processing',\n",
       "   '.',\n",
       "   'In',\n",
       "   '[',\n",
       "   '4',\n",
       "   ']',\n",
       "   ',',\n",
       "   'the',\n",
       "   'authors',\n",
       "   'explore',\n",
       "   'the',\n",
       "   'concept',\n",
       "   'of',\n",
       "   'virtual',\n",
       "   '\\n',\n",
       "   'devices',\n",
       "   'that',\n",
       "   'abstract',\n",
       "   'the',\n",
       "   'heterogeneity',\n",
       "   'of',\n",
       "   'multimedia',\n",
       "   'sensors',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'introduce',\n",
       "   'virtual',\n",
       "   'multimedia',\n",
       "   'sensors',\n",
       "   '(',\n",
       "   'VMS',\n",
       "   ')',\n",
       "   ',',\n",
       "   'which',\n",
       "   'we',\n",
       "   'will',\n",
       "   'refer',\n",
       "   'to',\n",
       "   '\\n',\n",
       "   'as',\n",
       "   'multimedia',\n",
       "   'VNFs',\n",
       "   'in',\n",
       "   'our',\n",
       "   'work',\n",
       "   '.',\n",
       "   'These',\n",
       "   'VMSs',\n",
       "   'enable',\n",
       "   'multimedia',\n",
       "   '\\n\\n',\n",
       "   'stream',\n",
       "   'processing',\n",
       "   'using',\n",
       "   'lightweight',\n",
       "   'virtualization',\n",
       "   '.',\n",
       "   'Furthermore',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'authors',\n",
       "   'demonstrated',\n",
       "   'the',\n",
       "   'feasibility',\n",
       "   'of',\n",
       "   'using',\n",
       "   'chained',\n",
       "   'VMSs',\n",
       "   'to',\n",
       "   '\\n',\n",
       "   'create',\n",
       "   'complex',\n",
       "   'multimedia',\n",
       "   'stream',\n",
       "   'processing',\n",
       "   'pipelines',\n",
       "   '.',\n",
       "   'However',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'this',\n",
       "   'characteristic',\n",
       "   'was',\n",
       "   'not',\n",
       "   'fully',\n",
       "   'explored',\n",
       "   'in',\n",
       "   'that',\n",
       "   'work',\n",
       "   '.',\n",
       "   'Meanwhile',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'in',\n",
       "   '[',\n",
       "   '11',\n",
       "   ']',\n",
       "   ',',\n",
       "   'the',\n",
       "   'authors',\n",
       "   'investigated',\n",
       "   'the',\n",
       "   'joint',\n",
       "   'load',\n",
       "   'balancing',\n",
       "   'and',\n",
       "   'autoscaling',\n",
       "   'of',\n",
       "   'multimedia',\n",
       "   'VNFs',\n",
       "   'in',\n",
       "   'edge',\n",
       "   'or',\n",
       "   'cloud',\n",
       "   'environments',\n",
       "   '.',\n",
       "   'In',\n",
       "   '\\n',\n",
       "   'this',\n",
       "   'context',\n",
       "   ',',\n",
       "   'virtualization',\n",
       "   'concepts',\n",
       "   'like',\n",
       "   'NFV',\n",
       "   'have',\n",
       "   'been',\n",
       "   'actively',\n",
       "   '\\n',\n",
       "   'integrated',\n",
       "   'into',\n",
       "   'different',\n",
       "   'studies',\n",
       "   'to',\n",
       "   'process',\n",
       "   'multimedia',\n",
       "   'streams',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'The',\n",
       "   'Service',\n",
       "   'Function',\n",
       "   'Chain',\n",
       "   '(',\n",
       "   'SFC',\n",
       "   ')',\n",
       "   'is',\n",
       "   'a',\n",
       "   'sequence',\n",
       "   'of',\n",
       "   'sorted',\n",
       "   'VNFs',\n",
       "   '\\n',\n",
       "   'associated',\n",
       "   'with',\n",
       "   'a',\n",
       "   'Service',\n",
       "   'Level',\n",
       "   'Agreement',\n",
       "   '(',\n",
       "   'SLA',\n",
       "   ')',\n",
       "   '[',\n",
       "   '5',\n",
       "   ']',\n",
       "   '.',\n",
       "   'By',\n",
       "   'chaining',\n",
       "   '\\n',\n",
       "   'multimedia',\n",
       "   'functions',\n",
       "   ',',\n",
       "   'it',\n",
       "   'is',\n",
       "   'possible',\n",
       "   'to',\n",
       "   'create',\n",
       "   'complex',\n",
       "   'multimedia',\n",
       "   '\\n',\n",
       "   'stream',\n",
       "   'processing',\n",
       "   'pipelines',\n",
       "   '.',\n",
       "   'When',\n",
       "   'the',\n",
       "   'VNFs',\n",
       "   'in',\n",
       "   'an',\n",
       "   'SFC',\n",
       "   'are',\n",
       "   'tailored',\n",
       "   '\\n',\n",
       "   'for',\n",
       "   'multimedia',\n",
       "   'processing',\n",
       "   ',',\n",
       "   'they',\n",
       "   'are',\n",
       "   'referred',\n",
       "   'to',\n",
       "   'as',\n",
       "   'multimedia',\n",
       "   'SFCs',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Despite',\n",
       "   'the',\n",
       "   'potential',\n",
       "   'of',\n",
       "   'multimedia',\n",
       "   'SFCs',\n",
       "   'in',\n",
       "   'various',\n",
       "   'fields',\n",
       "   '[',\n",
       "   '9',\n",
       "   ']',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'we',\n",
       "   'face',\n",
       "   'the',\n",
       "   'challenge',\n",
       "   'of',\n",
       "   'implementing',\n",
       "   'and',\n",
       "   'managing',\n",
       "   'them',\n",
       "   'easily',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'securely',\n",
       "   ',',\n",
       "   'and',\n",
       "   'efficiently',\n",
       "   '.',\n",
       "   'The',\n",
       "   'components',\n",
       "   'used',\n",
       "   'to',\n",
       "   'create',\n",
       "   'multimedia',\n",
       "   '\\n',\n",
       "   'SFCs',\n",
       "   'are',\n",
       "   'typically',\n",
       "   'developed',\n",
       "   'by',\n",
       "   'different',\n",
       "   'entities',\n",
       "   'using',\n",
       "   'multiple',\n",
       "   '\\n',\n",
       "   'technologies',\n",
       "   '[',\n",
       "   '14',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Thus',\n",
       "   ',',\n",
       "   'to',\n",
       "   'create',\n",
       "   'a',\n",
       "   'multimedia',\n",
       "   'SFC',\n",
       "   ',',\n",
       "   'users',\n",
       "   'must',\n",
       "   'have',\n",
       "   '\\n',\n",
       "   'a',\n",
       "   'moderate',\n",
       "   'level',\n",
       "   'of',\n",
       "   'knowledge',\n",
       "   'of',\n",
       "   'these',\n",
       "   'technologies',\n",
       "   '.',\n",
       "   'Moreover',\n",
       "   ',',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'interoperability',\n",
       "   'of',\n",
       "   'these',\n",
       "   'components',\n",
       "   'sometimes',\n",
       "   'is',\n",
       "   'often',\n",
       "   'limited',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'To',\n",
       "   'address',\n",
       "   'these',\n",
       "   'challenges',\n",
       "   ',',\n",
       "   'it',\n",
       "   'is',\n",
       "   'necessary',\n",
       "   'to',\n",
       "   'provide',\n",
       "   'a',\n",
       "   'layer',\n",
       "   '\\n',\n",
       "   'that',\n",
       "   'encapsulates',\n",
       "   'the',\n",
       "   'technical',\n",
       "   'details',\n",
       "   'of',\n",
       "   'multimedia',\n",
       "   'SFCs',\n",
       "   '.',\n",
       "   'There',\n",
       "   '\\n',\n",
       "   'are',\n",
       "   'different',\n",
       "   'alternatives',\n",
       "   'for',\n",
       "   'this',\n",
       "   ',',\n",
       "   'among',\n",
       "   'which',\n",
       "   'Domain',\n",
       "   '-',\n",
       "   'Specific',\n",
       "   '\\n',\n",
       "   'Languages',\n",
       "   '(',\n",
       "   'DSLs',\n",
       "   ')',\n",
       "   'stand',\n",
       "   'out',\n",
       "   '[',\n",
       "   '18',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Unlike',\n",
       "   'other',\n",
       "   'approaches',\n",
       "   ',',\n",
       "   'such',\n",
       "   'as',\n",
       "   '\\n',\n",
       "   'metamodels',\n",
       "   'or',\n",
       "   'conventional',\n",
       "   'programming',\n",
       "   'languages',\n",
       "   ',',\n",
       "   'DSLs',\n",
       "   'offer',\n",
       "   '\\n',\n",
       "   'greater',\n",
       "   'flexibility',\n",
       "   'and',\n",
       "   'simplicity',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'DSLs',\n",
       "   'are',\n",
       "   'language',\n",
       "   'specifications',\n",
       "   'designed',\n",
       "   'to',\n",
       "   'describe',\n",
       "   'the',\n",
       "   'characteristics',\n",
       "   ',',\n",
       "   'syntax',\n",
       "   ',',\n",
       "   'and',\n",
       "   'behavior',\n",
       "   'of',\n",
       "   'systems',\n",
       "   'in',\n",
       "   'specific',\n",
       "   'fields',\n",
       "   '[',\n",
       "   '15',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'These',\n",
       "   'languages',\n",
       "   'enable',\n",
       "   'intuitive',\n",
       "   'integration',\n",
       "   'and',\n",
       "   'management',\n",
       "   'by',\n",
       "   '\\n',\n",
       "   'providing',\n",
       "   'an',\n",
       "   'abstraction',\n",
       "   'layer',\n",
       "   'encapsulating',\n",
       "   'technical',\n",
       "   'details',\n",
       "   '.',\n",
       "   'This',\n",
       "   '\\n',\n",
       "   'allows',\n",
       "   'users',\n",
       "   'to',\n",
       "   ...],\n",
       "  'pos_tagger': '',\n",
       "  'lema': ['a',\n",
       "   'domain',\n",
       "   'specific',\n",
       "   'Language',\n",
       "   'for',\n",
       "   'Multimedia',\n",
       "   'Service',\n",
       "   'Function',\n",
       "   'chain',\n",
       "   'base',\n",
       "   'on',\n",
       "   'Virtualization',\n",
       "   'of',\n",
       "   'sensor',\n",
       "   'Franklin',\n",
       "   'Jordan',\n",
       "   'Ventura',\n",
       "   'Quico',\n",
       "   'fventuraq@midiacom.uff.br',\n",
       "   'Fluminense',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'Niterói',\n",
       "   'RJ',\n",
       "   'Brazil',\n",
       "   'Débora',\n",
       "   'Muchaluat',\n",
       "   'Saade',\n",
       "   'debora@midiacom.uff.br',\n",
       "   'Fluminense',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'Niterói',\n",
       "   'RJ',\n",
       "   'Brazil',\n",
       "   'ABSTRACT',\n",
       "   'Virtualization',\n",
       "   'be',\n",
       "   'a',\n",
       "   'widely',\n",
       "   'use',\n",
       "   'technology',\n",
       "   'that',\n",
       "   'can',\n",
       "   'abstract',\n",
       "   'the',\n",
       "   'complexity',\n",
       "   'of',\n",
       "   'heterogeneous',\n",
       "   'environment',\n",
       "   'such',\n",
       "   'as',\n",
       "   'the',\n",
       "   'internet',\n",
       "   'of',\n",
       "   'thing',\n",
       "   'IoT',\n",
       "   'and',\n",
       "   'multimedia',\n",
       "   'system',\n",
       "   'Multimedia',\n",
       "   'sensor',\n",
       "   'be',\n",
       "   'an',\n",
       "   'important',\n",
       "   'data',\n",
       "   'source',\n",
       "   'in',\n",
       "   'the',\n",
       "   'internet',\n",
       "   'of',\n",
       "   'thing',\n",
       "   'IoT',\n",
       "   'which',\n",
       "   'bring',\n",
       "   'the',\n",
       "   'internet',\n",
       "   'of',\n",
       "   'Media',\n",
       "   'Things',\n",
       "   'IoMT',\n",
       "   'paradigm',\n",
       "   'base',\n",
       "   'on',\n",
       "   'virtualization',\n",
       "   'and',\n",
       "   'IoMT',\n",
       "   'the',\n",
       "   'concept',\n",
       "   'of',\n",
       "   'a',\n",
       "   'multimedia',\n",
       "   'Virtual',\n",
       "   'Network',\n",
       "   'Function',\n",
       "   'multimedia',\n",
       "   'VNF',\n",
       "   'have',\n",
       "   'be',\n",
       "   'adopt',\n",
       "   'to',\n",
       "   'denote',\n",
       "   'the',\n",
       "   'virtualize',\n",
       "   'representation',\n",
       "   'of',\n",
       "   'device',\n",
       "   'and',\n",
       "   'also',\n",
       "   'software',\n",
       "   'component',\n",
       "   'that',\n",
       "   'process',\n",
       "   'multimedia',\n",
       "   'stream',\n",
       "   'in',\n",
       "   'many',\n",
       "   'scenario',\n",
       "   'multiple',\n",
       "   'process',\n",
       "   'must',\n",
       "   'be',\n",
       "   'apply',\n",
       "   'to',\n",
       "   'multimedia',\n",
       "   'stream',\n",
       "   'in',\n",
       "   'a',\n",
       "   'predefine',\n",
       "   'sequence',\n",
       "   'thus',\n",
       "   'create',\n",
       "   'the',\n",
       "   'concept',\n",
       "   'of',\n",
       "   'multimedia',\n",
       "   'Service',\n",
       "   'Function',\n",
       "   'Chain',\n",
       "   'multimedia',\n",
       "   'sfc',\n",
       "   'few',\n",
       "   'effort',\n",
       "   'have',\n",
       "   'be',\n",
       "   'make',\n",
       "   'in',\n",
       "   'the',\n",
       "   'literature',\n",
       "   'to',\n",
       "   'create',\n",
       "   'a',\n",
       "   'description',\n",
       "   'language',\n",
       "   'to',\n",
       "   'support',\n",
       "   'the',\n",
       "   'definition',\n",
       "   'of',\n",
       "   'multimedia',\n",
       "   'sfc',\n",
       "   'in',\n",
       "   'order',\n",
       "   'to',\n",
       "   'fill',\n",
       "   'this',\n",
       "   'gap',\n",
       "   'we',\n",
       "   'propose',\n",
       "   'a',\n",
       "   'Domain',\n",
       "   'Specific',\n",
       "   'Language',\n",
       "   'DSL',\n",
       "   'call',\n",
       "   'L',\n",
       "   'PRISM',\n",
       "   'this',\n",
       "   'dsl',\n",
       "   'can',\n",
       "   'be',\n",
       "   'use',\n",
       "   'as',\n",
       "   'a',\n",
       "   'conceptual',\n",
       "   'base',\n",
       "   'for',\n",
       "   'developer',\n",
       "   'to',\n",
       "   'implement',\n",
       "   'and',\n",
       "   'virtualize',\n",
       "   'multimedia',\n",
       "   'application',\n",
       "   'use',\n",
       "   'multimedia',\n",
       "   'vnf',\n",
       "   'we',\n",
       "   'also',\n",
       "   'present',\n",
       "   'a',\n",
       "   'Proof',\n",
       "   'of',\n",
       "   'Concept',\n",
       "   'PoC',\n",
       "   'that',\n",
       "   'use',\n",
       "   'l',\n",
       "   'prism',\n",
       "   'to',\n",
       "   'run',\n",
       "   'multimedia',\n",
       "   'sfc',\n",
       "   'our',\n",
       "   'DSL',\n",
       "   'and',\n",
       "   'PoC',\n",
       "   'be',\n",
       "   'evaluate',\n",
       "   'by',\n",
       "   'software',\n",
       "   'developer',\n",
       "   'and',\n",
       "   'the',\n",
       "   'result',\n",
       "   'show',\n",
       "   'that',\n",
       "   'adopt',\n",
       "   'l',\n",
       "   'prism',\n",
       "   'facilitate',\n",
       "   'the',\n",
       "   'definition',\n",
       "   'and',\n",
       "   'deployment',\n",
       "   'of',\n",
       "   'multimedia',\n",
       "   'sfc',\n",
       "   'base',\n",
       "   'on',\n",
       "   'multimedia',\n",
       "   'vnf',\n",
       "   'keyword',\n",
       "   'IoMT',\n",
       "   'IoT',\n",
       "   'VNF',\n",
       "   'SFC',\n",
       "   'DSL',\n",
       "   'L',\n",
       "   'PRISM',\n",
       "   '1',\n",
       "   'introduction',\n",
       "   'Virtualization',\n",
       "   'be',\n",
       "   'a',\n",
       "   'concept',\n",
       "   'that',\n",
       "   'have',\n",
       "   'pave',\n",
       "   'the',\n",
       "   'way',\n",
       "   'for',\n",
       "   'numerous',\n",
       "   'research',\n",
       "   'study',\n",
       "   'in',\n",
       "   'Cloud',\n",
       "   'Computing',\n",
       "   '5',\n",
       "   'g',\n",
       "   'and',\n",
       "   'IoT',\n",
       "   '14',\n",
       "   '24',\n",
       "   'this',\n",
       "   'technology',\n",
       "   'not',\n",
       "   'only',\n",
       "   'reduce',\n",
       "   'the',\n",
       "   'cost',\n",
       "   'of',\n",
       "   'implement',\n",
       "   'and',\n",
       "   'manage',\n",
       "   'infrastructure',\n",
       "   'but',\n",
       "   'also',\n",
       "   'serve',\n",
       "   'as',\n",
       "   'a',\n",
       "   'fundamental',\n",
       "   'pillar',\n",
       "   'in',\n",
       "   'paradigm',\n",
       "   'such',\n",
       "   'as',\n",
       "   'Network',\n",
       "   'Function',\n",
       "   'Virtualization',\n",
       "   'NFV',\n",
       "   '16',\n",
       "   '19',\n",
       "   'the',\n",
       "   'adoption',\n",
       "   'of',\n",
       "   'NFV',\n",
       "   'and',\n",
       "   'its',\n",
       "   'Virtual',\n",
       "   'Network',\n",
       "   'Function',\n",
       "   'VNF',\n",
       "   'component',\n",
       "   'have',\n",
       "   'prove',\n",
       "   'to',\n",
       "   'be',\n",
       "   'effective',\n",
       "   'in',\n",
       "   'reduce',\n",
       "   'the',\n",
       "   'consumption',\n",
       "   'of',\n",
       "   'computational',\n",
       "   'and',\n",
       "   'network',\n",
       "   'resource',\n",
       "   'furthermore',\n",
       "   'virtualization',\n",
       "   'increase',\n",
       "   'the',\n",
       "   'speed',\n",
       "   'and',\n",
       "   'efficiency',\n",
       "   'of',\n",
       "   'resource',\n",
       "   'provisioning',\n",
       "   'in',\n",
       "   'proceeding',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'web',\n",
       "   'WebMe',\n",
       "   'dia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   '2024',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'sbc',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '2753',\n",
       "   'Anselmo',\n",
       "   'L.',\n",
       "   'E.',\n",
       "   'Battisti',\n",
       "   'anselmo@midiacom.uff.br',\n",
       "   'Fluminense',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'Niterói',\n",
       "   'RJ',\n",
       "   'Brazil',\n",
       "   'Flavia',\n",
       "   'C.',\n",
       "   'Delicato',\n",
       "   'fdelicato@ic.uff.br',\n",
       "   'Fluminense',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'Niterói',\n",
       "   'RJ',\n",
       "   'Brazil',\n",
       "   'in',\n",
       "   'cloud',\n",
       "   'and',\n",
       "   'edge',\n",
       "   'environment',\n",
       "   'facilitate',\n",
       "   'the',\n",
       "   'management',\n",
       "   'and',\n",
       "   'orchestration',\n",
       "   'of',\n",
       "   'these',\n",
       "   'resource',\n",
       "   '24',\n",
       "   'many',\n",
       "   'investigation',\n",
       "   'combine',\n",
       "   'NFV',\n",
       "   'technology',\n",
       "   'with',\n",
       "   'multimedia',\n",
       "   'stream',\n",
       "   'processing',\n",
       "   'in',\n",
       "   '4',\n",
       "   'the',\n",
       "   'author',\n",
       "   'explore',\n",
       "   'the',\n",
       "   'concept',\n",
       "   'of',\n",
       "   'virtual',\n",
       "   'device',\n",
       "   'that',\n",
       "   'abstract',\n",
       "   'the',\n",
       "   'heterogeneity',\n",
       "   'of',\n",
       "   'multimedia',\n",
       "   'sensor',\n",
       "   'and',\n",
       "   'introduce',\n",
       "   'virtual',\n",
       "   'multimedia',\n",
       "   'sensor',\n",
       "   'VMS',\n",
       "   'which',\n",
       "   'we',\n",
       "   'will',\n",
       "   'refer',\n",
       "   'to',\n",
       "   'as',\n",
       "   'multimedia',\n",
       "   'vnf',\n",
       "   'in',\n",
       "   'our',\n",
       "   'work',\n",
       "   'these',\n",
       "   'VMSs',\n",
       "   'enable',\n",
       "   'multimedia',\n",
       "   'stream',\n",
       "   'processing',\n",
       "   'use',\n",
       "   'lightweight',\n",
       "   'virtualization',\n",
       "   'furthermore',\n",
       "   'the',\n",
       "   'author',\n",
       "   'demonstrate',\n",
       "   'the',\n",
       "   'feasibility',\n",
       "   'of',\n",
       "   'use',\n",
       "   'chain',\n",
       "   'VMSs',\n",
       "   'to',\n",
       "   'create',\n",
       "   'complex',\n",
       "   'multimedia',\n",
       "   'stream',\n",
       "   'processing',\n",
       "   'pipeline',\n",
       "   'however',\n",
       "   'this',\n",
       "   'characteristic',\n",
       "   'be',\n",
       "   'not',\n",
       "   'fully',\n",
       "   'explore',\n",
       "   'in',\n",
       "   'that',\n",
       "   'work',\n",
       "   'meanwhile',\n",
       "   'in',\n",
       "   '11',\n",
       "   'the',\n",
       "   'author',\n",
       "   'investigate',\n",
       "   'the',\n",
       "   'joint',\n",
       "   'load',\n",
       "   'balance',\n",
       "   'and',\n",
       "   'autoscaling',\n",
       "   'of',\n",
       "   'multimedia',\n",
       "   'vnf',\n",
       "   'in',\n",
       "   'edge',\n",
       "   'or',\n",
       "   'cloud',\n",
       "   'environment',\n",
       "   'in',\n",
       "   'this',\n",
       "   'context',\n",
       "   'virtualization',\n",
       "   'concept',\n",
       "   'like',\n",
       "   'NFV',\n",
       "   'have',\n",
       "   'be',\n",
       "   'actively',\n",
       "   'integrate',\n",
       "   'into',\n",
       "   'different',\n",
       "   'study',\n",
       "   'to',\n",
       "   'process',\n",
       "   'multimedia',\n",
       "   'stream',\n",
       "   'the',\n",
       "   'Service',\n",
       "   'Function',\n",
       "   'Chain',\n",
       "   'SFC',\n",
       "   'be',\n",
       "   'a',\n",
       "   'sequence',\n",
       "   'of',\n",
       "   'sorted',\n",
       "   'vnf',\n",
       "   'associate',\n",
       "   'with',\n",
       "   'a',\n",
       "   'Service',\n",
       "   'Level',\n",
       "   'Agreement',\n",
       "   'SLA',\n",
       "   '5',\n",
       "   'by',\n",
       "   'chain',\n",
       "   'multimedia',\n",
       "   'function',\n",
       "   'it',\n",
       "   'be',\n",
       "   'possible',\n",
       "   'to',\n",
       "   'create',\n",
       "   'complex',\n",
       "   'multimedia',\n",
       "   'stream',\n",
       "   'processing',\n",
       "   'pipeline',\n",
       "   'when',\n",
       "   'the',\n",
       "   'vnf',\n",
       "   'in',\n",
       "   'an',\n",
       "   'SFC',\n",
       "   'be',\n",
       "   'tailor',\n",
       "   'for',\n",
       "   'multimedia',\n",
       "   'processing',\n",
       "   'they',\n",
       "   'be',\n",
       "   'refer',\n",
       "   'to',\n",
       "   'as',\n",
       "   'multimedia',\n",
       "   'sfc',\n",
       "   'despite',\n",
       "   'the',\n",
       "   'potential',\n",
       "   'of',\n",
       "   'multimedia',\n",
       "   'sfc',\n",
       "   'in',\n",
       "   'various',\n",
       "   'field',\n",
       "   '9',\n",
       "   'we',\n",
       "   'face',\n",
       "   'the',\n",
       "   'challenge',\n",
       "   'of',\n",
       "   'implement',\n",
       "   'and',\n",
       "   'manage',\n",
       "   'they',\n",
       "   'easily',\n",
       "   'securely',\n",
       "   'and',\n",
       "   'efficiently',\n",
       "   'the',\n",
       "   'component',\n",
       "   'use',\n",
       "   'to',\n",
       "   'create',\n",
       "   'multimedia',\n",
       "   'sfc',\n",
       "   'be',\n",
       "   'typically',\n",
       "   'develop',\n",
       "   'by',\n",
       "   'different',\n",
       "   'entity',\n",
       "   'use',\n",
       "   'multiple',\n",
       "   'technology',\n",
       "   '14',\n",
       "   'thus',\n",
       "   'to',\n",
       "   'create',\n",
       "   'a',\n",
       "   'multimedia',\n",
       "   'sfc',\n",
       "   'user',\n",
       "   'must',\n",
       "   'have',\n",
       "   'a',\n",
       "   'moderate',\n",
       "   'level',\n",
       "   'of',\n",
       "   'knowledge',\n",
       "   'of',\n",
       "   'these',\n",
       "   'technology',\n",
       "   'moreover',\n",
       "   'the',\n",
       "   'interoperability',\n",
       "   'of',\n",
       "   'these',\n",
       "   'component',\n",
       "   'sometimes',\n",
       "   'be',\n",
       "   'often',\n",
       "   'limited',\n",
       "   'to',\n",
       "   'address',\n",
       "   'these',\n",
       "   'challenge',\n",
       "   'it',\n",
       "   'be',\n",
       "   'necessary',\n",
       "   'to',\n",
       "   'provide',\n",
       "   'a',\n",
       "   'layer',\n",
       "   'that',\n",
       "   'encapsulate',\n",
       "   'the',\n",
       "   'technical',\n",
       "   'detail',\n",
       "   'of',\n",
       "   'multimedia',\n",
       "   'sfc',\n",
       "   'there',\n",
       "   'be',\n",
       "   'different',\n",
       "   'alternative',\n",
       "   'for',\n",
       "   'this',\n",
       "   'among',\n",
       "   'which',\n",
       "   'Domain',\n",
       "   'Specific',\n",
       "   'Languages',\n",
       "   'dsl',\n",
       "   'stand',\n",
       "   'out',\n",
       "   '18',\n",
       "   'unlike',\n",
       "   'other',\n",
       "   'approach',\n",
       "   'such',\n",
       "   'as',\n",
       "   'metamodel',\n",
       "   'or',\n",
       "   'conventional',\n",
       "   'programming',\n",
       "   'language',\n",
       "   'dsl',\n",
       "   'offer',\n",
       "   'great',\n",
       "   'flexibility',\n",
       "   'and',\n",
       "   'simplicity',\n",
       "   'dsl',\n",
       "   'be',\n",
       "   'language',\n",
       "   'specification',\n",
       "   'design',\n",
       "   'to',\n",
       "   'describe',\n",
       "   'the',\n",
       "   'characteristic',\n",
       "   'syntax',\n",
       "   'and',\n",
       "   'behavior',\n",
       "   'of',\n",
       "   'system',\n",
       "   'in',\n",
       "   'specific',\n",
       "   'field',\n",
       "   '15',\n",
       "   'these',\n",
       "   'language',\n",
       "   'enable',\n",
       "   'intuitive',\n",
       "   'integration',\n",
       "   'and',\n",
       "   'management',\n",
       "   'by',\n",
       "   'provide',\n",
       "   'an',\n",
       "   'abstraction',\n",
       "   'layer',\n",
       "   'encapsulate',\n",
       "   'technical',\n",
       "   'detail',\n",
       "   'this',\n",
       "   'allow',\n",
       "   'user',\n",
       "   'to',\n",
       "   'focus',\n",
       "   'on',\n",
       "   'business',\n",
       "   'logic',\n",
       "   'and',\n",
       "   'specific',\n",
       "   'functionality',\n",
       "   'in',\n",
       "   'addition',\n",
       "   'dsl',\n",
       "   'facilitate',\n",
       "   'the',\n",
       "   'integration',\n",
       "   'of',\n",
       "   'new',\n",
       "   'or',\n",
       "   'external',\n",
       "   'component',\n",
       "   'more',\n",
       "   'efficiently',\n",
       "   'and',\n",
       "   'less',\n",
       "   'technically',\n",
       "   'although',\n",
       "   'there',\n",
       "   'have',\n",
       "   'be',\n",
       "   'significant',\n",
       "   'advance',\n",
       "   'in',\n",
       "   'the',\n",
       "   'literature',\n",
       "   'to',\n",
       "   'facilitate',\n",
       "   'the',\n",
       "   'definition',\n",
       "   'and',\n",
       "   'deployment',\n",
       "   'of',\n",
       "   'NFV',\n",
       "   'service',\n",
       "   'such',\n",
       "   'as',\n",
       "   '11',\n",
       "   'WebMedia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Ventura',\n",
       "   'Quico',\n",
       "   'et',\n",
       "   'al',\n",
       "   'the',\n",
       "   'TOSCA',\n",
       "   'nfv',\n",
       "   'metamodel',\n",
       "   '22',\n",
       "   'or',\n",
       "   'ETSI',\n",
       "   'gs',\n",
       "   'NFV',\n",
       "   'SOL',\n",
       "   '10',\n",
       "   'there',\n",
       "   'be',\n",
       "   'still',\n",
       "   'a',\n",
       "   'considerable',\n",
       "   'gap',\n",
       "   'specifically',\n",
       "   'regard',\n",
       "   'multimedia',\n",
       "   'sfc',\n",
       "   'the',\n",
       "   'need',\n",
       "   'to',\n",
       "   'address',\n",
       "   'multimedia',\n",
       "   'sfc',\n",
       "   'separately',\n",
       "   'arise',\n",
       "   'from',\n",
       "   'their',\n",
       "   'unique',\n",
       "   'nature',\n",
       "   'and',\n",
       "   'challenge',\n",
       "   'some',\n",
       "   'of',\n",
       "   'these',\n",
       "   'challenge',\n",
       "   'be',\n",
       "   'i',\n",
       "   'manage',\n",
       "   'large',\n",
       "   'volume',\n",
       "   'and',\n",
       "   'high',\n",
       "   'speed',\n",
       "   'datum',\n",
       "   'flow',\n",
       "   'ii',\n",
       "   'synchronize',\n",
       "   'multiple',\n",
       "   'format',\n",
       "   'and',\n",
       "   'medium',\n",
       "   'type',\n",
       "   'and',\n",
       "   'iii',\n",
       "   'meeting',\n",
       "   'requirement',\n",
       "   'for',\n",
       "   'real',\n",
       "   'time',\n",
       "   'processing',\n",
       "   'which',\n",
       "   'be',\n",
       "   'not',\n",
       "   'as',\n",
       "   'critical',\n",
       "   'in',\n",
       "   'traditional',\n",
       "   'NFVs',\n",
       "   '17',\n",
       "   'furthermore',\n",
       "   'multimedia',\n",
       "   'sfc',\n",
       "   'often',\n",
       "   'require',\n",
       "   'great',\n",
       "   'flexibility',\n",
       "   'in',\n",
       "   'term',\n",
       "   'of',\n",
       "   'scalability',\n",
       "   'and',\n",
       "   'adaptation',\n",
       "   'of',\n",
       "   'dynamic',\n",
       "   'change',\n",
       "   'in',\n",
       "   'resource',\n",
       "   'demand',\n",
       "   'therefore',\n",
       "   'although',\n",
       "   'existing',\n",
       "   'model',\n",
       "   'such',\n",
       "   'as',\n",
       "   'TOSCA',\n",
       "   'NFV',\n",
       "   'provide',\n",
       "   'mean',\n",
       "   'to',\n",
       "   'describe',\n",
       "   'sfc',\n",
       "   'in',\n",
       "   'general',\n",
       "   'the',\n",
       "   'unique',\n",
       "   'characteristic',\n",
       "   'of',\n",
       "   'multimedia',\n",
       "   'sfc',\n",
       "   'demand',\n",
       "   'a',\n",
       "   'specialized',\n",
       "   'approach',\n",
       "   'that',\n",
       "   'directly',\n",
       "   'address',\n",
       "   'their',\n",
       "   'specific',\n",
       "   'requirement',\n",
       "   'and',\n",
       "   'inherent',\n",
       "   'challenge',\n",
       "   'in',\n",
       "   'this',\n",
       "   'work',\n",
       "   'our',\n",
       "   'aim',\n",
       "   'be',\n",
       "   'to',\n",
       "   'fill',\n",
       "   'this',\n",
       "   'gap',\n",
       "   'by',\n",
       "   'propose',\n",
       "   'a',\n",
       "   'DSL',\n",
       "   'call',\n",
       "   'Language',\n",
       "   'for',\n",
       "   'programming',\n",
       "   'iot',\n",
       "   'sensor',\n",
       "   'for',\n",
       "   'Multimedia',\n",
       "   'LPRISM',\n",
       "   'which',\n",
       "   'serve',\n",
       "   'as',\n",
       "   'a',\n",
       "   'conceptual',\n",
       "   'foundation',\n",
       "   'for',\n",
       "   'describe',\n",
       "   'multimedia',\n",
       "   'sfc',\n",
       "   'particularly',\n",
       "   'in',\n",
       "   'the',\n",
       "   'emerge',\n",
       "   'field',\n",
       "   'of',\n",
       "   'the',\n",
       "   'IoMT',\n",
       "   'the',\n",
       "   'decision',\n",
       "   'to',\n",
       "   'develop',\n",
       "   'l',\n",
       "   'PRISM',\n",
       "   'instead',\n",
       "   'of',\n",
       "   'extend',\n",
       "   'exist',\n",
       "   'solution',\n",
       "   'like',\n",
       "   'TOSCA',\n",
       "   'NFV',\n",
       "   'be',\n",
       "   'drive',\n",
       "   'by',\n",
       "   'the',\n",
       "   'unique',\n",
       "   'demand',\n",
       "   'of',\n",
       "   'multimedia',\n",
       "   'sfc',\n",
       "   'such',\n",
       "   'as',\n",
       "   'the',\n",
       "   'inherent',\n",
       "   'complexity',\n",
       "   'in',\n",
       "   'their',\n",
       "   'definition',\n",
       "   'and',\n",
       "   'configuration',\n",
       "   'the',\n",
       "   'need',\n",
       "   'for',\n",
       "   'flexibility',\n",
       "   'in',\n",
       "   'their',\n",
       "   'component',\n",
       "   'and',\n",
       "   'adaptability',\n",
       "   'to',\n",
       "   'specific',\n",
       "   'environment',\n",
       "   'our',\n",
       "   'DSL',\n",
       "   'include',\n",
       "   'a',\n",
       "   'data',\n",
       "   'model',\n",
       "   'that',\n",
       "   'provide',\n",
       "   ...]},\n",
       " {'titulo': 'Constructing a KBQA Framework: Design and Implementation',\n",
       "  'informacoes_url': '',\n",
       "  'idioma': 'english',\n",
       "  'storage_key': '../articles/original/english/985-24747-1-10-20240923.pdf',\n",
       "  'author': 'Rômulo Chrispim de Mello; Jorão Gomes Jr.; Jairo Francisco de Souza; and Victor Ströele',\n",
       "  'data_publicacao': '11-09-2024',\n",
       "  'resumo': 'The exponential growth of data on the internet has made information retrieval increasingly challenging. Knowledge-based QuestionAnswering (KBQA) framework offers an efficient solution that quickly provides accurate and relevant information. However, these frameworks face significant challenges, especially when dealing with complex queries involving multiple entities and properties. This paper studies KBQA frameworks, focusing on improving entity recognition, property extraction, and query generation using advanced Natural Language Processing (NLP) and Artificial Intelligence (AI) techniques. We implemented and evaluated combination tools for extracting entities and properties, with the combination of models achieving the best performance. Our evaluation metrics included entity and property retrieval, SPARQL query completeness, and accuracy. The results demonstrated the effectiveness of our approach, with high accuracy rates in identifying entities and properties. ###',\n",
       "  'keywords': 'KBQA, Complex Questions, Entity Recognition, Property Extraction, SPARQL',\n",
       "  'referencias': ['[1] Shuang Chen, Qian Liu, Zhiwei Yu, Chin-Yew Lin, Jian-Guang Lou, and Feng\\nJiang. 2021. ReTraCk: A flexible and efficient framework for knowledge base\\nquestion answering. In *Proceedings of the 59th annual meeting of the association*\\n*for computational linguistics and the 11th international joint conference on natural*\\n*language processing: system demonstrations* . 325–336.',\n",
       "   '[2] Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth\\nMurisasco. 2023. Complex QA and language models hybrid architectures, Survey.\\n*arXiv preprint arXiv:2302.09051* (2023).',\n",
       "   '[3] Akshay Kumar Dileep, Anurag Mishra, Ria Mehta, Siddharth Uppal, Jaydeep\\nChakraborty, and Srividya K. Bansal. 2021. Template-based Question Answering\\nanalysis on the LC-QuAD2.0 Dataset. In *2021 IEEE 15th International Conference*\\n*on Semantic Computing (ICSC)* . 443–448. https://doi.org/10.1109/ICSC50631.2021.\\n00079\\n\\n\\n96\\n\\n\\n-----\\n\\nConstructing a KBQA Framework: Design and Implementation WebMedia’2024, Juiz de Fora, Brazil',\n",
       "   '[4] Eleftherios Dimitrakis, Konstantinos Sgontzos, and Yannis Tzitzikas. 2020. A\\nsurvey on question answering systems over linked data and documents. *Journal*\\n*of intelligent information systems* 55 (2020), 233–259.',\n",
       "   '[5] Mohnish Dubey, Debayan Banerjee, Abdelrahman Abdelkawi, and Jens Lehmann.\\n2019. Lc-quad 2.0: A large dataset for complex question answering over wikidata\\nand dbpedia. In *International Semantic Web Conference* . Springer, 69–78.',\n",
       "   '[6] Paolo Ferragina and Ugo Scaiella. 2010. TAGME: On-the-Fly Annotation of\\nShort Text Fragments (by Wikipedia Entities). In *Proceedings of the 19th ACM*\\n*International Conference on Information and Knowledge Management* (Toronto,\\nON, Canada) *(CIKM ’10)* . Association for Computing Machinery, New York, NY,\\nUSA, 1625–1628. https://doi.org/10.1145/1871437.1871689',\n",
       "   '[7] Jorão Gomes, Rômulo Chrispim de Mello, Victor Ströele, and Jairo Francisco de\\nSouza. 2022. A Hereditary Attentive Template-based Approach for Complex\\nKnowledge Base Question Answering Systems. *Expert Systems with Applications*\\n205 (2022), 117725. https://doi.org/10.1016/j.eswa.2022.117725',\n",
       "   '[8] Jorão Gomes Jr, Rômulo Chrispim de Mello, Victor Ströele, and Jairo Francisco\\nde Souza. 2022. A study of approaches to answering complex questions over\\nknowledge bases. *Knowledge and Information Systems* 64, 11 (2022), 2849–2881.',\n",
       "   '[9] Jorão Gomes Jr., Rômulo Chrispim de Mello, Victor Ströele, and Jairo Francisco\\nde Souza. 2021. LC-QuAD 2.1. https://doi.org/10.5281/zenodo.5508297',\n",
       "   '[10] Xixin Hu, Xuan Wu, Yiheng Shu, and Yuzhong Qu. 2022. Logical Form Generation\\nvia Multi-task Learning for Complex Question Answering over Knowledge Bases.\\nIn *Proceedings of the 29th International Conference on Computational Linguistics* .\\nInternational Committee on Computational Linguistics, Gyeongju, Republic of\\nKorea, 1687–1696. https://aclanthology.org/2022.coling-1.145',\n",
       "   '[11] Heewon Jang, Yeongtaek Oh, Seunghee Jin, Haemin Jung, Hyesoo Kong, Dokyung\\nLee, Dongkyu Jeon, and Wooju Kim. 2017. KBQA: Constructing Structured\\nQuery Graph from Keyword Query for Semantic Search. In *Proceedings of the*\\n*International Conference on Electronic Commerce* (Pangyo, Seongnam, Republic of\\nKorea) *(ICEC ’17)* . Association for Computing Machinery, New York, NY, USA,\\nArticle 8, 8 pages. https://doi.org/10.1145/3154943.3154955',\n",
       "   '[12] Yunshi Lan, Gaole He, Jinhao Jiang, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen.\\n2021. A Survey on Complex Knowledge Base Question Answering: Methods,\\nChallenges and Solutions. *CoRR* abs/2105.11644 (2021). arXiv:2105.11644 https:\\n//arxiv.org/abs/2105.11644',\n",
       "   '[13] Nandana Mihindukulasooriya, Gaetano Rossiello, Pavan Kapanipathi, Ibrahim\\nAbdelaziz, Srinivas Ravishankar, Mo Yu, Alfio Gliozzo, Salim Roukos, and\\nAlexander G. Gray. 2020. Leveraging Semantic Parsing for Relation Linking over Knowledge Bases. *CoRR* abs/2009.07726 (2020). arXiv:2009.07726\\nhttps://arxiv.org/abs/2009.07726',\n",
       "   '[14] Saeedeh Momtazi and Zahra Abbasiantaeb. 2022. *Question Answering over Text*\\n*and Knowledge Base* . Springer Nature.',\n",
       "   '[15] Sumit Neelam, Udit Sharma, Hima Karanam, Shajith Ikbal, Pavan Kapanipathi,\\nIbrahim Abdelaziz, Nandana Mihindukulasooriya, Young-Suk Lee, Santosh Srivastava, Cezar Pendus, et al . 2022. A benchmark for generalizable and interpretable temporal question answering over knowledge bases. *arXiv preprint*\\n*arXiv:2201.05793* (2022).',\n",
       "   '[16] Ngonga Ngomo. 2018. 9th challenge on question answering over linked data\\n(QALD-9). *language* 7, 1 (2018), 58–64.',\n",
       "   '[17] Kechen Qin, Yu Wang, Cheng Li, Kalpa Gunaratna, Hongxia Jin, Virgil Pavlu, and\\nJaved A. Aslam. 2020. A Complex KBQA System using Multiple Reasoning Paths.\\n*CoRR* abs/2005.10970 (2020). arXiv:2005.10970 https://arxiv.org/abs/2005.10970',\n",
       "   '[18] Gaetano Rossiello, Nandana Mihindukulasooriya, Ibrahim Abdelaziz, Mihaela\\nBornea, Alfio Gliozzo, Tahira Naseem, and Pavan Kapanipathi. 2021. Generative\\nRelation Linking for Question Answering over Knowledge Bases. In *The Semantic*\\n*Web – ISWC 2021*, Andreas Hotho, Eva Blomqvist, Stefan Dietze, Achille Fokoue,\\nYing Ding, Payam Barnaghi, Armin Haller, Mauro Dragoni, and Harith Alani\\n(Eds.). Springer International Publishing, Cham, 321–337.',\n",
       "   '[19] Ahmad Sakor, Kuldeep Singh, Anery Patel, and Maria-Esther Vidal. 2019.\\nFALCON 2.0: An Entity and Relation Linking Tool over Wikidata. *CoRR*\\nabs/1912.11270 (2019). arXiv:1912.11270 http://arxiv.org/abs/1912.11270',\n",
       "   '[20] Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, and Guilin Qi.\\n2023. Evaluation of ChatGPT as a Question Answering System for Answering\\nComplex Questions. *arXiv preprint arXiv:2303.07992* (2023).',\n",
       "   '[21] Priyansh Trivedi, Gaurav Maheshwari, Mohnish Dubey, and Jens Lehmann. 2017.\\nLc-quad: A corpus for complex question answering over knowledge graphs. In *In-*\\n*ternational Semantic Web Conference* . Springer, Springer International Publishing,\\nCham, 210–218.',\n",
       "   '[22] Ricardo Usbeck, Axel-Cyrille Ngonga Ngomo, Bastian Haarmann, Anastasia\\nKrithara, Michael Röder, and Giulio Napolitano. 2017. 7th open challenge on\\nquestion answering over linked data (QALD-7). In *Semantic Web Challenges: 4th*\\n*SemWebEval Challenge at ESWC 2017, Portoroz, Slovenia, May 28-June 1, 2017,*\\n*Revised Selected Papers* . Springer, 59–69.',\n",
       "   '[23] Zhiwen Xie, Zhao Zeng, Guangyou Zhou, and Tingting He. 2016. Knowledge\\nbase question answering based on deep learning models. In *Natural Language*\\n*Understanding and Intelligent Applications: 5th CCF Conference on Natural Lan-*\\n*guage Processing and Chinese Computing, NLPCC 2016, and 24th International*\\n*Conference on Computer Processing of Oriental Languages, ICCPOL 2016, Kunming,*\\n\\n\\n*China, December 2–6, 2016, Proceedings 24* . Springer, 300–311.',\n",
       "   '[24] Xi Ye, Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou, and Caiming Xiong. 2021.\\nRng-kbqa: Generation augmented iterative ranking for knowledge base question\\nanswering. *arXiv preprint arXiv:2109.08678* (2021).\\n\\n\\n97\\n\\n\\n-----'],\n",
       "  'text': '# **Constructing a KBQA Framework: Design and Implementation**\\n\\n## Rômulo Chrispim de Mello\\n#### romulomello@ice.ufjf.br Department of Computer Science Federal University of Juiz de Fora Juiz de Fora – MG, Brazil\\n## Jairo Francisco de Souza\\n#### jairo.souza@ice.ufjf.br Department of Computer Science Federal University of Juiz de Fora Juiz de Fora – MG, Brazil\\n### **ABSTRACT**\\n\\nThe exponential growth of data on the internet has made information retrieval increasingly challenging. Knowledge-based QuestionAnswering (KBQA) framework offers an efficient solution that\\nquickly provides accurate and relevant information. However, these\\nframeworks face significant challenges, especially when dealing\\nwith complex queries involving multiple entities and properties.\\nThis paper studies KBQA frameworks, focusing on improving entity recognition, property extraction, and query generation using\\nadvanced Natural Language Processing (NLP) and Artificial Intelligence (AI) techniques. We implemented and evaluated combination\\ntools for extracting entities and properties, with the combination\\nof models achieving the best performance. Our evaluation metrics\\nincluded entity and property retrieval, SPARQL query completeness, and accuracy. The results demonstrated the effectiveness of\\nour approach, with high accuracy rates in identifying entities and\\nproperties.\\n### **KEYWORDS**\\n\\nKBQA, Complex Questions, Entity Recognition, Property Extraction, SPARQL\\n### **1 INTRODUCTION**\\n\\nThe large volume of data available on the internet has made the\\ntask of finding relevant information even more challenging [ 11 ].\\nIn this context, new information retrieval methods have allowed\\nfor more intelligent searches, taking into account the context of\\nthe search. Additionally, advances in Natural Language Processing\\n(NLP) research have enabled a better understanding of the search\\ncontext, allowing Knowledge-based Question-Answering (KBQA)\\nframeworks to emerge as an efficient solution to meet this demand\\n\\n[12].\\nThe complexity of the questions users ask is one of the main\\nchallenges faced by KBQA frameworks [ 17 ]. Questions can be complex for various reasons, such as the presence of multiple concepts,\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n© 2024 SBC – Brazilian Computing Society.\\nISSN 2966-2753\\n\\n## Jorão Gomes Jr.\\n#### jorao.gomes.junior@wu.ac.at Institute for Digital Ecosystems Vienna University of Economics and Business Vienna, Austria\\n## Victor Ströele\\n#### victor.stroele@ice.ufjf.br Department of Computer Science Federal University of Juiz de Fora Juiz de Fora – MG, Brazil\\n\\nthe need to understand the context in which the question is created, and the dependence on additional information that was not\\nmentioned in the question. These factors can lead to ambiguities in\\nquestions and, consequently, to imprecise results.\\nNatural language questions can vary greatly in complexity. Simple questions might involve straightforward retrieval of facts, such\\nas “What is the capital of France?”. However, complex questions\\noften require the integration of multiple pieces of information, reasoning over data, and understanding nuanced context. For example,\\nthe question “Which films directed by Quentin Tarantino were\\nnominated for an Oscar?” requires the system to identify multiple\\nentities (“Quentin Tarantino”, “films”, and “Oscar”) and understand\\ntheir relationships [ 23 ]. The complexity increases when questions\\ninvolve conditional statements, comparative structures, or temporal aspects. For instance, “What was the population of New York\\nCity before 2000?” needs temporal reasoning and access to historical data. Similarly, questions like “Is the Eiffel Tower taller than\\nthe Statue of Liberty?” require comparative reasoning and precise\\nentity linking.\\nUnderstanding the context in which a question is asked is crucial\\nfor accurate KBQA. The same term can have different meanings\\nbased on context, making disambiguation a significant challenge.\\nFor example, the word “Java” can refer to a programming language,\\nan island in Indonesia, or a type of coffee. Determining the correct\\ninterpretation based on the surrounding context is essential for\\nproviding accurate answers [ 10 ]. Moreover, questions often rely\\non implicit context that is not explicitly stated. For instance, in\\nthe question “What is the country’s capital?”, the system must\\ninfer which country is being referred to from prior context or user\\ninteraction history. This requires the KBQA system to maintain and\\nutilize contextual information dynamically.\\nLarge language models (LLMs) such as GPT have recently gained\\npopularity as Question-Answering (QA) systems, demonstrating\\nimpressive results in answering questions posed in natural language.\\nThese models are trained on massive amounts of text data, allowing\\nthem to understand and generate human-like responses to various\\nprompts [ 2 ]. However, it is important to note that QA systems\\nsolely using LLMs are limited by their training data [ 20 ]. They\\nare not designed to access external knowledge repositories and\\ncannot provide information not included in their training data. For\\ninstance, if a question is asked about an event that occurred after\\n\\n\\n89\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Mello et al.\\n\\n\\nthe LLM’s training data was collected, it may be unable to provide\\n\\nan accurate answer.\\n\\nOn the other hand, KBQA systems are designed to access and\\nsearch knowledge repositories, providing access to always up-todate information [ 14 ]. These systems can retrieve information\\nfrom various structured and unstructured data sources, including databases and ontologies [ 4 ]. With their ability to access a\\nlarge and constantly updated knowledge base [ 15 ], KBQA systems\\nare more suitable for answering complex questions that require\\ndomain-specific knowledge or information about recent events.\\nComplex Knowledge-based Question-Answering (C-KBQA) systems use natural language processing and artificial intelligence\\ntechniques [ 8 ]. Natural language processing allows the system to\\nunderstand and interpret users’ questions, while artificial intelligence enables the system to learn the characteristics of those\\nquestions, their entities, properties, and relationships and to return\\nthe appropriate values. Additionally, these systems have a robust\\nand up-to-date knowledge base, which allows the retrieval and\\npresentation of relevant and current information to users [15].\\nKBQA systems typically involve many steps in query processing,\\nfrom entity recognition to SPARQL query generation. In this work,\\nwe propose a KBQA framework that features a practical and flexible\\npipeline for complex knowledge-based question answering (KBQA).\\nThis framework can serve as a model for other systems of this type,\\nincluding adaptations to other languages.\\nThe proposed pipeline is designed to handle complex questions\\ninvolving multiple entities and properties and generate accurate\\nand complete SPARQL queries. A central feature of this pipeline is\\nits flexibility, allowing the replacement of entity recognition and\\nproperty extraction models with new ones that may offer better\\nperformance or accuracy. Additionally, the pipeline can incorporate\\nrefinements in SPARQL queries, using qualifier constraints for specific properties, increasing the completeness and accuracy of the\\nanswers. We implemented methods to retrieve all properties related\\nto an entity and developed a system to dynamically fill placeholders\\nin templates with the correct values extracted from the knowledge\\nbase. These methods ensure the pipeline can easily adapt to new\\nrequirements and improvements, maintaining its effectiveness and\\nefficiency.\\nThis structured approach ensures the system can handle complex\\nqueries, decompose them, identify relevant entities and relationships, and construct precise SPARQL queries to provide accurate\\nresults. Using entity recognition, relation extraction, and template\\nmatching, combined with effective ranking and slot filling, enhances\\nthe accuracy of knowledge-based question answering. By structuring the pipeline in a modular and flexible way, it can easily integrate\\nnew models and rules, ensuring continuous improvements in accuracy and efficiency. Thus, the pipeline demonstrates how to handle\\ncomplex queries effectively and provides a solid foundation for\\nfuture adaptations and innovations in the field of KBQA systems.\\nThis paper is organized as follows: Section 2 presents related\\nwork, discussing existing approaches and their contributions to the\\nfield. Section 3 describes the materials and methods used in this\\nstudy, including dataset preprocessing, template grouping by semantic proximity, dummy template creation, and the tools used for\\nentity and property extraction. Section 4 presents and discusses the\\nobtained results, highlighting the tool combinations that achieved\\n\\n\\nthe best performance. Section 5 concludes the work, discussing the\\nimplications of the results and suggesting future research directions.\\n### **2 RELATED WORK**\\n\\nResearch in Knowledge-Based Question Answering (KBQA) has explored various approaches to enhance the precision and efficiency\\nof these systems, particularly when dealing with complex questions. This section reviews significant contributions across three\\nmain areas: entity recognition, relation extraction, and template\\nmatching.\\n### **2.1 Entity Recognition**\\n\\nEntity recognition is a fundamental task in KBQA, involving identifying entities mentioned in the user’s query. One notable work\\nin this area is TagMe [ 6 ], which introduces a technique for annotating short text fragments with relevant Wikipedia hyperlinks.\\nThe system leverages Wikipedia’s extensive collection of articles\\nand anchor texts to provide informative and accurate annotations.\\nThe main technique used involves point identification, sense disambiguation, and annotation. Point identification analyzes the input\\ntext to identify potential points that can be linked to Wikipedia\\narticles. Sense disambiguation selects each point’s most relevant\\nWikipedia page, considering context and statistical information.\\nFinally, annotation adds hyperlinks to corresponding Wikipedia\\narticles, allowing users to access additional information and context\\nsimply by clicking on the annotated points. TagMe demonstrates superior performance and speed compared to other systems, making\\nit a valuable tool for entity recognition in short texts.\\nAnother significant contribution in this field is Falcon [ 19 ], a\\nrule-based approach for linking entities and relationships in Wikidata. Falcon employs core principles of English morphology, such\\nas tokenization and N-gram tessellation, to link entity and relation\\nsurface forms in short sentences to Wikidata entries. This method\\n\\nincludes a local knowledge base composed of DBpedia entities to enhance the recognition and linking process. Falcon provides a ranked\\nlist of entities and relations annotated with their Internationalized\\nResource Identifier (IRI) in Wikidata, aiding the NLP community in\\nentity and relation recognition. The approach outperforms existing\\nbaselines in entity linking tasks, demonstrating high F-score values and robustness across various datasets like QALD-9 [ 16 ] and\\nLC-QuAD 2.0 [5].\\n### **2.2 Relation Extraction**\\n\\nRelation extraction is another crucial component of KBQA systems,\\nfocusing on identifying and linking relationships between entities\\nwithin a query. SLING [ 13 ] is a semantic analysis framework designed to link text relationships to knowledge bases accurately. The\\napproach integrates multiple methods, including statistical Abstract\\nMeaning Representation (AMR) mapping, distant supervision data\\ngeneration, and various relation-linking modules. The statistical\\nAMR mapping technique is pivotal in identifying relationships by\\nnormalizing syntactic variations between sentences and providing strong predicates. Distant supervision data generation creates\\ntraining examples mapped to corresponding knowledge base relations, enhancing the system’s learning process. SLING leverages\\n\\n\\n90\\n\\n\\n-----\\n\\nConstructing a KBQA Framework: Design and Implementation WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\ntransformer-based architectures to encode AMR graphs and question text for relation linking, achieving state-of-the-art performance\\nacross datasets such as QALD-7 [ 22 ], QALD-9 [ 16 ], and LC-QuAD\\n1.0 [21].\\nAnother innovative approach in relation extraction is presented\\nby [ 18 ], which proposes a sequence-to-sequence model enhanced\\nwith structured data from the target knowledge base. This model\\ngenerates a sequence of relations based on the input question text,\\nenriched by an entity-linking system that queries the knowledge\\nbase to retrieve candidate relations. The model’s decoder then\\n\\nuses the enriched input representation to generate a structured\\nsequence of argument-relation pairs, considering the contextual\\ninformation and candidate relations. This approach significantly\\nimproves relation-linking performance in question-answering systems, demonstrating notable enhancements over existing methods.\\n### **2.3 Template Matching**\\n\\nTemplate matching involves identifying patterns in user questions\\nand matching them to predefined templates, facilitating the generation of structured and well-formatted responses. In [ 3 ], the authors\\nexplore machine learning models and preprocessing techniques\\nto classify natural language questions into appropriate templates\\nusing the LC-QUAD 2.0 dataset. They train classifiers such as XGBoost and Random Forest, utilizing Part-of-Speech (POS) tagging\\nand FastText for preprocessing. POS tagging assigns grammatical\\ntags to words, helping the system understand the syntactic structure of questions, while FastText captures the semantic meaning\\nof words through embeddings. The combination of XGBoost and\\nPOS+FastText preprocessing achieves superior accuracy in classifying questions into relevant templates, showcasing the effectiveness\\nof template-based question answering.\\nAnother noteworthy study is presented by [ 7 ], which introduces\\na hereditary attention mechanism combined with template matching to enhance semantic extraction from questions. This approach\\ncategorizes complex questions into answer templates, leveraging\\nhierarchical structures within the questions. The hereditary attention mechanism operates bottom-up, where each neural network\\ncell inherits attention from another cell, capturing and prioritizing\\nthe most relevant information at different levels of the question’s\\nstructure. This method improves the robustness and accuracy of\\nKBQA systems, providing a reliable technique for answering complex questions from knowledge bases.\\n### **2.4 Frameworks**\\n\\nRecent advancements in KBQA framework have highlighted the\\neffectiveness of integrating various methodologies to improve performance in complex queries. Two works in this area are the RnGKBQA and ReTraCk framework, which present approaches to enhance the accuracy and generalization capabilities of KBQA framework.\\n\\nThe RnG-KBQA system, developed by [ 24 ], addresses the limitations of traditional KBQA systems that struggle with unseen knowledge base (KB) schema items. RnG-KBQA combines a ranking-based\\napproach with a generation model to enhance coverage and generalization. The system first ranks candidate logical forms based on\\nthe question. Then, it uses a generation model conditioned on the\\n\\n\\nquestion and top-ranked candidates to create the final logical form.\\nThis dual approach significantly improves performance, achieving\\nstate-of-the-art results on the GRAILQA and WEBQSP datasets,\\nwith notable improvements in zero-shot generalization.\\nHowever, RnG-KBQA also presents some limitations. The complexity of combining ranking and generation can lead to longer\\nprocessing times and increased computational resource requirements. Additionally, the effectiveness of the generation model may\\nbe limited by the quality of the training data, affecting the system’s\\nability to handle ambiguous or poorly formulated queries.\\nThe ReTraCk, developed by [ 1 ], introduces a flexible framework\\nthat integrates multiple stages for entity recognition, relation extraction, and ranking. ReTraCk emphasizes the importance of a\\nmodular design, allowing for easy integration of different models\\nand techniques at each stage. The system has shown remarkable\\nperformance in various benchmarks, demonstrating its adaptability\\nand efficiency in handling diverse KBQA tasks. The approach focuses on iterative refinement and ranking to enhance the accuracy\\nof the generated answers, contributing to the development of a\\nmore robust KBQA framework.\\nDespite its advantages, ReTraCk also faces challenges. Integrating and adjusting multiple models and techniques can increase\\nthe system’s complexity, making maintenance and updates more\\ndifficult. Additionally, since ReTraCk relies on multiple processing\\nstages, any error in one of these stages can compromise the accuracy of the final answer. The modular approach, while flexible, can\\nresult in inconsistencies when different modules are not perfectly\\naligned. Moreover, ReTraCk is a resource-intensive system requiring considerable computational resources, which can be a barrier\\nto deployment in production environments with limited resources.\\nThese works underscore the importance of combining ranking\\nand generation techniques to overcome the limitations of the traditional KBQA framework, paving the way for more accurate and\\ngeneralizable solutions. By integrating advanced NLP and machine\\nlearning models, both RnG-KBQA and ReTraCk contribute significantly to the field, offering valuable insights and methodologies\\nfor future research in KBQA. However, the identified limitations\\nof these systems highlight the ongoing need for refinement and\\ninnovation to improve the efficiency and effectiveness of KBQA\\nsystems.\\nThis work shares the same principles of these frameworks, such\\nas flexibility and their use for different datasets. However, our\\napproach stands out by focusing on complex queries, which often\\nrequire the integration of multiple pieces of information, reasoning\\nover data, and understanding nuanced context. The solution allows\\nnew entity recognition and property extraction models if needed.\\nWe implemented methods to retrieve all properties related to an\\nentity and developed a system to dynamically fill placeholders in\\ntemplates with the correct values extracted from the knowledge\\nbase. These methods ensure the pipeline can easily adapt to new\\nrequirements and improvements. Finally, the system’s structured\\ndesign can decompose complex queries, identify relevant entities\\nand relationships, and construct precise SPARQL queries using\\nentity recognition, relation extraction, template matching, ranking,\\nand slot-filling methods.\\n\\n\\n91\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Mello et al.\\n\\n### **3 MATERIALS AND METHOD**\\n\\nTo validate our complex question-answering framework, we used\\nthe LC-QUAD 2.1 dataset [9], a refined and cleaned version of LCQUAD 2.0. LC-QUAD 2.1 was chosen due to its structure, which\\nincludes question templates and SPARQL queries with “dummy”\\nelements, facilitating subsequent filling with specific information.\\nThis dataset was created based on the improvements made by [7].\\nThe LC-QUAD 2.0 dataset [ 5 ] was created using the Amazon\\nMechanical Turk crowdsourcing platform, resulting in over 30,000\\ncomplex questions intended to be answered by knowledge-based\\nquestion-answering (KBQA) systems. However, the original dataset\\npresented several inconsistencies, such as duplicate questions, malformed questions, and extreme variations in question length. To\\naddress these issues, [ 7 ] developed LC-QUAD 2.1. This new dataset\\nunderwent a cleaning and standardization process, removing duplicate and malformed questions and adjusting question lengths to\\nensure greater consistency. This process resulted in a cleaner and\\nmore reliable dataset for testing KBQA systems.\\nThe questions in this dataset are complex due to their multifaceted nature, involving multiple entities, relationships, and advanced\\noperations. These questions often require combining data from\\ndifferent sources, applying temporal filters, and specific counting\\noperations. Additionally, many questions use qualifiers to provide\\nmore detailed, contextually rich answers. Complexity also includes\\nthe need for context disambiguation and handling multiple intents\\nin a single question. It also covers various question types, including\\nBoolean questions, counting operations, and questions requiring\\nstring operations and temporal aspects. This diversity and complexity are crucial for evaluating the robustness and effectiveness of\\nKBQA systems, ensuring they can handle realistic and challenging\\nqueries.\\nWe then started working on grouping templates by semantic\\nproximity. This process involved identifying common elements in\\nSPARQL queries, such as projections, jumps, filters, and logical\\noperators, and organizing them into coherent groups based on their\\ncomplexity and structure. Projections define the specific data to be\\nreturned, while jumps represent the intermediate steps needed to\\nobtain that data by navigating relationships within the knowledge\\ngraph. Filters narrow down results based on criteria like numeric\\ncomparisons or string matches, and logical operators such as LIMIT,\\nORDER BY, COUNT, and DISTINCT modify the presentation of\\nresults.\\n\\nTo group the templates, we applied NLP techniques to analyze\\nthe relationships between these components and measure their\\nsemantic similarities. This allowed us to categorize the templates\\ninto groups that reflect both their structural and semantic characteristics. For example, Group 0 includes simple ASK queries that\\nverify the existence of relationships, while Group 1 also uses ASK\\nqueries but involves multiple jumps between entities to verify more\\ncomplex relationships. Group 2 focuses on selecting distinct results.\\nAs the complexity increases, groups such as 3 through 7 involve\\ncounting entities, applying filters, and ordering results. The more\\nadvanced groups, 8 through 12, handle multiple relationships between entities, require complex string matching, temporal filtering,\\nand often combine multiple values and nested conditions, significantly increasing query complexity.\\n\\n\\nThese grouped templates ensure that similar queries are organized in a way that preserves their logical structure, facilitating\\nefficient retrieval and adaptation for different questions. Empirical\\nvalidation using real datasets confirmed the effectiveness of these\\ngroupings, resulting in improved template retrieval at runtime, allowing the system to quickly and accurately handle a wide range\\nof complex questions.\\n\\n**Table 1: Examples of Grouped Templates**\\n\\n|Group|ID|SPARQL Template|\\n|---|---|---|\\n|0|0.1|ASK WHERE { DUMMY_S DUMMY_P DUMMY_O }|\\n||0.2|ASK WHERE { DUMMY_S DUMMY_P ?obj FILTER(?obj = DUMMY_F) }|\\n||0.3|ASK WHERE { DUMMY_S DUMMY_P ?obj FILTER(?obj > DUMMY_F) }|\\n||0.4|ASK WHERE { DUMMY_S DUMMY_P ?obj FILTER(?obj < DUMMY_F) }|\\n|2|2.1|SELECT DISTINCT ?answer WHERE { ?answer DUMMY_P DUMMY_O }|\\n||2.2|SELECT DISTINCT ?answer WHERE { DUMMY_S DUMMY_P ?answer }|\\n\\n\\n\\nAs shown in Table 1, these examples illustrate how templates\\nwith similar structures and semantic functions are grouped to facilitate retrieval and adaptation. By grouping templates in this way,\\nthe system can reuse and adapt predefined templates more efficiently, answering a wide range of complex questions accurately\\nand quickly.\\nThe creation of dummy templates was another fundamental aspect of the work. These templates are generic SPARQL queries\\nwhere placeholders, such as DUMMY_S for the subject, DUMMY_P\\nfor the predicate, and DUMMY_O for the object replace specific\\nelements. Analyzing existing SPARQL queries allowed the identification of common patterns, which were generalized into representative dummy templates. These templates were then grouped\\nbased on semantic similarities, using NLP techniques to organize\\nthe templates efficiently.\\nThe dummy elements in LC-QUAD 2.1 are essential for our\\nframework because they allow a structured and systematic slotfilling process while generating the final SPARQL queries. These\\ndummies are replaced by actual values after the correct entities,\\nproperties, and filters have been identified and extracted. Moreover,\\nqueries with dummies are retrieved through models trained by [ 7 ]\\nand [ 3 ]. These models were developed to identify and structure\\ncomplex questions, facilitating the creation of SPARQL queries that\\nuse dummies to represent entities and properties to be filled later.\\nTo use this dataset in the framework, we performed initial processing starting from the gold SPARQL query for each question.\\nWe extracted entities, properties, and filters. At the end of the\\npipeline processing, this extracted information is used to evaluate\\nthe system, checking whether all expected entities, properties, and\\nfilters or only a portion were found. This allows us to measure\\nthe precision and efficiency of the framework by comparing the\\nexpected information, ensuring that the system is aligned with the\\ngold SPARQL query.\\n\\n\\n92\\n\\n\\n-----\\n\\nConstructing a KBQA Framework: Design and Implementation WebMedia’2024, Juiz de Fora, Brazil\\n\\n### **3.1 Tools for Entity and Property Extraction**\\n\\nWith information about the templates, entities, and properties\\nneeded to answer each question, we began searching for tools\\nthat could efficiently perform this extraction. We identified three\\nmain tools: DeepPavlov, Falcon 2.0, and Spacy. Each of these tools\\nhas unique features that contribute to the accuracy and coverage\\nof our system.\\nDeepPavlov is an open-source NLP library that offers a variety of\\npre-trained models for different NLP tasks, including named entity\\nrecognition (NER) and entity linking. The DeepPavlov library is\\nknown for its flexibility and efficiency, allowing customization of\\nmodels as needed.\\nSpecifically, we used the Entity Linking model from DeepPavlov.\\nThis model operates in two main stages. The first is named entity\\nrecognition (NER), where the model identifies entities in the text\\nusing a combination of linguistic rules and deep learning models.\\nIt applies tokenization techniques, where the text is divided into\\ntokens, which are analyzed to identify possible entities. Then, entity\\ndisambiguation, where the Entity Linking model links these entities\\nto specific entries in a knowledge base, such as Wikidata. This\\nprocess uses word embeddings, which are word representation\\nvectors, to calculate similarities between the identified entities in\\nthe text and the candidates in the knowledge base, selecting the\\nbest match.\\nFalcon 2.0 links entities and relations in short texts specifically\\ndesigned to work with Wikidata. According to [ 19 ], Falcon 2.0 uses a\\nrule-based linguistic approach. The tool performs tokenization and\\ncompounding, dividing the text into tokens using English morphology rules. Tokens representing entities or relations are identified,\\nand compound tokens (like “operating income”) are treated as a\\nsingle unit. It also uses N-Gram tiling techniques to combine tokens close to the text and can form a composite entity or relation.\\nWhen necessary, the model can also split N-Grams to refine the\\nidentification of entities and relations.\\nFalcon 2.0 generates a list of candidate entities and possible\\nrelations from Wikidata for each identified token. This is done by\\nconsulting a local knowledge base that contains alignments of labels\\nand aliases of entities and relations from Wikidata. The generated\\ncandidates are then ranked based on similarity and the probability\\nof a correct match. The model uses rules and an inference process\\nto determine the most likely candidates. Falcon 2.0 stands out for its\\nability to simultaneously link entities and relations, which is crucial\\nfor answering complex questions involving multiple entities and\\ntheir interrelations. The tool is available as an online API, making\\nit accessible and easy to integrate into our system.\\nFinally, Spacy is a widely used open-source NLP library known\\nfor its speed and efficiency. It offers robust models for tasks such\\nas named entity recognition, dependency parsing, and text classification. We used Spacy’s entity linking model. The text is processed\\nthrough a pipeline that includes tokenization, lemmatization, POS\\ntagging (part-of-speech tagging), and dependency parsing. Each of\\nthese steps contributes to accurately identifying entities in the text.\\nSpacy uses a deep learning model trained on large annotated\\ndatasets to identify entities in the text. This model recognizes entities in various contexts and domains. After identifying the entities,\\nthe entity linking model associates them with a knowledge base,\\n\\n\\nsuch as Wikidata. Spacy uses entity embeddings and a vector similarity approach to find the best match for each identified entity.\\nThe library allows significant customizations, enabling adjustments\\nto model parameters and adding new entities and relations to the\\nknowledge base as needed.\\n### **3.2 Tools Integration**\\n\\nAfter selecting the tools, we extracted entities and properties from\\neach question using the described tools. Each tool was applied\\nindependently, and the results were compared to identify complementarities and redundancies. This process was fundamental to\\nensuring that all relevant entities and properties were captured,\\nincreasing the accuracy and coverage of our system.\\n\\n*3.2.1* *Additional Properties Retrieval.* We noticed that many properties were still not captured despite using these tools. To address\\nthis, we implemented a method to retrieve all properties related to\\nan entity, treating it both as a subject and as an object.\\nTo retrieve these additional properties, we performed specific\\nSPARQL queries. When the entity was treated as a subject, the\\nquery searched for all properties where the entity was the subject\\nof the relationship. Conversely, when the entity was treated as an\\nobject, the query searched for all properties where the entity was\\nthe object of the relationship. These querying processes allowed\\nus to recover a broader set of properties related to the identified\\nentities.\\nThe queries were executed for each identified entity, and the\\nresults returned the related properties and descriptions. These additional properties were then integrated into the original dataset,\\nimproving the coverage and accuracy of our system. This additional\\nprocess allowed us to expand the set of captured properties, enhancing the comprehensiveness and precision of our system. However,\\nthere was still a type of property that we could not accurately\\ncapture.\\n\\n*3.2.2* *Qualifiers Constraints for Specific Properties.* Our system\\ncould not generate correct answers for specific templates even\\nafter using the tools and retrieving additional properties. We implemented an additional approach to address these cases using qualifier\\nconstraints of specific properties. Qualifiers are metadata that add\\ncontext to a statement in a knowledge base like Wikidata, specifying additional conditions that must be met. These qualifiers can\\nbe essential for correctly understanding the application of certain\\nproperties in specific contexts.\\nTo identify and use these qualifiers, we developed specific SPARQL\\nqueries. These queries searched for the constraints associated with\\nparticular properties, allowing us to refine data extraction further\\nand improve the accuracy of the answers generated by our system.\\nThe query was structured to identify “qualifier value” constraints\\nassociated with a specific property. This query selects objects and\\ntheir labels associated with the specified property’s constraints.\\nWe ran this query for problematic properties identified during\\nthe query generation process. By retrieving these constraints, we\\ncould identify the additional conditions needed for the correct property application. These qualifier constraints significantly improved\\n\\n\\n93\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Mello et al.\\n\\n\\nour system’s ability to generate accurate answers for the problematic templates. This allowed us to overcome previous limitations\\nand increase the framework’s overall precision.\\n\\n*3.2.3* *Ranking.* Although we had a relatively small number of entities, the number of properties was very large. This made it difficult\\nto combine entities with properties, resulting in many possible\\nSPARQL queries. To address this problem, we used the relation\\nranking module of DeepPavlov to filter and select the most relevant\\nproperties for each question.\\nThe relation ranking module of DeepPavlov is designed to take a\\nquestion and a set of properties as input and then rank them based\\non their relevance using a ranking model. The module assigns a\\nspecific probability to each property, allowing the filtering of the\\nmost relevant ones for the given question. This process is essential\\nto reduce the complexity of slot filling, where combining all entities with all possible properties would result in a huge number of\\nqueries.\\nThe relation ranking process uses word embeddings to represent\\nwords in vector space semantically. These embeddings are loaded\\nfrom pre-trained models such as fastText, used in our case. The\\nranking module receives the set of previously extracted properties\\nfor each question. It processes these properties along with the question and calculates the probabilities of the properties being relevant\\nto the question. The softmax layer in the model is responsible for\\nassigning probabilities, allowing only the most probable properties to be selected. Thus, for each question, we selected the top 5\\nproperties from the ranking. This filtered the entities, keeping only\\nthose related to the selected properties. This process reduced the\\nnumber of properties to be considered and restricted the relevant\\nentities, making the combination of entities and properties more\\nmanageable and precise. This approach was necessary to reduce the\\ncomplexity of slot filling and avoid generating excessive SPARQL\\nqueries. By focusing on the most relevant properties and entities, we\\nimproved efficiency, ensuring the system could adequately respond\\nto complex questions.\\n\\n*3.2.4* *Filters.* In addition to extracting entities and properties, a\\ncrucial step in our system was the extraction of filters. Filters are\\nadditional components of SPARQL queries that restrict results based\\non specific criteria, such as integer values, dates, or other conditions.\\nIdentifying these filters is essential for generating correct SPARQL\\nqueries.\\nSince we already knew which templates contained filters, we\\ndeveloped rules based on regular expressions (regex) specific to each\\ntemplate type. Each set of filters was associated with these specific\\ntemplates. Regex are tools used to identify specific patterns within\\ntexts. First, we analyzed the templates to identify common filter\\npatterns, such as dates and numbers. Then, we defined specific regex\\nto capture these values in each template type and implemented\\nthese rules in the processing pipeline. For example, we used regex\\nto identify dates or integer numbers in questions that contained\\nthese as filters. We tested these rules on a validation set to ensure\\nthe correct extraction of filters and adjusted the regex as necessary\\nto handle variations in question patterns.\\nAfter extraction, the filters were integrated into the SPARQL\\nquery processing pipeline. Each filter was associated with the corresponding entities and properties within the context of the specific\\n\\n\\ntemplates, completing the query construction. This process allowed\\nour system to handle complex questions with specific conditions,\\nensuring that the generated SPARQL queries were accurate. The\\nregex-based approach specific to each template type proved efficient\\nin identifying and extracting filters directly from the questions, improving the system’s ability to accurately respond to a wide range\\nof questions.\\n### **3.3 Pipeline**\\n\\nOur KBQA system was designed to efficiently handle complex\\nqueries by breaking the process into several stages. Figure 1 depicts the complete workflow for answering the query: “Which films\\ndirected by Quentin Tarantino were nominated for an Oscar?”\\n\\n**Question**\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n**Figure 1: Workflow of the KBQA system for the question:**\\n**“Which films directed by Quentin Tarantino were nominated**\\n**for an Oscar?”**\\n\\nInitially, the input question is parsed to understand its structure\\nand intent. Key entities such as “Quentin Tarantino” and “Oscar”\\nare recognized through Named Entity Recognition (NER) and subsequently linked to their corresponding entries in the knowledge\\nbase (in our case, the Wikidata). Relationships between these entities and other relevant attributes are then extracted. These entities\\n\\nand relations are ranked based on their relevance to the query,\\nprioritizing properties like “nominated for” (P1411) and “director”\\n(P57).\\nNext, the query is matched to a predefined template to help generate the appropriate SPARQL query. Specific slots in the SPARQL\\nquery are filled with the identified entities and properties. The\\ncompleted SPARQL query is executed against the knowledge base,\\nretrieving the relevant data. For instance, the query *SELECT ?s*\\n*WHERE ?s wdt:P57 wd:Q3772 . ?s wdt:P1411 wd:Q19020* fetches\\nfilms directed by Quentin Tarantino and nominated for an Oscar.\\nThe results are then processed and presented, identifying films\\nlike “Pulp Fiction”, “Inglourious Basterds”, and “Django Unchained”.\\n\\n\\n94\\n\\n\\n-----\\n\\nConstructing a KBQA Framework: Design and Implementation WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\nThis structured approach ensures the system can handle complex\\nqueries by breaking down the question, identifying relevant entities\\nand relations, and constructing precise SPARQL queries to provide\\naccurate results. Using entity recognition, relation extraction, and\\ntemplate matching, combined with effective ranking and slot filling,\\nenhances the accuracy of knowledge-based question answering.\\nThe central idea of our framework is to provide robust flexibility, allowing the insertion and removal of models and rules as\\nneeded. This includes replacing existing entity recognition systems\\nwith new, potentially more accurate models, and allowing multiple\\nsystems to operate in parallel for greater accuracy. The property\\nextraction component is tunable, allowing you to use different models or updated rules to refine SPARQL queries. The classification\\nsystem can be modified in some parameters or completely replaced\\nto prioritize the most relevant entities and properties.\\nThis flexibility is crucial for adapting the system to diverse needs\\nand contexts, enabling continuous improvements in accuracy and\\nefficiency. By allowing you to add and remove models, with advances in NLP and AI techniques, the framework ensures it can\\nhandle complex queries, identify relevant entities and relationships,\\nand build accurate SPARQL queries to provide accurate results.\\nThis structured approach increases the accuracy of answers to\\nknowledge-based questions.\\n### **3.4 Metrics Description**\\n\\nTo evaluate the effectiveness of our KBQA system, we used three\\nmetrics that allow us to measure both the precision and coverage\\nof the answers provided: Correct Entities, Correct Properties, and\\nCorret SPARQL. The **Correct Entities** are the number of entities correctly identified in the queries. Identifying entities accurately is crucial because entities represent the key components of a\\nquery. For example, in the query “Which films directed by Quentin\\nTarantino were nominated for an Oscar?”, “Quentin Tarantino” and\\n“Oscar” are entities that need to be correctly identified. The **Cor-**\\n**rect Properties** are the number of properties correctly identified\\nin the queries. Properties describe the relationships or attributes\\nof entities. For example, in the query “Which films directed by\\nQuentin Tarantino were nominated for an Oscar?”, “directed by”\\nand “nominated for” are properties that must be identified accurately. Finally, the **Correct SPARQL** is the percentage of queries\\nwhere the SPARQL query was generated correctly. This metric\\nis critical as it reflects the overall ability of the system to understand and translate natural language queries into correct SPARQL\\nqueries, which are necessary for retrieving accurate information\\nfrom knowledge bases.\\nThese metrics are essential for a comprehensive evaluation of\\nthe KBQA system. They provide a detailed breakdown of where\\nthe system performs well and where there are gaps. For example,\\nhigh entity accuracy but low property accuracy would suggest that\\nwhile the system is good at recognizing subjects, it struggles with\\nunderstanding the relationships between them.\\nBy analyzing metrics such as correct entities and properties, we\\ncan identify specific areas where the system needs improvement.\\nFor example, if many properties are incorrect, we might need to\\nenhance our property extraction algorithms. Consider the query\\n“Which films directed by Quentin Tarantino were nominated for\\n\\n\\n**Table 2: Correct Entities, Properties, and SPARQL queries**\\n**found**\\n\\n**Grou** **p** **Entities** **(** **%** **)** **Pro** **p** **erties** **(** **%** **)** **SPAR** **Q** **L** **(** **%** **)**\\n\\n0 82.32 67.09 62.94\\n\\n1 84.92 86.67 51.67\\n\\n2 76.66 52.47 37.14\\n\\n3 94.30 74.68 73.42\\n\\n4 65.01 54.92 14.29\\n\\n5 81.76 55.97 14.32\\n\\n6 42.62 24.42 0.00\\n\\n7 96.76 51.18 10.03\\n\\n8 97.50 84.38 76.25\\n\\n9 53.96 7.17 2.26\\n\\n10 95.06 66.67 30.86\\n\\n11 55.06 32.91 0.00\\n\\n12 84.46 50.90 14.86\\n\\n**Avera** **g** **e** **77.72** **54.57** **29.85**\\n**Avera** **g** **e w/ filter** **85.87** **64.49** **38.58**\\n\\nan Oscar?”. This query involves identifying the entity “Quentin\\nTarantino” and the properties “directed by” and “nominated for”.\\nThe SPARQL query generated from this should accurately reflect\\nthese components to retrieve the correct films from the knowledge\\nbase.\\n\\n**Dummy Query:**\\n\\nSELECT ?ent WHERE { ?ent DUMMY_P DUMMY_O . ?ent DUMMY_P ?obj\\n\\n} LIMIT DUMMY_F\\n\\n**Complete SPARQL Query:**\\n\\nSELECT ?film WHERE {\\n\\n?film wdt:P57 wd:Q3772. # P57 represents \"directed by\" and\\n\\nQ3772 is Quentin Tarantino\\n\\n?film wdt:P1411 wd:Q19020. # P1411 represents \"nominated\\n\\nfor\" and Q19020 is Oscar\\n\\n} LIMIT 10\\n\\nIn this example, if the entities “Quentin Tarantino” and “Oscar” are not identified correctly, or if the properties “directed by”\\nand “nominated for” are not mapped correctly to their SPARQL\\nrepresentations, the query will fail to return the correct answer.\\n### **4 RESULTS AND DISCUSSION**\\n\\nTable 2 presents detailed results for the different tested tool combinations, highlighting the combination that showed the best performance: DeepPavlov, Falcon, and SpaCy.\\nThe combination of DeepPavlov, Falcon, and SpaCy was selected\\nafter testing with other tools, which generally did not yield good\\nresults. These three tools proved to be the most efficient, complementing each other where one might fail individually. Other\\noptions were discarded due to recurring issues in achieving the\\nnecessary accuracy. Thus, this combination ensured a more reliable\\nand balanced performance. The following sections discuss the main\\nresults observed.\\n\\n\\n95\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Mello et al.\\n\\n### **4.1 Entity Recognition**\\n\\nEntity recognition is crucial for the accuracy of KBQA queries. The\\ncombination of tools was highly effective in identifying entities,\\nachieving an entity accuracy rate of up to 95.06% in some template\\ngroups. This high rate of correct entity identification demonstrates\\nthe robustness of the tools in handling diverse and complex queries.\\nThe average entity accuracy rate was 77.72%, and when filtering the\\ngroups with the greatest difficulty (groups 6, 9, and 11), it increased\\nto 85.87%. However, there is still room for improvement in reducing\\nthe number of missing entities, which can further enhance the\\noverall framework performance.\\n### **4.2 Property Extraction**\\n\\nProperty extraction showed strong performance, with accuracy\\nrates reaching up to 86.67%. Accurately extracting properties is essential for generating precise and relevant answers to user queries.\\nHowever, the average property accuracy rate (54.57%) was lower\\nthan that of entities (77.72%), which can be explained because property extraction depends on the correct identification of entities. If\\nthe entity is incorrect, the property will likely be incorrect. By filtering the groups with the greatest difficulty, the average property\\naccuracy rate increases to 64.49%. The results indicate that the tools\\neffectively understand and extract the required properties from\\nqueries. Continuous refinement of property extraction models will\\nbe beneficial in addressing this gap.\\n### **4.3 Overall Performance**\\n\\nThe overall performance of the KBQA framework, reflected in the\\nSPARQL accuracy rates, varied among different template groups.\\nThe highest SPARQL accuracy achieved was 76.25%, indicating that\\nthe system can generate highly accurate SPARQL queries under\\noptimal conditions. The overall average SPARQL accuracy was\\n29.85%, and when filtering the groups with the greatest difficulty,\\nthis average increases to 38.58%. This demonstrates the potential of\\nthe tools to provide reliable and precise answers to complex queries.\\nHowever, further enhancements in entity and property extraction\\nwill be critical to achieving consistently high performance across\\nall queries.\\n### **4.4 Discussion**\\n\\nThe results obtained have significant implications for the development of the KBQA framework. The 0 accuracy in some groups\\nreflects the difficulty of handling complex questions, where multiple\\ncontextual meanings, intricate relationships between entities, and\\nthe need to infer implicit information make the task challenging.\\nImprovements in context disambiguation and entity extraction are\\nessential, as incorrect identifications lead to inaccurate answers.\\nThese challenges explain why other works often avoid complex\\nquestions, focusing on simpler issues where existing tools are more\\neffective. Additionally, integrating inference mechanisms is crucial\\nto capture implicit contextual information, providing more complete answers. Finally, enhancing scalability and performance is\\nvital to ensure that the framework can handle large volumes of\\ndata.\\n\\nAlthough the combination of DeepPavlov, Falcon, and SpaCy has\\nshown promising results, there is still much room for improvement.\\n\\n\\nSo far, no fine-tuning has been performed on the models used, but\\nthis practice could bring significant improvements, especially in\\nthe groups where we encountered failures. The analysis of the\\nresults highlights key areas for future advancements, which will\\nbe essential for dealing with complex questions and advancing\\nknowledge-based question-answering frameworks.\\n### **5 FINAL REMARKS**\\n\\nThis work addresses the challenges and advancements in developing a practical and flexible pipeline for KBQA. We proposed a\\npipeline that can serve as a model for other KBQA systems, including adaptations to other languages. The pipeline is designed to\\nhandle complex questions involving multiple entities and properties, generating accurate and complete SPARQL queries.\\nThe pipeline allows entity recognition and property extraction\\nmodels to be replaced with new ones that may offer better performance or accuracy. We also implemented refinements in SPARQL\\nqueries, using qualifier constraints for specific properties, increasing the completeness and accuracy of the answers. Additionally, we\\ndeveloped methods to retrieve all properties related to an entity and\\ndynamically fill placeholders in templates with the correct values\\nextracted from the knowledge base.\\nOur structured approach ensures the system can handle complex\\nqueries, decompose them, identify relevant entities and relationships, and construct precise SPARQL queries to provide accurate\\nresults. Entity recognition, relation extraction, and template matching, combined with effective ranking and slot filling, have increased\\nthe accuracy of knowledge-based question answering.\\nThe results highlight the effectiveness of the DeepPavlov, Falcon,\\nand SpaCy combination, which demonstrated superior performance\\nin identifying and extracting entities and properties. However, there\\nis still room for continuous improvement, especially in reducing\\nthe number of unidentified entities and properties. Future research\\nshould improve context disambiguation techniques, enhance property extraction models, refine SPARQL queries for properties and\\nqualifiers, and integrate more advanced inference mechanisms.\\nBy structuring the pipeline in a modular and flexible way, it\\ncan easily integrate new models and rules, ensuring continuous\\nimprovements in accuracy and efficiency. Thus, the pipeline not\\nonly demonstrates how to effectively handle complex queries but\\nalso provides a solid foundation for future adaptations and innovations in the field of KBQA systems. Integrating KBQA with other\\nemerging technologies, such as the Internet of Things (IoT), can\\nopen new possibilities for practical applications, further increasing\\nthe applicability and effectiveness of these systems.\\n### **REFERENCES**\\n\\n[1] Shuang Chen, Qian Liu, Zhiwei Yu, Chin-Yew Lin, Jian-Guang Lou, and Feng\\nJiang. 2021. ReTraCk: A flexible and efficient framework for knowledge base\\nquestion answering. In *Proceedings of the 59th annual meeting of the association*\\n*for computational linguistics and the 11th international joint conference on natural*\\n*language processing: system demonstrations* . 325–336.\\n\\n[2] Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth\\nMurisasco. 2023. Complex QA and language models hybrid architectures, Survey.\\n*arXiv preprint arXiv:2302.09051* (2023).\\n\\n[3] Akshay Kumar Dileep, Anurag Mishra, Ria Mehta, Siddharth Uppal, Jaydeep\\nChakraborty, and Srividya K. Bansal. 2021. Template-based Question Answering\\nanalysis on the LC-QuAD2.0 Dataset. In *2021 IEEE 15th International Conference*\\n*on Semantic Computing (ICSC)* . 443–448. https://doi.org/10.1109/ICSC50631.2021.\\n00079\\n\\n\\n96\\n\\n\\n-----\\n\\nConstructing a KBQA Framework: Design and Implementation WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\n\\n[4] Eleftherios Dimitrakis, Konstantinos Sgontzos, and Yannis Tzitzikas. 2020. A\\nsurvey on question answering systems over linked data and documents. *Journal*\\n*of intelligent information systems* 55 (2020), 233–259.\\n\\n[5] Mohnish Dubey, Debayan Banerjee, Abdelrahman Abdelkawi, and Jens Lehmann.\\n2019. Lc-quad 2.0: A large dataset for complex question answering over wikidata\\nand dbpedia. In *International Semantic Web Conference* . Springer, 69–78.\\n\\n[6] Paolo Ferragina and Ugo Scaiella. 2010. TAGME: On-the-Fly Annotation of\\nShort Text Fragments (by Wikipedia Entities). In *Proceedings of the 19th ACM*\\n*International Conference on Information and Knowledge Management* (Toronto,\\nON, Canada) *(CIKM ’10)* . Association for Computing Machinery, New York, NY,\\nUSA, 1625–1628. https://doi.org/10.1145/1871437.1871689\\n\\n[7] Jorão Gomes, Rômulo Chrispim de Mello, Victor Ströele, and Jairo Francisco de\\nSouza. 2022. A Hereditary Attentive Template-based Approach for Complex\\nKnowledge Base Question Answering Systems. *Expert Systems with Applications*\\n205 (2022), 117725. https://doi.org/10.1016/j.eswa.2022.117725\\n\\n[8] Jorão Gomes Jr, Rômulo Chrispim de Mello, Victor Ströele, and Jairo Francisco\\nde Souza. 2022. A study of approaches to answering complex questions over\\nknowledge bases. *Knowledge and Information Systems* 64, 11 (2022), 2849–2881.\\n\\n[9] Jorão Gomes Jr., Rômulo Chrispim de Mello, Victor Ströele, and Jairo Francisco\\nde Souza. 2021. LC-QuAD 2.1. https://doi.org/10.5281/zenodo.5508297\\n\\n[10] Xixin Hu, Xuan Wu, Yiheng Shu, and Yuzhong Qu. 2022. Logical Form Generation\\nvia Multi-task Learning for Complex Question Answering over Knowledge Bases.\\nIn *Proceedings of the 29th International Conference on Computational Linguistics* .\\nInternational Committee on Computational Linguistics, Gyeongju, Republic of\\nKorea, 1687–1696. https://aclanthology.org/2022.coling-1.145\\n\\n[11] Heewon Jang, Yeongtaek Oh, Seunghee Jin, Haemin Jung, Hyesoo Kong, Dokyung\\nLee, Dongkyu Jeon, and Wooju Kim. 2017. KBQA: Constructing Structured\\nQuery Graph from Keyword Query for Semantic Search. In *Proceedings of the*\\n*International Conference on Electronic Commerce* (Pangyo, Seongnam, Republic of\\nKorea) *(ICEC ’17)* . Association for Computing Machinery, New York, NY, USA,\\nArticle 8, 8 pages. https://doi.org/10.1145/3154943.3154955\\n\\n[12] Yunshi Lan, Gaole He, Jinhao Jiang, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen.\\n2021. A Survey on Complex Knowledge Base Question Answering: Methods,\\nChallenges and Solutions. *CoRR* abs/2105.11644 (2021). arXiv:2105.11644 https:\\n//arxiv.org/abs/2105.11644\\n\\n[13] Nandana Mihindukulasooriya, Gaetano Rossiello, Pavan Kapanipathi, Ibrahim\\nAbdelaziz, Srinivas Ravishankar, Mo Yu, Alfio Gliozzo, Salim Roukos, and\\nAlexander G. Gray. 2020. Leveraging Semantic Parsing for Relation Linking over Knowledge Bases. *CoRR* abs/2009.07726 (2020). arXiv:2009.07726\\nhttps://arxiv.org/abs/2009.07726\\n\\n[14] Saeedeh Momtazi and Zahra Abbasiantaeb. 2022. *Question Answering over Text*\\n*and Knowledge Base* . Springer Nature.\\n\\n[15] Sumit Neelam, Udit Sharma, Hima Karanam, Shajith Ikbal, Pavan Kapanipathi,\\nIbrahim Abdelaziz, Nandana Mihindukulasooriya, Young-Suk Lee, Santosh Srivastava, Cezar Pendus, et al . 2022. A benchmark for generalizable and interpretable temporal question answering over knowledge bases. *arXiv preprint*\\n*arXiv:2201.05793* (2022).\\n\\n[16] Ngonga Ngomo. 2018. 9th challenge on question answering over linked data\\n(QALD-9). *language* 7, 1 (2018), 58–64.\\n\\n[17] Kechen Qin, Yu Wang, Cheng Li, Kalpa Gunaratna, Hongxia Jin, Virgil Pavlu, and\\nJaved A. Aslam. 2020. A Complex KBQA System using Multiple Reasoning Paths.\\n*CoRR* abs/2005.10970 (2020). arXiv:2005.10970 https://arxiv.org/abs/2005.10970\\n\\n[18] Gaetano Rossiello, Nandana Mihindukulasooriya, Ibrahim Abdelaziz, Mihaela\\nBornea, Alfio Gliozzo, Tahira Naseem, and Pavan Kapanipathi. 2021. Generative\\nRelation Linking for Question Answering over Knowledge Bases. In *The Semantic*\\n*Web – ISWC 2021*, Andreas Hotho, Eva Blomqvist, Stefan Dietze, Achille Fokoue,\\nYing Ding, Payam Barnaghi, Armin Haller, Mauro Dragoni, and Harith Alani\\n(Eds.). Springer International Publishing, Cham, 321–337.\\n\\n[19] Ahmad Sakor, Kuldeep Singh, Anery Patel, and Maria-Esther Vidal. 2019.\\nFALCON 2.0: An Entity and Relation Linking Tool over Wikidata. *CoRR*\\nabs/1912.11270 (2019). arXiv:1912.11270 http://arxiv.org/abs/1912.11270\\n\\n[20] Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, and Guilin Qi.\\n2023. Evaluation of ChatGPT as a Question Answering System for Answering\\nComplex Questions. *arXiv preprint arXiv:2303.07992* (2023).\\n\\n[21] Priyansh Trivedi, Gaurav Maheshwari, Mohnish Dubey, and Jens Lehmann. 2017.\\nLc-quad: A corpus for complex question answering over knowledge graphs. In *In-*\\n*ternational Semantic Web Conference* . Springer, Springer International Publishing,\\nCham, 210–218.\\n\\n[22] Ricardo Usbeck, Axel-Cyrille Ngonga Ngomo, Bastian Haarmann, Anastasia\\nKrithara, Michael Röder, and Giulio Napolitano. 2017. 7th open challenge on\\nquestion answering over linked data (QALD-7). In *Semantic Web Challenges: 4th*\\n*SemWebEval Challenge at ESWC 2017, Portoroz, Slovenia, May 28-June 1, 2017,*\\n*Revised Selected Papers* . Springer, 59–69.\\n\\n[23] Zhiwen Xie, Zhao Zeng, Guangyou Zhou, and Tingting He. 2016. Knowledge\\nbase question answering based on deep learning models. In *Natural Language*\\n*Understanding and Intelligent Applications: 5th CCF Conference on Natural Lan-*\\n*guage Processing and Chinese Computing, NLPCC 2016, and 24th International*\\n*Conference on Computer Processing of Oriental Languages, ICCPOL 2016, Kunming,*\\n\\n\\n*China, December 2–6, 2016, Proceedings 24* . Springer, 300–311.\\n\\n[24] Xi Ye, Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou, and Caiming Xiong. 2021.\\nRng-kbqa: Generation augmented iterative ranking for knowledge base question\\nanswering. *arXiv preprint arXiv:2109.08678* (2021).\\n\\n\\n97\\n\\n\\n-----\\n\\n',\n",
       "  'artigo_tokenizado': ['#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Constructing',\n",
       "   'a',\n",
       "   'KBQA',\n",
       "   'Framework',\n",
       "   ':',\n",
       "   'Design',\n",
       "   'and',\n",
       "   'Implementation',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Rômulo',\n",
       "   'Chrispim',\n",
       "   'de',\n",
       "   'Mello',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'romulomello@ice.ufjf.br',\n",
       "   'Department',\n",
       "   'of',\n",
       "   'Computer',\n",
       "   'Science',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   '–',\n",
       "   'MG',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Jairo',\n",
       "   'Francisco',\n",
       "   'de',\n",
       "   'Souza',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'jairo.souza@ice.ufjf.br',\n",
       "   'Department',\n",
       "   'of',\n",
       "   'Computer',\n",
       "   'Science',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   '–',\n",
       "   'MG',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'ABSTRACT',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'The',\n",
       "   'exponential',\n",
       "   'growth',\n",
       "   'of',\n",
       "   'data',\n",
       "   'on',\n",
       "   'the',\n",
       "   'internet',\n",
       "   'has',\n",
       "   'made',\n",
       "   'information',\n",
       "   'retrieval',\n",
       "   'increasingly',\n",
       "   'challenging',\n",
       "   '.',\n",
       "   'Knowledge',\n",
       "   '-',\n",
       "   'based',\n",
       "   'QuestionAnswering',\n",
       "   '(',\n",
       "   'KBQA',\n",
       "   ')',\n",
       "   'framework',\n",
       "   'offers',\n",
       "   'an',\n",
       "   'efficient',\n",
       "   'solution',\n",
       "   'that',\n",
       "   '\\n',\n",
       "   'quickly',\n",
       "   'provides',\n",
       "   'accurate',\n",
       "   'and',\n",
       "   'relevant',\n",
       "   'information',\n",
       "   '.',\n",
       "   'However',\n",
       "   ',',\n",
       "   'these',\n",
       "   '\\n',\n",
       "   'frameworks',\n",
       "   'face',\n",
       "   'significant',\n",
       "   'challenges',\n",
       "   ',',\n",
       "   'especially',\n",
       "   'when',\n",
       "   'dealing',\n",
       "   '\\n',\n",
       "   'with',\n",
       "   'complex',\n",
       "   'queries',\n",
       "   'involving',\n",
       "   'multiple',\n",
       "   'entities',\n",
       "   'and',\n",
       "   'properties',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'This',\n",
       "   'paper',\n",
       "   'studies',\n",
       "   'KBQA',\n",
       "   'frameworks',\n",
       "   ',',\n",
       "   'focusing',\n",
       "   'on',\n",
       "   'improving',\n",
       "   'entity',\n",
       "   'recognition',\n",
       "   ',',\n",
       "   'property',\n",
       "   'extraction',\n",
       "   ',',\n",
       "   'and',\n",
       "   'query',\n",
       "   'generation',\n",
       "   'using',\n",
       "   '\\n',\n",
       "   'advanced',\n",
       "   'Natural',\n",
       "   'Language',\n",
       "   'Processing',\n",
       "   '(',\n",
       "   'NLP',\n",
       "   ')',\n",
       "   'and',\n",
       "   'Artificial',\n",
       "   'Intelligence',\n",
       "   '(',\n",
       "   'AI',\n",
       "   ')',\n",
       "   'techniques',\n",
       "   '.',\n",
       "   'We',\n",
       "   'implemented',\n",
       "   'and',\n",
       "   'evaluated',\n",
       "   'combination',\n",
       "   '\\n',\n",
       "   'tools',\n",
       "   'for',\n",
       "   'extracting',\n",
       "   'entities',\n",
       "   'and',\n",
       "   'properties',\n",
       "   ',',\n",
       "   'with',\n",
       "   'the',\n",
       "   'combination',\n",
       "   '\\n',\n",
       "   'of',\n",
       "   'models',\n",
       "   'achieving',\n",
       "   'the',\n",
       "   'best',\n",
       "   'performance',\n",
       "   '.',\n",
       "   'Our',\n",
       "   'evaluation',\n",
       "   'metrics',\n",
       "   '\\n',\n",
       "   'included',\n",
       "   'entity',\n",
       "   'and',\n",
       "   'property',\n",
       "   'retrieval',\n",
       "   ',',\n",
       "   'SPARQL',\n",
       "   'query',\n",
       "   'completeness',\n",
       "   ',',\n",
       "   'and',\n",
       "   'accuracy',\n",
       "   '.',\n",
       "   'The',\n",
       "   'results',\n",
       "   'demonstrated',\n",
       "   'the',\n",
       "   'effectiveness',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'our',\n",
       "   'approach',\n",
       "   ',',\n",
       "   'with',\n",
       "   'high',\n",
       "   'accuracy',\n",
       "   'rates',\n",
       "   'in',\n",
       "   'identifying',\n",
       "   'entities',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'properties',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'KEYWORDS',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'KBQA',\n",
       "   ',',\n",
       "   'Complex',\n",
       "   'Questions',\n",
       "   ',',\n",
       "   'Entity',\n",
       "   'Recognition',\n",
       "   ',',\n",
       "   'Property',\n",
       "   'Extraction',\n",
       "   ',',\n",
       "   'SPARQL',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   '1',\n",
       "   'INTRODUCTION',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'The',\n",
       "   'large',\n",
       "   'volume',\n",
       "   'of',\n",
       "   'data',\n",
       "   'available',\n",
       "   'on',\n",
       "   'the',\n",
       "   'internet',\n",
       "   'has',\n",
       "   'made',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'task',\n",
       "   'of',\n",
       "   'finding',\n",
       "   'relevant',\n",
       "   'information',\n",
       "   'even',\n",
       "   'more',\n",
       "   'challenging',\n",
       "   '[',\n",
       "   '11',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'In',\n",
       "   'this',\n",
       "   'context',\n",
       "   ',',\n",
       "   'new',\n",
       "   'information',\n",
       "   'retrieval',\n",
       "   'methods',\n",
       "   'have',\n",
       "   'allowed',\n",
       "   '\\n',\n",
       "   'for',\n",
       "   'more',\n",
       "   'intelligent',\n",
       "   'searches',\n",
       "   ',',\n",
       "   'taking',\n",
       "   'into',\n",
       "   'account',\n",
       "   'the',\n",
       "   'context',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'search',\n",
       "   '.',\n",
       "   'Additionally',\n",
       "   ',',\n",
       "   'advances',\n",
       "   'in',\n",
       "   'Natural',\n",
       "   'Language',\n",
       "   'Processing',\n",
       "   '\\n',\n",
       "   '(',\n",
       "   'NLP',\n",
       "   ')',\n",
       "   'research',\n",
       "   'have',\n",
       "   'enabled',\n",
       "   'a',\n",
       "   'better',\n",
       "   'understanding',\n",
       "   'of',\n",
       "   'the',\n",
       "   'search',\n",
       "   '\\n',\n",
       "   'context',\n",
       "   ',',\n",
       "   'allowing',\n",
       "   'Knowledge',\n",
       "   '-',\n",
       "   'based',\n",
       "   'Question',\n",
       "   '-',\n",
       "   'Answering',\n",
       "   '(',\n",
       "   'KBQA',\n",
       "   ')',\n",
       "   '\\n',\n",
       "   'frameworks',\n",
       "   'to',\n",
       "   'emerge',\n",
       "   'as',\n",
       "   'an',\n",
       "   'efficient',\n",
       "   'solution',\n",
       "   'to',\n",
       "   'meet',\n",
       "   'this',\n",
       "   'demand',\n",
       "   '\\n\\n',\n",
       "   '[',\n",
       "   '12',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'The',\n",
       "   'complexity',\n",
       "   'of',\n",
       "   'the',\n",
       "   'questions',\n",
       "   'users',\n",
       "   'ask',\n",
       "   'is',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'main',\n",
       "   '\\n',\n",
       "   'challenges',\n",
       "   'faced',\n",
       "   'by',\n",
       "   'KBQA',\n",
       "   'frameworks',\n",
       "   '[',\n",
       "   '17',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Questions',\n",
       "   'can',\n",
       "   'be',\n",
       "   'complex',\n",
       "   'for',\n",
       "   'various',\n",
       "   'reasons',\n",
       "   ',',\n",
       "   'such',\n",
       "   'as',\n",
       "   'the',\n",
       "   'presence',\n",
       "   'of',\n",
       "   'multiple',\n",
       "   'concepts',\n",
       "   ',',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   ':',\n",
       "   'Proceedings',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Web',\n",
       "   '(',\n",
       "   'WebMe',\n",
       "   '\\n',\n",
       "   'dia’2024',\n",
       "   ')',\n",
       "   '.',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '.',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   ':',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   ',',\n",
       "   '2024',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'SBC',\n",
       "   '–',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '-',\n",
       "   '2753',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Jorão',\n",
       "   'Gomes',\n",
       "   'Jr.',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'jorao.gomes.junior@wu.ac.at',\n",
       "   'Institute',\n",
       "   'for',\n",
       "   'Digital',\n",
       "   'Ecosystems',\n",
       "   'Vienna',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Economics',\n",
       "   'and',\n",
       "   'Business',\n",
       "   'Vienna',\n",
       "   ',',\n",
       "   'Austria',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Victor',\n",
       "   'Ströele',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'victor.stroele@ice.ufjf.br',\n",
       "   'Department',\n",
       "   'of',\n",
       "   'Computer',\n",
       "   'Science',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   '–',\n",
       "   'MG',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '\\n\\n',\n",
       "   'the',\n",
       "   'need',\n",
       "   'to',\n",
       "   'understand',\n",
       "   'the',\n",
       "   'context',\n",
       "   'in',\n",
       "   'which',\n",
       "   'the',\n",
       "   'question',\n",
       "   'is',\n",
       "   'created',\n",
       "   ',',\n",
       "   'and',\n",
       "   'the',\n",
       "   'dependence',\n",
       "   'on',\n",
       "   'additional',\n",
       "   'information',\n",
       "   'that',\n",
       "   'was',\n",
       "   'not',\n",
       "   '\\n',\n",
       "   'mentioned',\n",
       "   'in',\n",
       "   'the',\n",
       "   'question',\n",
       "   '.',\n",
       "   'These',\n",
       "   'factors',\n",
       "   'can',\n",
       "   'lead',\n",
       "   'to',\n",
       "   'ambiguities',\n",
       "   'in',\n",
       "   '\\n',\n",
       "   'questions',\n",
       "   'and',\n",
       "   ',',\n",
       "   'consequently',\n",
       "   ',',\n",
       "   'to',\n",
       "   'imprecise',\n",
       "   'results',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Natural',\n",
       "   'language',\n",
       "   'questions',\n",
       "   'can',\n",
       "   'vary',\n",
       "   'greatly',\n",
       "   'in',\n",
       "   'complexity',\n",
       "   '.',\n",
       "   'Simple',\n",
       "   'questions',\n",
       "   'might',\n",
       "   'involve',\n",
       "   'straightforward',\n",
       "   'retrieval',\n",
       "   'of',\n",
       "   'facts',\n",
       "   ',',\n",
       "   'such',\n",
       "   '\\n',\n",
       "   'as',\n",
       "   '“',\n",
       "   'What',\n",
       "   'is',\n",
       "   'the',\n",
       "   'capital',\n",
       "   'of',\n",
       "   'France',\n",
       "   '?',\n",
       "   '”',\n",
       "   '.',\n",
       "   'However',\n",
       "   ',',\n",
       "   'complex',\n",
       "   'questions',\n",
       "   '\\n',\n",
       "   'often',\n",
       "   'require',\n",
       "   'the',\n",
       "   'integration',\n",
       "   'of',\n",
       "   'multiple',\n",
       "   'pieces',\n",
       "   'of',\n",
       "   'information',\n",
       "   ',',\n",
       "   'reasoning',\n",
       "   'over',\n",
       "   'data',\n",
       "   ',',\n",
       "   'and',\n",
       "   'understanding',\n",
       "   'nuanced',\n",
       "   'context',\n",
       "   '.',\n",
       "   'For',\n",
       "   'example',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'question',\n",
       "   '“',\n",
       "   'Which',\n",
       "   'films',\n",
       "   'directed',\n",
       "   'by',\n",
       "   'Quentin',\n",
       "   'Tarantino',\n",
       "   'were',\n",
       "   '\\n',\n",
       "   'nominated',\n",
       "   'for',\n",
       "   'an',\n",
       "   'Oscar',\n",
       "   '?',\n",
       "   '”',\n",
       "   'requires',\n",
       "   'the',\n",
       "   'system',\n",
       "   'to',\n",
       "   'identify',\n",
       "   'multiple',\n",
       "   '\\n',\n",
       "   'entities',\n",
       "   '(',\n",
       "   '“',\n",
       "   'Quentin',\n",
       "   'Tarantino',\n",
       "   '”',\n",
       "   ',',\n",
       "   '“',\n",
       "   'films',\n",
       "   '”',\n",
       "   ',',\n",
       "   'and',\n",
       "   '“',\n",
       "   'Oscar',\n",
       "   '”',\n",
       "   ')',\n",
       "   'and',\n",
       "   'understand',\n",
       "   '\\n',\n",
       "   'their',\n",
       "   'relationships',\n",
       "   '[',\n",
       "   '23',\n",
       "   ']',\n",
       "   '.',\n",
       "   'The',\n",
       "   'complexity',\n",
       "   'increases',\n",
       "   'when',\n",
       "   'questions',\n",
       "   '\\n',\n",
       "   'involve',\n",
       "   'conditional',\n",
       "   'statements',\n",
       "   ',',\n",
       "   'comparative',\n",
       "   'structures',\n",
       "   ',',\n",
       "   'or',\n",
       "   'temporal',\n",
       "   'aspects',\n",
       "   '.',\n",
       "   'For',\n",
       "   'instance',\n",
       "   ',',\n",
       "   '“',\n",
       "   'What',\n",
       "   'was',\n",
       "   'the',\n",
       "   'population',\n",
       "   'of',\n",
       "   'New',\n",
       "   'York',\n",
       "   '\\n',\n",
       "   'City',\n",
       "   'before',\n",
       "   '2000',\n",
       "   '?',\n",
       "   '”',\n",
       "   'needs',\n",
       "   'temporal',\n",
       "   'reasoning',\n",
       "   'and',\n",
       "   'access',\n",
       "   'to',\n",
       "   'historical',\n",
       "   'data',\n",
       "   '.',\n",
       "   'Similarly',\n",
       "   ',',\n",
       "   'questions',\n",
       "   'like',\n",
       "   '“',\n",
       "   'Is',\n",
       "   'the',\n",
       "   'Eiffel',\n",
       "   'Tower',\n",
       "   'taller',\n",
       "   'than',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'Statue',\n",
       "   'of',\n",
       "   'Liberty',\n",
       "   '?',\n",
       "   '”',\n",
       "   'require',\n",
       "   'comparative',\n",
       "   'reasoning',\n",
       "   'and',\n",
       "   'precise',\n",
       "   '\\n',\n",
       "   'entity',\n",
       "   'linking',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Understanding',\n",
       "   'the',\n",
       "   'context',\n",
       "   'in',\n",
       "   'which',\n",
       "   'a',\n",
       "   'question',\n",
       "   'is',\n",
       "   'asked',\n",
       "   'is',\n",
       "   'crucial',\n",
       "   '\\n',\n",
       "   'for',\n",
       "   'accurate',\n",
       "   'KBQA',\n",
       "   '.',\n",
       "   'The',\n",
       "   'same',\n",
       "   'term',\n",
       "   'can',\n",
       "   'have',\n",
       "   'different',\n",
       "   'meanings',\n",
       "   '\\n',\n",
       "   'based',\n",
       "   'on',\n",
       "   'context',\n",
       "   ',',\n",
       "   'making',\n",
       "   'disambiguation',\n",
       "   'a',\n",
       "   'significant',\n",
       "   'challenge',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'For',\n",
       "   'example',\n",
       "   ',',\n",
       "   'the',\n",
       "   'word',\n",
       "   '“',\n",
       "   'Java',\n",
       "   '”',\n",
       "   'can',\n",
       "   'refer',\n",
       "   'to',\n",
       "   'a',\n",
       "   'programming',\n",
       "   'language',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'an',\n",
       "   'island',\n",
       "   'in',\n",
       "   'Indonesia',\n",
       "   ',',\n",
       "   'or',\n",
       "   'a',\n",
       "   'type',\n",
       "   'of',\n",
       "   'coffee',\n",
       "   '.',\n",
       "   'Determining',\n",
       "   'the',\n",
       "   'correct',\n",
       "   '\\n',\n",
       "   'interpretation',\n",
       "   'based',\n",
       "   'on',\n",
       "   'the',\n",
       "   'surrounding',\n",
       "   'context',\n",
       "   'is',\n",
       "   'essential',\n",
       "   'for',\n",
       "   '\\n',\n",
       "   'providing',\n",
       "   'accurate',\n",
       "   'answers',\n",
       "   '[',\n",
       "   '10',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Moreover',\n",
       "   ',',\n",
       "   'questions',\n",
       "   'often',\n",
       "   'rely',\n",
       "   '\\n',\n",
       "   'on',\n",
       "   'implicit',\n",
       "   'context',\n",
       "   'that',\n",
       "   'is',\n",
       "   'not',\n",
       "   'explicitly',\n",
       "   'stated',\n",
       "   '.',\n",
       "   'For',\n",
       "   'instance',\n",
       "   ',',\n",
       "   'in',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'question',\n",
       "   '“',\n",
       "   'What',\n",
       "   'is',\n",
       "   'the',\n",
       "   'country',\n",
       "   '’s',\n",
       "   'capital',\n",
       "   '?',\n",
       "   '”',\n",
       "   ',',\n",
       "   'the',\n",
       "   'system',\n",
       "   'must',\n",
       "   '\\n',\n",
       "   'infer',\n",
       "   'which',\n",
       "   'country',\n",
       "   'is',\n",
       "   'being',\n",
       "   'referred',\n",
       "   'to',\n",
       "   'from',\n",
       "   'prior',\n",
       "   'context',\n",
       "   'or',\n",
       "   'user',\n",
       "   '\\n',\n",
       "   'interaction',\n",
       "   'history',\n",
       "   '.',\n",
       "   'This',\n",
       "   'requires',\n",
       "   'the',\n",
       "   'KBQA',\n",
       "   'system',\n",
       "   'to',\n",
       "   'maintain',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'utilize',\n",
       "   'contextual',\n",
       "   'information',\n",
       "   'dynamically',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Large',\n",
       "   'language',\n",
       "   'models',\n",
       "   '(',\n",
       "   'LLMs',\n",
       "   ')',\n",
       "   'such',\n",
       "   'as',\n",
       "   'GPT',\n",
       "   'have',\n",
       "   'recently',\n",
       "   'gained',\n",
       "   '\\n',\n",
       "   'popularity',\n",
       "   'as',\n",
       "   'Question',\n",
       "   '-',\n",
       "   'Answering',\n",
       "   '(',\n",
       "   'QA',\n",
       "   ')',\n",
       "   'systems',\n",
       "   ',',\n",
       "   'demonstrating',\n",
       "   '\\n',\n",
       "   'impressive',\n",
       "   'results',\n",
       "   'in',\n",
       "   'answering',\n",
       "   'questions',\n",
       "   'posed',\n",
       "   'in',\n",
       "   'natural',\n",
       "   'language',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'These',\n",
       "   'models',\n",
       "   'are',\n",
       "   'trained',\n",
       "   'on',\n",
       "   'massive',\n",
       "   'amounts',\n",
       "   'of',\n",
       "   'text',\n",
       "   'data',\n",
       "   ',',\n",
       "   'allowing',\n",
       "   '\\n',\n",
       "   'them',\n",
       "   'to',\n",
       "   'understand',\n",
       "   'and',\n",
       "   'generate',\n",
       "   'human',\n",
       "   '-',\n",
       "   'like',\n",
       "   'responses',\n",
       "   'to',\n",
       "   'various',\n",
       "   '\\n',\n",
       "   'prompts',\n",
       "   '[',\n",
       "   '2',\n",
       "   ']',\n",
       "   '.',\n",
       "   'However',\n",
       "   ',',\n",
       "   'it',\n",
       "   'is',\n",
       "   'important',\n",
       "   'to',\n",
       "   'note',\n",
       "   'that',\n",
       "   'QA',\n",
       "   'systems',\n",
       "   '\\n',\n",
       "   'solely',\n",
       "   'using',\n",
       "   'LLMs',\n",
       "   'are',\n",
       "   'limited',\n",
       "   'by',\n",
       "   'their',\n",
       "   'training',\n",
       "   'data',\n",
       "   '[',\n",
       "   '20',\n",
       "   ']',\n",
       "   '.',\n",
       "   'They',\n",
       "   '\\n',\n",
       "   'are',\n",
       "   'not',\n",
       "   'designed',\n",
       "   'to',\n",
       "   'access',\n",
       "   'external',\n",
       "   'knowledge',\n",
       "   'repositories',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'can',\n",
       "   'not',\n",
       "   'provide',\n",
       "   'information',\n",
       "   'not',\n",
       "   'included',\n",
       "   'in',\n",
       "   'their',\n",
       "   ...],\n",
       "  'pos_tagger': '',\n",
       "  'lema': ['construct',\n",
       "   'a',\n",
       "   'kbqa',\n",
       "   'framework',\n",
       "   'design',\n",
       "   'and',\n",
       "   'Implementation',\n",
       "   'Rômulo',\n",
       "   'Chrispim',\n",
       "   'de',\n",
       "   'Mello',\n",
       "   'romulomello@ice.ufjf.br',\n",
       "   'Department',\n",
       "   'of',\n",
       "   'Computer',\n",
       "   'Science',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'MG',\n",
       "   'Brazil',\n",
       "   'Jairo',\n",
       "   'Francisco',\n",
       "   'de',\n",
       "   'Souza',\n",
       "   'jairo.souza@ice.ufjf.br',\n",
       "   'Department',\n",
       "   'of',\n",
       "   'Computer',\n",
       "   'Science',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'MG',\n",
       "   'Brazil',\n",
       "   'ABSTRACT',\n",
       "   'the',\n",
       "   'exponential',\n",
       "   'growth',\n",
       "   'of',\n",
       "   'datum',\n",
       "   'on',\n",
       "   'the',\n",
       "   'internet',\n",
       "   'have',\n",
       "   'make',\n",
       "   'information',\n",
       "   'retrieval',\n",
       "   'increasingly',\n",
       "   'challenging',\n",
       "   'Knowledge',\n",
       "   'base',\n",
       "   'QuestionAnswering',\n",
       "   'KBQA',\n",
       "   'framework',\n",
       "   'offer',\n",
       "   'an',\n",
       "   'efficient',\n",
       "   'solution',\n",
       "   'that',\n",
       "   'quickly',\n",
       "   'provide',\n",
       "   'accurate',\n",
       "   'and',\n",
       "   'relevant',\n",
       "   'information',\n",
       "   'however',\n",
       "   'these',\n",
       "   'framework',\n",
       "   'face',\n",
       "   'significant',\n",
       "   'challenge',\n",
       "   'especially',\n",
       "   'when',\n",
       "   'deal',\n",
       "   'with',\n",
       "   'complex',\n",
       "   'query',\n",
       "   'involve',\n",
       "   'multiple',\n",
       "   'entity',\n",
       "   'and',\n",
       "   'property',\n",
       "   'this',\n",
       "   'paper',\n",
       "   'study',\n",
       "   'kbqa',\n",
       "   'framework',\n",
       "   'focus',\n",
       "   'on',\n",
       "   'improve',\n",
       "   'entity',\n",
       "   'recognition',\n",
       "   'property',\n",
       "   'extraction',\n",
       "   'and',\n",
       "   'query',\n",
       "   'generation',\n",
       "   'use',\n",
       "   'advanced',\n",
       "   'Natural',\n",
       "   'Language',\n",
       "   'Processing',\n",
       "   'NLP',\n",
       "   'and',\n",
       "   'Artificial',\n",
       "   'Intelligence',\n",
       "   'AI',\n",
       "   'technique',\n",
       "   'we',\n",
       "   'implement',\n",
       "   'and',\n",
       "   'evaluate',\n",
       "   'combination',\n",
       "   'tool',\n",
       "   'for',\n",
       "   'extract',\n",
       "   'entity',\n",
       "   'and',\n",
       "   'property',\n",
       "   'with',\n",
       "   'the',\n",
       "   'combination',\n",
       "   'of',\n",
       "   'model',\n",
       "   'achieve',\n",
       "   'the',\n",
       "   'good',\n",
       "   'performance',\n",
       "   'our',\n",
       "   'evaluation',\n",
       "   'metric',\n",
       "   'include',\n",
       "   'entity',\n",
       "   'and',\n",
       "   'property',\n",
       "   'retrieval',\n",
       "   'SPARQL',\n",
       "   'query',\n",
       "   'completeness',\n",
       "   'and',\n",
       "   'accuracy',\n",
       "   'the',\n",
       "   'result',\n",
       "   'demonstrate',\n",
       "   'the',\n",
       "   'effectiveness',\n",
       "   'of',\n",
       "   'our',\n",
       "   'approach',\n",
       "   'with',\n",
       "   'high',\n",
       "   'accuracy',\n",
       "   'rate',\n",
       "   'in',\n",
       "   'identify',\n",
       "   'entity',\n",
       "   'and',\n",
       "   'property',\n",
       "   'keyword',\n",
       "   'KBQA',\n",
       "   'Complex',\n",
       "   'Questions',\n",
       "   'Entity',\n",
       "   'Recognition',\n",
       "   'Property',\n",
       "   'Extraction',\n",
       "   'sparql',\n",
       "   '1',\n",
       "   'introduction',\n",
       "   'the',\n",
       "   'large',\n",
       "   'volume',\n",
       "   'of',\n",
       "   'datum',\n",
       "   'available',\n",
       "   'on',\n",
       "   'the',\n",
       "   'internet',\n",
       "   'have',\n",
       "   'make',\n",
       "   'the',\n",
       "   'task',\n",
       "   'of',\n",
       "   'find',\n",
       "   'relevant',\n",
       "   'information',\n",
       "   'even',\n",
       "   'more',\n",
       "   'challenging',\n",
       "   '11',\n",
       "   'in',\n",
       "   'this',\n",
       "   'context',\n",
       "   'new',\n",
       "   'information',\n",
       "   'retrieval',\n",
       "   'method',\n",
       "   'have',\n",
       "   'allow',\n",
       "   'for',\n",
       "   'more',\n",
       "   'intelligent',\n",
       "   'search',\n",
       "   'take',\n",
       "   'into',\n",
       "   'account',\n",
       "   'the',\n",
       "   'context',\n",
       "   'of',\n",
       "   'the',\n",
       "   'search',\n",
       "   'additionally',\n",
       "   'advance',\n",
       "   'in',\n",
       "   'Natural',\n",
       "   'Language',\n",
       "   'Processing',\n",
       "   'NLP',\n",
       "   'research',\n",
       "   'have',\n",
       "   'enable',\n",
       "   'a',\n",
       "   'well',\n",
       "   'understanding',\n",
       "   'of',\n",
       "   'the',\n",
       "   'search',\n",
       "   'context',\n",
       "   'allow',\n",
       "   'Knowledge',\n",
       "   'base',\n",
       "   'Question',\n",
       "   'answering',\n",
       "   'KBQA',\n",
       "   'framework',\n",
       "   'to',\n",
       "   'emerge',\n",
       "   'as',\n",
       "   'an',\n",
       "   'efficient',\n",
       "   'solution',\n",
       "   'to',\n",
       "   'meet',\n",
       "   'this',\n",
       "   'demand',\n",
       "   '12',\n",
       "   'the',\n",
       "   'complexity',\n",
       "   'of',\n",
       "   'the',\n",
       "   'question',\n",
       "   'user',\n",
       "   'ask',\n",
       "   'be',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'main',\n",
       "   'challenge',\n",
       "   'face',\n",
       "   'by',\n",
       "   'KBQA',\n",
       "   'framework',\n",
       "   '17',\n",
       "   'question',\n",
       "   'can',\n",
       "   'be',\n",
       "   'complex',\n",
       "   'for',\n",
       "   'various',\n",
       "   'reason',\n",
       "   'such',\n",
       "   'as',\n",
       "   'the',\n",
       "   'presence',\n",
       "   'of',\n",
       "   'multiple',\n",
       "   'concept',\n",
       "   'in',\n",
       "   'proceeding',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'web',\n",
       "   'WebMe',\n",
       "   'dia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   '2024',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'sbc',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '2753',\n",
       "   'Jorão',\n",
       "   'Gomes',\n",
       "   'Jr.',\n",
       "   'jorao.gomes.junior@wu.ac.at',\n",
       "   'Institute',\n",
       "   'for',\n",
       "   'Digital',\n",
       "   'Ecosystems',\n",
       "   'Vienna',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Economics',\n",
       "   'and',\n",
       "   'Business',\n",
       "   'Vienna',\n",
       "   'Austria',\n",
       "   'Victor',\n",
       "   'Ströele',\n",
       "   'victor.stroele@ice.ufjf.br',\n",
       "   'Department',\n",
       "   'of',\n",
       "   'Computer',\n",
       "   'Science',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'MG',\n",
       "   'Brazil',\n",
       "   'the',\n",
       "   'need',\n",
       "   'to',\n",
       "   'understand',\n",
       "   'the',\n",
       "   'context',\n",
       "   'in',\n",
       "   'which',\n",
       "   'the',\n",
       "   'question',\n",
       "   'be',\n",
       "   'create',\n",
       "   'and',\n",
       "   'the',\n",
       "   'dependence',\n",
       "   'on',\n",
       "   'additional',\n",
       "   'information',\n",
       "   'that',\n",
       "   'be',\n",
       "   'not',\n",
       "   'mention',\n",
       "   'in',\n",
       "   'the',\n",
       "   'question',\n",
       "   'these',\n",
       "   'factor',\n",
       "   'can',\n",
       "   'lead',\n",
       "   'to',\n",
       "   'ambiguity',\n",
       "   'in',\n",
       "   'question',\n",
       "   'and',\n",
       "   'consequently',\n",
       "   'to',\n",
       "   'imprecise',\n",
       "   'result',\n",
       "   'natural',\n",
       "   'language',\n",
       "   'question',\n",
       "   'can',\n",
       "   'vary',\n",
       "   'greatly',\n",
       "   'in',\n",
       "   'complexity',\n",
       "   'simple',\n",
       "   'question',\n",
       "   'might',\n",
       "   'involve',\n",
       "   'straightforward',\n",
       "   'retrieval',\n",
       "   'of',\n",
       "   'fact',\n",
       "   'such',\n",
       "   'as',\n",
       "   'what',\n",
       "   'be',\n",
       "   'the',\n",
       "   'capital',\n",
       "   'of',\n",
       "   'France',\n",
       "   'however',\n",
       "   'complex',\n",
       "   'question',\n",
       "   'often',\n",
       "   'require',\n",
       "   'the',\n",
       "   'integration',\n",
       "   'of',\n",
       "   'multiple',\n",
       "   'piece',\n",
       "   'of',\n",
       "   'information',\n",
       "   'reason',\n",
       "   'over',\n",
       "   'datum',\n",
       "   'and',\n",
       "   'understand',\n",
       "   'nuanced',\n",
       "   'context',\n",
       "   'for',\n",
       "   'example',\n",
       "   'the',\n",
       "   'question',\n",
       "   'which',\n",
       "   'film',\n",
       "   'direct',\n",
       "   'by',\n",
       "   'Quentin',\n",
       "   'Tarantino',\n",
       "   'be',\n",
       "   'nominate',\n",
       "   'for',\n",
       "   'an',\n",
       "   'Oscar',\n",
       "   'require',\n",
       "   'the',\n",
       "   'system',\n",
       "   'to',\n",
       "   'identify',\n",
       "   'multiple',\n",
       "   'entity',\n",
       "   'Quentin',\n",
       "   'Tarantino',\n",
       "   'film',\n",
       "   'and',\n",
       "   'Oscar',\n",
       "   'and',\n",
       "   'understand',\n",
       "   'their',\n",
       "   'relationship',\n",
       "   '23',\n",
       "   'the',\n",
       "   'complexity',\n",
       "   'increase',\n",
       "   'when',\n",
       "   'question',\n",
       "   'involve',\n",
       "   'conditional',\n",
       "   'statement',\n",
       "   'comparative',\n",
       "   'structure',\n",
       "   'or',\n",
       "   'temporal',\n",
       "   'aspect',\n",
       "   'for',\n",
       "   'instance',\n",
       "   'what',\n",
       "   'be',\n",
       "   'the',\n",
       "   'population',\n",
       "   'of',\n",
       "   'New',\n",
       "   'York',\n",
       "   'City',\n",
       "   'before',\n",
       "   '2000',\n",
       "   'need',\n",
       "   'temporal',\n",
       "   'reasoning',\n",
       "   'and',\n",
       "   'access',\n",
       "   'to',\n",
       "   'historical',\n",
       "   'datum',\n",
       "   'similarly',\n",
       "   'question',\n",
       "   'like',\n",
       "   'be',\n",
       "   'the',\n",
       "   'Eiffel',\n",
       "   'tower',\n",
       "   'tall',\n",
       "   'than',\n",
       "   'the',\n",
       "   'Statue',\n",
       "   'of',\n",
       "   'Liberty',\n",
       "   'require',\n",
       "   'comparative',\n",
       "   'reasoning',\n",
       "   'and',\n",
       "   'precise',\n",
       "   'entity',\n",
       "   'link',\n",
       "   'understand',\n",
       "   'the',\n",
       "   'context',\n",
       "   'in',\n",
       "   'which',\n",
       "   'a',\n",
       "   'question',\n",
       "   'be',\n",
       "   'ask',\n",
       "   'be',\n",
       "   'crucial',\n",
       "   'for',\n",
       "   'accurate',\n",
       "   'kbqa',\n",
       "   'the',\n",
       "   'same',\n",
       "   'term',\n",
       "   'can',\n",
       "   'have',\n",
       "   'different',\n",
       "   'meaning',\n",
       "   'base',\n",
       "   'on',\n",
       "   'context',\n",
       "   'make',\n",
       "   'disambiguation',\n",
       "   'a',\n",
       "   'significant',\n",
       "   'challenge',\n",
       "   'for',\n",
       "   'example',\n",
       "   'the',\n",
       "   'word',\n",
       "   'Java',\n",
       "   'can',\n",
       "   'refer',\n",
       "   'to',\n",
       "   'a',\n",
       "   'programming',\n",
       "   'language',\n",
       "   'an',\n",
       "   'island',\n",
       "   'in',\n",
       "   'Indonesia',\n",
       "   'or',\n",
       "   'a',\n",
       "   'type',\n",
       "   'of',\n",
       "   'coffee',\n",
       "   'determine',\n",
       "   'the',\n",
       "   'correct',\n",
       "   'interpretation',\n",
       "   'base',\n",
       "   'on',\n",
       "   'the',\n",
       "   'surround',\n",
       "   'context',\n",
       "   'be',\n",
       "   'essential',\n",
       "   'for',\n",
       "   'provide',\n",
       "   'accurate',\n",
       "   'answer',\n",
       "   '10',\n",
       "   'moreover',\n",
       "   'question',\n",
       "   'often',\n",
       "   'rely',\n",
       "   'on',\n",
       "   'implicit',\n",
       "   'context',\n",
       "   'that',\n",
       "   'be',\n",
       "   'not',\n",
       "   'explicitly',\n",
       "   'state',\n",
       "   'for',\n",
       "   'instance',\n",
       "   'in',\n",
       "   'the',\n",
       "   'question',\n",
       "   'what',\n",
       "   'be',\n",
       "   'the',\n",
       "   'country',\n",
       "   '’s',\n",
       "   'capital',\n",
       "   'the',\n",
       "   'system',\n",
       "   'must',\n",
       "   'infer',\n",
       "   'which',\n",
       "   'country',\n",
       "   'be',\n",
       "   'be',\n",
       "   'refer',\n",
       "   'to',\n",
       "   'from',\n",
       "   'prior',\n",
       "   'context',\n",
       "   'or',\n",
       "   'user',\n",
       "   'interaction',\n",
       "   'history',\n",
       "   'this',\n",
       "   'require',\n",
       "   'the',\n",
       "   'KBQA',\n",
       "   'system',\n",
       "   'to',\n",
       "   'maintain',\n",
       "   'and',\n",
       "   'utilize',\n",
       "   'contextual',\n",
       "   'information',\n",
       "   'dynamically',\n",
       "   'large',\n",
       "   'language',\n",
       "   'model',\n",
       "   'LLMs',\n",
       "   'such',\n",
       "   'as',\n",
       "   'GPT',\n",
       "   'have',\n",
       "   'recently',\n",
       "   'gain',\n",
       "   'popularity',\n",
       "   'as',\n",
       "   'question',\n",
       "   'answering',\n",
       "   'QA',\n",
       "   'system',\n",
       "   'demonstrate',\n",
       "   'impressive',\n",
       "   'result',\n",
       "   'in',\n",
       "   'answer',\n",
       "   'question',\n",
       "   'pose',\n",
       "   'in',\n",
       "   'natural',\n",
       "   'language',\n",
       "   'these',\n",
       "   'model',\n",
       "   'be',\n",
       "   'train',\n",
       "   'on',\n",
       "   'massive',\n",
       "   'amount',\n",
       "   'of',\n",
       "   'text',\n",
       "   'datum',\n",
       "   'allow',\n",
       "   'they',\n",
       "   'to',\n",
       "   'understand',\n",
       "   'and',\n",
       "   'generate',\n",
       "   'human',\n",
       "   'like',\n",
       "   'response',\n",
       "   'to',\n",
       "   'various',\n",
       "   'prompt',\n",
       "   '2',\n",
       "   'however',\n",
       "   'it',\n",
       "   'be',\n",
       "   'important',\n",
       "   'to',\n",
       "   'note',\n",
       "   'that',\n",
       "   'QA',\n",
       "   'system',\n",
       "   'solely',\n",
       "   'use',\n",
       "   'LLMs',\n",
       "   'be',\n",
       "   'limit',\n",
       "   'by',\n",
       "   'their',\n",
       "   'training',\n",
       "   'datum',\n",
       "   '20',\n",
       "   'they',\n",
       "   'be',\n",
       "   'not',\n",
       "   'design',\n",
       "   'to',\n",
       "   'access',\n",
       "   'external',\n",
       "   'knowledge',\n",
       "   'repository',\n",
       "   'and',\n",
       "   'can',\n",
       "   'not',\n",
       "   'provide',\n",
       "   'information',\n",
       "   'not',\n",
       "   'include',\n",
       "   'in',\n",
       "   'their',\n",
       "   'training',\n",
       "   'datum',\n",
       "   'for',\n",
       "   'instance',\n",
       "   'if',\n",
       "   'a',\n",
       "   'question',\n",
       "   'be',\n",
       "   'ask',\n",
       "   'about',\n",
       "   'an',\n",
       "   'event',\n",
       "   'that',\n",
       "   'occur',\n",
       "   'after',\n",
       "   '89',\n",
       "   'WebMedia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Mello',\n",
       "   'et',\n",
       "   'al',\n",
       "   'the',\n",
       "   'LLM',\n",
       "   '’s',\n",
       "   'training',\n",
       "   'datum',\n",
       "   'be',\n",
       "   'collect',\n",
       "   'it',\n",
       "   'may',\n",
       "   'be',\n",
       "   'unable',\n",
       "   'to',\n",
       "   'provide',\n",
       "   'an',\n",
       "   'accurate',\n",
       "   'answer',\n",
       "   'on',\n",
       "   'the',\n",
       "   'other',\n",
       "   'hand',\n",
       "   'KBQA',\n",
       "   'system',\n",
       "   'be',\n",
       "   'design',\n",
       "   'to',\n",
       "   'access',\n",
       "   'and',\n",
       "   'search',\n",
       "   'knowledge',\n",
       "   'repository',\n",
       "   'provide',\n",
       "   'access',\n",
       "   'to',\n",
       "   'always',\n",
       "   'up',\n",
       "   'todate',\n",
       "   'information',\n",
       "   '14',\n",
       "   'these',\n",
       "   'system',\n",
       "   'can',\n",
       "   'retrieve',\n",
       "   'information',\n",
       "   'from',\n",
       "   'various',\n",
       "   'structured',\n",
       "   'and',\n",
       "   'unstructured',\n",
       "   'data',\n",
       "   'source',\n",
       "   'include',\n",
       "   'database',\n",
       "   'and',\n",
       "   'ontology',\n",
       "   '4',\n",
       "   'with',\n",
       "   'their',\n",
       "   'ability',\n",
       "   'to',\n",
       "   'access',\n",
       "   'a',\n",
       "   'large',\n",
       "   'and',\n",
       "   'constantly',\n",
       "   'update',\n",
       "   'knowledge',\n",
       "   'base',\n",
       "   '15',\n",
       "   'KBQA',\n",
       "   'system',\n",
       "   'be',\n",
       "   'more',\n",
       "   'suitable',\n",
       "   'for',\n",
       "   'answer',\n",
       "   'complex',\n",
       "   'question',\n",
       "   'that',\n",
       "   'require',\n",
       "   'domain',\n",
       "   'specific',\n",
       "   'knowledge',\n",
       "   'or',\n",
       "   'information',\n",
       "   'about',\n",
       "   'recent',\n",
       "   'event',\n",
       "   'Complex',\n",
       "   'Knowledge',\n",
       "   'base',\n",
       "   'Question',\n",
       "   'answering',\n",
       "   'c',\n",
       "   'KBQA',\n",
       "   'system',\n",
       "   'use',\n",
       "   'natural',\n",
       "   'language',\n",
       "   'processing',\n",
       "   'and',\n",
       "   'artificial',\n",
       "   'intelligence',\n",
       "   'technique',\n",
       "   '8',\n",
       "   'natural',\n",
       "   'language',\n",
       "   'processing',\n",
       "   'allow',\n",
       "   'the',\n",
       "   'system',\n",
       "   'to',\n",
       "   'understand',\n",
       "   'and',\n",
       "   'interpret',\n",
       "   'user',\n",
       "   'question',\n",
       "   'while',\n",
       "   'artificial',\n",
       "   'intelligence',\n",
       "   'enable',\n",
       "   'the',\n",
       "   'system',\n",
       "   'to',\n",
       "   'learn',\n",
       "   'the',\n",
       "   'characteristic',\n",
       "   'of',\n",
       "   'those',\n",
       "   'question',\n",
       "   'their',\n",
       "   'entity',\n",
       "   'property',\n",
       "   'and',\n",
       "   'relationship',\n",
       "   'and',\n",
       "   'to',\n",
       "   'return',\n",
       "   'the',\n",
       "   'appropriate',\n",
       "   'value',\n",
       "   'additionally',\n",
       "   'these',\n",
       "   'system',\n",
       "   'have',\n",
       "   'a',\n",
       "   'robust',\n",
       "   'and',\n",
       "   'up',\n",
       "   'to',\n",
       "   'date',\n",
       "   'knowledge',\n",
       "   'base',\n",
       "   'which',\n",
       "   'allow',\n",
       "   'the',\n",
       "   'retrieval',\n",
       "   'and',\n",
       "   'presentation',\n",
       "   'of',\n",
       "   'relevant',\n",
       "   'and',\n",
       "   'current',\n",
       "   'information',\n",
       "   'to',\n",
       "   'user',\n",
       "   '15',\n",
       "   'KBQA',\n",
       "   'system',\n",
       "   'typically',\n",
       "   'involve',\n",
       "   'many',\n",
       "   'step',\n",
       "   'in',\n",
       "   'query',\n",
       "   'processing',\n",
       "   'from',\n",
       "   'entity',\n",
       "   'recognition',\n",
       "   'to',\n",
       "   'SPARQL',\n",
       "   'query',\n",
       "   'generation',\n",
       "   'in',\n",
       "   'this',\n",
       "   'work',\n",
       "   'we',\n",
       "   'propose',\n",
       "   'a',\n",
       "   'KBQA',\n",
       "   'framework',\n",
       "   'that',\n",
       "   'feature',\n",
       "   'a',\n",
       "   'practical',\n",
       "   'and',\n",
       "   'flexible',\n",
       "   'pipeline',\n",
       "   'for',\n",
       "   'complex',\n",
       "   'knowledge',\n",
       "   'base',\n",
       "   'question',\n",
       "   'answer',\n",
       "   'KBQA',\n",
       "   'this',\n",
       "   'framework',\n",
       "   'can',\n",
       "   'serve',\n",
       "   'as',\n",
       "   'a',\n",
       "   'model',\n",
       "   'for',\n",
       "   'other',\n",
       "   'system',\n",
       "   'of',\n",
       "   'this',\n",
       "   'type',\n",
       "   'include',\n",
       "   'adaptation',\n",
       "   'to',\n",
       "   'other',\n",
       "   'language',\n",
       "   'the',\n",
       "   'propose',\n",
       "   'pipeline',\n",
       "   'be',\n",
       "   'design',\n",
       "   'to',\n",
       "   'handle',\n",
       "   'complex',\n",
       "   'question',\n",
       "   'involve',\n",
       "   'multiple',\n",
       "   'entity',\n",
       "   'and',\n",
       "   'property',\n",
       "   'and',\n",
       "   'generate',\n",
       "   'accurate',\n",
       "   'and',\n",
       "   'complete',\n",
       "   'SPARQL',\n",
       "   'query',\n",
       "   'a',\n",
       "   'central',\n",
       "   'feature',\n",
       "   'of',\n",
       "   'this',\n",
       "   'pipeline',\n",
       "   'be',\n",
       "   ...]},\n",
       " {'titulo': 'Automatic Time-aware Recognition of Brazilian Sign Language Based on Dynamic Time Warping',\n",
       "  'informacoes_url': '',\n",
       "  'idioma': 'english',\n",
       "  'storage_key': '../articles/original/english/985-24745-1-10-20240923.pdf',\n",
       "  'author': 'Lucas de S. Arcanjo; Lucas F. Coelho; Silvio Jamil F. Guimarães; Zenilton K. G. do Patrocínio Jr; and Leonardo Vilela Cardoso',\n",
       "  'data_publicacao': '11-09-2024',\n",
       "  'resumo': 'The Brazilian Sign Language (Libras) is a crucial communication medium for the deaf community in Brazil, yet it poses significant challenges for recognition and translation tasks. This paper presents a novel approach using Fast Dynamic Time Warping (FastDTW) [1]  for recognizing Libras signs in video streams. This approach aims to bridge the communication gap between deaf and hearing individuals, enhancing accessibility and reducing social marginalization. The methodology leverages MediaPipe to extract key hand and body landmarks, which are then used to compute angular features for accurate sign recognition. Experiments were conducted on the MINDS-Libras dataset, and the results demonstrated a high recognition accuracy, outperforming traditional methods. Furthermore, when the proposed model is applied to the INCLUDE-50 dataset containing signs from a different sign language, it performs competitively without relying on deep learning techniques. ###',\n",
       "  'keywords': 'Computer Vision, Sign Language Recognition, Gesture Recognition, Dynamic Time Warping, MediaPipe, Libras, Brazilian Sign Language.',\n",
       "  'referencias': ['[1] Sunusi Bala Abdullahi and Kosin Chamnongthai. 2022. American sign language\\nwords recognition using spatio-temporal prosodic and angle features: A sequential learning approach. *IEEE Access* 10 (2022), 15911–15923.',\n",
       "   '[2] Ibrahim Adepoju Adeyanju, Oluwaseyi Olawale Bello, and Mutiu Adesina Adegboye. 2021. Machine learning methods for sign language recognition: A critical\\nreview and analysis. *Intelligent Systems with Applications* 12 (2021), 200056.',\n",
       "   '[3] Nikolaos Arvanitis, Evangelos Sartinas, and Dimitrios Kosmopoulos. 2023.\\nProcrustes-DTW: Dynamic Time Warping Variant for the Recognition of Sign\\nLanguage Utterances. In *2023 IEEE International Conference on Acoustics, Speech,*\\n*and Signal Processing Workshops (ICASSPW)* . IEEE, 1–5.',\n",
       "   '[4] Valentin Bazarevsky, Ivan Grishchenko, Karthik Raveendran, Tyler Zhu, Fan\\nZhang, and Matthias Grundmann. 2020. Blazepose: On-device real-time body\\npose tracking. *arXiv preprint arXiv:2006.10204* (2020).',\n",
       "   '[5] Lucinda Ferreira Brito. 2010. *Por uma gramática de línguas de sinais* . TB-Edições\\nTempo Brasileiro.',\n",
       "   '[6] Juan Cheng, Fulin Wei, Yu Liu, Chang Li, Qiang Chen, and Xun Chen. 2020.\\nChinese Sign Language Recognition Based on DTW-Distance-Mapping Features.\\n*Mathematical Problems in Engineering* 2020, 1 (2020), 8953670.',\n",
       "   '[7] Bruno Costa, Jean Freire, Hamilton Cavalcante, Márcia Homci, Adriana Castro,\\nRaimundo Viégas Jr, Bianchi Meiguins, and Jefferson Morais. 2017. Fault Classification on Transmission Lines Using KNN-DTW. 174–187. https://doi.org/10.\\n1007/978-3-319-62392-4_13',\n",
       "   '[8] Diego RB da Silva, Tiago Maritan U Araujo, Thais Gaudencio do Rêgo, and\\nManuella Aschoff Cavalcanti Brandão. 2020. A Two-Stream Model Based on 3D\\nConvolutional Neural Networks for the Recognition of Brazilian Sign Language\\nin the Health Context. In *Proceedings of the Brazilian Symposium on Multimedia*\\n*and the Web* . 5–12.',\n",
       "   '[9] Giulia Zanon De Castro, Rubia Reis Guerra, and Frederico Gadelha Guimarães.\\n2023. Automatic translation of sign language with multi-stream 3D CNN and\\ngeneration of artificial depth maps. *Expert Systems with Applications* 215 (2023),\\n119394.',\n",
       "   '[10] Edwin Escobedo, Lourdes Ramirez, and Guillermo Camara. 2019. Dynamic Sign\\nLanguage Recognition Based on Convolutional Neural Networks and Texture\\nMaps. In *2019 32nd SIBGRAPI Conference on Graphics, Patterns and Images (SIB-*\\n*GRAPI)* . 265–272. https://doi.org/10.1109/SIBGRAPI.2019.00043',\n",
       "   '[11] Google. 2024. MediaPipe Pose. https://ai.google.dev/edge/mediapipe/solutions/\\nvision/pose_landmarker',\n",
       "   '[12] Rohit J Kate. 2016. Using dynamic time warping distances as features for improved\\ntime series classification. *Data mining and knowledge discovery* 30 (2016), 283–\\n312.',\n",
       "   '[13] Deep R. Kothadiya, Chintan M. Bhatt, T. Saba, A. Rehman, and Saeed Ali Omer Bahaj. 2023. SIGNFORMER: DeepVision Transformer for Sign Language Recognition.\\n*IEEE Access* 11 (2023), 4730–4739. https://doi.org/10.1109/ACCESS.2022.3231130',\n",
       "   '[14] Boon Giin Lee and Su Min Lee. 2017. Smart wearable hand device for sign\\nlanguage interpretation system with sensors fusion. *IEEE Sensors Journal* 18, 3\\n(2017), 1224–1232.',\n",
       "   '[15] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja,\\nMichael Hays, Fan Zhang, Chuo-Ling Chang, Ming Guang Yong, Juhyun Lee,\\net al . 2019. Mediapipe: A framework for building perception pipelines. *arXiv*\\n*preprint arXiv:1906.08172* (2019).',\n",
       "   '[16] Marc Marais, Dane Brown, James Connan, and Alden Boby. 2022. Improving\\nsigner-independence using pose estimation and transfer learning for sign language recognition. In *International Advanced Computing Conference* . Springer,\\n415–428.',\n",
       "   '[17] Syed Atif Mehdi and Yasir Niaz Khan. 2002. Sign language recognition using sensor gloves. In *Proceedings of the 9th International Conference on Neural Information*\\n*Processing, 2002. ICONIP’02.*, Vol. 5. IEEE, 2204–2206.',\n",
       "   '[18] Ferda Ofli, Rizwan Chaudhry, Gregorij Kurillo, René Vidal, and Ruzena Bajcsy.\\n2014. Sequence of the most informative joints (smij): A new representation for\\nhuman skeletal action recognition. *Journal of Visual Communication and Image*\\n*Representation* 25, 1 (2014), 24–38.\\n\\n\\n78\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Arcanjo et al.',\n",
       "   '[19] Luiza Maria Borges Oliveira. 2012. *Cartilha do Censo 2010 – Pessoas com Deficiên-*\\n*cia* . Secretaria de Direitos Humanos da Presidência da República, Brasília.',\n",
       "   '[20] Wesley L Passos, Gabriel M Araujo, Jonathan N Gois, and Amaro A de Lima. 2021.\\nA gait energy image-based system for Brazilian sign language recognition. *IEEE*\\n*Transactions on Circuits and Systems I: Regular Papers* 68, 11 (2021), 4761–4771.',\n",
       "   '[21] Razieh Rastgoo, Kourosh Kiani, and Sergio Escalera. 2021. Sign language recognition: A deep survey. *Expert Systems with Applications* 164 (2021), 113794.',\n",
       "   '[22] Tamires Martins Rezende, Sílvia Grasiella Moreira Almeida, and Frederico Gadelha Guimarães. 2021. Development and validation of a Brazilian\\nsign language database for human gesture recognition. *Neural Computing and*\\n*Applications* 33, 16 (01 Aug 2021), 10449–10467. https://doi.org/10.1007/s00521021-05802-4',\n",
       "   '[23] Hiroaki Sakoe and Seibi Chiba. 1978. Dynamic programming algorithm optimization for spoken word recognition. *IEEE transactions on acoustics, speech, and*\\n*signal processing* 26, 1 (1978), 43–49.',\n",
       "   '[24] Stan Salvador and Philip Chan. 2007. Toward accurate dynamic time warping in\\nlinear time and space. *Intelligent Data Analysis* 11, 5 (2007), 561–580.',\n",
       "   '[25] Maria Fernanda Neves Silveira de Souza, Amanda Miranda Brito Araújo, Luiza\\nFernandes Fonseca Sandes, Daniel Antunes Freitas, Wellington Danilo Soares,\\nRaquel Schwenck de Mello Vianna, and Árlen Almeida Duarte de Sousa. 2017.\\nPrincipais dificuldades e obstáculos enfrentados pela comunidade surda no acesso\\nà saúde: uma revisão integrativa de literatura. *Revista Cefac* 19 (2017), 395–405.',\n",
       "   '[26] Advaith Sridhar, Rohith Gandhi Ganesan, Pratyush Kumar, and Mitesh Khapra.\\n2020. Include: A large scale dataset for indian sign language recognition. In\\n*Proceedings of the 28th ACM international conference on multimedia* . 1366–1375.',\n",
       "   '[27] Barathi Subramanian, Bekhzod Olimov, Shraddha M Naik, Sangchul Kim, KilHoum Park, and Jeonghong Kim. 2022. An integrated mediapipe-optimized GRU\\nmodel for Indian sign language recognition. *Scientific Reports* 12, 1 (2022), 11964.',\n",
       "   '[28] Jimin Tan, Jianan Yang, Sai Wu, Gang Chen, and Jake Zhao. 2021. A critical look\\nat the current train/test split in machine learning. *ArXiv preprint ArXiv:2106.04525*\\n(2021).',\n",
       "   '[29] Akshit Tayade and Swapnil Patil. 2021. Real-time Vernacular Sign Language\\nRecognition using MediaPipe and Machine Learning. *International Journal of*\\n*Research Publication and Reviews* 2, 5 (2021), 9–17. https://doi.org/10.13140/RG.2.\\n2.32364.03203',\n",
       "   '[30] Ankita Wadhawan and Parteek Kumar. 2020. Deep learning-based sign language\\nrecognition system for static signs. *Neural Computing and Applications* 32 (2020),\\n7957 – 7968. https://doi.org/10.1007/s00521-019-04691-y',\n",
       "   '[31] Tzu-Tsung Wong. 2015. Performance evaluation of classification algorithms\\nby k-fold and leave-one-out cross validation. *Pattern Recognition* 48, 9 (2015),\\n2839–2846. https://doi.org/10.1016/j.patcog.2015.03.009',\n",
       "   '[32] Fan Zhang, Valentin Bazarevsky, Andrey Vakunov, Andrei Tkachenka, George\\nSung, Chuo-Ling Chang, and Matthias Grundmann. 2020. Mediapipe hands:\\nOn-device real-time hand tracking. *arXiv preprint arXiv:2006.10214* (2020).\\n\\n\\n79\\n\\n\\n-----'],\n",
       "  'text': '# **Automatic Time-aware Recognition of Brazilian Sign Language** **Based on Dynamic Time Warping**\\n\\n## Lucas de S. Arcanjo\\n#### larcanjo@sga.pucminas.br Laboratory of Image and Multimedia Data Science (IMScience), Pontifícia Universidade Católica de Minas Gerais (PUC Minas) Belo Horizonte, Minas Gerais\\n\\n## Lucas F. Coelho\\n#### lucas.coelho.1296135@sga.pucminas.br Laboratory of Image and Multimedia Data Science (IMScience), Pontifícia Universidade Católica de Minas Gerais (PUC Minas) Belo Horizonte, Minas Gerais\\n\\n## Silvio Jamil F. Guimarães\\n#### sjamil@pucminas.br Laboratory of Image and Multimedia Data Science (IMScience), Pontifícia Universidade Católica de Minas Gerais (PUC Minas) Belo Horizonte, Minas Gerais\\n\\n## Zenilton K. G. do Patrocínio Jr\\n#### zenilton@pucminas.br Laboratory of Image and Multimedia Data Science (IMScience), Pontifícia Universidade Católica de Minas Gerais (PUC Minas) Belo Horizonte, Minas Gerais\\n### **ABSTRACT**\\n\\nThe Brazilian Sign Language (Libras) is a crucial communication\\nmedium for the deaf community in Brazil, yet it poses significant\\nchallenges for recognition and translation tasks. This paper presents\\na novel approach using Fast Dynamic Time Warping (FastDTW) [1]\\n\\nfor recognizing Libras signs in video streams. This approach aims\\nto bridge the communication gap between deaf and hearing individuals, enhancing accessibility and reducing social marginalization.\\nThe methodology leverages MediaPipe to extract key hand and\\nbody landmarks, which are then used to compute angular features\\nfor accurate sign recognition. Experiments were conducted on the\\nMINDS-Libras dataset, and the results demonstrated a high recognition accuracy, outperforming traditional methods. Furthermore,\\nwhen the proposed model is applied to the INCLUDE-50 dataset\\ncontaining signs from a different sign language, it performs competitively without relying on deep learning techniques.\\n### **KEYWORDS**\\n\\nComputer Vision, Sign Language Recognition, Gesture Recognition, Dynamic Time Warping, MediaPipe, Libras, Brazilian Sign\\nLanguage.\\n### **1 INTRODUCTION**\\n\\nThe Brazilian Sign Language (Libras [2] ) recognition is a complex\\nresearch field that has attracted considerable interest in computer\\nvision and multimedia communities [ 2, 21 ]. One of the challenges in\\nLibras recognition task is the content description of signers based\\n\\n1 Code available on https://github.com/IMScience-PPGINF-PucMinas/libras-signrecognition\\n2 In Brazilian Portuguese – Língua Brasileira de Sinais\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n© 2024 SBC – Brazilian Computing Society.\\nISSN 2966-2753\\n\\n## Leonardo Vilela Cardoso\\n#### leonardocardoso@pucminas.br Laboratory of Image and Multimedia Data Science (IMScience), Pontifícia Universidade Católica de Minas Gerais (PUC Minas) Belo Horizonte, Minas Gerais\\n\\non ground truth (GT) annotations created by multiple individuals. The variability introduced by multiple annotators often results\\nin a GT containing diverse perspectives of the events depicted in\\nthe signer’s video, thereby highlighting different body movements\\naccording to the annotators’ fluency in Libras [ 9, 22 ]. According\\nto the Brazilian Institute of Geography and Statistics – IBGE [ 19 ],\\nabout 10 million people have hearing problems, with approximately\\n3 million being completely deaf and living in Brazil. While communication applications and tools have been widely developed in\\nrecent decades, deaf people face numerous problems using these\\ntechnologies. Outside the technological field, communication barriers also manifest, and this is the greatest difficulty in providing\\nservices to hearing-impaired individuals [21, 25].\\nThe communication barrier faced by deaf individuals hinders\\nequitable access to essential services. A system utilizing pattern\\nrecognition techniques could bring significant benefits to communication between deaf and hearing people, facilitating interaction in\\nvarious contexts and contributing to the reduction of the marginalization of this community.\\nTwo different strategies can be applied in sign language recognition: device-based and computer vision-based methods [ 2, 21 ].\\nDevice-based approaches utilize specialized hardware such as data\\ngloves, depth-sensing cameras, and other wearable sensors to capture sign language gestures [ 22 ]. These devices can provide precise\\ndata but often at the cost of user comfort and affordability. On\\nthe other hand, computer vision-based methods leverage regular\\ncameras or webcams to capture gestures, offering a more natural\\nand cost-effective solution [ 8 ]. These methods often employ neural\\nnetworks and diverse machine-learning techniques to recognize\\nsigns.\\nA key distinction across computer vision works is the use of\\nspatio-temporal features. Some of them rely exclusively on images.\\nFor this task, Convolutional Neural Networks (CNNs) have shown\\nhigh accuracy in recognizing static signs, achieving up to 99.90%\\naccuracy on grayscale images [ 13, 30 ]. However, in video, temporal\\n\\n\\n72\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Arcanjo et al.\\n\\ndata processing is critical for sign recognition. The order of the\\ngestures is crucial, and the static processing of an image does not\\nsolve the real communication problem.\\nThe main idea of this study is to enhance the accuracy of recognizing Libras signs from video streams and translating them\\ninto Portuguese words using FastDTW techniques. This task involves several subtasks, including signs interpretation for hands,\\nfingers, torso, and the positioning of phalanges within images. Consequently, this work seeks to identify specific Libras gestures within\\nvideo sequences and accurately map them to their corresponding\\nPortuguese words, thereby bridging the two languages. Additionally, a central component of this research is to evaluate the effectiveness of FastDTW in this translation process. The goal is not\\nonly to assess the technique’s feasibility but also to optimize its\\npractical application for facilitating communication between Libras\\nusers and Portuguese speakers.\\nThis paper is structured as follows. The theoretical background is\\npresented in Section 2, while Section 3 discusses the related works.\\nSection 4 details the proposed method, while Section 5 presents and\\nanalyzes the results. Finally, Section 6 draws some conclusions.\\n### **2 BACKGROUND** **2.1 Dynamic Time Warping**\\n\\n\\nDynamic Time Warping (DTW) is a technique for measuring the\\nsimilarity between time series, introduced by Sakoe and Chiba [23]\\ninitially for speech applications. Time series analysis is a critical\\ntask in various domains. Accurate measurement of the similarity\\nbetween time series is essential for classification, clustering, and\\nanomaly detection. Two common techniques for measuring similarity between time series are Euclidean Distance (ED) and DTW.\\nWhile ED is simple and efficient, it has significant limitations that\\nDTW addresses more effectively.\\nFigure 1 demonstrates the difference between ED and DTW.\\nComparing the two signals, it is possible to observe the distortion\\ncaused by ED, whereas DTW tends to capture the temporal relationships between the two compared series. ED is a straightforward\\nmethod for calculating the similarity between time series. Given\\ntime series *𝑋* = [ *𝑥* 1 *,𝑥* 2 *, . . .,𝑥* *𝑛* ] and *𝑌* = [ *𝑦* 1 *,𝑦* 2 *, . . .,𝑦* *𝑛* ] of equal\\nlength *𝑛*, the ED is defined as:\\n\\n\\n**Figure 1: Comparison between DTW and ED [7], in which** *𝑑*\\n**represents a distance between two signs, as degree difference,**\\n**and** *𝑡* **the time variation**\\n\\n.\\n\\nA warping path *𝑊* is defined as a sequence of matrix elements\\nrepresenting a mapping between *𝑋* and *𝑌* :\\n\\n*𝑊* = [ *𝑤* 1 *,𝑤* 2 *, . . .,𝑤* *𝐾* ] *,* where *𝑤* *𝑘* = ( *𝑖, 𝑗* ) *.* (3)\\n\\nThe warping path must satisfy the following conditions: (i) *bound-*\\n*ary condition* : the path starts at the bottom-left corner and ends at\\nthe top-right corner of the matrix; (ii) *continuity* : the path steps\\nmust be contiguous; and (iii) *monotonicity* : the path indices must\\nbe non-decreasing. The idea is to find the path that minimizes the\\ncumulative distance:\\n\\n\\nThe optimal path is determined using dynamic programming.\\nThe recursive formula to fill the cost matrix is:\\n\\n*𝐷* ( *𝑖, 𝑗* ) = ( *𝑥* *𝑖* − *𝑦* *𝑗* ) [2] +min{ *𝐷* ( *𝑖* −1 *, 𝑗* ) *, 𝐷* ( *𝑖, 𝑗* −1) *, 𝐷* ( *𝑖* −1 *, 𝑗* −1)} *.* (5)\\n\\nThe FastDTW algorithm, introduced by [ 24 ], is an optimized\\nversion of the original DTW. Unlike DTW, which requires filling\\nthe entire cost matrix and has a computational complexity of *𝑂* ( *𝑁* [2] ),\\nFastDTW achieves *𝑂* ( *𝑁* ) complexity by strategically reducing the\\nnumber of calculations needed to fill the cost matrix. This significant\\nreduction in computational overhead makes FastDTW much faster.\\nThe FastDTW algorithm comprises three key operations: (i) coarsening, which shrinks the time series into a smaller representation\\nby averaging adjacent pairs of points, effectively halving the size\\nof the series; (ii) projection, which uses the warp path from a lower\\n\\n\\n�� *𝐾*\\n\\n*𝐷* ( *𝑤* *𝑘* ) *.* (4)\\n\\n� *𝑘* =1\\n∑︁\\n\\n\\n*𝐸𝐷* ( *𝑋,𝑌* ) =\\n\\n\\n~~�~~ *𝑛*\\n\\n( *𝑥* *𝑖* − *𝑦* *𝑖* ) [2] *.* (1)\\n\\n� *𝑖* =1\\n∑︁\\n\\n\\n*𝐷𝑇𝑊* ( *𝑋,𝑌* ) = min\\n*𝑊*\\n\\n\\nWhile ED is computationally efficient, it is sensitive to shifts and\\ndistortions in the time axis. Thus, if one time series is a slightly\\nshifted version of another, ED may indicate a large dissimilarity,\\neven if the series appears visually similar.\\nDTW was designed to overcome the limitations of ED by allowing for elastic shifting along the time axis. This makes DTW\\nmore robust to variations in time series that are misaligned or\\nhave different lengths [ 12 ]. Given time series *𝑋* = [ *𝑥* 1 *,𝑥* 2 *, . . .,𝑥* *𝑛* ]\\nand *𝑌* = [ *𝑦* 1 *,𝑦* 2 *, . . .,𝑦* *𝑚* ], the DTW algorithm constructs an *𝑛* × *𝑚*\\ncost matrix *𝐷* where each element *𝐷* ( *𝑖, 𝑗* ) represents the squared\\ndistance between points *𝑥* *𝑖* and *𝑦* *𝑗* :\\n\\n*𝐷* ( *𝑖, 𝑗* ) = ( *𝑥* *𝑖* − *𝑦* *𝑗* ) [2] *.* (2)\\n\\n\\n73\\n\\n\\n-----\\n\\nAutomatic Time-aware Recognition of Brazilian Sign Language Based on Dynamic Time Warping WebMedia’2024, Juiz de Fora, Brazil\\n\\nresolution as an initial guess for the higher resolution; and (iii) refinement, which fine-tunes the projected warp path through local\\nadjustments controlled by a radius parameter.\\n### **2.2 MediaPipe**\\n\\n\\nMediaPipe is a comprehensive framework developed by Google that\\nenables developers to create multi-modal, cross-platform applied\\nmachine learning (ML) pipelines. Designed to handle various data\\ntypes – including video, audio, and time series – MediaPipe is a\\nversatile tool for numerous applications. It is renowned for its robust\\ncollection of human body detection and tracking models, trained\\non some of the most extensive and diverse datasets available [15].\\nMediaPipe presents a hand-tracking solution to recognize and extract landmarks from RGB images. The first model detects the palm\\nwithin an image and provides an accurately cropped palm image,\\nwhich is then passed to the landmark model. This step reduces the\\nneed for extensive data augmentation, such as rotations, flipping,\\nand scaling, and focuses the model’s power on precise landmark localization. The Hand Landmark model processes the detected palm\\nregions to precisely localize 21 three-dimensional hand-knuckle\\ncoordinates (x, y, z), as shown in Figure 2. This model maps coordinates even to partially visible hands and does not incorporate\\ninformation regarding facial modifications [32].\\nThe pose solution aims to detect and track the human body’s\\nskeletal structure. The model identifies 33 key landmarks on the\\nbody, including the face, shoulders, elbows, wrists, hips, knees, and\\nankles. This comprehensive set of landmarks allows for detailed\\nand accurate body pose estimation, which is crucial for applications\\nsuch as fitness tracking, augmented reality, and animation [4].\\nFigure 3 presents the tracking of MediaPipe Holistic with Hands\\nand Pose used to detect landmarks on a frame, combining the capabilities of hand tracking, pose estimation, and face mesh into a\\nsingle unified pipeline. This simultaneous tracking of the body pose,\\nhand movements, and facial expressions provides a comprehensive\\nunderstanding of human motion and interaction.\\n\\n**Figure 2: Hand landmarks identified by MediaPipe. Each**\\n**number represents a finger joint [11].**\\n\\n\\n**Figure 3: MediaPipe Holistic with Hands and Pose enabled**\\n**to detect landmarks on a video frame.**\\n### **3 RELATED WORKS**\\n\\nSome techniques have been developed for sign language recognition\\nusing different strategies and datasets. In early works, data gloves\\nwith embedded sensors captured intricate finger and hand movements and orientations. These gloves provide precise measurements,\\nunaffected by external agents such as light or magnetic fields. However, data gloves are often uncomfortable and expensive, limiting\\ntheir suitability for extended use, despite their accuracy [ 14, 17, 21 ].\\nDepth-sensing cameras like the Microsoft Kinect and Leap Motion Controller capture both RGB and depth information, allowing\\nfor more detailed gesture analysis. Adeyanju et al . [2] demonstrated\\nthe effectiveness of these devices in sign language recognition, noting their ability to capture fine-grained details of hand movements\\nand spatial orientation. Despite that, these devices also add to the\\noverall cost and complexity of the system.\\nDynamic videos incorporating movements for signs have been\\nwidely used in computer vision-based methods. De Castro et al . [9]\\nused RGB cameras along with the MINDS-Libras dataset and 3D\\nCNNs to extract spatial and temporal features through 3D convolution operations, achieving an average accuracy of 91%. In addition to\\nusing RGB cameras, more advanced techniques incorporate depth\\nsensors and infrared cameras to enhance recognition accuracy. Escobedo et al . [10] suggested a method that combines RGB-D data\\nwith texture maps to capture hand location and movement. This\\napproach integrates multimodal data into a three-stream CNN architecture, allowing for robust feature extraction and achieving\\nsuperior performance compared to traditional RGB-based methods.\\nTheir system demonstrated improvements in recognizing dynamic\\nsigns, highlighting the advantages of incorporating depth informa\\ntion.\\n\\nMethods utilizing MediaPipe have gained attention for their realtime capabilities and ease of deployment [ 16, 27 ]. Tayade and Patil\\n\\n[29] conducted a study on real-time letter recognition using MediaPipe with Support Vector Machine (SVM). They utilized datasets\\nfrom American, Indian, Italian, and Turkish sign languages for\\ntraining and evaluation, achieving an average accuracy of 99%.\\nOne of the common challenges in creating Sign Language Recognition (SLR) models is the lack of extensive and high-quality datasets\\n\\n\\n74\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Arcanjo et al.\\n\\n\\nfor certain sign languages [ 22 ]. Training models that extensively\\nuse deep learning techniques can be computationally expensive\\ndue to high data dependency. In this way, DTW is an effective strategy for this task, as it measures the similarity between temporal\\nsequences based on their distance [ 3, 28 ]. Cheng et al . [6] utilized\\nDTW for distance mapping in Chinese sign language recognition.\\nThey combined SVM and DTW, achieving an accuracy of 99.03%\\nin a dataset of 11 signs, demonstrating the effectiveness of this\\napproach for recognizing complex signs.\\nThe literature on SLR reveals that various approaches and techniques have been employed in different contexts and scenarios.\\nAmong the related works, the use of MediaPipe stands out, demonstrating promising results in real-time gesture detection with high\\naccuracy. These studies provide important insights and useful perspectives that contribute to the development and improvement of\\nthis work. Considering the lessons learned from related works, the\\nproposed project aims to use MediaPipe to extract relevant gesture features and employ a DTW-based model to recognize a set of\\nsigns, thereby improving communication between deaf and hearing\\nindividuals.\\n### **4 METHODS**\\n\\nSign language is not merely a collection of simple gestures; rather, it\\nis a complex linguistic system defined by multiple parameters that\\nallow for the encoding of a broad range of meanings. According\\nto Brito [5], the primary parameters in sign language include hand\\nconfiguration, articulation point, and movement.\\nHand configuration refers to the distinct shapes that hands can\\nassume to generate signs. The specific shapes and configurations of\\nthe hands can vary widely between different sign languages. The\\narticulation point or location involves the space in front of the body\\n(neutral space) or specific body regions, such as the head, waist, and\\nshoulders, where signs are articulated. The location of the sign is\\ncrucial as it provides context and meaning. Movement is a complex\\nparameter involving various forms and directions, including pulsing\\nmotion, movements of the finger joints, and directional movements\\nin space. The displacement of the hands, fingers, and arms over\\ntime plays a significant role in conveying the sign’s meaning.\\nDepending on the context, some parameters might not be necessary for interpretation. For instance, *hand orientation* refers to the\\ndirection of the palm during the sign, which can face up, down, towards the body, forward, left, or right. Orientation helps distinguish\\nbetween signs that may have similar hand configurations and movements. Facial expressions and other non-manual expressions are\\nessential for providing additional context and emphasis. They can\\nconvey emotions and differentiate between types of sentences such\\nas affirmative, interrogative, exclamatory, and negative statements.\\nFigure 4 shows a flowchart of the proposed Libras recognition\\nframework based on landmarks extraction and computing distances\\nbetween multiple time series. In the first step, the video input frame\\nis cropped using a simple region of interest (ROI) to remove potential noise caused by the borders of the videos during the recognition\\ntask. This reduction effectively trims 10% of the vertical distance\\non each video frame.\\n\\nNext, we extract the landmarks using MediaPipe Holistic. At\\nthis stage, we execute with *𝑚𝑖𝑛* _ *𝑑𝑒𝑡𝑒𝑐𝑡𝑖𝑜𝑛* _ *𝑐𝑜𝑛𝑓𝑖𝑑𝑒𝑛𝑐𝑒* = 0 *.* 5 and\\n\\n\\n*𝑚𝑖𝑛* _ *𝑡𝑟𝑎𝑐𝑘𝑖𝑛𝑔* _ *𝑐𝑜𝑛𝑓𝑖𝑑𝑒𝑛𝑐𝑒* = 0 *.* 5, utilizing the Hands and Pose models. The hand model provides 21 three-dimensional landmarks for\\neach hand present in a frame, while the pose model provides 33\\nlandmarks indicating the positions of the body.\\nTo process the results from the landmarks, we flatten all the\\nlandmarks provided by MediaPipe Holistic, resulting in a single\\narray of coordinates. We clean up the data by filling possible null\\nvalues with 0. This step is important to avoid any noise during\\ndetection, as blurred images (during rapid movements) can lead to\\nissues in the detection [27].\\nWith the landmarks ready, we execute the feature extraction process on the landmarks data. Figure 5 illustrates the hand landmarks\\nidentified by MediaPipe. These points are used to calculate the angles between finger joints, allowing the model to recognize different\\nsigns based on the configuration of fingers and hands. Each sign\\ncan be considered a composition of different poses, where each pose\\nis characterized by a particular set of angles. This concept has been\\napplied in several works in human action recognition [ 1, 18 ]. Each\\nfeature vector is composed of hands angles with several points, and\\npose angles. To calculate the value of an angle *𝜔* from the landmark\\n3D values we can use the dot product :\\n\\n\\n�\\n\\n\\n*𝜔* = arccos\\n\\n\\n�\\n\\n\\n*𝐵𝐶* - *𝐶𝐷*\\n\\n| *𝐵𝐶* || *𝐶𝐷* |\\n\\n\\n(6)\\n\\n\\nin which *𝐵𝐶* and *𝐶𝐷* are segments composing a joint.\\nOnce the feature vectors are defined, we use an unsupervised\\nlearning approach to provide the necessary information to the\\nmodel before the tests. Leave-One-Person-Out Cross-Validation\\n\\n(LOPOCV) is a validation technique that is well-suited for scenarios\\nthat involve gesture recognition tasks [ 31 ]. We divide the dataset so\\nthat one individual’s data is completely left out of the training set\\nand used exclusively for testing. This process is repeated for each\\nindividual in the dataset, ensuring that each person’s data is used\\nas a test set exactly once. The model is trained on the remaining\\nindividuals’ data during each iteration.\\nThe model uses FastDTW to compute the distance between time\\nseries and perform sign recognition. This approach compares the\\nsign to be recognized with all known signs and measures the distance among them. The FastDTW algorithm is executed with the\\nradius parameter set to 1, which defines the size of the neighborhood when expanding the path. After computing all distances, the\\nclosest known sign name is used in the output of the task.\\n### **5 RESULTS**\\n\\nWe evaluated the proposed method on MINDS-Libras [ 22 ] and\\nINCLUDE-50 [ 26 ] datasets. The MINDS-Libras dataset was used to\\noptimize the model, including adjustments to the region of interest\\nin the videos and configuring the model to better recognize the\\nsigns used. Once the model was established, it was applied to both\\ndatasets to test in different scenarios and compare to the state-ofthe-art results.\\n\\nThe MINDS-Libras dataset [ 22 ] consists of 1,200 data sequences\\ndistributed into 20 classes. Each class represents a distinct sign\\nfrom Libras, including both static and dynamic signs. The signs\\nwere captured using two types of sensors: a Canon EOS Rebel t5i\\nDSLR camera and a Microsoft Kinect v2 sensor. As a result, each\\n\\n\\n75\\n\\n\\n-----\\n\\nAutomatic Time-aware Recognition of Brazilian Sign Language Based on Dynamic Time Warping WebMedia’2024, Juiz de Fora, Brazil\\n\\n**Figure 4: Outline of the proposed method for Libras recognition.**\\n\\n\\nrecording contains a 1920 × 1080 RGB video (captured by the DSLR\\ncamera) and 640 × 480 RGB-D data (captured by the Kinect sensor),\\nboth recorded at 30 frames per second (fps). However, we used only\\nthe full HD RGB data in this work.\\n\\n**Figure 5: The feature extracted from the hand: joint angles**\\n**and fingertip positions. The blue points indicate the finger-**\\n**tip positions on which the 3D displacements are computed.**\\n**The red points indicate the joints on which the angles are**\\n**computed.**\\n\\n\\nWe used overall accuracy and F1-score as performance metrics\\nto evaluate our model. Accuracy is the proportion of true positive\\n( *𝑇𝑃* ) and true negative ( *𝑇𝑁* ) results among the total number of\\ncases examined. It is calculated using the formula:\\n\\n*𝑇𝑃* + *𝑇𝑁*\\nAccuracy = (7)\\n*𝑇𝑃* + *𝑇𝑁* + *𝐹𝑃* + *𝐹𝑁*\\n\\nin which *𝐹𝑃* and *𝐹𝑁* represent false positives and false negatives,\\nrespectively. The F1-score is the harmonic mean of precision and\\nrecall, balancing the two metrics. It is calculated as:\\n\\nF1-Score = 2 × [Precision][ ×][ Recall] (8)\\n\\nPrecision + Recall\\n\\nPrecision is the ratio of correctly predicted positive observations\\nto the total predicted positives, while Recall is the ratio of correctly\\npredicted positive observations to all observations of the actual\\nclass.\\n\\nTable 1 shows the models’ performance metrics. The results in\\nTable 1 were obtained using the Leave-One-Person-Out Cross-Vali-da-tion (LOOCV) technique, a method proposed by De Castro\\net al . [9] . The signers from the dataset were divided into 12 groups,\\nwith 11 for validation and 1 for testing. This approach is applied\\nto achieve an accuracy of 0.86 ± 0.08, using only RGB data as\\ninput to the network and without employing deep learning or data\\naugmentation to improve the results. Additionally, the results from\\nDe Castro et al . [9] and Passos et al . [20], both of which use data\\naugmentation, are presented for comparison. Despite not using\\ndata augmentation, our approach continues to yield competitive\\n\\n\\n76\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Arcanjo et al.\\n\\n\\n**Table 1: Comparison of methods for recognizing signs using**\\n**MINDS-Libras dataset.**\\n\\nMethod Aug Dataset Accuracy F1-Score\\n\\nDe Castro et al. [9] Yes MINDS-Libras 0.91 ± 0.07 0.90\\nPassos et al. [20] Yes MINDS-Libras 0.85 ± 0.02  \\nOurs No MINDS-Libras 0.86 ± 0.08 0.87\\n\\n**Figure 6: Confusion matrix across all runs on the MINDS-**\\n**Libras dataset.**\\n\\nresults, primarily because it does not require a large amount of\\nnewly produced information for training.\\nThe confusion matrix in Figure 6 illustrates the performance of\\nour model across all runs on the MINDS-Libras dataset. The matrix\\n\\nshows that some signs have a higher rate of confusion. For example,\\n“Yellow” was misclassified as “Candy” in 0.16 of the cases, and\\n“Bathroom,” which has the lowest accuracy at 0.47, was misclassified\\nas “To Happen” in 0.25 of the cases. These areas highlight where\\nadditional contextual features, such as facial expressions and the\\nposition of hands relative to the face, could improve recognition\\n\\naccuracy.\\nTable 2 shows the best case for precision, recall, and F1-score\\nachieved by Signer 12 using the LOOCV method. The model demonstrates an overall accuracy of 0.96 achieving perfect scores in precision, recall, and F1-score for 16 classes, indicating that it recognized\\nthese signs without any false positives or false negatives.\\nHowever, certain signs, such as “Bank”, “Frog”, “Bathroom”, and\\n“Bad”, show lower precision and recall values. These lower precision and recall values can be attributed to the similarity in hand\\nconfigurations or movements with other signs. For example, “Bank”\\nis confused with “Bad” due to the high similarity between the signs\\nand the fact that the model considers the angles formed by the trunk\\nand hands but not the position of the hand relative to the face, which\\nhinders the precise identification of certain signs. Another example\\nis “Frog” being confused with “Bathroom”, which could have been\\n\\n\\n**Table 2: Best case for precision, recall, and F1-score for each**\\n**class (Signer 12) using the MINDS-Libras dataset with the**\\n**LOOCV method.**\\n\\nPrecision Recall F1-Score Support\\n\\nTo happen 1.00 1.00 1.00 5\\nBank 0.71 1.00 0.83 5\\n\\nMirror 1.00 1.00 1.00 5\\n\\nTo know 1.00 1.00 1.00 5\\n\\nFive 1.00 1.00 1.00 5\\n\\nApple 1.00 1.00 1.00 5\\n\\nCorner 1.00 1.00 1.00 5\\n\\nBathroom 1.00 0.60 0.75 5\\n\\nStudent 1.00 1.00 1.00 5\\n\\nFrog 0.71 1.00 0.83 5\\nTo enjoy 1.00 1.00 1.00 5\\nTo know 1.00 1.00 1.00 5\\n\\nFear 1.00 1.00 1.00 5\\n\\nAmerica 1.00 1.00 1.00 5\\n\\nWill 1.00 1.00 1.00 5\\n\\nYellow 1.00 1.00 1.00 5\\n\\nVaccine 1.00 1.00 1.00 5\\n\\nSon 1.00 1.00 1.00 5\\n\\nNoise 1.00 1.00 1.00 5\\n\\nBad 1.00 0.60 0.75 5\\n\\nAccuracy 0.96 100\\n\\navoided if the model had considered facial expressions during sign\\nidentification.\\nTo compare our model with the proposed model for validating\\nthe MINDS dataset, it was necessary to use the same separation\\ntechnique proposed in MINDS, with a random split of 75% of the\\ndataset used for training and 25% used for validation. An important\\npoint to highlight about this technique is that it can cause overfitting\\nin the model, which can hinder the development of the model and its\\nability to generalize. This issue is mentioned in De Castro et al . [9],\\nwhich discusses overfitting and proposes the previously mentioned\\nLOOCV technique. Table 3 shows the results using the random split;\\nour model achieved an accuracy of 0.98 ± 0.01, which is superior\\nto that obtained by the model proposed in Rezende et al . [22] . It is\\nimportant to note that our model does not use data augmentation\\ntechniques, which are often used to artificially increase the size and\\nvariability of the training data to improve model performance.\\nThe INCLUDE-50 dataset, compiled by Sridhar et al . [26], comprises 263 distinct classes of signs in Indian Sign Language (ISL).\\nThese classes are organized into 15 categories including clothes,\\ncolors, adjectives, and pronouns. Each signer performs each sign\\n\\n**Table 3: Comparison of methods for recognizing Libras signs**\\n**using a 75% training and 25% testing split utilizing MINDS**\\n**dataset.**\\n\\nMethod Aug Dataset Accuracy F1-Score\\n\\nRezende et al. [22] Yes MINDS-Libras 0.93 ± 0.02 0.93\\n\\nOurs No MINDS-Libras 0.98 ± 0.01 0.98\\n\\n\\n77\\n\\n\\n-----\\n\\nAutomatic Time-aware Recognition of Brazilian Sign Language Based on Dynamic Time Warping WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\n**Table 4: Comparison of methods for recognizing signs using**\\n**INCLUDE-50 dataset.**\\n\\nMethod Aug Dataset Accuracy\\n\\nDe Castro et al. [9] Yes INCLUDE-50 0.95\\nSridhar et al. [26] Yes INCLUDE-50 0.94\\nSridhar et al. [26] No INCLUDE-50 0.74\\n\\nOurs No INCLUDE-50 0.90\\n\\n2 to 6 times, resulting in 4,287 videos. For quick evaluation, the\\nINCLUDE-50 subset, also created by Sridhar et al . [26], includes 50\\nsign categories with 958 videos. In our study, we utilize this subset\\nto validate our methodology. Sridhar et al . [26] previously defined\\nthe videos that make up the training and test sets. We employed\\nthe same division proposed by them.\\nThe results for the INCLUDE-50 dataset are shown in Table 4.\\n\\nThe proposed model achieved an accuracy of 0.90, demonstrating\\nthe effectiveness of our approach in different sign languages and\\nvalidating its applicability in various scenarios. The comparison\\nwith other methods, such as the work by De Castro et al . [9] and\\nSridhar et al . [26], shows that our approach performs competitively,\\nparticularly in terms of accuracy, without relying on deep learning\\ntechniques, especially when compared to the method proposed\\nby Sridhar et al . [26] without data augmentation, achieving an\\nimprovement of approximately 21.5%.\\nThese findings highlight the robustness and generalizability of\\nour proposed method across different datasets and sign languages.\\nThe use of data augmentation has a significant impact on the performance of models. For instance, Sridhar et al . [26] achieved an accuracy of 0.94 with augmentation, but only 0.74 without it. Despite\\nnot using data augmentation, our model achieved a competitive\\naccuracy of 0.90, showcasing the effectiveness of our approach.\\nFurther studies can build upon this work to explore more sophisticated models and techniques, aiming for even higher accuracy and\\nbroader applicability in real-world scenarios.\\n### **6 CONCLUSION**\\n\\nThis research presents an unsupervised method for sign recognition.\\nThe model that uses FastDTW, together with angle measurement,\\ndemonstrated good generalization capacity across two datasets,\\nwithout the use of data augmentation. Our approach uses visual\\ninformation to describe signs without the analysis of depth sensors\\nor gloves, facilitating its dissemination to different languages and\\ndatasets.\\n\\nFor future work, we plan to incorporate data augmentation techniques, which have shown significant improvements in accuracy.\\nOne possible model enhancement is to include the position of the\\nhand relative to the face as a feature to be measured, as well as\\nidentifying contextual cues such as facial expressions and other\\nparts of the body. This can be essential for understanding the sign\\nand further improving the model’s accuracy. Another aspect we\\nintend to address is the weighting of angles, allowing more important angles to have greater influence, thereby aiding the model’s\\nidentification process.\\n\\n### **ACKNOWLEDGMENTS**\\n\\nThe authors thank the Pontifícia Universidade Católica de Minas\\n\\nGerais – PUC-Minas, Coordenação de Aperfeiçoamento de Pessoal\\nde Nível Superior – CAPES (Grant PROAP 88887.842889/2023-00\\n– PUC/MG, Grant PDPG 88887.708960/2022-00 – PUC/MG - Informática, Grant STIC-AMSUD 88887.878869/2023-00 and Finance\\nCode 001), the Conselho Nacional de Desenvolvimento Científico\\ne Tecnológico – CNPq (Grants 407242/2021-0, 306573/2022-9 and\\n442950/2023-3), and Fundação de Apoio à Pesquisa do Estado de\\nMinas Gerais – FAPEMIG (Grant APQ-01079-23).\\n### **REFERENCES**\\n\\n[1] Sunusi Bala Abdullahi and Kosin Chamnongthai. 2022. American sign language\\nwords recognition using spatio-temporal prosodic and angle features: A sequential learning approach. *IEEE Access* 10 (2022), 15911–15923.\\n\\n[2] Ibrahim Adepoju Adeyanju, Oluwaseyi Olawale Bello, and Mutiu Adesina Adegboye. 2021. Machine learning methods for sign language recognition: A critical\\nreview and analysis. *Intelligent Systems with Applications* 12 (2021), 200056.\\n\\n[3] Nikolaos Arvanitis, Evangelos Sartinas, and Dimitrios Kosmopoulos. 2023.\\nProcrustes-DTW: Dynamic Time Warping Variant for the Recognition of Sign\\nLanguage Utterances. In *2023 IEEE International Conference on Acoustics, Speech,*\\n*and Signal Processing Workshops (ICASSPW)* . IEEE, 1–5.\\n\\n[4] Valentin Bazarevsky, Ivan Grishchenko, Karthik Raveendran, Tyler Zhu, Fan\\nZhang, and Matthias Grundmann. 2020. Blazepose: On-device real-time body\\npose tracking. *arXiv preprint arXiv:2006.10204* (2020).\\n\\n[5] Lucinda Ferreira Brito. 2010. *Por uma gramática de línguas de sinais* . TB-Edições\\nTempo Brasileiro.\\n\\n[6] Juan Cheng, Fulin Wei, Yu Liu, Chang Li, Qiang Chen, and Xun Chen. 2020.\\nChinese Sign Language Recognition Based on DTW-Distance-Mapping Features.\\n*Mathematical Problems in Engineering* 2020, 1 (2020), 8953670.\\n\\n[7] Bruno Costa, Jean Freire, Hamilton Cavalcante, Márcia Homci, Adriana Castro,\\nRaimundo Viégas Jr, Bianchi Meiguins, and Jefferson Morais. 2017. Fault Classification on Transmission Lines Using KNN-DTW. 174–187. https://doi.org/10.\\n1007/978-3-319-62392-4_13\\n\\n[8] Diego RB da Silva, Tiago Maritan U Araujo, Thais Gaudencio do Rêgo, and\\nManuella Aschoff Cavalcanti Brandão. 2020. A Two-Stream Model Based on 3D\\nConvolutional Neural Networks for the Recognition of Brazilian Sign Language\\nin the Health Context. In *Proceedings of the Brazilian Symposium on Multimedia*\\n*and the Web* . 5–12.\\n\\n[9] Giulia Zanon De Castro, Rubia Reis Guerra, and Frederico Gadelha Guimarães.\\n2023. Automatic translation of sign language with multi-stream 3D CNN and\\ngeneration of artificial depth maps. *Expert Systems with Applications* 215 (2023),\\n119394.\\n\\n[10] Edwin Escobedo, Lourdes Ramirez, and Guillermo Camara. 2019. Dynamic Sign\\nLanguage Recognition Based on Convolutional Neural Networks and Texture\\nMaps. In *2019 32nd SIBGRAPI Conference on Graphics, Patterns and Images (SIB-*\\n*GRAPI)* . 265–272. https://doi.org/10.1109/SIBGRAPI.2019.00043\\n\\n[11] Google. 2024. MediaPipe Pose. https://ai.google.dev/edge/mediapipe/solutions/\\nvision/pose_landmarker\\n\\n[12] Rohit J Kate. 2016. Using dynamic time warping distances as features for improved\\ntime series classification. *Data mining and knowledge discovery* 30 (2016), 283–\\n312.\\n\\n[13] Deep R. Kothadiya, Chintan M. Bhatt, T. Saba, A. Rehman, and Saeed Ali Omer Bahaj. 2023. SIGNFORMER: DeepVision Transformer for Sign Language Recognition.\\n*IEEE Access* 11 (2023), 4730–4739. https://doi.org/10.1109/ACCESS.2022.3231130\\n\\n[14] Boon Giin Lee and Su Min Lee. 2017. Smart wearable hand device for sign\\nlanguage interpretation system with sensors fusion. *IEEE Sensors Journal* 18, 3\\n(2017), 1224–1232.\\n\\n[15] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja,\\nMichael Hays, Fan Zhang, Chuo-Ling Chang, Ming Guang Yong, Juhyun Lee,\\net al . 2019. Mediapipe: A framework for building perception pipelines. *arXiv*\\n*preprint arXiv:1906.08172* (2019).\\n\\n[16] Marc Marais, Dane Brown, James Connan, and Alden Boby. 2022. Improving\\nsigner-independence using pose estimation and transfer learning for sign language recognition. In *International Advanced Computing Conference* . Springer,\\n415–428.\\n\\n[17] Syed Atif Mehdi and Yasir Niaz Khan. 2002. Sign language recognition using sensor gloves. In *Proceedings of the 9th International Conference on Neural Information*\\n*Processing, 2002. ICONIP’02.*, Vol. 5. IEEE, 2204–2206.\\n\\n[18] Ferda Ofli, Rizwan Chaudhry, Gregorij Kurillo, René Vidal, and Ruzena Bajcsy.\\n2014. Sequence of the most informative joints (smij): A new representation for\\nhuman skeletal action recognition. *Journal of Visual Communication and Image*\\n*Representation* 25, 1 (2014), 24–38.\\n\\n\\n78\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Arcanjo et al.\\n\\n\\n\\n[19] Luiza Maria Borges Oliveira. 2012. *Cartilha do Censo 2010 – Pessoas com Deficiên-*\\n*cia* . Secretaria de Direitos Humanos da Presidência da República, Brasília.\\n\\n[20] Wesley L Passos, Gabriel M Araujo, Jonathan N Gois, and Amaro A de Lima. 2021.\\nA gait energy image-based system for Brazilian sign language recognition. *IEEE*\\n*Transactions on Circuits and Systems I: Regular Papers* 68, 11 (2021), 4761–4771.\\n\\n[21] Razieh Rastgoo, Kourosh Kiani, and Sergio Escalera. 2021. Sign language recognition: A deep survey. *Expert Systems with Applications* 164 (2021), 113794.\\n\\n[22] Tamires Martins Rezende, Sílvia Grasiella Moreira Almeida, and Frederico Gadelha Guimarães. 2021. Development and validation of a Brazilian\\nsign language database for human gesture recognition. *Neural Computing and*\\n*Applications* 33, 16 (01 Aug 2021), 10449–10467. https://doi.org/10.1007/s00521021-05802-4\\n\\n[23] Hiroaki Sakoe and Seibi Chiba. 1978. Dynamic programming algorithm optimization for spoken word recognition. *IEEE transactions on acoustics, speech, and*\\n*signal processing* 26, 1 (1978), 43–49.\\n\\n[24] Stan Salvador and Philip Chan. 2007. Toward accurate dynamic time warping in\\nlinear time and space. *Intelligent Data Analysis* 11, 5 (2007), 561–580.\\n\\n[25] Maria Fernanda Neves Silveira de Souza, Amanda Miranda Brito Araújo, Luiza\\nFernandes Fonseca Sandes, Daniel Antunes Freitas, Wellington Danilo Soares,\\nRaquel Schwenck de Mello Vianna, and Árlen Almeida Duarte de Sousa. 2017.\\nPrincipais dificuldades e obstáculos enfrentados pela comunidade surda no acesso\\nà saúde: uma revisão integrativa de literatura. *Revista Cefac* 19 (2017), 395–405.\\n\\n\\n\\n[26] Advaith Sridhar, Rohith Gandhi Ganesan, Pratyush Kumar, and Mitesh Khapra.\\n2020. Include: A large scale dataset for indian sign language recognition. In\\n*Proceedings of the 28th ACM international conference on multimedia* . 1366–1375.\\n\\n[27] Barathi Subramanian, Bekhzod Olimov, Shraddha M Naik, Sangchul Kim, KilHoum Park, and Jeonghong Kim. 2022. An integrated mediapipe-optimized GRU\\nmodel for Indian sign language recognition. *Scientific Reports* 12, 1 (2022), 11964.\\n\\n[28] Jimin Tan, Jianan Yang, Sai Wu, Gang Chen, and Jake Zhao. 2021. A critical look\\nat the current train/test split in machine learning. *ArXiv preprint ArXiv:2106.04525*\\n(2021).\\n\\n[29] Akshit Tayade and Swapnil Patil. 2021. Real-time Vernacular Sign Language\\nRecognition using MediaPipe and Machine Learning. *International Journal of*\\n*Research Publication and Reviews* 2, 5 (2021), 9–17. https://doi.org/10.13140/RG.2.\\n2.32364.03203\\n\\n[30] Ankita Wadhawan and Parteek Kumar. 2020. Deep learning-based sign language\\nrecognition system for static signs. *Neural Computing and Applications* 32 (2020),\\n7957 – 7968. https://doi.org/10.1007/s00521-019-04691-y\\n\\n[31] Tzu-Tsung Wong. 2015. Performance evaluation of classification algorithms\\nby k-fold and leave-one-out cross validation. *Pattern Recognition* 48, 9 (2015),\\n2839–2846. https://doi.org/10.1016/j.patcog.2015.03.009\\n\\n[32] Fan Zhang, Valentin Bazarevsky, Andrey Vakunov, Andrei Tkachenka, George\\nSung, Chuo-Ling Chang, and Matthias Grundmann. 2020. Mediapipe hands:\\nOn-device real-time hand tracking. *arXiv preprint arXiv:2006.10214* (2020).\\n\\n\\n79\\n\\n\\n-----\\n\\n',\n",
       "  'artigo_tokenizado': ['#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Automatic',\n",
       "   'Time',\n",
       "   '-',\n",
       "   'aware',\n",
       "   'Recognition',\n",
       "   'of',\n",
       "   'Brazilian',\n",
       "   'Sign',\n",
       "   'Language',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Based',\n",
       "   'on',\n",
       "   'Dynamic',\n",
       "   'Time',\n",
       "   'Warping',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Lucas',\n",
       "   'de',\n",
       "   'S.',\n",
       "   'Arcanjo',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'larcanjo@sga.pucminas.br',\n",
       "   'Laboratory',\n",
       "   'of',\n",
       "   'Image',\n",
       "   'and',\n",
       "   'Multimedia',\n",
       "   'Data',\n",
       "   'Science',\n",
       "   '(',\n",
       "   'IMScience',\n",
       "   ')',\n",
       "   ',',\n",
       "   'Pontifícia',\n",
       "   'Universidade',\n",
       "   'Católica',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   '(',\n",
       "   'PUC',\n",
       "   'Minas',\n",
       "   ')',\n",
       "   'Belo',\n",
       "   'Horizonte',\n",
       "   ',',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Lucas',\n",
       "   'F.',\n",
       "   'Coelho',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'lucas.coelho.1296135@sga.pucminas.br',\n",
       "   'Laboratory',\n",
       "   'of',\n",
       "   'Image',\n",
       "   'and',\n",
       "   'Multimedia',\n",
       "   'Data',\n",
       "   'Science',\n",
       "   '(',\n",
       "   'IMScience',\n",
       "   ')',\n",
       "   ',',\n",
       "   'Pontifícia',\n",
       "   'Universidade',\n",
       "   'Católica',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   '(',\n",
       "   'PUC',\n",
       "   'Minas',\n",
       "   ')',\n",
       "   'Belo',\n",
       "   'Horizonte',\n",
       "   ',',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Silvio',\n",
       "   'Jamil',\n",
       "   'F.',\n",
       "   'Guimarães',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'sjamil@pucminas.br',\n",
       "   'Laboratory',\n",
       "   'of',\n",
       "   'Image',\n",
       "   'and',\n",
       "   'Multimedia',\n",
       "   'Data',\n",
       "   'Science',\n",
       "   '(',\n",
       "   'IMScience',\n",
       "   ')',\n",
       "   ',',\n",
       "   'Pontifícia',\n",
       "   'Universidade',\n",
       "   'Católica',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   '(',\n",
       "   'PUC',\n",
       "   'Minas',\n",
       "   ')',\n",
       "   'Belo',\n",
       "   'Horizonte',\n",
       "   ',',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Zenilton',\n",
       "   'K.',\n",
       "   'G.',\n",
       "   'do',\n",
       "   'Patrocínio',\n",
       "   'Jr',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'zenilton@pucminas.br',\n",
       "   'Laboratory',\n",
       "   'of',\n",
       "   'Image',\n",
       "   'and',\n",
       "   'Multimedia',\n",
       "   'Data',\n",
       "   'Science',\n",
       "   '(',\n",
       "   'IMScience',\n",
       "   ')',\n",
       "   ',',\n",
       "   'Pontifícia',\n",
       "   'Universidade',\n",
       "   'Católica',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   '(',\n",
       "   'PUC',\n",
       "   'Minas',\n",
       "   ')',\n",
       "   'Belo',\n",
       "   'Horizonte',\n",
       "   ',',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'ABSTRACT',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'The',\n",
       "   'Brazilian',\n",
       "   'Sign',\n",
       "   'Language',\n",
       "   '(',\n",
       "   'Libras',\n",
       "   ')',\n",
       "   'is',\n",
       "   'a',\n",
       "   'crucial',\n",
       "   'communication',\n",
       "   '\\n',\n",
       "   'medium',\n",
       "   'for',\n",
       "   'the',\n",
       "   'deaf',\n",
       "   'community',\n",
       "   'in',\n",
       "   'Brazil',\n",
       "   ',',\n",
       "   'yet',\n",
       "   'it',\n",
       "   'poses',\n",
       "   'significant',\n",
       "   '\\n',\n",
       "   'challenges',\n",
       "   'for',\n",
       "   'recognition',\n",
       "   'and',\n",
       "   'translation',\n",
       "   'tasks',\n",
       "   '.',\n",
       "   'This',\n",
       "   'paper',\n",
       "   'presents',\n",
       "   '\\n',\n",
       "   'a',\n",
       "   'novel',\n",
       "   'approach',\n",
       "   'using',\n",
       "   'Fast',\n",
       "   'Dynamic',\n",
       "   'Time',\n",
       "   'Warping',\n",
       "   '(',\n",
       "   'FastDTW',\n",
       "   ')',\n",
       "   '[',\n",
       "   '1',\n",
       "   ']',\n",
       "   '\\n\\n',\n",
       "   'for',\n",
       "   'recognizing',\n",
       "   'Libras',\n",
       "   'signs',\n",
       "   'in',\n",
       "   'video',\n",
       "   'streams',\n",
       "   '.',\n",
       "   'This',\n",
       "   'approach',\n",
       "   'aims',\n",
       "   '\\n',\n",
       "   'to',\n",
       "   'bridge',\n",
       "   'the',\n",
       "   'communication',\n",
       "   'gap',\n",
       "   'between',\n",
       "   'deaf',\n",
       "   'and',\n",
       "   'hearing',\n",
       "   'individuals',\n",
       "   ',',\n",
       "   'enhancing',\n",
       "   'accessibility',\n",
       "   'and',\n",
       "   'reducing',\n",
       "   'social',\n",
       "   'marginalization',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'The',\n",
       "   'methodology',\n",
       "   'leverages',\n",
       "   'MediaPipe',\n",
       "   'to',\n",
       "   'extract',\n",
       "   'key',\n",
       "   'hand',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'body',\n",
       "   'landmarks',\n",
       "   ',',\n",
       "   'which',\n",
       "   'are',\n",
       "   'then',\n",
       "   'used',\n",
       "   'to',\n",
       "   'compute',\n",
       "   'angular',\n",
       "   'features',\n",
       "   '\\n',\n",
       "   'for',\n",
       "   'accurate',\n",
       "   'sign',\n",
       "   'recognition',\n",
       "   '.',\n",
       "   'Experiments',\n",
       "   'were',\n",
       "   'conducted',\n",
       "   'on',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'MINDS',\n",
       "   '-',\n",
       "   'Libras',\n",
       "   'dataset',\n",
       "   ',',\n",
       "   'and',\n",
       "   'the',\n",
       "   'results',\n",
       "   'demonstrated',\n",
       "   'a',\n",
       "   'high',\n",
       "   'recognition',\n",
       "   'accuracy',\n",
       "   ',',\n",
       "   'outperforming',\n",
       "   'traditional',\n",
       "   'methods',\n",
       "   '.',\n",
       "   'Furthermore',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'when',\n",
       "   'the',\n",
       "   'proposed',\n",
       "   'model',\n",
       "   'is',\n",
       "   'applied',\n",
       "   'to',\n",
       "   'the',\n",
       "   'INCLUDE-50',\n",
       "   'dataset',\n",
       "   '\\n',\n",
       "   'containing',\n",
       "   'signs',\n",
       "   'from',\n",
       "   'a',\n",
       "   'different',\n",
       "   'sign',\n",
       "   'language',\n",
       "   ',',\n",
       "   'it',\n",
       "   'performs',\n",
       "   'competitively',\n",
       "   'without',\n",
       "   'relying',\n",
       "   'on',\n",
       "   'deep',\n",
       "   'learning',\n",
       "   'techniques',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'KEYWORDS',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Computer',\n",
       "   'Vision',\n",
       "   ',',\n",
       "   'Sign',\n",
       "   'Language',\n",
       "   'Recognition',\n",
       "   ',',\n",
       "   'Gesture',\n",
       "   'Recognition',\n",
       "   ',',\n",
       "   'Dynamic',\n",
       "   'Time',\n",
       "   'Warping',\n",
       "   ',',\n",
       "   'MediaPipe',\n",
       "   ',',\n",
       "   'Libras',\n",
       "   ',',\n",
       "   'Brazilian',\n",
       "   'Sign',\n",
       "   '\\n',\n",
       "   'Language',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   '1',\n",
       "   'INTRODUCTION',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'The',\n",
       "   'Brazilian',\n",
       "   'Sign',\n",
       "   'Language',\n",
       "   '(',\n",
       "   'Libras',\n",
       "   '[',\n",
       "   '2',\n",
       "   ']',\n",
       "   ')',\n",
       "   'recognition',\n",
       "   'is',\n",
       "   'a',\n",
       "   'complex',\n",
       "   '\\n',\n",
       "   'research',\n",
       "   'field',\n",
       "   'that',\n",
       "   'has',\n",
       "   'attracted',\n",
       "   'considerable',\n",
       "   'interest',\n",
       "   'in',\n",
       "   'computer',\n",
       "   '\\n',\n",
       "   'vision',\n",
       "   'and',\n",
       "   'multimedia',\n",
       "   'communities',\n",
       "   '[',\n",
       "   '2',\n",
       "   ',',\n",
       "   '21',\n",
       "   ']',\n",
       "   '.',\n",
       "   'One',\n",
       "   'of',\n",
       "   'the',\n",
       "   'challenges',\n",
       "   'in',\n",
       "   '\\n',\n",
       "   'Libras',\n",
       "   'recognition',\n",
       "   'task',\n",
       "   'is',\n",
       "   'the',\n",
       "   'content',\n",
       "   'description',\n",
       "   'of',\n",
       "   'signers',\n",
       "   'based',\n",
       "   '\\n\\n',\n",
       "   '1',\n",
       "   'Code',\n",
       "   'available',\n",
       "   'on',\n",
       "   'https://github.com/IMScience-PPGINF-PucMinas/libras-signrecognition',\n",
       "   '\\n',\n",
       "   '2',\n",
       "   'In',\n",
       "   'Brazilian',\n",
       "   'Portuguese',\n",
       "   '–',\n",
       "   'Língua',\n",
       "   'Brasileira',\n",
       "   'de',\n",
       "   'Sinais',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   ':',\n",
       "   'Proceedings',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Web',\n",
       "   '(',\n",
       "   'WebMe',\n",
       "   '\\n',\n",
       "   'dia’2024',\n",
       "   ')',\n",
       "   '.',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '.',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   ':',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   ',',\n",
       "   '2024',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'SBC',\n",
       "   '–',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '-',\n",
       "   '2753',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Leonardo',\n",
       "   'Vilela',\n",
       "   'Cardoso',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'leonardocardoso@pucminas.br',\n",
       "   'Laboratory',\n",
       "   'of',\n",
       "   'Image',\n",
       "   'and',\n",
       "   'Multimedia',\n",
       "   'Data',\n",
       "   'Science',\n",
       "   '(',\n",
       "   'IMScience',\n",
       "   ')',\n",
       "   ',',\n",
       "   'Pontifícia',\n",
       "   'Universidade',\n",
       "   'Católica',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   '(',\n",
       "   'PUC',\n",
       "   'Minas',\n",
       "   ')',\n",
       "   'Belo',\n",
       "   'Horizonte',\n",
       "   ',',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   '\\n\\n',\n",
       "   'on',\n",
       "   'ground',\n",
       "   'truth',\n",
       "   '(',\n",
       "   'GT',\n",
       "   ')',\n",
       "   'annotations',\n",
       "   'created',\n",
       "   'by',\n",
       "   'multiple',\n",
       "   'individuals',\n",
       "   '.',\n",
       "   'The',\n",
       "   'variability',\n",
       "   'introduced',\n",
       "   'by',\n",
       "   'multiple',\n",
       "   'annotators',\n",
       "   'often',\n",
       "   'results',\n",
       "   '\\n',\n",
       "   'in',\n",
       "   'a',\n",
       "   'GT',\n",
       "   'containing',\n",
       "   'diverse',\n",
       "   'perspectives',\n",
       "   'of',\n",
       "   'the',\n",
       "   'events',\n",
       "   'depicted',\n",
       "   'in',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'signer',\n",
       "   '’s',\n",
       "   'video',\n",
       "   ',',\n",
       "   'thereby',\n",
       "   'highlighting',\n",
       "   'different',\n",
       "   'body',\n",
       "   'movements',\n",
       "   '\\n',\n",
       "   'according',\n",
       "   'to',\n",
       "   'the',\n",
       "   'annotators',\n",
       "   '’',\n",
       "   'fluency',\n",
       "   'in',\n",
       "   'Libras',\n",
       "   '[',\n",
       "   '9',\n",
       "   ',',\n",
       "   '22',\n",
       "   ']',\n",
       "   '.',\n",
       "   'According',\n",
       "   '\\n',\n",
       "   'to',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Institute',\n",
       "   'of',\n",
       "   'Geography',\n",
       "   'and',\n",
       "   'Statistics',\n",
       "   '–',\n",
       "   'IBGE',\n",
       "   '[',\n",
       "   '19',\n",
       "   ']',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'about',\n",
       "   '10',\n",
       "   'million',\n",
       "   'people',\n",
       "   'have',\n",
       "   'hearing',\n",
       "   'problems',\n",
       "   ',',\n",
       "   'with',\n",
       "   'approximately',\n",
       "   '\\n',\n",
       "   '3',\n",
       "   'million',\n",
       "   'being',\n",
       "   'completely',\n",
       "   'deaf',\n",
       "   'and',\n",
       "   'living',\n",
       "   'in',\n",
       "   'Brazil',\n",
       "   '.',\n",
       "   'While',\n",
       "   'communication',\n",
       "   'applications',\n",
       "   'and',\n",
       "   'tools',\n",
       "   'have',\n",
       "   'been',\n",
       "   'widely',\n",
       "   'developed',\n",
       "   'in',\n",
       "   '\\n',\n",
       "   'recent',\n",
       "   'decades',\n",
       "   ',',\n",
       "   'deaf',\n",
       "   'people',\n",
       "   'face',\n",
       "   'numerous',\n",
       "   'problems',\n",
       "   'using',\n",
       "   'these',\n",
       "   '\\n',\n",
       "   'technologies',\n",
       "   '.',\n",
       "   'Outside',\n",
       "   'the',\n",
       "   'technological',\n",
       "   'field',\n",
       "   ',',\n",
       "   'communication',\n",
       "   'barriers',\n",
       "   'also',\n",
       "   'manifest',\n",
       "   ',',\n",
       "   'and',\n",
       "   'this',\n",
       "   'is',\n",
       "   'the',\n",
       "   'greatest',\n",
       "   'difficulty',\n",
       "   'in',\n",
       "   'providing',\n",
       "   '\\n',\n",
       "   'services',\n",
       "   'to',\n",
       "   'hearing',\n",
       "   '-',\n",
       "   'impaired',\n",
       "   'individuals',\n",
       "   '[',\n",
       "   '21',\n",
       "   ',',\n",
       "   '25',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'The',\n",
       "   'communication',\n",
       "   'barrier',\n",
       "   'faced',\n",
       "   'by',\n",
       "   'deaf',\n",
       "   'individuals',\n",
       "   'hinders',\n",
       "   '\\n',\n",
       "   'equitable',\n",
       "   'access',\n",
       "   'to',\n",
       "   'essential',\n",
       "   'services',\n",
       "   '.',\n",
       "   'A',\n",
       "   'system',\n",
       "   'utilizing',\n",
       "   'pattern',\n",
       "   '\\n',\n",
       "   'recognition',\n",
       "   'techniques',\n",
       "   'could',\n",
       "   'bring',\n",
       "   'significant',\n",
       "   'benefits',\n",
       "   'to',\n",
       "   'communication',\n",
       "   'between',\n",
       "   'deaf',\n",
       "   'and',\n",
       "   'hearing',\n",
       "   'people',\n",
       "   ',',\n",
       "   'facilitating',\n",
       "   'interaction',\n",
       "   'in',\n",
       "   '\\n',\n",
       "   'various',\n",
       "   'contexts',\n",
       "   'and',\n",
       "   'contributing',\n",
       "   'to',\n",
       "   'the',\n",
       "   'reduction',\n",
       "   'of',\n",
       "   'the',\n",
       "   'marginalization',\n",
       "   'of',\n",
       "   'this',\n",
       "   'community',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Two',\n",
       "   'different',\n",
       "   'strategies',\n",
       "   'can',\n",
       "   'be',\n",
       "   'applied',\n",
       "   'in',\n",
       "   'sign',\n",
       "   'language',\n",
       "   'recognition',\n",
       "   ':',\n",
       "   'device',\n",
       "   '-',\n",
       "   'based',\n",
       "   'and',\n",
       "   'computer',\n",
       "   'vision',\n",
       "   '-',\n",
       "   'based',\n",
       "   'methods',\n",
       "   '[',\n",
       "   '2',\n",
       "   ',',\n",
       "   '21',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Device',\n",
       "   '-',\n",
       "   'based',\n",
       "   'approaches',\n",
       "   'utilize',\n",
       "   'specialized',\n",
       "   'hardware',\n",
       "   'such',\n",
       "   'as',\n",
       "   'data',\n",
       "   '\\n',\n",
       "   'gloves',\n",
       "   ',',\n",
       "   'depth',\n",
       "   '-',\n",
       "   'sensing',\n",
       "   'cameras',\n",
       "   ',',\n",
       "   'and',\n",
       "   'other',\n",
       "   'wearable',\n",
       "   'sensors',\n",
       "   'to',\n",
       "   'capture',\n",
       "   'sign',\n",
       "   'language',\n",
       "   'gestures',\n",
       "   '[',\n",
       "   '22',\n",
       "   ']',\n",
       "   '.',\n",
       "   'These',\n",
       "   'devices',\n",
       "   'can',\n",
       "   'provide',\n",
       "   'precise',\n",
       "   '\\n',\n",
       "   'data',\n",
       "   'but',\n",
       "   'often',\n",
       "   'at',\n",
       "   'the',\n",
       "   'cost',\n",
       "   'of',\n",
       "   'user',\n",
       "   'comfort',\n",
       "   'and',\n",
       "   'affordability',\n",
       "   '.',\n",
       "   'On',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'other',\n",
       "   'hand',\n",
       "   ',',\n",
       "   'computer',\n",
       "   'vision',\n",
       "   '-',\n",
       "   'based',\n",
       "   'methods',\n",
       "   'leverage',\n",
       "   'regular',\n",
       "   '\\n',\n",
       "   'cameras',\n",
       "   'or',\n",
       "   'webcams',\n",
       "   'to',\n",
       "   'capture',\n",
       "   'gestures',\n",
       "   ',',\n",
       "   'offering',\n",
       "   'a',\n",
       "   'more',\n",
       "   'natural',\n",
       "   '\\n',\n",
       "   'and',\n",
       "   'cost',\n",
       "   '-',\n",
       "   'effective',\n",
       "   'solution',\n",
       "   '[',\n",
       "   '8',\n",
       "   ']',\n",
       "   '.',\n",
       "   'These',\n",
       "   'methods',\n",
       "   'often',\n",
       "   'employ',\n",
       "   'neural',\n",
       "   '\\n',\n",
       "   'networks',\n",
       "   'and',\n",
       "   'diverse',\n",
       "   'machine',\n",
       "   '-',\n",
       "   'learning',\n",
       "   'techniques',\n",
       "   'to',\n",
       "   'recognize',\n",
       "   '\\n',\n",
       "   'signs',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'A',\n",
       "   'key',\n",
       "   'distinction',\n",
       "   'across',\n",
       "   'computer',\n",
       "   'vision',\n",
       "   'works',\n",
       "   'is',\n",
       "   'the',\n",
       "   'use',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'spatio',\n",
       "   '-',\n",
       "   'temporal',\n",
       "   'features',\n",
       "   '.',\n",
       "   'Some',\n",
       "   'of',\n",
       "   'them',\n",
       "   'rely',\n",
       "   'exclusively',\n",
       "   'on',\n",
       "   'images',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'For',\n",
       "   'this',\n",
       "   'task',\n",
       "   ',',\n",
       "   'Convolutional',\n",
       "   'Neural',\n",
       "   'Networks',\n",
       "   '(',\n",
       "   'CNNs',\n",
       "   ')',\n",
       "   'have',\n",
       "   'shown',\n",
       "   '\\n',\n",
       "   'high',\n",
       "   'accuracy',\n",
       "   'in',\n",
       "   'recognizing',\n",
       "   'static',\n",
       "   'signs',\n",
       "   ',',\n",
       "   'achieving',\n",
       "   'up',\n",
       "   'to',\n",
       "   '99.90',\n",
       "   '%',\n",
       "   '\\n',\n",
       "   'accuracy',\n",
       "   'on',\n",
       "   'grayscale',\n",
       "   'images',\n",
       "   '[',\n",
       "   '13',\n",
       "   ',',\n",
       "   '30',\n",
       "   ']',\n",
       "   '.',\n",
       "   'However',\n",
       "   ',',\n",
       "   'in',\n",
       "   'video',\n",
       "   ',',\n",
       "   'temporal',\n",
       "   '\\n\\n\\n',\n",
       "   '72',\n",
       "   '\\n\\n\\n',\n",
       "   '-----',\n",
       "   '\\n\\n',\n",
       "   'WebMedia’2024',\n",
       "   ',',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   'Arcanjo',\n",
       "   'et',\n",
       "   'al',\n",
       "   '.',\n",
       "   '\\n\\n',\n",
       "   'data',\n",
       "   'processing',\n",
       "   'is',\n",
       "   'critical',\n",
       "   'for',\n",
       "   'sign',\n",
       "   'recognition',\n",
       "   '.',\n",
       "   'The',\n",
       "   'order',\n",
       "   'of',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'gestures',\n",
       "   'is',\n",
       "   'crucial',\n",
       "   ',',\n",
       "   'and',\n",
       "   'the',\n",
       "   'static',\n",
       "   'processing',\n",
       "   'of',\n",
       "   'an',\n",
       "   'image',\n",
       "   'does',\n",
       "   'not',\n",
       "   '\\n',\n",
       "   ...],\n",
       "  'pos_tagger': '',\n",
       "  'lema': ['automatic',\n",
       "   'time',\n",
       "   'aware',\n",
       "   'Recognition',\n",
       "   'of',\n",
       "   'Brazilian',\n",
       "   'Sign',\n",
       "   'Language',\n",
       "   'base',\n",
       "   'on',\n",
       "   'dynamic',\n",
       "   'time',\n",
       "   'Warping',\n",
       "   'Lucas',\n",
       "   'de',\n",
       "   'S.',\n",
       "   'Arcanjo',\n",
       "   'larcanjo@sga.pucminas.br',\n",
       "   'Laboratory',\n",
       "   'of',\n",
       "   'Image',\n",
       "   'and',\n",
       "   'Multimedia',\n",
       "   'Data',\n",
       "   'Science',\n",
       "   'IMScience',\n",
       "   'Pontifícia',\n",
       "   'Universidade',\n",
       "   'Católica',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'PUC',\n",
       "   'Minas',\n",
       "   'Belo',\n",
       "   'Horizonte',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'Lucas',\n",
       "   'F.',\n",
       "   'Coelho',\n",
       "   'lucas.coelho.1296135@sga.pucminas.br',\n",
       "   'Laboratory',\n",
       "   'of',\n",
       "   'Image',\n",
       "   'and',\n",
       "   'Multimedia',\n",
       "   'Data',\n",
       "   'Science',\n",
       "   'IMScience',\n",
       "   'Pontifícia',\n",
       "   'Universidade',\n",
       "   'Católica',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'PUC',\n",
       "   'Minas',\n",
       "   'Belo',\n",
       "   'Horizonte',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'Silvio',\n",
       "   'Jamil',\n",
       "   'F.',\n",
       "   'Guimarães',\n",
       "   'sjamil@pucminas.br',\n",
       "   'Laboratory',\n",
       "   'of',\n",
       "   'Image',\n",
       "   'and',\n",
       "   'Multimedia',\n",
       "   'Data',\n",
       "   'Science',\n",
       "   'IMScience',\n",
       "   'Pontifícia',\n",
       "   'Universidade',\n",
       "   'Católica',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'PUC',\n",
       "   'Minas',\n",
       "   'Belo',\n",
       "   'Horizonte',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'Zenilton',\n",
       "   'K.',\n",
       "   'G.',\n",
       "   'do',\n",
       "   'Patrocínio',\n",
       "   'Jr',\n",
       "   'zenilton@pucminas.br',\n",
       "   'Laboratory',\n",
       "   'of',\n",
       "   'Image',\n",
       "   'and',\n",
       "   'Multimedia',\n",
       "   'Data',\n",
       "   'Science',\n",
       "   'IMScience',\n",
       "   'Pontifícia',\n",
       "   'Universidade',\n",
       "   'Católica',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'PUC',\n",
       "   'Minas',\n",
       "   'Belo',\n",
       "   'Horizonte',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'ABSTRACT',\n",
       "   'the',\n",
       "   'brazilian',\n",
       "   'Sign',\n",
       "   'Language',\n",
       "   'Libras',\n",
       "   'be',\n",
       "   'a',\n",
       "   'crucial',\n",
       "   'communication',\n",
       "   'medium',\n",
       "   'for',\n",
       "   'the',\n",
       "   'deaf',\n",
       "   'community',\n",
       "   'in',\n",
       "   'Brazil',\n",
       "   'yet',\n",
       "   'it',\n",
       "   'pose',\n",
       "   'significant',\n",
       "   'challenge',\n",
       "   'for',\n",
       "   'recognition',\n",
       "   'and',\n",
       "   'translation',\n",
       "   'task',\n",
       "   'this',\n",
       "   'paper',\n",
       "   'present',\n",
       "   'a',\n",
       "   'novel',\n",
       "   'approach',\n",
       "   'use',\n",
       "   'fast',\n",
       "   'dynamic',\n",
       "   'time',\n",
       "   'Warping',\n",
       "   'FastDTW',\n",
       "   '1',\n",
       "   'for',\n",
       "   'recognize',\n",
       "   'Libras',\n",
       "   'sign',\n",
       "   'in',\n",
       "   'video',\n",
       "   'stream',\n",
       "   'this',\n",
       "   'approach',\n",
       "   'aim',\n",
       "   'to',\n",
       "   'bridge',\n",
       "   'the',\n",
       "   'communication',\n",
       "   'gap',\n",
       "   'between',\n",
       "   'deaf',\n",
       "   'and',\n",
       "   'hear',\n",
       "   'individual',\n",
       "   'enhance',\n",
       "   'accessibility',\n",
       "   'and',\n",
       "   'reduce',\n",
       "   'social',\n",
       "   'marginalization',\n",
       "   'the',\n",
       "   'methodology',\n",
       "   'leverage',\n",
       "   'MediaPipe',\n",
       "   'to',\n",
       "   'extract',\n",
       "   'key',\n",
       "   'hand',\n",
       "   'and',\n",
       "   'body',\n",
       "   'landmark',\n",
       "   'which',\n",
       "   'be',\n",
       "   'then',\n",
       "   'use',\n",
       "   'to',\n",
       "   'compute',\n",
       "   'angular',\n",
       "   'feature',\n",
       "   'for',\n",
       "   'accurate',\n",
       "   'sign',\n",
       "   'recognition',\n",
       "   'experiment',\n",
       "   'be',\n",
       "   'conduct',\n",
       "   'on',\n",
       "   'the',\n",
       "   'MINDS',\n",
       "   'Libras',\n",
       "   'dataset',\n",
       "   'and',\n",
       "   'the',\n",
       "   'result',\n",
       "   'demonstrate',\n",
       "   'a',\n",
       "   'high',\n",
       "   'recognition',\n",
       "   'accuracy',\n",
       "   'outperform',\n",
       "   'traditional',\n",
       "   'method',\n",
       "   'furthermore',\n",
       "   'when',\n",
       "   'the',\n",
       "   'propose',\n",
       "   'model',\n",
       "   'be',\n",
       "   'apply',\n",
       "   'to',\n",
       "   'the',\n",
       "   'include-50',\n",
       "   'dataset',\n",
       "   'contain',\n",
       "   'sign',\n",
       "   'from',\n",
       "   'a',\n",
       "   'different',\n",
       "   'sign',\n",
       "   'language',\n",
       "   'it',\n",
       "   'perform',\n",
       "   'competitively',\n",
       "   'without',\n",
       "   'rely',\n",
       "   'on',\n",
       "   'deep',\n",
       "   'learning',\n",
       "   'technique',\n",
       "   'keyword',\n",
       "   'Computer',\n",
       "   'Vision',\n",
       "   'Sign',\n",
       "   'Language',\n",
       "   'Recognition',\n",
       "   'Gesture',\n",
       "   'Recognition',\n",
       "   'dynamic',\n",
       "   'Time',\n",
       "   'Warping',\n",
       "   'MediaPipe',\n",
       "   'Libras',\n",
       "   'Brazilian',\n",
       "   'Sign',\n",
       "   'Language',\n",
       "   '1',\n",
       "   'introduction',\n",
       "   'the',\n",
       "   'brazilian',\n",
       "   'Sign',\n",
       "   'Language',\n",
       "   'Libras',\n",
       "   '2',\n",
       "   'recognition',\n",
       "   'be',\n",
       "   'a',\n",
       "   'complex',\n",
       "   'research',\n",
       "   'field',\n",
       "   'that',\n",
       "   'have',\n",
       "   'attract',\n",
       "   'considerable',\n",
       "   'interest',\n",
       "   'in',\n",
       "   'computer',\n",
       "   'vision',\n",
       "   'and',\n",
       "   'multimedia',\n",
       "   'community',\n",
       "   '2',\n",
       "   '21',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'challenge',\n",
       "   'in',\n",
       "   'Libras',\n",
       "   'recognition',\n",
       "   'task',\n",
       "   'be',\n",
       "   'the',\n",
       "   'content',\n",
       "   'description',\n",
       "   'of',\n",
       "   'signer',\n",
       "   'base',\n",
       "   '1',\n",
       "   'Code',\n",
       "   'available',\n",
       "   'on',\n",
       "   'https://github.com/imscience-ppginf-pucminas/libras-signrecognition',\n",
       "   '2',\n",
       "   'in',\n",
       "   'brazilian',\n",
       "   'portuguese',\n",
       "   'Língua',\n",
       "   'Brasileira',\n",
       "   'de',\n",
       "   'Sinais',\n",
       "   'in',\n",
       "   'proceeding',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'web',\n",
       "   'WebMe',\n",
       "   'dia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   '2024',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'sbc',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '2753',\n",
       "   'Leonardo',\n",
       "   'Vilela',\n",
       "   'Cardoso',\n",
       "   'leonardocardoso@pucminas.br',\n",
       "   'Laboratory',\n",
       "   'of',\n",
       "   'Image',\n",
       "   'and',\n",
       "   'Multimedia',\n",
       "   'Data',\n",
       "   'Science',\n",
       "   'IMScience',\n",
       "   'Pontifícia',\n",
       "   'Universidade',\n",
       "   'Católica',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'PUC',\n",
       "   'Minas',\n",
       "   'Belo',\n",
       "   'Horizonte',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'on',\n",
       "   'ground',\n",
       "   'truth',\n",
       "   'GT',\n",
       "   'annotation',\n",
       "   'create',\n",
       "   'by',\n",
       "   'multiple',\n",
       "   'individual',\n",
       "   'the',\n",
       "   'variability',\n",
       "   'introduce',\n",
       "   'by',\n",
       "   'multiple',\n",
       "   'annotator',\n",
       "   'often',\n",
       "   'result',\n",
       "   'in',\n",
       "   'a',\n",
       "   'GT',\n",
       "   'contain',\n",
       "   'diverse',\n",
       "   'perspective',\n",
       "   'of',\n",
       "   'the',\n",
       "   'event',\n",
       "   'depict',\n",
       "   'in',\n",
       "   'the',\n",
       "   'signer',\n",
       "   '’s',\n",
       "   'video',\n",
       "   'thereby',\n",
       "   'highlight',\n",
       "   'different',\n",
       "   'body',\n",
       "   'movement',\n",
       "   'accord',\n",
       "   'to',\n",
       "   'the',\n",
       "   'annotator',\n",
       "   'fluency',\n",
       "   'in',\n",
       "   'Libras',\n",
       "   '9',\n",
       "   '22',\n",
       "   'accord',\n",
       "   'to',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Institute',\n",
       "   'of',\n",
       "   'Geography',\n",
       "   'and',\n",
       "   'Statistics',\n",
       "   'IBGE',\n",
       "   '19',\n",
       "   'about',\n",
       "   '10',\n",
       "   'million',\n",
       "   'people',\n",
       "   'have',\n",
       "   'hear',\n",
       "   'problem',\n",
       "   'with',\n",
       "   'approximately',\n",
       "   '3',\n",
       "   'million',\n",
       "   'be',\n",
       "   'completely',\n",
       "   'deaf',\n",
       "   'and',\n",
       "   'live',\n",
       "   'in',\n",
       "   'Brazil',\n",
       "   'while',\n",
       "   'communication',\n",
       "   'application',\n",
       "   'and',\n",
       "   'tool',\n",
       "   'have',\n",
       "   'be',\n",
       "   'widely',\n",
       "   'develop',\n",
       "   'in',\n",
       "   'recent',\n",
       "   'decade',\n",
       "   'deaf',\n",
       "   'people',\n",
       "   'face',\n",
       "   'numerous',\n",
       "   'problem',\n",
       "   'use',\n",
       "   'these',\n",
       "   'technology',\n",
       "   'outside',\n",
       "   'the',\n",
       "   'technological',\n",
       "   'field',\n",
       "   'communication',\n",
       "   'barrier',\n",
       "   'also',\n",
       "   'manif',\n",
       "   'and',\n",
       "   'this',\n",
       "   'be',\n",
       "   'the',\n",
       "   'great',\n",
       "   'difficulty',\n",
       "   'in',\n",
       "   'provide',\n",
       "   'service',\n",
       "   'to',\n",
       "   'hearing',\n",
       "   'impair',\n",
       "   'individual',\n",
       "   '21',\n",
       "   '25',\n",
       "   'the',\n",
       "   'communication',\n",
       "   'barrier',\n",
       "   'face',\n",
       "   'by',\n",
       "   'deaf',\n",
       "   'individual',\n",
       "   'hinder',\n",
       "   'equitable',\n",
       "   'access',\n",
       "   'to',\n",
       "   'essential',\n",
       "   'service',\n",
       "   'a',\n",
       "   'system',\n",
       "   'utilize',\n",
       "   'pattern',\n",
       "   'recognition',\n",
       "   'technique',\n",
       "   'could',\n",
       "   'bring',\n",
       "   'significant',\n",
       "   'benefit',\n",
       "   'to',\n",
       "   'communication',\n",
       "   'between',\n",
       "   'deaf',\n",
       "   'and',\n",
       "   'hear',\n",
       "   'people',\n",
       "   'facilitate',\n",
       "   'interaction',\n",
       "   'in',\n",
       "   'various',\n",
       "   'contexts',\n",
       "   'and',\n",
       "   'contribute',\n",
       "   'to',\n",
       "   'the',\n",
       "   'reduction',\n",
       "   'of',\n",
       "   'the',\n",
       "   'marginalization',\n",
       "   'of',\n",
       "   'this',\n",
       "   'community',\n",
       "   'two',\n",
       "   'different',\n",
       "   'strategy',\n",
       "   'can',\n",
       "   'be',\n",
       "   'apply',\n",
       "   'in',\n",
       "   'sign',\n",
       "   'language',\n",
       "   'recognition',\n",
       "   'device',\n",
       "   'base',\n",
       "   'and',\n",
       "   'computer',\n",
       "   'vision',\n",
       "   'base',\n",
       "   'method',\n",
       "   '2',\n",
       "   '21',\n",
       "   'device',\n",
       "   'base',\n",
       "   'approach',\n",
       "   'utilize',\n",
       "   'specialized',\n",
       "   'hardware',\n",
       "   'such',\n",
       "   'as',\n",
       "   'datum',\n",
       "   'glove',\n",
       "   'depth',\n",
       "   'sense',\n",
       "   'camera',\n",
       "   'and',\n",
       "   'other',\n",
       "   'wearable',\n",
       "   'sensor',\n",
       "   'to',\n",
       "   'capture',\n",
       "   'sign',\n",
       "   'language',\n",
       "   'gesture',\n",
       "   '22',\n",
       "   'these',\n",
       "   'device',\n",
       "   'can',\n",
       "   'provide',\n",
       "   'precise',\n",
       "   'datum',\n",
       "   'but',\n",
       "   'often',\n",
       "   'at',\n",
       "   'the',\n",
       "   'cost',\n",
       "   'of',\n",
       "   'user',\n",
       "   'comfort',\n",
       "   'and',\n",
       "   'affordability',\n",
       "   'on',\n",
       "   'the',\n",
       "   'other',\n",
       "   'hand',\n",
       "   'computer',\n",
       "   'vision',\n",
       "   'base',\n",
       "   'method',\n",
       "   'leverage',\n",
       "   'regular',\n",
       "   'camera',\n",
       "   'or',\n",
       "   'webcam',\n",
       "   'to',\n",
       "   'capture',\n",
       "   'gesture',\n",
       "   'offer',\n",
       "   'a',\n",
       "   'more',\n",
       "   'natural',\n",
       "   'and',\n",
       "   'cost',\n",
       "   'effective',\n",
       "   'solution',\n",
       "   '8',\n",
       "   'these',\n",
       "   'method',\n",
       "   'often',\n",
       "   'employ',\n",
       "   'neural',\n",
       "   'network',\n",
       "   'and',\n",
       "   'diverse',\n",
       "   'machine',\n",
       "   'learn',\n",
       "   'technique',\n",
       "   'to',\n",
       "   'recognize',\n",
       "   'sign',\n",
       "   'a',\n",
       "   'key',\n",
       "   'distinction',\n",
       "   'across',\n",
       "   'computer',\n",
       "   'vision',\n",
       "   'work',\n",
       "   'be',\n",
       "   'the',\n",
       "   'use',\n",
       "   'of',\n",
       "   'spatio',\n",
       "   'temporal',\n",
       "   'feature',\n",
       "   'some',\n",
       "   'of',\n",
       "   'they',\n",
       "   'rely',\n",
       "   'exclusively',\n",
       "   'on',\n",
       "   'image',\n",
       "   'for',\n",
       "   'this',\n",
       "   'task',\n",
       "   'Convolutional',\n",
       "   'Neural',\n",
       "   'Networks',\n",
       "   'CNNs',\n",
       "   'have',\n",
       "   'show',\n",
       "   'high',\n",
       "   'accuracy',\n",
       "   'in',\n",
       "   'recognize',\n",
       "   'static',\n",
       "   'sign',\n",
       "   'achieve',\n",
       "   'up',\n",
       "   'to',\n",
       "   '99.90',\n",
       "   'accuracy',\n",
       "   'on',\n",
       "   'grayscale',\n",
       "   'image',\n",
       "   '13',\n",
       "   '30',\n",
       "   'however',\n",
       "   'in',\n",
       "   'video',\n",
       "   'temporal',\n",
       "   '72',\n",
       "   'WebMedia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Arcanjo',\n",
       "   'et',\n",
       "   'al',\n",
       "   'datum',\n",
       "   'processing',\n",
       "   'be',\n",
       "   'critical',\n",
       "   'for',\n",
       "   'sign',\n",
       "   'recognition',\n",
       "   'the',\n",
       "   'order',\n",
       "   'of',\n",
       "   'the',\n",
       "   'gesture',\n",
       "   'be',\n",
       "   'crucial',\n",
       "   'and',\n",
       "   'the',\n",
       "   'static',\n",
       "   'processing',\n",
       "   'of',\n",
       "   'an',\n",
       "   'image',\n",
       "   'do',\n",
       "   'not',\n",
       "   'solve',\n",
       "   'the',\n",
       "   'real',\n",
       "   'communication',\n",
       "   'problem',\n",
       "   'the',\n",
       "   'main',\n",
       "   'idea',\n",
       "   'of',\n",
       "   'this',\n",
       "   'study',\n",
       "   'be',\n",
       "   'to',\n",
       "   'enhance',\n",
       "   'the',\n",
       "   'accuracy',\n",
       "   'of',\n",
       "   'recognize',\n",
       "   'Libras',\n",
       "   'sign',\n",
       "   'from',\n",
       "   'video',\n",
       "   'stream',\n",
       "   'and',\n",
       "   'translate',\n",
       "   'they',\n",
       "   'into',\n",
       "   'portuguese',\n",
       "   'word',\n",
       "   'use',\n",
       "   'FastDTW',\n",
       "   'technique',\n",
       "   'this',\n",
       "   'task',\n",
       "   'involve',\n",
       "   'several',\n",
       "   'subtask',\n",
       "   'include',\n",
       "   'sign',\n",
       "   'interpretation',\n",
       "   'for',\n",
       "   'hand',\n",
       "   'finger',\n",
       "   'torso',\n",
       "   'and',\n",
       "   'the',\n",
       "   'positioning',\n",
       "   'of',\n",
       "   'phalanx',\n",
       "   'within',\n",
       "   'image',\n",
       "   'consequently',\n",
       "   'this',\n",
       "   'work',\n",
       "   'seek',\n",
       "   'to',\n",
       "   'identify',\n",
       "   'specific',\n",
       "   'Libras',\n",
       "   'gesture',\n",
       "   'within',\n",
       "   'video',\n",
       "   'sequence',\n",
       "   'and',\n",
       "   'accurately',\n",
       "   'map',\n",
       "   'they',\n",
       "   'to',\n",
       "   'their',\n",
       "   'corresponding',\n",
       "   'portuguese',\n",
       "   'word',\n",
       "   'thereby',\n",
       "   'bridge',\n",
       "   'the',\n",
       "   'two',\n",
       "   'language',\n",
       "   'additionally',\n",
       "   'a',\n",
       "   'central',\n",
       "   'component',\n",
       "   'of',\n",
       "   'this',\n",
       "   'research',\n",
       "   'be',\n",
       "   'to',\n",
       "   'evaluate',\n",
       "   'the',\n",
       "   'effectiveness',\n",
       "   'of',\n",
       "   'FastDTW',\n",
       "   'in',\n",
       "   'this',\n",
       "   'translation',\n",
       "   'process',\n",
       "   'the',\n",
       "   'goal',\n",
       "   'be',\n",
       "   'not',\n",
       "   'only',\n",
       "   'to',\n",
       "   'assess',\n",
       "   'the',\n",
       "   'technique',\n",
       "   '’s',\n",
       "   'feasibility',\n",
       "   'but',\n",
       "   'also',\n",
       "   'to',\n",
       "   'optimize',\n",
       "   'its',\n",
       "   'practical',\n",
       "   'application',\n",
       "   'for',\n",
       "   'facilitate',\n",
       "   'communication',\n",
       "   'between',\n",
       "   'Libras',\n",
       "   'user',\n",
       "   'and',\n",
       "   'portuguese',\n",
       "   'speaker',\n",
       "   'this',\n",
       "   'paper',\n",
       "   'be',\n",
       "   'structure',\n",
       "   'as',\n",
       "   'follow',\n",
       "   'the',\n",
       "   'theoretical',\n",
       "   'background',\n",
       "   'be',\n",
       "   'present',\n",
       "   'in',\n",
       "   'Section',\n",
       "   '2',\n",
       "   'while',\n",
       "   'section',\n",
       "   '3',\n",
       "   'discuss',\n",
       "   'the',\n",
       "   'relate',\n",
       "   'work',\n",
       "   'section',\n",
       "   '4',\n",
       "   'detail',\n",
       "   'the',\n",
       "   'propose',\n",
       "   'method',\n",
       "   'while',\n",
       "   'section',\n",
       "   '5',\n",
       "   'present',\n",
       "   'and',\n",
       "   'analyze',\n",
       "   'the',\n",
       "   'result',\n",
       "   'finally',\n",
       "   'section',\n",
       "   '6',\n",
       "   'draw',\n",
       "   'some',\n",
       "   'conclusion',\n",
       "   '2',\n",
       "   'BACKGROUND',\n",
       "   '2.1',\n",
       "   'dynamic',\n",
       "   'time',\n",
       "   'Warping',\n",
       "   'dynamic',\n",
       "   'time',\n",
       "   'Warping',\n",
       "   'DTW',\n",
       "   'be',\n",
       "   'a',\n",
       "   'technique',\n",
       "   'for',\n",
       "   'measure',\n",
       "   'the',\n",
       "   'similarity',\n",
       "   'between',\n",
       "   'time',\n",
       "   'series',\n",
       "   'introduce',\n",
       "   'by',\n",
       "   'Sakoe',\n",
       "   'and',\n",
       "   'Chiba',\n",
       "   '23',\n",
       "   'initially',\n",
       "   'for',\n",
       "   'speech',\n",
       "   'application',\n",
       "   'time',\n",
       "   'series',\n",
       "   'analysis',\n",
       "   'be',\n",
       "   'a',\n",
       "   'critical',\n",
       "   'task',\n",
       "   'in',\n",
       "   'various',\n",
       "   'domain',\n",
       "   'accurate',\n",
       "   'measurement',\n",
       "   'of',\n",
       "   'the',\n",
       "   'similarity',\n",
       "   'between',\n",
       "   'time',\n",
       "   'series',\n",
       "   'be',\n",
       "   'essential',\n",
       "   'for',\n",
       "   'classification',\n",
       "   'cluster',\n",
       "   'and',\n",
       "   'anomaly',\n",
       "   'detection',\n",
       "   'two',\n",
       "   'common',\n",
       "   'technique',\n",
       "   'for',\n",
       "   'measure',\n",
       "   'similarity',\n",
       "   'between',\n",
       "   'time',\n",
       "   'series',\n",
       "   'be',\n",
       "   'Euclidean',\n",
       "   'Distance',\n",
       "   'ED',\n",
       "   'and',\n",
       "   'DTW',\n",
       "   'while',\n",
       "   'ED',\n",
       "   'be',\n",
       "   'simple',\n",
       "   'and',\n",
       "   'efficient',\n",
       "   'it',\n",
       "   'have',\n",
       "   'significant',\n",
       "   'limitation',\n",
       "   'that',\n",
       "   'DTW',\n",
       "   'address',\n",
       "   'more',\n",
       "   'effectively',\n",
       "   'figure',\n",
       "   '1',\n",
       "   'demonstrate',\n",
       "   'the',\n",
       "   'difference',\n",
       "   'between',\n",
       "   'ED',\n",
       "   'and',\n",
       "   'DTW',\n",
       "   'compare',\n",
       "   'the',\n",
       "   'two',\n",
       "   'signal',\n",
       "   'it',\n",
       "   'be',\n",
       "   'possible',\n",
       "   'to',\n",
       "   'observe',\n",
       "   'the',\n",
       "   'distortion',\n",
       "   'cause',\n",
       "   'by',\n",
       "   'ED',\n",
       "   'whereas',\n",
       "   'DTW',\n",
       "   'tend',\n",
       "   'to',\n",
       "   'capture',\n",
       "   'the',\n",
       "   'temporal',\n",
       "   'relationship',\n",
       "   'between',\n",
       "   ...]},\n",
       " {'titulo': 'Characterization of the Brazilian musical landscape: A study of regional preferences based on the Spotify charts',\n",
       "  'informacoes_url': '',\n",
       "  'idioma': 'english',\n",
       "  'storage_key': '../articles/original/english/985-24746-1-10-20240923.pdf',\n",
       "  'author': 'Filipe A. S. Moura; Carlos H. G. Ferreira; and Helen C. S. C. Lima',\n",
       "  'data_publicacao': '11-09-2024',\n",
       "  'resumo': 'In the digital age, streaming services such as Spotify have changed the way people consume music, highlighting the enormous influence these platforms have on the market. In the highly competitive music industry, it is crucial for independent artists in particular to maintain their popularity. This is especially true in countries like Brazil, where geographical and cultural differences influence music consumption patterns. Understanding these patterns is essential for effective marketing and production strategies. Despite previous research on music consumption, genre preferences and user behavior, there is a lack of detailed studies on the geographical and cultural distribution of music preferences in Brazil. Our study fills this gap by examining musical genre preferences and acoustic features of tracks across Brazilian regions over two years. We collected Spotify chart data from 2022 and 2023, modeled bipartite genre-city networks, and used backbone extraction methods to highlight significant genre preferences. Temporal analysis revealed patterns and persistence of musical preferences across cities, while clustering techniques revealed regional and cultural differences in acoustic features. Our results show that genre preferences are stable across Brazilian regions, with important genres emphasized by backbone networks. Persistence analysis suggests minimal changes over time, except during major holidays. Furthermore, Brazilian city clusters exhibit distinct acoustic patterns regardless of music genres, with notable differences in features such as liveliness, speechiness, and valence. This research provides new insights into regional musical diversity in Brazil and paves the way for future studies on cultural and geographical influences on music preferences. ###',\n",
       "  'keywords': 'Music Preferences, Music Genre Networks, Regional Analysis, Spotify Charts, Music Data Mining',\n",
       "  'referencias': ['[1] Moonis Ali and Savvas Zannettou. 2024. From Isolation to Desolation: Investigating Self-Harm Discussions in Incel Communities. 18 (May 2024), 43–56.\\nhttps://doi.org/10.1609/icwsm.v18i1.31296',\n",
       "   '[2] Carlos Araujo, Marco Cristo, and Rafael Giusti. 2019. Predicting Music Popularity\\non Streaming Platforms. In *Anais do XVII Simpósio Brasileiro de Computação*\\n*Musical* (São João del-Rei). SBC, Porto Alegre, RS, Brasil, 141–148. https://doi.\\norg/10.5753/sbcm.2019.10436',\n",
       "   '[3] Mariana Lopes Barata and Pedro Simoes Coelho. 2021. Music streaming services:\\nunderstanding the drivers of customer purchase and intention to recommend.\\n*Heliyon* 7, 8 (2021).',\n",
       "   '[4] Gabriel Barbosa, Bruna Melo, Gabriel Oliveira, Mariana Silva, Danilo Seufitelli, and Mirella Moro. 2021. Hot Streaks in the Brazilian Music Market:\\nA Comparison Between Physical and Digital Eras. In *Anais do XVIII Simpósio*\\n*Brasileiro de Computação Musical* (Recife). SBC, Porto Alegre, RS, Brasil, 152–159.\\nhttps://doi.org/10.5753/sbcm.2021.19440',\n",
       "   '[5] Dogan Basaran and Keti Ventura. 2022. Exploring Digital Marketing In Entertainment Industry: A Case Of A Digital Music Platform. *Journal of Management*\\n*Marketing and Logistics* 9, 3 (2022), 115–126.',\n",
       "   '[6] Pablo Bello and David Garcia. 2021. Cultural Divergence in popular music: the increasing diversity of music consumption on Spotify across countries. *Humanities*\\n*and Social Sciences Communications* 8, 1 (2021), 1–8.',\n",
       "   '[7] Pauwke Berkers. 2012. Gendered scrobbling: Listening behaviour of young adults\\non Last. fm. *Interactions: Studies in Communication & Culture* 2, 3 (2012), 279–296.',\n",
       "   '[8] Jean-Samuel Beuscart, Samuel Coavoux, and Jean-Baptiste Garrocq. 2023. Listening to music videos on YouTube. Digital consumption practices and the environmental impact of streaming. *Journal of Consumer Culture* (2023), 654–671.',\n",
       "   '[9] Michele Coscia. 2021. Noise Corrected Sampling of Online Social Networks. *ACM*\\n*Transactions on Knowledge Discovery from Data (TKDD)* 15, 2 (2021), 1–21.',\n",
       "   '[10] Anne Danielsen and Yngvar Kjus. 2019. The mediated festival: Live music as\\ntrigger of streaming and social media engagement. *Convergence* (2019).',\n",
       "   '[11] Robert Dorfman. 1979. A formula for the Gini coefficient. *The review of economics*\\n*and statistics* (1979), 146–149.',\n",
       "   '[12] Maura Edmond. 2014. Here we go again: Music videos after YouTube. *Television*\\n*& New Media* 15, 4 (2014), 305–320.',\n",
       "   '[13] Falina Enriquez. 2022. Pernambuco and Bahia’s Musical “War”: Contemporary\\nMusic, Intraregional Rivalry, and Branding in Northeastern Brazil. *Luso-Brazilian*\\n*Review* 59, 1 (2022), 22–60.',\n",
       "   '[14] Ghazal Fazelnia, Eric Simon, Ian Anderson, Benjamin Carterette, and Mounia Lalmas. 2022. Variational User Modeling with Slow and Fast Features. In *Proceedings*\\n*of the Fifteenth ACM International Conference on Web Search and Data Mining*\\n(Virtual Event, AZ, USA) *(WSDM ’22)* . Association for Computing Machinery,\\nNew York, NY, USA, 271–279. https://doi.org/10.1145/3488560.3498477',\n",
       "   '[15] Andres Ferraro, Xavier Serra, and Christine Bauer. 2021. What is fair? Exploring\\nthe artists’ perspective on the fairness of music streaming platforms. In *IFIP*\\n*conference on human-computer interaction* . Springer, 562–584.',\n",
       "   '[16] Carlos Henrique Gomes Ferreira, Fabricio Murai, Ana P. C. Silva, Martino Trevisan, Luca Vassio, Idilio Drago, Marco Mellia, and Jussara M. Almeida. 2022. On\\nnetwork backbone extraction for modeling online collective behavior. *PLOS ONE*\\n17, 9 (09 2022), 1–36. https://doi.org/10.1371/journal.pone.0274218',\n",
       "   '[17] Jonathon Grasse. 2021. Musical Spaces and Deep Regionalism in Minas Gerais,\\nBrazil. In *Musical Spaces* . Jenny Stanford Publishing, 5–21.',\n",
       "   '[18] Jiawei Han, Jian Pei, and Hanghang Tong. 2022. *Data mining: concepts and*\\n*techniques* . Morgan kaufmann.',\n",
       "   '[19] John A Hartigan and Manchek A Wong. 1979. Algorithm AS 136: A k-means\\nclustering algorithm. *Journal of the royal statistical society. series c (applied*\\n*statistics)* 28, 1 (1979), 100–108.',\n",
       "   '[20] David Hesmondhalgh. 2022. Streaming’s effects on music culture: Old anxieties\\nand new simplifications. *Cultural Sociology* 16, 1 (2022), 3–24.',\n",
       "   '[21] David C Howell. 1992. *Statistical methods for psychology* . PWS-Kent Publishing\\nCo.',\n",
       "   '[22] Julie Jiang, Aditiya Ponnada, Ang Li, Ben Lacker, and Samuel F Way. 2024. A\\nGenre-Based Analysis of New Music Streaming at Scale. In *Proceedings of the*\\n*16th ACM Web Science Conference* (<conf-loc>, <city>Stuttgart</city>, <country>Germany</country>, </conf-loc>) *(WEBSCI ’24)* . Association for Computing Machinery, New York, NY, USA, 191–201. https://doi.org/10.1145/3614419.\\n3644002',\n",
       "   '[23] Ian T Jolliffe and Jorge Cadima. 2016. Principal component analysis: a review\\nand recent developments. *Philosophical transactions of the royal society A: Mathe-*\\n*matical, Physical and Engineering Sciences* 374, 2065 (2016), 20150202.',\n",
       "   '[24] Maarit Kinnunen, Harri Homi, and Antti Honkanen. 2020. Social sustainability\\nin adolescents’ music event attendance. *Sustainability* 12, 22 (2020), 9419.',\n",
       "   '[25] Conrad Lee and Padraig Cunningham. 2012. The Geographic Flow of Music. (04\\n2012). https://doi.org/10.1109/ASONAM.2012.237',\n",
       "   '[26] Conrad Lee and Padraig Cunningham. 2023. Number of music streaming subscribers worldwide from the 1st half of 2019 to 3rd quarter 2023. (2023). https:\\n//www.statista.com/statistics/669113/number-music-streaming-subscribers/',\n",
       "   '[27] Elisabeth Lex, Dominik Kowald, and Markus Schedl. 2020. Modeling Popularity\\nand Temporal Drift of Music Genre Preferences. *Trans. Int. Soc. Music. Inf. Retr.* 3,\\n1 (2020), 17–30.',\n",
       "   '[28] Renan S Linhares, José M Rosa, Carlos HG Ferreira, Fabricio Murai, Gabriel\\nNobre, and Jussara Almeida. 2022. Uncovering coordinated communities on\\ntwitter during the 2020 us election. In *2022 IEEE/ACM International Conference*\\n*on Advances in Social Networks Analysis and Mining (ASONAM)* . IEEE, 80–87.',\n",
       "   '[29] Riccardo Marcaccioli and Giacomo Livan. 2019. A pólya urn approach to information filtering in complex networks. *Nature communications* (2019).',\n",
       "   '[30] Maria Luiza Botelho Mondelli, Luiz M. R. Gadelha Jr., and Artur Ziviani. 2018.\\nO Que os Países Escutam: Analisando a Rede de Gêneros Musicais ao Redor do\\nMundo. In *Anais do VII Brazilian Workshop on Social Network Analysis and Mining*\\n(Natal). SBC, Porto Alegre, RS, Brasil. https://doi.org/10.5753/brasnam.2018.3586',\n",
       "   '[31] Jeremy Wade Morris. 2020. Music platforms and the optimization of culture.\\n*Social Media+ Society* 6, 3 (2020), 2056305120940690.',\n",
       "   '[32] Shane Murphy. 2020. Music marketing in the digital music industries–An autoethnographic exploration of opportunities and challenges for independent\\nmusicians. *International Journal of Music Business Research* 9, 1 (2020), 7–40.',\n",
       "   '[33] Houssam Nassif, Kemal Oral Cansizlar, Mitchell Goodman, and SVN Vishwanathan. 2018. Diversifying music recommendations. *arXiv preprint*\\n*arXiv:1810.01482* (2018).',\n",
       "   '[34] Zachary P Neal. 2022. backbone: An R package to extract network backbones.\\n*PloS one* 17, 5 (2022), e0269137.',\n",
       "   '[35] Robert Nisbet, John Elder, and Gary D Miner. 2009. *Handbook of statistical analysis*\\n*and data mining applications* . Academic press.',\n",
       "   '[36] Ramy A Rahimi and Kyung-Hye Park. 2020. A comparative study of internet\\narchitecture and applications of online music streaming services: The impact\\non the global music industry growth. In *2020 8th International Conference on*\\n*Information and Communication Technology (ICoICT)* . IEEE, 1–6.',\n",
       "   '[37] Jing Ren and Robert J. Kauffman. [n. d.]. Understanding music track popularity\\nin a social network.(2017). In *Proceedings of the 25th European Conference on*\\n*Information Systems ECIS, Guimarães, Portugal, June* . 5–10.',\n",
       "   '[38] Peter J Rousseeuw. 1987. Silhouettes: a graphical aid to the interpretation and\\nvalidation of cluster analysis. *Journal of computational and applied mathematics*\\n20 (1987), 53–65.',\n",
       "   '[39] M Ángeles Serrano, Marián Boguná, and Alessandro Vespignani. 2009. Extracting\\nthe multiscale backbone of complex weighted networks. *Proceedings of the*\\n*national academy of sciences* 106, 16 (2009), 6483–6488.',\n",
       "   '[40] Yading Song, Simon Dixon, and Marcus Pearce. 2012. A survey of music recommendation systems and future perspectives. In *9th international symposium on*\\n*computer music modeling and retrieval*, Vol. 4. Citeseer, 395–410.',\n",
       "   '[41] Vilde Schanke Sundet and Marika Lüders. 2023. “Young people are on YouTube”:\\nindustry notions on streaming and youth as a new media generation. *Journal of*\\n*Media Business Studies* 20, 3 (2023), 223–240.',\n",
       "   '[42] Fernando Terroso-Saenz, Jesús Soto, and Andres Muñoz. 2023. Music Mobility\\nPatterns: How Songs Propagate Around the World Through Spotify. *Pattern*\\n*Recognition* 143 (2023), 109807.',\n",
       "   '[43] Pier Paolo Tricomi, Luca Pajola, Luca Pasa, and Mauro Conti. 2024. \"All of\\nMe\": Mining Users’ Attributes from their Public Spotify Playlists. In *Companion*\\n*Proceedings of the ACM on Web Conference 2024* . Association for Computing\\nMachinery, New York, NY, USA.',\n",
       "   '[44] Gerald Van Belle. 2011. *Statistical rules of thumb* . John Wiley & Sons.',\n",
       "   '[45] Gabriel Vaz de Melo, Ana Machado, and Lucas Carvalho. 2020. Music consumption in Brazil: an analysis of streaming reproductions. *PragMATIZES - Revista*\\n*Latino-Americana de Estudos em Cultura* 10 (09 2020), 141.',\n",
       "   '[46] Samuel F. Way, Jean Garcia-Gathright, and Henriette Cramer. 2020. Local Trends\\nin Global Music Streaming. *Proceedings of the International AAAI Conference on*\\n*Web and Social Media* 14, 1 (May 2020), 705–714. https://doi.org/10.1609/icwsm.\\nv14i1.7336',\n",
       "   '[47] Jacob Wolbert. 2023. Multiple Brasilidades: Musician-Market Negotiations within\\nthe Brazilian Midstream. *Journal of Popular Music Studies* 35, 3 (2023), 102–127.\\n\\n\\n88\\n\\n\\n-----'],\n",
       "  'text': '# **Characterization of the Brazilian musical landscape: A study of** **regional preferences based on the Spotify charts**\\n\\n## Filipe A. S. Moura\\n#### filipe.asm@aluno.ufop.edu.br\\n\\nDepartamento de Computação e Sistemas\\nUniversidade Federal de Ouro Preto\\n### **ABSTRACT**\\n\\n## Carlos H. G. Ferreira\\n#### chgferreira@ufop.edu.br\\n\\nDepartamento de Computação e Sistemas\\nUniversidade Federal de Ouro Preto\\n\\n## Helen C. S. C. Lima\\n#### helen@ufop.edu.br\\n\\nDepartamento de Computação e Sistemas\\nUniversidade Federal de Ouro Preto\\n\\n\\nIn the digital age, streaming services such as Spotify have changed\\nthe way people consume music, highlighting the enormous influence these platforms have on the market. In the highly competitive\\nmusic industry, it is crucial for independent artists in particular to\\nmaintain their popularity. This is especially true in countries like\\nBrazil, where geographical and cultural differences influence music\\nconsumption patterns. Understanding these patterns is essential for\\neffective marketing and production strategies. Despite previous research on music consumption, genre preferences and user behavior,\\nthere is a lack of detailed studies on the geographical and cultural\\ndistribution of music preferences in Brazil. Our study fills this gap\\nby examining musical genre preferences and acoustic features of\\ntracks across Brazilian regions over two years. We collected Spotify chart data from 2022 and 2023, modeled bipartite genre-city\\nnetworks, and used backbone extraction methods to highlight significant genre preferences. Temporal analysis revealed patterns and\\npersistence of musical preferences across cities, while clustering\\ntechniques revealed regional and cultural differences in acoustic\\nfeatures. Our results show that genre preferences are stable across\\nBrazilian regions, with important genres emphasized by backbone\\nnetworks. Persistence analysis suggests minimal changes over time,\\nexcept during major holidays. Furthermore, Brazilian city clusters\\nexhibit distinct acoustic patterns regardless of music genres, with\\nnotable differences in features such as liveliness, speechiness, and\\nvalence. This research provides new insights into regional musical\\ndiversity in Brazil and paves the way for future studies on cultural\\nand geographical influences on music preferences.\\n### **KEYWORDS**\\n\\nMusic Preferences, Music Genre Networks, Regional Analysis, Spotify Charts, Music Data Mining\\n### **1 INTRODUCTION**\\n\\nIn the digital era, streaming services have revolutionized the way\\npeople consume media content, particularly music [ 3, 8, 12 ]. The\\nnumber of music streaming subscribers worldwide increased from\\n616 million at the end of the second quarter of 2022 to 713 million\\nin the third quarter of 2023 [ 26 ]. As a result, paid music streaming\\nsubscriptions have become the norm for many music fans and\\nhave led to impressive growth in subscriber numbers in recent\\nyears. This surge in popularity has led to music streaming services\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n© 2024 SBC – Brazilian Computing Society.\\nISSN 2966-2753\\n\\n\\naccounting for a significant portion of the music industry’s revenue,\\ndemonstrating their significant influence on the market and the\\nsteady global expansion of these platforms [ 4, 8, 36 ]. Among the\\nvarious music streaming services, Spotify stands out as a profitable\\nplayer. In 2023, Spotify paid out more than 9 billion dollars to\\nartists, one of the highest annual payouts by a single provider\\nin history [1] . What factors contribute to the persistence of these\\nregional patterns?\\nIn parallel, the music industry is highly competitive, and maintaining popularity and influence is crucial for artists, especially for\\nindependent ones [ 32 ]. Expertise in production and promotion offers substantial advantages in this landscape [ 5 ]. In large countries\\nsuch as Brazil, this knowledge is even more important due to the\\ngreat geographic and cultural diversity [ 45 ]. A music popular in\\nthe South may not resonate in the North, and vice versa. Thanks\\nto extensive data from streaming platforms such as Spotify, it is\\nnow possible to understand the geographical distribution of music preferences [ 30 ]. Mapping regional popular music is the first\\nstep toward identifying the profile of a region and testing relevant\\nmarketing strategies [ 45 ]. This profiling, carried out through a temporal analysis, assesses the common characteristics associated with\\npopular music and their cultural significance.\\nIndeed, several prior efforts have concentrated on studying music popularity within groups [ 2, 37 ], geographical distribution of\\nmusic preferences [ 7, 25, 30, 45 ], user demographics and personality [ 6, 43 ], and music diversity [ 20, 31, 46 ]. However, these studies\\ndo not capture the regional behaviors of these groups in terms of\\ngenre preferences, especially in the Brazilian context. Given the\\nsignificant role that music streaming platforms play in the distribution and consumption of music, it is important to understand\\nthe composition of musical preferences in different regions [ 6, 15 ].\\nIn Brazil, with its diverse cultural landscape, this understanding is\\nparticularly important for artists, producers, and marketers who\\nwant to tailor their strategies to regional tastes [ 13, 17, 47 ]. With\\nthis in mind, this study seeks to fill this gap by examining and\\ncharacterizing the musical genre preferences and audio features\\nof tracks in different regions of Brazil over a two-year period on\\nSpotify charts. In this way, we aim to gain valuable insights into the\\nregional musical landscape that can improve marketing strategies,\\nproduction decisions, and promotional activities. Our study aims\\nto address the following research questions:\\n\\n**—RQ1:** How do preferences for musical genres differ in different\\nBrazilian cities over the years? What factors contribute to the persistence of these regional patterns?\\n\\n1 Spotify royalty data 2023: https://apnews.com/article/spotify-loud-clear-report8ddab5a6e03f65233b0f9ed80eb99e0c\\n\\n\\n80\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Filipe A. S. Moura, Carlos H. G. Ferreira, and Helen C. S. C. Lima\\n\\n\\n**—RQ2:** To what extent do the acoustic features of the tracks reflect\\nregional and cultural patterns in Brazil, independent of their musical\\ngenres? Furthermore, are there acoustic differences within the same\\nmusical genre across different regions?\\n\\nTo tackle these research questions, we collected data from Spotify\\ncharts for the years 2022 and 2023. We modeled genre-city bipartite networks and applied a state-of-the-art backbone extraction\\nmethod to highlight significant genre preferences while respecting\\nthe heterogeneity of musical tastes across different cities. Using\\nthis refined topology, we conducted a temporal analysis to identify patterns and persistence in musical preferences, addressing\\nRQ1. For RQ2, we generated features from the backbone network\\nand applied clustering techniques to uncover regional and cultural\\nvariations in the intrinsic acoustic features of tracks. This involved\\nprofiling clusters based on their acoustic characteristics and identifying significant differences within the same genre in different\\nregions. Our results show that preferences for musical genres in\\nBrazilian cities exhibit a stable set of preferred genres, with backbone networks highlighting the most important ones. Moreover, we\\nobserved balanced preferences, while persistence analysis indicated\\nminimal changes over time, except during important holidays. Regarding RQ2, Brazilian city clusters showed different audio patterns\\nin tracks, regardless of musical genres. We found significant differences in features such as *liveness*, *speechiness*, and *valence* between\\nclusters with similar genre preferences, indicating regional and\\ncultural differences in musical preferences and acoustic nuances\\nwithin the same genres in different regions.\\nThe remainder of this paper is organized as follows. In Section\\n2, we discuss related works. In Section 3, we describe the method\\nof data crawling and the generated dataset. Section 4 details our\\nmethodology in four steps. In Section 5, we present our results\\nin discuss our findings for RQ1 and RQ2. Finally, in Section 6, we\\nconclude and propose ideas for future research.\\n### **2 RELATED WORKS**\\n\\nSome works aim to understand the distribution of music consumption in a population [ 7, 42, 43, 46 ], focusing on various aspects.\\nThese include consumption patterns [ 37, 42 ], genre preferences\\n\\n[ 22, 25, 27, 30, 45 ], user behavior [ 8, 14 ], the temporal evolution\\nof music preferences [ 12, 41 ], the impact of marketing campaigns\\n\\n[ 5, 32 ], the influence of social events [ 10, 24 ], user demographics\\nand personality [ 6, 43 ], the effects of algorithmic recommendations\\n\\n[33, 40], and musical diversity [20, 31].\\nFocusing on consumption patterns, Ren and Kauffman [37] built\\na machine learning model to predict the popularity of new releases\\non streaming platforms, uncovering patterns that remain common\\nto popular music, such as tracks staying at the top of the charts\\nfor long periods. Similarly, Terroso-Saenz et al . [42] developed an\\nalgorithm to detect music propagation patterns on Spotify across\\ndifferent countries, finding strong correlations with cultural and\\nsocial similarities. Our goal is different, as we want to identify the\\nmost popular music in each city, excluding national hits and making\\ncorrelations between cities based on their popular music to describe\\npossible geographical and cultural connections.\\nIn terms of genre preferences, Mondelli et al . [30] used a networkbased approach to analyze communities of countries with similar\\n\\n\\nmusical genre preferences, finding that most communities are culturally and/or geographically related. They also showed that Brazil\\nis unique, with its top genres primarily originating from its own\\nculture. Jiang et al . [22] found that new releases with functional\\npurposes are less consumed, and popular genres like *pop* do not\\nalways equate to high consumption rates. Our study focuses on\\ngenre preferences in the different regions of Brazil. It emphasizes\\nthe country’s cultural and geographical diversity and reinforces\\nthe idea that the most popular music genres in Brazil come from\\nwithin the country.\\nLee and Cunningham [25] presented hierarchical groups of countries with similar genre preferences, identifying leader-follower relations where leaders are defined by their historical production and\\nconsumption of a genre. Although determining the origin of genres\\nin the Brazilian regions is not our primary research question, we\\naddress it by recognizing sub-genres that indicate regional origins\\nand identifying national genres that are predominantly listened to\\nin specific areas. Similarly, Vaz de Melo et al . [45] identified digital music consumption patterns in each state but did not reveal\\ndifferences. Our goal is to highlight these differences by analyzing the track acoustic features of similar genres and sub-genres in\\ndifferent regions, connecting users geographically through similar\\npreferences, and revealing regional differences.\\nRegarding user demographics and personality, Tricomi et al . [43]\\nlinked Spotify users’ playlists to their demographic and personality\\ntraits, finding that similar playlists are created by users with similar\\nprofiles. Bello and Garcia [6] explored cross-country diversity in\\nmusic charts over four years, revealing a trend towards greater\\ndiversity in global digital music consumption. In the area of musical\\ndiversity, Way et al . [46] found that the preference for local content\\nincreased in various genres from 2014 to 2019. Hesmondhalgh [20]\\nargued that streaming platforms encourage music to have certain\\ncharacteristics and thus influence the listener’s aesthetic experience.\\nMorris [31] investigated the “platform effects” on users’ musical\\ntastes and found that music acts like data, putting pressure on\\nmusicians and producers to adapt to platform trends.\\nOur study contributes to the literature by providing a longitudinal analysis of the geographical and cultural distribution of music\\npreferences in Brazil. Using data from a two-year period, we examine how genre preferences and track acoustic features differ from\\nregion to region, offering new insights into the regional diversity\\nof musical tastes in Brazil. Our research fills a gap in the understanding of regional music consumption and provides a basis for\\nfuture studies on cultural and geographical influences on music\\npreferences.\\n### **3 DATASET**\\n\\nTo analyze Brazil’s musical preferences geographically, we use\\nthe Spotify Charts [2] . This service summarizes the daily Top 200\\nmost listened to songs in a specific region, referred to as a “chart”.\\nWe collected the weekly rankings of all available Brazilian cities\\non the platform from 2022 to 2023. Each chart contains essential\\ninformation about all tracks, including the track name, artist names,\\nand the Spotify identifier for each track. Additionally, we used these\\nidentifiers to query for acoustic features and genres of each track\\n\\n2 Spotify Charts: https://charts.spotify.com/charts/overview/global\\n\\n\\n81\\n\\n\\n-----\\n\\nCharacterization of the Brazilian musical landscape WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\nusing the Spotify API for Developers [3] . Some core features described\\nby Spotify are as follows:\\n\\n**—Acousticness** measures from 0.0 to 1.0 whether the track is\\n\\nacoustic; 1.0 indicates an acoustic song, while 0.0 means a song\\nwithout any acoustic elements.\\n**—Danceability** describes from 0.0 to 1.0 how suitable the track is\\nfor dancing based on a combination of musical elements; higher\\nvalues indicate greater danceability.\\n**—Energy** measures from 0.0 to 1.0 how intense a track is.\\n**—Liveness** detects the presence of an audience in the recording\\non a scale between 0.0 and 1.0.\\n\\n**—Speechiness** detects the presence of spoken words in a track;\\nthe more exclusively speech-like the recording, the closer to 1.0 the\\nattribute value.\\n\\n**—Valence** ranges from 0.0 to 1.0 describing the positivity of a track.\\nValues close to 1.0 indicate happy, positive, and euphoric songs,\\nwhile low values sound more negative, sad, depressed, or angry.\\n\\nThe dataset includes 5190 unique tracks, 487 genres, and 2056\\nartists. Geographically, we cover 17 cities across 16 states, namely:\\nBelém, Belo Horizonte, Brasília, Campinas, Campo Grande, Cuiabá,\\nCuritiba, Florianópolis, Fortaleza, Goiânia, Manaus, Porto Alegre,\\nRecife, Rio de Janeiro, Salvador, São Paulo, and Uberlândia. Unfortunately, other cities – especially the countryside – could not be\\ncovered by this study due to their absence on the Spotify Charts platform. However, those mentioned above represent the five regions of\\nBrazil (North, Northeast, Midwest, Southeast, and South), providing\\na comprehensive geographical perspective of the country’s music\\npreferences. Thus, we extend the literature with a detailed analysis\\nof the temporal, geographical and cultural distribution of music\\npreferences in Brazil across different regions.\\n### **4 METHODOLOGY**\\n\\nThis section presents our methodology, including the steps for\\nanswering each RQ.\\n### **4.1 Modeling musical genre preferences**\\n\\nAiming to answer our RQ’s, we start by discretizing the two-year\\nperiod into bimonthly windows. We then adopted network models for each period Δ *𝜏*, specifically bipartite networks. Formally,\\nthe bipartite network B [∆] *[𝜏]* = ( *𝑉* *𝐺𝑒𝑛𝑟𝑒* *,𝑉* *𝐶𝑖𝑡𝑦* *, 𝐸* [Δ] *[𝜏]* ) consists of the\\nfollowing elements: *𝑉* *𝐺𝑒𝑛𝑟𝑒* is the set of all music genres, *𝑉* *𝐶𝑖𝑡𝑦* is\\nthe set of all cities, and *𝐸* [Δ] *[𝜏]* is the set of directed edges ( *𝑔,𝑐,𝑤* *𝑔𝑐* [Δ] *[𝜏]* [)] [,]\\n\\nwhere *𝑔* ∈ *𝑉* *𝐺𝑒𝑛𝑟𝑒* and *𝑐* ∈ *𝑉* *𝐶𝑖𝑡𝑦* . The weight *𝑤* *𝑔𝑐* [Δ] *[𝜏]* [represents the]\\nnumber of unique tracks of a genre *𝑔* that appear in the Spotify\\nchart of city *𝑐* during the period Δ *𝜏* .\\nThen, we use instances of B [∆] *[𝜏]* to identify cities with common\\ngenre preferences. However, our analysis revealed that these networks often resulted in highly dense structures, where each node\\nwas strongly linked with many others. This density made it challenging to extract meaningful information about common preferences.\\nThis phenomenon is similar to the observations in prior efforts\\n\\n[ 30 ], who also faced difficulties due to the dense nature of their\\nnetworks. They found that certain genres were universally popular,\\n\\n3 Spotify API for Developers: https://developer.spotify.com/documentation/web-api\\n\\n\\nappearing in charts across multiple countries, resulting in an overly\\nconnected network.\\n\\nTo address the issue of overly dense networks, we employ a network backbone extraction method [ 16, 29, 34 ]. Since not all edges\\nare equally significant for comprehension, as tracks and genres can\\nbe influenced by side effects such as singer popularity, we propose\\nadopting *backbone* extraction algorithms to filter through the noise\\nand unveil only the most relevant edges, resulting in a subset of the\\ninitial network [ 9 ]. There are several backbone extraction methods\\nin the literature [ 16 ]. Some of these methods are used to explore network heterogeneity by identifying salient edges with significantly\\nhigher weights based on local individual patterns (e.g., Polya Urn\\nFilter and Disparity Filter) or global network patterns (e.g., Thresholding), representing persistent and repetitive interactions [ 28, 39 ].\\nLocal methods, in particular, are probabilistic methods that build\\n*null models* for each node and can capture, from a local perspective,\\nhow salient an edge is according to its weight. A review of these\\nmethods for this context is available in the literature [16].\\nWe determined that the most valuable edges – also called *salient*\\nedges – in our context are not necessarily the weakest or strongest\\nacross all cities but those that have exceptional weights from the\\nperspective of each city. In other words, these are the genres that\\nappear most frequently in the charts of the individual cities, taking\\ninto account the popularity of the city and the genre. The main\\nassumption is that these edges will not exhibit a uniform behavior\\nacross all cities but will show specific deviation patterns for smaller\\nsets of cities. Thus, we employ the Polya Urn Filter [ 29 ], a backbone\\nextraction method inspired by the Pólya urn combinatorial model.\\nThis method takes into account the reinforcement hypothesis of\\ninteractions between the same two nodes over time (i.e., a genre\\nwith a high presence in the listened genres of a city over the studied\\ntime windows is a salient link), presuming these edges are maintained and reinforced [ 29 ]. The method is controlled by a given\\n*alpha*, which is used to determine the probability for the statistical\\nsignificance of an edge according to the null model of the Pólya-Urn\\nfilter, and by a reinforcement parameter, which may be self-tuned.\\nWe advocate the use of this method, as opposed to the thresholds used by other authors [ 1 ], because it is advantageous when\\nobserving cities and genre preferences, as it takes into account\\nthe heterogeneity of the data. For example, a genre may appear 10\\ntimes in the chart of one city and 100 times in another, creating\\nedges with the corresponding weights. Both edges can remain in\\nthe backbone and connect the genre in both cities. This is possible\\nbecause from the perspective of the first city, 10 occurrences may\\nbe significant when other genres only occur 1, 2 or 3 times. In this\\nway, this approach respects the heterogeneity of edge weights in\\nthe networks.\\n\\nThe execution of the Polya Urn Filter reveals the most statistically significant edges of the network, and we retained edges with\\na *p-value <* 0 *.* 05, corresponding to a 95% confidence level. By extracting the backbone of B [∆] *[𝜏]*, we generated B [∆] *[𝜏]*\\nBackbone [, identifying]\\nthe most listened-to genres in each city for each period. Overall,\\nwe aim to capture significant patterns in genre preferences across\\ndifferent cities using B [∆] *[𝜏]*\\nBackbone [.]\\nGiven the bimonthly sequence of bipartite backbone networks\\nB [∆] *[𝜏]*\\nBackbone [, we start by analyzing how genre preferences vary and]\\n\\n\\n82\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Filipe A. S. Moura, Carlos H. G. Ferreira, and Helen C. S. C. Lima\\n\\n\\npersist across cities. We focus on the distribution of genre popularity\\nin each city (i.e., whether all genres in the backbone are listened to\\nwith the same intensity). To measure this distribution variation, we\\ncalculated the Gini index, a well-established conventional measure\\nof income inequality [ 11 ], for the genres in the backbone for each\\ncity. First, for each city and time window Δ *𝜏*, we sorted the genre\\nlink weights in its corresponding backbone and used them as input\\n\\n\\nto the following Gini equation *𝐺* = [2] *𝑛* [�] [�] *[𝑛]* *𝑖* = *[𝑛]* *𝑖* = 1 1 *[𝑖]* [·] *[𝑦]* *[𝑦]* *[𝑖][𝑖]* [−] *[𝑛]* *𝑛* [+][1] [.]\\n\\nIn this definition, *𝑛* is the total number of genres, *𝑦* *𝑖* is the *𝑖* -th\\nlink weight in the sorted set, [�] *[𝑛]* *𝑖* =1 *[𝑦]* *[𝑖]* [is the sum of all weights, and]\\n� *𝑛𝑖* =1 *[𝑖]* [·] *[𝑦]* *[𝑖]* [is the weighted sum of the values in the set, where] *[ 𝑖]* [is the]\\nindex of the value. The second term, *[𝑛]* *𝑛* [+][1] [, is the normalization that]\\n\\nguarantees that the value is between 0 and 1, where 0 means equally\\ndistributed and 1 means unequally distributed. In this way, we want\\nto find out if the cities still have differences in these preferences\\naccording to their preferred genres in the backbone, essentially\\nidentifying the most preferred among the preferred genres.\\nNext, we analyzed the persistence of genre preferences over time\\nby examining the fraction of genres that remain in the backbone\\nacross sequential periods Δ *𝜏* and Δ *𝜏* +1 for each city. Our main idea\\nhere is to determine whether cities have a well-defined collection of\\npreferred genres that persist over the entire period. These analyses\\nallow us to understand how genre preferences vary across cities\\nin Brazil over the years, addressing **RQ1** . They also help us to\\nidentify the regional factors that contribute to the persistence of\\nthese patterns, as described in the next section.\\n### **4.2 Modeling track acoustic features and** **regional patterns**\\n\\nRecall that the goal of RQ2 is to understand the track acoustic features to reveal regional patterns of the preferred tracks in different\\ncities and even within a genre in different regions. To achieve this,\\nwe first use the information about music and genre preferences\\nobtained from the backbones, as described in the previous section.\\nHowever, the edges of such backbones do not encode the many\\ndimensions that make up the track acoustic features (detailed in\\nSection 3), so multivariate analysis is required. Given the persistence of the genres analyzed in RQ1, as we will discuss in Section\\n5, we assume that cities show very small variation of preferred\\ngenres listened to over time, making temporal analysis redundant.\\nTherefore, we opted for analyzing a unique aggregated view of the\\nregional time period on the track acoustic features. Then, we use\\nthe genre preference information from the backbones from RQ1,\\ngather the track acoustic features, perform feature engineering, and\\nuse K-means clustering on these features to create clusters that\\ntackle our RQ2, as detailed below.\\nFrom the backbones B [∆] *[𝜏]*\\nBackbone [, which capture the preferences of]\\ngenres, for each city *𝑐* *𝑖* a set of tracks *𝑆* *𝑐* *𝑖* is built. This set *𝑆* *𝑐* *𝑖* consists\\nof the tracks from genres that belong to the backbone of genres\\nlinked to city *𝑐* *𝑖* in *𝑉* *𝐶𝑖𝑡𝑦* . However, we noticed that the same genre,\\npreferred by two or more cities according to our backbone, may or\\nmay not have become preferred because of the same tracks. Thus,\\nwe observed that popular tracks of various genres appear in many\\ncities, while others are exclusive to a given city. In the first case,\\nthese tracks are national hits of the genre, which do not contribute\\n\\n\\nto revealing specific regional patterns. In the second case, they\\nare tracks capable of revealing regional particularities, which are\\nclosely related to our objectives here. Based on this observation, for\\neach city *𝑐* *𝑖*, we have discarded instances of tracks that are listened\\nto in at least one other city (e.g., a track is only kept in the set if\\nit is exclusively represented in the chart by the city in question).\\nThus, each city has a representative and exclusive set of tracks that\\nbelong to different genres and were preferred by it over the entire\\nperiod analyzed.\\nNonetheless, we have retained a significant number of exclusive\\ntracks for each city that are potentially capable of revealing the\\npatterns of interest. More specifically, the number of tracks in the\\nbackbone for each city has decreased from 700–900 to 340–570.\\nWe found that most of the removed tracks belonged to very popular genres in Brazil, such as *sertanejo* and *funk*, including their\\nsub-genres (e.g. *agronejo*, *funk-mtg* ). These genres accounted for\\nabout 45% and 10% of the removed tracks for each city, respectively.\\nAnother frequently removed genre was *arrocha*, which accounted\\nfor 10% of the removed tracks.\\n\\nAfter this, for each city *𝑐* *𝑖* and each track in its respective set\\n*𝑆* *𝑐* *𝑖*, we gathered the values of the track acoustic features explained\\nin Section 3, such as acousticness, valence, danceability, etc. From\\nthe distribution of values for each acoustic feature in the set *𝑆* *𝑐* *𝑖* of\\ncity *𝑐* *𝑖*, we extracted the four moments: *mean*, *variance*, *skewness*,\\nand *kurtosis* . The intuition behind extracting these moments is to\\ncapture different aspects of the distribution of acoustic features\\nfor each city beyond the mean, allowing to obtain more consistent\\nclusters [ 18, 35 ]. With this, we obtained a final feature matrix of 17\\nCities x 28 features (7 acoustic features x 4 moments). By analyzing\\nthis matrix, we expected to identify regional and cultural patterns\\nin track preferences, providing a detailed view of the predominant\\nacoustic characteristics in cities aggregated by region.\\nTo cluster the cities according to the audio characteristics of their\\npreferred tracks, we started by applying Principal Component Analysis (PCA) to the final matrix of 17 Cities x 28 features (7 acoustic\\nfeatures x 4 moments) [ 18 ]. PCA was used to reduce the dimensionality of the data while retaining most of the explained variance\\n\\n[ 23 ]. Ultimately, we adopted 5 principal components, which corresponded to 82% of the explained variance. With the reduced data\\nmatrix, we then used the K-means clustering algorithm to group the\\ncities based on the acoustic characteristics of their preferred tracks\\n\\n[ 18, 19 ]. Additionally, we used the silhouette index to determine\\nthe optimal number of clusters *𝑘*, which was found to be *𝑘* = 6,\\nwith a value of 0.38 indicating fair clustering [38].\\nFinally, we characterized each cluster based on the most popular\\ngenres and the track acoustic features. This alignment addresses\\n**RQ2** : the first aspect illustrates the favorite genres of a group of\\ncities, while the second aspect reveals the similarities in the track\\nacoustic features between these clusters. However, clusters may\\nexhibit statistically equivalent average acoustic properties. To distinguish statistically significant differences, we employed a one-way\\nanalysis of variance (ANOVA) [21, 44].\\n### **5 RESULTS**\\n\\nThis section presents our results. First, we present a topological\\ncharacterization of the networks modeled. Then we present the\\nresults for our RQ1 and finally for RQ2.\\n\\n\\n83\\n\\n\\n-----\\n\\nCharacterization of the Brazilian musical landscape WebMedia’2024, Juiz de Fora, Brazil\\n\\n### **5.1 Topological analysis of genre preference** **networks**\\n\\nAs described in Section 4, we extracted the backbone for all instances of B [∆] *[𝜏]* with a *p-value <* 0 *.* 05 to determine the preferred\\ngenres of each city. Networks were grouped by year and the average\\nand standard deviation for some metrics are displayed in Table 1 of\\nsix networks per year. On average, the backbone retained about 25%\\nof the nodes and 11% of the edges. This significant reduction in network size allows us to focus on the most salient genre preferences\\nfor each city, as shown in Figure 1.\\n\\n**Table 1: Topology of the original and backbone networks for**\\n**all instances of** B [∆] *[𝜏]* **in 2022 and 2023.**\\n\\n**Metric** **Ori** **g** **inal** **Backbone**\\n\\n**2022** **2023** **2022** **2023**\\n\\n# *𝑉* *𝐶𝑖𝑡* *𝑦* 17 17 17 17\\n# *𝑉* *𝐺𝑒𝑛𝑟𝑒* 118 ± 6.6 112 ± 20.7 25.6 ± 2.6 28.3 ± 4.2\\nˆ # Ed g es 1422 ± 176.8 1211.5 ± 203.7 161.8 ± 12.7 154.5 ± 23.3\\nˆ *𝑘* *𝑖𝑛* ( *𝑉* *𝐶𝑖𝑡* *𝑦* ) 76.8 ± 12.8 63.8 ± 12.7 9.4 ± 3.4 8.9 ± 3.3\\n*𝑘* *𝑜𝑢𝑡* ( *𝑉* *𝐺𝑒𝑛𝑟𝑒* ) 11.3 ± 6.4 9.9 ± 6.6 6.19 ± 6.18 5.4 ± 5.8\\n\\nWe observe the number of nodes and averaged metrics in the\\nbimonthly original networks is quite similar between the two years,\\nwith 118±6.6 in 2022 and 112±20.7 in 2023. There is a significant\\ndecrease in the backbone networks, with an average of 25.6±2.6\\nnodes in 2022 and 28.3±4.2 nodes in 2023. The average number\\nof edges also decreased from 1422±176.8 and 1211.5±203.7 in the\\noriginal networks to 161.8±12.7 and 154.5±23.3 in the backbone\\nnetworks for 2022 and 2023, respectively. The average in-degree\\n( *𝑘* [ˆ] *𝑖𝑛* ) for *𝑉* *𝐶𝑖𝑡𝑦* (cities) decreased from 76.8±12.8 and 63.8±12.7 in\\nthe original networks to 9.4±3.4 and 8.9±3.3 in the backbone networks, respectively. Similarly, the average out-degree ( *𝑘* [ˆ] *𝑜𝑢𝑡* ) for\\n*𝑉* *𝐺𝑒𝑛𝑟𝑒* (genres) decreased from 11.3±6.4 and 9.9±6.6 to 6.19±6.18\\nand 5.4±5.8, respectively. The significant reduction in the number of\\nnodes and edges in the backbone networks emphasises the sparsity\\nand more focused nature of the backbone, which is essential for\\nidentifying the most relevant genre preferences for each city.\\n\\n200\\n\\n150\\n\\n100\\n\\n50\\n\\n0\\n\\n|Col1|Col2|\\n|---|---|\\n|||\\n\\n\\n\\nCities\\n\\n**Figure 1: Genres present in the** B [∆] Backbone *[𝜏]* **[compared with all]**\\n**genres listened to in each city.**\\n\\n\\n**Figure 2:** B [∆] *[𝜏]* **, which represent the time windows of**\\nΔ *𝜏* = **January/February 2022.**\\n### **5.2 Analysis of musical genre preferences**\\n\\nWe then move on to analyze the distributed preference of genres\\nin the backbone via the Gini index to determine whether cities\\n\\nlisten to their favorite genres with similar intensity in the months\\nanalyzed. Figure 4 shows these results in the form of a heatmap. In\\nthis heatmap, the *𝑥* -axis represents the different cities and the *𝑦* -axis\\nrepresents the bimonthly time windows from January/February\\n2022 to November/December 2023. Each cell in the heatmap shows\\nthe Gini index for a specific city and time window, with the color\\nintensity indicating the degree of inequality in genre preferences.\\nThe Gini index values range from 0 to 1, with values closer to 0\\nindicating a more even distribution of genre preferences and values\\ncloser to 1 indicating a more uneven distribution.\\nThe Gini index values for cities are mostly between 0.1 and 0.4,\\nsuggesting overall balanced genre preferences. However, this balance could mask nuances in the distribution. For example, a Gini\\nindex of 0.1 could mean that there are two sets of genres that are\\nvery popular, while others are leveled down. The heatmap allows\\n\\n\\nWe also note that although most cities listened more than 150\\nunique genres, only a maximum of 30 were considered urban preferences in the two years. To illustrate these topological patterns of\\nrevealed genre preferences, we show in Figures 2 and 3 the original\\nnetwork view for our first bipartite network corresponding to the\\nperiod January/February 2022. The darker the edge, the stronger\\nthe preference of this genre for the connected city. If we follow our\\nhypothesis that such genres are maintained and reinforced in the\\nnetwork behavior, we see that out of the 142 genres in the original network, only 17 remain in the backbone network, indicating\\nthat each city has a limited number of genres that are consistently\\npreferred over time.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n84\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Filipe A. S. Moura, Carlos H. G. Ferreira, and Helen C. S. C. Lima\\n\\n\\n\\n\\n1 *.* 0\\n\\n0 *.* 9\\n\\n0 *.* 8\\n\\n0 *.* 7\\n\\n0 *.* 6\\n\\n\\n\\n\\nJan,Feb/2022 - Mar,Apr/2022\\n\\nMar,Apr/2022 - May,Jun/2022\\n\\nMay,Jun/2022 - Jul,Aug/2022\\n\\nJul,Aug/2022 - Sep,Oct/2022\\n\\nSep,Oct/2022 - Nov,Dec/2022\\n\\nNov,Dec/2022 - Jan,Feb/2023\\n\\nJan,Feb/2023 - Mar,Apr/2023\\n\\nMar,Apr/2023 - May,Jun/2023\\n\\nMay,Jun/2023 - Jul,Aug/2023\\n\\nJul,Aug/2023 - Sep,Oct/2023\\n\\nSep,Oct/2023 - Nov,Dec/2023\\n\\n\\n\\n\\n\\n\\n\\n\\nCampo Grande\\n\\n\\n\\ntrap pesado\\n\\n\\n|0.88|0.67|0.86|1.0|0.82|1.0|0.73|0.8|0.73|0.78|0.82|0.67|0.92|0.83|0.82|0.88|0.86|\\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n|1.0|0.85|1.0|0.85|0.82|0.81|0.91|0.89|0.9|1.0|0.75|1.0|0.89|0.91|1.0|1.0|1.0|\\n|1.0|0.9|1.0|0.92|0.9|0.8|1.0|1.0|1.0|1.0|1.0|0.82|0.89|0.91|1.0|1.0|1.0|\\n|1.0|0.9|0.83|1.0|0.91|0.65|0.8|0.78|0.78|1.0|0.91|0.92|0.89|0.83|0.78|1.0|1.0|\\n|1.0|0.53|1.0|0.9|1.0|0.76|1.0|1.0|1.0|1.0|1.0|1.0|0.88|0.92|1.0|1.0|1.0|\\n|1.0|0.89|1.0|0.89|0.75|0.88|0.89|0.78|1.0|0.83|0.91|0.91|0.62|0.8|0.71|1.0|1.0|\\n|1.0|1.0|0.83|0.89|0.9|0.92|0.89|0.88|1.0|1.0|0.69|0.64|0.9|0.82|0.83|1.0|1.0|\\n|1.0|0.89|0.75|0.78|1.0|0.86|1.0|1.0|1.0|0.83|0.9|1.0|1.0|1.0|1.0|1.0|1.0|\\n|1.0|0.89|0.86|0.64|0.8|0.81|0.86|0.86|0.86|1.0|0.8|0.7|0.88|0.89|0.71|1.0|1.0|\\n|1.0|0.88|1.0|0.9|0.62|0.82|0.88|1.0|1.0|0.86|0.75|0.69|0.58|0.8|0.86|1.0|1.0|\\n|0.83|0.8|0.6|0.91|0.75|0.71|0.78|0.7|0.78|0.88|0.69|0.72|0.8|0.5|0.78|1.0|0.71|\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n**Figure 3:** B [∆] *[𝜏]*\\nBackbone **[, which represent the time windows of]**\\nΔ *𝜏* = **January/February 2022.**\\n\\n\\nJan,Feb/2022\\n\\nMar,Apr/2022\\n\\nMay,Jun/2022\\n\\nJul,Aug/2022\\n\\nSep,Oct/2022\\n\\nNov,Dec/2022\\n\\nJan,Feb/2023\\n\\nMar,Apr/2023\\n\\nMay,Jun/2023\\n\\nJul,Aug/2023\\n\\nSep,Oct/2023\\n\\nNov,Dec/2023\\n\\n\\n\\n0 *.* 40\\n\\n0 *.* 35\\n\\n0 *.* 30\\n\\n0 *.* 25\\n\\n0 *.* 20\\n\\n0 *.* 15\\n\\n0 *.* 10\\n\\n|0.22|0.260.|270.26|0.25|0.43|0.26|0.250.|260.26|0.21|0.260|.340.24|0.24|0.22|0.28|\\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n|0.22|0.370.|230.25|0.21|0.2|0.2|0.210.|270.21|0.2|0.240|.170.19|0.2|0.22|0.21|\\n|0.13|0.230.|140.26|0.28|0.22|0.23|0.240.|220.15|0.17|0.210|.170.18|0.26|0.13|0.14|\\n|0.12|0.190.|140.24|0.21|0.24|0.19|0.230.|220.15|0.22|0.180|.110.17|0.19|0.11|0.14|\\n|0.12|0.220.|140.22|0.19|0.22|0.23|0.230.|210.15|0.17|0.2 0|.120.18|0.2|0.11|0.14|\\n|0.13|0.180.|140.13|0.21|0.32|0.2|0.064 0.|130.14|0.21|0.170|.120.18|0.14|0.12|0.12|\\n|0.11|0.210.|120.21|0.17|0.2|0.26|0.12 0|.1 0.12|0.17|0.1 0|.310.35|0.12|0.11|0.11|\\n|0.1|0.10.0|99 0.13|0.19|0.22|0.16|0.17 0|.1 0.11|0.078|0.350|.120.34|0.1|0.091|0.1|\\n|0.099|0.13 0.|10.055|0.12|0.3|0.096|0.10.0|96 0.12|0.079|0.140|.170.14|0.11|0.091|0.1|\\n|0.11|0.160.|110.17|0.18|0.39|0.15|0.160.|110.13|0.26|0.099 0|.160.12|0.12|0.11|0.11|\\n|0.12|0.160.|120.16|0.16|0.37|0.15|0.120.|110.14|0.24|0.290|.290.17|0.12|0.11|0.12|\\n|0.12|0.240.|210.24|0.27|0.32|0.23|0.260.|280.27|0.37|0.330|.260.39|0.22|0.11|0.13|\\n\\n\\nCities\\n\\n**Figure 4: Heatmap of the Gini index for each city across**\\n**bimonthly time windows.**\\n\\nus to observe patterns and changes in the distribution of genre\\npreferences over time and in different cities. From the heatmap, we\\ncan see that certain cities such as Rio de Janeiro, Manaus and Belém\\nhave relatively higher Gini index values, indicating a more unequal\\ndistribution of genre preferences in some time windows. On the\\nother hand, cities such as Cuiabá, Campo Grande and Uberlândia\\nshow more balanced genre preferences with lower Gini index values throughout the analyzed period. We thus observe sometimes\\nconsistency, sometimes variation in genre preferences between different cities and time windows, which helps us to understand the\\ndistribution of music tastes in the Brazilian cities analyzed.\\nWe analyzed the persistence of genres in the backbone over time,\\naiming to identify whether cities listen to the same genres with\\n\\n\\n0 *.* 5\\n\\nCities\\n\\n**Figure 5: Heatmap of the genre persistence (%) of** Δ *𝜏* **in** Δ *𝜏* +1\\n**for each city**\\n\\nconsistent intensity, as shown in Figure 5. In this heatmap, the\\n*x-axis* represent the cities and the *y-axis* represent the compared\\nbimonthly time windows Δ *𝜏* and Δ *𝜏* +1 . Each cell shows the percentage of genres from Δ *𝜏* that are still present in Δ *𝜏* +1 for each city. This\\npersistence analysis revealed two main findings. First, all cities have\\na well-defined group of preferred genres, indicated by a high persistence degree. Second, certain months showed significant changes\\nin genre preferences compared to the previous months in most\\ncities, except for Cuiabá, Campo Grande, and Uberlândia. These\\nmonths are January/February, March/April, September/October,\\nand November/December, which correspond to popular holidays\\nin Brazil. January/February and March/April coincide with the\\ncarnival period, characterized by nationwide parties and specific\\nmusic genres. September/October and November/December are\\nthe last four months of the year, associated with Christmas and\\nNew Year’s Eve celebrations. These holidays could increase the\\npopularity of certain genres or change the acoustic characteristics\\nof genres during these periods.\\nAlthough we found seasonal differences, we did not formally\\nestablish a causal connection between the genres in these seasonal\\nwindows and the acoustic features, but a potential link could exist.\\nBased on these findings, we divided the cities into two groups: those\\nthat consistently listen to the same genres over the years and those\\nwith seasonal variations. Cuiabá, Campo Grande, and Uberlândia\\nstand out as cities with no significant changes in both years. These\\ncities are geographically close, but other geographically close cities,\\nsuch as São Paulo, Campinas, and Belo Horizonte, do not exhibit\\nthe same behavior. Besides Cuiabá, Campo Grande, and Uberlândia,\\nall other cities show seasonal variances, mostly between 10% and\\n30% in different genres. Overall, the favorite genres of each city\\nvary very little, indicating that the preferences are stable with some\\nseasonal influences.\\nTo summarize, preferences for music genres in Brazilian cities\\nshow a stable set of preferred genres, with backbone networks\\nhighlighting the most important ones. The Gini index indicates\\nbalanced preferences, while persistence analysis reveals minimal\\nchanges over time, except during major holidays. Overall, genre\\npreferences remain consistent, with cultural events contributing to\\noccasional variations.\\n\\n\\n85\\n\\n\\n-----\\n\\nCharacterization of the Brazilian musical landscape WebMedia’2024, Juiz de Fora, Brazil\\n### **5.3 Analysis of track acoustic features and** **regional patterns**\\n\\nIn our RQ2, we wanted to understand to what extent the intrinsic\\nacoustic features of tracks are independent of their musical genre\\nand whether there are acoustic differences between different regions within the same musical genre. As explained in Section 4, we\\napplied K-means clustering and used the average silhouette score\\nto evaluate the quality of the resulting clusters on track acoustic\\nfeatures, uncovering 6 clusters composed of the following cities:\\n\\n  - **Cluster 1:** Florianópolis, Porto Alegre, Curitiba, and Belo\\n\\nHorizonte\\n\\n  - **Cluster 2:** Goiânia, Brasília, and Uberlândia\\n\\n  - **Cluster 3:** Recife, Fortaleza, and Salvador\\n\\n  - **Cluster 4:** Rio de Janeiro, Belém, and Manaus\\n\\n  - **Cluster 5:** Cuiabá and Campo Grande\\n\\n  - **Cluster 6:** São Paulo and Campinas\\n\\n\\nWe start by presenting in Figure 6 the geographic distribution\\nof each cluster in the Brazilian territory. Geographically, we can\\ncharacterize Cluster 1 as the Southern of Brazil, with the addition\\nof Belo Horizonte. This is a distinctive pattern since Belo Horizonte is in the Southeast but clusters with Southern cities. Cluster 2\\n\\nlikely represents the Central region, including part of the Midwest\\nand the city of Uberlândia, bridging the Midwest and Southeast.\\nCluster 3, consisting of Recife, Fortaleza, and Salvador, aligns well\\nwith expectations as these are Northeastern cities, showing regional acoustic similarities. Cluster 4 contains Rio de Janeiro along\\nwith Belém and Manaus, indicating a notable divergence as Rio de\\nJaneiro typically aligns with Southeastern cities. This cluster suggests that geographic distance was not a barrier to grouping these\\ncities acoustically. Cluster 5 represents part of Brazil’s Midwest\\nwith Cuiabá and Campo Grande, reflecting acoustic similarities\\nwithin this region. Finally, Cluster 6 includes São Paulo and Campinas, suggesting that these two cities from the state of São Paulo\\nhave unique acoustic features that set them apart from other Southeastern cities, further emphasizing the distinctiveness within the\\nSoutheast region itself. These clusters reveal both expected and\\nunexpected patterns, illustrating the complex regional and cultural\\nvariations in the acoustic features of tracks across Brazilian cities.\\n\\nWe move to our analysis of the exclusive tracks in each cluster,\\nresulting in the classification of the genre preferences, as shown in\\nFigure 7. The heatmap illustrates the distribution of genre preferences across different clusters. The *𝑥* -axis represents the clusters,\\nwhile the *𝑦* -axis represents the genres. Each cell shows the percentage of tracks in the cluster that belong to a particular genre. For\\nvisualization purposes, we selected genres for the heatmap where\\nthe sum of all cells in a given row (genre) is at least 10% of what is\\nlistened to in the clusters. This threshold provided a good trade-off\\nbetween highlighting the most popular genres of the clusters and\\nincluding the sub-genres of these top genres. A higher threshold\\nwould have resulted in only the top genres being displayed, obscuring sub-genres, while a lower threshold would have displayed less\\npopular genres, complicating visualization and interpretation.\\nWe found that Clusters 2 and 5 share similar genre preferences,\\nfocused on *sertanejo* and related styles. Similarly, Cluster 1 and\\nCluster 6 both prefer *funk* and its sub-genres. Although these clusters are geographically close, they suggest track acoustic features\\n\\n\\n0 *.* 00\\n\\n|4.40|0.20|0.22|10.51|0.00|0.00|\\n|---|---|---|---|---|---|\\n|2.20|9.15|0.67|0.00|18.97|0.00|\\n|3.30|10.16|19.69|2.39|13.79|4.17|\\n|7.69|4.27|2.68|5.73|1.72|11.46|\\n|13.74|1.63|0.67|0.32|0.00|9.38|\\n|7.69|2.85|1.12|3.03|0.00|2.08|\\n|2.75|2.03|0.22|0.64|0.57|10.42|\\n|1.65|4.27|26.62|0.96|1.72|0.00|\\n|1.65|15.45|1.57|1.91|20.69|2.08|\\n|0.55|14.43|0.89|0.64|20.69|1.04|\\n|1.10|1.02|0.00|3.66|0.00|7.29|\\n|1.10|1.22|3.13|7.64|0.00|12.50|\\n|0.55|0.41|0.22|2.71|0.57|10.42|\\n|0.00|11.38|0.22|0.00|18.97|0.00|\\n\\n\\n\\nClusters\\n\\n**Figure 7: Most relevant genres in each cluster (%).**\\n\\ndifferences within the same group of genres. In contrast, Cluster 3\\nand Cluster 4 did not show significant similarities with any other\\ncluster, despite their proximity to each other. Cluster 3 strongly\\nprefers *forró* and *arrocha*, while Cluster 4 has a more general genre\\npreference.\\nGiven the identified genre patterns, we analyzed significant differences in track acoustic features between clusters with similar\\n\\ngenre preferences. We used one-way ANOVA with a *p-value* of 0.05\\nto identify differences. This analysis revealed significant differences\\nin 9 of 13 track acoustic features between at least two clusters. We\\nused the average of these features to profile and compare clusters\\nwith similar genre preferences, as shown in Figure 8.\\nIn these radar plots, each axis represents one of the acoustic\\nfeatures (e.g., acousticness, valence, danceability). The values on\\nthe axes indicate the average feature value for each cluster. The\\n\\n\\n**Figure 6: Geographical distribution of the cities in the indi-**\\n**vidual clusters, with each color representing a cluster.**\\n\\n\\npop\\n\\nagronejo\\n\\narrocha\\n\\nfunk carioca\\n\\nfunk mtg\\n\\nfunk rj\\n\\nfunk paulista\\n\\nforro\\n\\nsertanejo universitario\\n\\nsertanejo\\n\\ntrap funk\\n\\ntrap brasileiro\\n\\nfunk consciente\\n\\nsertanejo pop\\n\\n\\n\\n0 *.* 25\\n\\n0 *.* 20\\n\\n0 *.* 15\\n\\n0 *.* 10\\n\\n0 *.* 05\\n\\n\\n86\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Filipe A. S. Moura, Carlos H. G. Ferreira, and Helen C. S. C. Lima\\n\\n\\n0 *.* 00\\n\\n\\n1 *.* 00\\n\\n0 *.* 75\\n\\n0 *.* 50\\n\\n0 *.* 25\\n\\n0 *.* 00\\n\\n\\nvalence\\n\\nliveness\\n\\n\\nacousticness\\n\\n1.0\\n\\nspeechiness\\n\\n\\ndanceability\\n\\nenergy\\n\\n\\nvalence\\n\\nliveness\\n\\n\\nacousticness\\n\\n1.0\\n\\nspeechiness\\n\\n\\ndanceability\\n\\nenergy\\n\\n\\nvalence\\n\\nliveness\\n\\n\\nacousticness\\n\\n1.0\\n\\nspeechiness\\n\\n\\ndanceability\\n\\nenergy\\n\\n\\n1 *.* 00\\n\\n0 *.* 75\\n\\n\\n0 *.* 50\\n\\n0 *.* 25\\n\\n\\n0 *.* 8\\n\\n0 *.* 6\\n\\n0 *.* 4\\n\\n0 *.* 2\\n\\n0 *.* 0\\n\\n\\nCluster 1 Cluster 2 Cluster 3 Cluster 4 Cluster 5 Cluster 6\\n\\n**Figure 8: Average tracks’ audio features across clusters.**\\n\\nfirst plot compares Cluster 2 (Goiânia, Brasília, and Uberlândia)\\nand Cluster 5 (Cuiabá and Campo Grande), which have preferences\\nfor *sertanejo* and related styles. The notable difference is in *live-*\\n*ness*, while other features are relatively similar. Both clusters show\\nhigh *energy*, indicating a preference for energetic national country\\nmusic. The second plot compares Cluster 1 (Florianópolis, Porto\\nAlegre, Curitiba, and Belo Horizonte) and Cluster 6 (São Paulo\\nand Campinas), which are associated with *funk* and its sub-genres.\\nThe main differences are in *speechiness*, *acousticness*, and *energy* .\\nHowever, *danceability* stands out as a core feature for *funk* across\\nboth clusters. Finally, the third plot compares Cluster 3 (Recife,\\nFortaleza, and Salvador) and Cluster 4 (Rio de Janeiro, Belém, and\\nManaus), which do not share common genre preferences. Cluster 3\\nhas higher values for most features than Cluster 4, but both clusters\\ncharacterize their regions — the North with Rio de Janeiro and the\\nNortheast — as very energetic and dance-oriented.\\nLastly, we deeply explored the distribution of the outstanding\\ntrack acoustic features of each comparison made in Figure 8, as\\nshown in Figure 9. These boxplots provide a detailed view of the\\ndistribution for *liveness*, *speechiness*, and *valence* in all clusters. The\\nfirst plot illustrates the *liveness* feature for all clusters. Clusters\\n2 (Goiânia, Brasília, and Uberlândia) and 5 (Cuiabá and Campo\\nGrande), which share similar *sertanejo* preferences, contrast significantly with the other clusters. This suggests that *sertanejo* tracks\\nare more associated with live performances compared to other genres. The second plot presents the *speechiness* feature. Clusters 1\\n(Florianópolis, Porto Alegre, Curitiba and Belo Horizonte) and 6\\n(São Paulo and Campinas), the *funk* clusters, show a higher lyrical\\npresence in their favorite tracks. This suggests that these clusters\\nvalue lyrical content more and differentiate the types of *funk* they\\nprefer. The *funk* of Cluster 1 ( *mtg, bh, carioca* ) focuses on rhythm\\nand energy, while Cluster 6 emphasizes a more lyrical funk ( *paulista,*\\n*consciente* ). The third plot shows the *valence* feature. Cluster 4 (Rio\\nde Janeiro, Belém, and Manaus) shows a more dispersed distribution, indicating a preference for more “negative” music compared\\nto the other clusters.\\nTo summarise, the Brazilian city clusters show different acoustic\\npatterns in the tracks, regardless of the musical genres. Significant\\ndifferences were observed in features such as *liveness*, *speechiness*,\\nand *valence* between clusters with similar genre preferences. This\\nreveals regional and cultural differences in musical preferences and\\nacoustic nuances within the same genres in different regions.\\n### **6 CONCLUSION AND FUTURE WORKS**\\n\\nIn this study, we collected and analyzed data on digital music consumption from Spotify, focusing on various cities in Brazil over\\n\\n\\nCluster 1 Cluster 2 Cluster 3 Cluster 4 Cluster 5 Cluster 6\\n\\n**Figure 9: Distribution of values for the features** ***liveness*** **,**\\n***speechiness*** **, and** ***valence*** **across clusters.**\\n\\nthe period from 2022 to 2023. This analysis aimed to understand\\nthe regional and cultural patterns in musical preferences and how\\nthese preferences are reflected in both genre and acoustic characteristics. Our approach involved modeling bipartite networks of\\ngenres and cities, extracting their backbones to highlight significant\\npreferences, and performing detailed temporal and cluster analyses.\\nThe primary objectives were to address two research questions.\\nIn response to RQ1, we discovered that each city has a well-defined\\nset of favorite genres that remain relatively stable over time. These\\nfavorite genres are consistently preferred, but there are occasional\\nseasonal variations influenced by major holidays like Christmas,\\nNew Year’s Eve, and Carnival. It follows the results of previous\\nstudies which shows Brazil as one of the countries that most listen\\n\\nto national genres and how diverse their preference can be over\\ndifferent regions [ 6, 25, 30 ]. Our analysis has shown that the cities\\ncan be divided into two groups: one group, which includes Cuiabá,\\nCampo Grande, and Uberlândia, consume their strict favorite genres\\nthroughout the year, while the other group has seasonal preferences\\nin addition to their favorite genres. This distinction sheds light on\\nhow cultural events influence musical tastes in different regions of\\nBrazil.\\n\\nRegarding RQ2, our clustering based on track acoustic features\\nrevealed distinct regional patterns. We identified six clusters of\\ncities, some of which showed strong geographical ties, while others,\\nsuch as the cluster including Rio de Janeiro, Belém, and Manaus,\\ndemonstrated that geographical distance was not a barrier to similar\\nacoustic preferences, even their genre consumption are significantly\\ndifferent, as shown by previous authors [ 25, 30, 45 ]. Notably, we\\nfound that clusters with similar genre preferences could exhibit\\nsignificant acoustic differences. For example, Cluster 5 (Cuiabá\\nand Campo Grande) and Cluster 2 (Goiânia, Brasília, and Uberlândia) both prefer *sertanejo*, but with different acoustic profiles.\\nThis suggests that even within the same genre, there are regional\\ndifferences in acoustic preferences that reflect broader cultural and\\nregional influences. Our results show the importance of considering\\nboth genre and acoustic features to understand regional musical\\npreferences. These findings can be valuable for the development\\nof targeted marketing strategies and the improvement of music\\nrecommendation systems.\\nFor future work, we plan to scale the analysis to a global level\\nto gain even deeper insights into the cultural and regional diversity of musical preferences and to use the backbone extraction\\nmethodology to cope with the heterogeneity of global music data.\\n\\n\\n87\\n\\n\\n-----\\n\\nCharacterization of the Brazilian musical landscape WebMedia’2024, Juiz de Fora, Brazil\\n\\n### **REFERENCES**\\n\\n[1] Moonis Ali and Savvas Zannettou. 2024. From Isolation to Desolation: Investigating Self-Harm Discussions in Incel Communities. 18 (May 2024), 43–56.\\nhttps://doi.org/10.1609/icwsm.v18i1.31296\\n\\n[2] Carlos Araujo, Marco Cristo, and Rafael Giusti. 2019. Predicting Music Popularity\\non Streaming Platforms. In *Anais do XVII Simpósio Brasileiro de Computação*\\n*Musical* (São João del-Rei). SBC, Porto Alegre, RS, Brasil, 141–148. https://doi.\\norg/10.5753/sbcm.2019.10436\\n\\n[3] Mariana Lopes Barata and Pedro Simoes Coelho. 2021. Music streaming services:\\nunderstanding the drivers of customer purchase and intention to recommend.\\n*Heliyon* 7, 8 (2021).\\n\\n[4] Gabriel Barbosa, Bruna Melo, Gabriel Oliveira, Mariana Silva, Danilo Seufitelli, and Mirella Moro. 2021. Hot Streaks in the Brazilian Music Market:\\nA Comparison Between Physical and Digital Eras. In *Anais do XVIII Simpósio*\\n*Brasileiro de Computação Musical* (Recife). SBC, Porto Alegre, RS, Brasil, 152–159.\\nhttps://doi.org/10.5753/sbcm.2021.19440\\n\\n[5] Dogan Basaran and Keti Ventura. 2022. Exploring Digital Marketing In Entertainment Industry: A Case Of A Digital Music Platform. *Journal of Management*\\n*Marketing and Logistics* 9, 3 (2022), 115–126.\\n\\n[6] Pablo Bello and David Garcia. 2021. Cultural Divergence in popular music: the increasing diversity of music consumption on Spotify across countries. *Humanities*\\n*and Social Sciences Communications* 8, 1 (2021), 1–8.\\n\\n[7] Pauwke Berkers. 2012. Gendered scrobbling: Listening behaviour of young adults\\non Last. fm. *Interactions: Studies in Communication & Culture* 2, 3 (2012), 279–296.\\n\\n[8] Jean-Samuel Beuscart, Samuel Coavoux, and Jean-Baptiste Garrocq. 2023. Listening to music videos on YouTube. Digital consumption practices and the environmental impact of streaming. *Journal of Consumer Culture* (2023), 654–671.\\n\\n[9] Michele Coscia. 2021. Noise Corrected Sampling of Online Social Networks. *ACM*\\n*Transactions on Knowledge Discovery from Data (TKDD)* 15, 2 (2021), 1–21.\\n\\n[10] Anne Danielsen and Yngvar Kjus. 2019. The mediated festival: Live music as\\ntrigger of streaming and social media engagement. *Convergence* (2019).\\n\\n[11] Robert Dorfman. 1979. A formula for the Gini coefficient. *The review of economics*\\n*and statistics* (1979), 146–149.\\n\\n[12] Maura Edmond. 2014. Here we go again: Music videos after YouTube. *Television*\\n*& New Media* 15, 4 (2014), 305–320.\\n\\n[13] Falina Enriquez. 2022. Pernambuco and Bahia’s Musical “War”: Contemporary\\nMusic, Intraregional Rivalry, and Branding in Northeastern Brazil. *Luso-Brazilian*\\n*Review* 59, 1 (2022), 22–60.\\n\\n[14] Ghazal Fazelnia, Eric Simon, Ian Anderson, Benjamin Carterette, and Mounia Lalmas. 2022. Variational User Modeling with Slow and Fast Features. In *Proceedings*\\n*of the Fifteenth ACM International Conference on Web Search and Data Mining*\\n(Virtual Event, AZ, USA) *(WSDM ’22)* . Association for Computing Machinery,\\nNew York, NY, USA, 271–279. https://doi.org/10.1145/3488560.3498477\\n\\n[15] Andres Ferraro, Xavier Serra, and Christine Bauer. 2021. What is fair? Exploring\\nthe artists’ perspective on the fairness of music streaming platforms. In *IFIP*\\n*conference on human-computer interaction* . Springer, 562–584.\\n\\n[16] Carlos Henrique Gomes Ferreira, Fabricio Murai, Ana P. C. Silva, Martino Trevisan, Luca Vassio, Idilio Drago, Marco Mellia, and Jussara M. Almeida. 2022. On\\nnetwork backbone extraction for modeling online collective behavior. *PLOS ONE*\\n17, 9 (09 2022), 1–36. https://doi.org/10.1371/journal.pone.0274218\\n\\n[17] Jonathon Grasse. 2021. Musical Spaces and Deep Regionalism in Minas Gerais,\\nBrazil. In *Musical Spaces* . Jenny Stanford Publishing, 5–21.\\n\\n[18] Jiawei Han, Jian Pei, and Hanghang Tong. 2022. *Data mining: concepts and*\\n*techniques* . Morgan kaufmann.\\n\\n[19] John A Hartigan and Manchek A Wong. 1979. Algorithm AS 136: A k-means\\nclustering algorithm. *Journal of the royal statistical society. series c (applied*\\n*statistics)* 28, 1 (1979), 100–108.\\n\\n[20] David Hesmondhalgh. 2022. Streaming’s effects on music culture: Old anxieties\\nand new simplifications. *Cultural Sociology* 16, 1 (2022), 3–24.\\n\\n[21] David C Howell. 1992. *Statistical methods for psychology* . PWS-Kent Publishing\\nCo.\\n\\n[22] Julie Jiang, Aditiya Ponnada, Ang Li, Ben Lacker, and Samuel F Way. 2024. A\\nGenre-Based Analysis of New Music Streaming at Scale. In *Proceedings of the*\\n*16th ACM Web Science Conference* (<conf-loc>, <city>Stuttgart</city>, <country>Germany</country>, </conf-loc>) *(WEBSCI ’24)* . Association for Computing Machinery, New York, NY, USA, 191–201. https://doi.org/10.1145/3614419.\\n3644002\\n\\n[23] Ian T Jolliffe and Jorge Cadima. 2016. Principal component analysis: a review\\nand recent developments. *Philosophical transactions of the royal society A: Mathe-*\\n*matical, Physical and Engineering Sciences* 374, 2065 (2016), 20150202.\\n\\n[24] Maarit Kinnunen, Harri Homi, and Antti Honkanen. 2020. Social sustainability\\nin adolescents’ music event attendance. *Sustainability* 12, 22 (2020), 9419.\\n\\n[25] Conrad Lee and Padraig Cunningham. 2012. The Geographic Flow of Music. (04\\n2012). https://doi.org/10.1109/ASONAM.2012.237\\n\\n[26] Conrad Lee and Padraig Cunningham. 2023. Number of music streaming subscribers worldwide from the 1st half of 2019 to 3rd quarter 2023. (2023). https:\\n//www.statista.com/statistics/669113/number-music-streaming-subscribers/\\n\\n\\n\\n[27] Elisabeth Lex, Dominik Kowald, and Markus Schedl. 2020. Modeling Popularity\\nand Temporal Drift of Music Genre Preferences. *Trans. Int. Soc. Music. Inf. Retr.* 3,\\n1 (2020), 17–30.\\n\\n[28] Renan S Linhares, José M Rosa, Carlos HG Ferreira, Fabricio Murai, Gabriel\\nNobre, and Jussara Almeida. 2022. Uncovering coordinated communities on\\ntwitter during the 2020 us election. In *2022 IEEE/ACM International Conference*\\n*on Advances in Social Networks Analysis and Mining (ASONAM)* . IEEE, 80–87.\\n\\n[29] Riccardo Marcaccioli and Giacomo Livan. 2019. A pólya urn approach to information filtering in complex networks. *Nature communications* (2019).\\n\\n[30] Maria Luiza Botelho Mondelli, Luiz M. R. Gadelha Jr., and Artur Ziviani. 2018.\\nO Que os Países Escutam: Analisando a Rede de Gêneros Musicais ao Redor do\\nMundo. In *Anais do VII Brazilian Workshop on Social Network Analysis and Mining*\\n(Natal). SBC, Porto Alegre, RS, Brasil. https://doi.org/10.5753/brasnam.2018.3586\\n\\n[31] Jeremy Wade Morris. 2020. Music platforms and the optimization of culture.\\n*Social Media+ Society* 6, 3 (2020), 2056305120940690.\\n\\n[32] Shane Murphy. 2020. Music marketing in the digital music industries–An autoethnographic exploration of opportunities and challenges for independent\\nmusicians. *International Journal of Music Business Research* 9, 1 (2020), 7–40.\\n\\n[33] Houssam Nassif, Kemal Oral Cansizlar, Mitchell Goodman, and SVN Vishwanathan. 2018. Diversifying music recommendations. *arXiv preprint*\\n*arXiv:1810.01482* (2018).\\n\\n[34] Zachary P Neal. 2022. backbone: An R package to extract network backbones.\\n*PloS one* 17, 5 (2022), e0269137.\\n\\n[35] Robert Nisbet, John Elder, and Gary D Miner. 2009. *Handbook of statistical analysis*\\n*and data mining applications* . Academic press.\\n\\n[36] Ramy A Rahimi and Kyung-Hye Park. 2020. A comparative study of internet\\narchitecture and applications of online music streaming services: The impact\\non the global music industry growth. In *2020 8th International Conference on*\\n*Information and Communication Technology (ICoICT)* . IEEE, 1–6.\\n\\n[37] Jing Ren and Robert J. Kauffman. [n. d.]. Understanding music track popularity\\nin a social network.(2017). In *Proceedings of the 25th European Conference on*\\n*Information Systems ECIS, Guimarães, Portugal, June* . 5–10.\\n\\n[38] Peter J Rousseeuw. 1987. Silhouettes: a graphical aid to the interpretation and\\nvalidation of cluster analysis. *Journal of computational and applied mathematics*\\n20 (1987), 53–65.\\n\\n[39] M Ángeles Serrano, Marián Boguná, and Alessandro Vespignani. 2009. Extracting\\nthe multiscale backbone of complex weighted networks. *Proceedings of the*\\n*national academy of sciences* 106, 16 (2009), 6483–6488.\\n\\n[40] Yading Song, Simon Dixon, and Marcus Pearce. 2012. A survey of music recommendation systems and future perspectives. In *9th international symposium on*\\n*computer music modeling and retrieval*, Vol. 4. Citeseer, 395–410.\\n\\n[41] Vilde Schanke Sundet and Marika Lüders. 2023. “Young people are on YouTube”:\\nindustry notions on streaming and youth as a new media generation. *Journal of*\\n*Media Business Studies* 20, 3 (2023), 223–240.\\n\\n[42] Fernando Terroso-Saenz, Jesús Soto, and Andres Muñoz. 2023. Music Mobility\\nPatterns: How Songs Propagate Around the World Through Spotify. *Pattern*\\n*Recognition* 143 (2023), 109807.\\n\\n[43] Pier Paolo Tricomi, Luca Pajola, Luca Pasa, and Mauro Conti. 2024. \"All of\\nMe\": Mining Users’ Attributes from their Public Spotify Playlists. In *Companion*\\n*Proceedings of the ACM on Web Conference 2024* . Association for Computing\\nMachinery, New York, NY, USA.\\n\\n[44] Gerald Van Belle. 2011. *Statistical rules of thumb* . John Wiley & Sons.\\n\\n[45] Gabriel Vaz de Melo, Ana Machado, and Lucas Carvalho. 2020. Music consumption in Brazil: an analysis of streaming reproductions. *PragMATIZES - Revista*\\n*Latino-Americana de Estudos em Cultura* 10 (09 2020), 141.\\n\\n[46] Samuel F. Way, Jean Garcia-Gathright, and Henriette Cramer. 2020. Local Trends\\nin Global Music Streaming. *Proceedings of the International AAAI Conference on*\\n*Web and Social Media* 14, 1 (May 2020), 705–714. https://doi.org/10.1609/icwsm.\\nv14i1.7336\\n\\n[47] Jacob Wolbert. 2023. Multiple Brasilidades: Musician-Market Negotiations within\\nthe Brazilian Midstream. *Journal of Popular Music Studies* 35, 3 (2023), 102–127.\\n\\n\\n88\\n\\n\\n-----\\n\\n',\n",
       "  'artigo_tokenizado': ['#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Characterization',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'musical',\n",
       "   'landscape',\n",
       "   ':',\n",
       "   'A',\n",
       "   'study',\n",
       "   'of',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   'regional',\n",
       "   'preferences',\n",
       "   'based',\n",
       "   'on',\n",
       "   'the',\n",
       "   'Spotify',\n",
       "   'charts',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Filipe',\n",
       "   'A.',\n",
       "   'S.',\n",
       "   'Moura',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'filipe.asm@aluno.ufop.edu.br',\n",
       "   '\\n\\n',\n",
       "   'Departamento',\n",
       "   'de',\n",
       "   'Computação',\n",
       "   'e',\n",
       "   'Sistemas',\n",
       "   '\\n',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Ouro',\n",
       "   'Preto',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'ABSTRACT',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Carlos',\n",
       "   'H.',\n",
       "   'G.',\n",
       "   'Ferreira',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'chgferreira@ufop.edu.br',\n",
       "   '\\n\\n',\n",
       "   'Departamento',\n",
       "   'de',\n",
       "   'Computação',\n",
       "   'e',\n",
       "   'Sistemas',\n",
       "   '\\n',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Ouro',\n",
       "   'Preto',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Helen',\n",
       "   'C.',\n",
       "   'S.',\n",
       "   'C.',\n",
       "   'Lima',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'helen@ufop.edu.br',\n",
       "   '\\n\\n',\n",
       "   'Departamento',\n",
       "   'de',\n",
       "   'Computação',\n",
       "   'e',\n",
       "   'Sistemas',\n",
       "   '\\n',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Ouro',\n",
       "   'Preto',\n",
       "   '\\n\\n\\n',\n",
       "   'In',\n",
       "   'the',\n",
       "   'digital',\n",
       "   'age',\n",
       "   ',',\n",
       "   'streaming',\n",
       "   'services',\n",
       "   'such',\n",
       "   'as',\n",
       "   'Spotify',\n",
       "   'have',\n",
       "   'changed',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'way',\n",
       "   'people',\n",
       "   'consume',\n",
       "   'music',\n",
       "   ',',\n",
       "   'highlighting',\n",
       "   'the',\n",
       "   'enormous',\n",
       "   'influence',\n",
       "   'these',\n",
       "   'platforms',\n",
       "   'have',\n",
       "   'on',\n",
       "   'the',\n",
       "   'market',\n",
       "   '.',\n",
       "   'In',\n",
       "   'the',\n",
       "   'highly',\n",
       "   'competitive',\n",
       "   '\\n',\n",
       "   'music',\n",
       "   'industry',\n",
       "   ',',\n",
       "   'it',\n",
       "   'is',\n",
       "   'crucial',\n",
       "   'for',\n",
       "   'independent',\n",
       "   'artists',\n",
       "   'in',\n",
       "   'particular',\n",
       "   'to',\n",
       "   '\\n',\n",
       "   'maintain',\n",
       "   'their',\n",
       "   'popularity',\n",
       "   '.',\n",
       "   'This',\n",
       "   'is',\n",
       "   'especially',\n",
       "   'true',\n",
       "   'in',\n",
       "   'countries',\n",
       "   'like',\n",
       "   '\\n',\n",
       "   'Brazil',\n",
       "   ',',\n",
       "   'where',\n",
       "   'geographical',\n",
       "   'and',\n",
       "   'cultural',\n",
       "   'differences',\n",
       "   'influence',\n",
       "   'music',\n",
       "   '\\n',\n",
       "   'consumption',\n",
       "   'patterns',\n",
       "   '.',\n",
       "   'Understanding',\n",
       "   'these',\n",
       "   'patterns',\n",
       "   'is',\n",
       "   'essential',\n",
       "   'for',\n",
       "   '\\n',\n",
       "   'effective',\n",
       "   'marketing',\n",
       "   'and',\n",
       "   'production',\n",
       "   'strategies',\n",
       "   '.',\n",
       "   'Despite',\n",
       "   'previous',\n",
       "   'research',\n",
       "   'on',\n",
       "   'music',\n",
       "   'consumption',\n",
       "   ',',\n",
       "   'genre',\n",
       "   'preferences',\n",
       "   'and',\n",
       "   'user',\n",
       "   'behavior',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'there',\n",
       "   'is',\n",
       "   'a',\n",
       "   'lack',\n",
       "   'of',\n",
       "   'detailed',\n",
       "   'studies',\n",
       "   'on',\n",
       "   'the',\n",
       "   'geographical',\n",
       "   'and',\n",
       "   'cultural',\n",
       "   '\\n',\n",
       "   'distribution',\n",
       "   'of',\n",
       "   'music',\n",
       "   'preferences',\n",
       "   'in',\n",
       "   'Brazil',\n",
       "   '.',\n",
       "   'Our',\n",
       "   'study',\n",
       "   'fills',\n",
       "   'this',\n",
       "   'gap',\n",
       "   '\\n',\n",
       "   'by',\n",
       "   'examining',\n",
       "   'musical',\n",
       "   'genre',\n",
       "   'preferences',\n",
       "   'and',\n",
       "   'acoustic',\n",
       "   'features',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'tracks',\n",
       "   'across',\n",
       "   'Brazilian',\n",
       "   'regions',\n",
       "   'over',\n",
       "   'two',\n",
       "   'years',\n",
       "   '.',\n",
       "   'We',\n",
       "   'collected',\n",
       "   'Spotify',\n",
       "   'chart',\n",
       "   'data',\n",
       "   'from',\n",
       "   '2022',\n",
       "   'and',\n",
       "   '2023',\n",
       "   ',',\n",
       "   'modeled',\n",
       "   'bipartite',\n",
       "   'genre',\n",
       "   '-',\n",
       "   'city',\n",
       "   '\\n',\n",
       "   'networks',\n",
       "   ',',\n",
       "   'and',\n",
       "   'used',\n",
       "   'backbone',\n",
       "   'extraction',\n",
       "   'methods',\n",
       "   'to',\n",
       "   'highlight',\n",
       "   'significant',\n",
       "   'genre',\n",
       "   'preferences',\n",
       "   '.',\n",
       "   'Temporal',\n",
       "   'analysis',\n",
       "   'revealed',\n",
       "   'patterns',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'persistence',\n",
       "   'of',\n",
       "   'musical',\n",
       "   'preferences',\n",
       "   'across',\n",
       "   'cities',\n",
       "   ',',\n",
       "   'while',\n",
       "   'clustering',\n",
       "   '\\n',\n",
       "   'techniques',\n",
       "   'revealed',\n",
       "   'regional',\n",
       "   'and',\n",
       "   'cultural',\n",
       "   'differences',\n",
       "   'in',\n",
       "   'acoustic',\n",
       "   '\\n',\n",
       "   'features',\n",
       "   '.',\n",
       "   'Our',\n",
       "   'results',\n",
       "   'show',\n",
       "   'that',\n",
       "   'genre',\n",
       "   'preferences',\n",
       "   'are',\n",
       "   'stable',\n",
       "   'across',\n",
       "   '\\n',\n",
       "   'Brazilian',\n",
       "   'regions',\n",
       "   ',',\n",
       "   'with',\n",
       "   'important',\n",
       "   'genres',\n",
       "   'emphasized',\n",
       "   'by',\n",
       "   'backbone',\n",
       "   '\\n',\n",
       "   'networks',\n",
       "   '.',\n",
       "   'Persistence',\n",
       "   'analysis',\n",
       "   'suggests',\n",
       "   'minimal',\n",
       "   'changes',\n",
       "   'over',\n",
       "   'time',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'except',\n",
       "   'during',\n",
       "   'major',\n",
       "   'holidays',\n",
       "   '.',\n",
       "   'Furthermore',\n",
       "   ',',\n",
       "   'Brazilian',\n",
       "   'city',\n",
       "   'clusters',\n",
       "   '\\n',\n",
       "   'exhibit',\n",
       "   'distinct',\n",
       "   'acoustic',\n",
       "   'patterns',\n",
       "   'regardless',\n",
       "   'of',\n",
       "   'music',\n",
       "   'genres',\n",
       "   ',',\n",
       "   'with',\n",
       "   '\\n',\n",
       "   'notable',\n",
       "   'differences',\n",
       "   'in',\n",
       "   'features',\n",
       "   'such',\n",
       "   'as',\n",
       "   'liveliness',\n",
       "   ',',\n",
       "   'speechiness',\n",
       "   ',',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'valence',\n",
       "   '.',\n",
       "   'This',\n",
       "   'research',\n",
       "   'provides',\n",
       "   'new',\n",
       "   'insights',\n",
       "   'into',\n",
       "   'regional',\n",
       "   'musical',\n",
       "   '\\n',\n",
       "   'diversity',\n",
       "   'in',\n",
       "   'Brazil',\n",
       "   'and',\n",
       "   'paves',\n",
       "   'the',\n",
       "   'way',\n",
       "   'for',\n",
       "   'future',\n",
       "   'studies',\n",
       "   'on',\n",
       "   'cultural',\n",
       "   '\\n',\n",
       "   'and',\n",
       "   'geographical',\n",
       "   'influences',\n",
       "   'on',\n",
       "   'music',\n",
       "   'preferences',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'KEYWORDS',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Music',\n",
       "   'Preferences',\n",
       "   ',',\n",
       "   'Music',\n",
       "   'Genre',\n",
       "   'Networks',\n",
       "   ',',\n",
       "   'Regional',\n",
       "   'Analysis',\n",
       "   ',',\n",
       "   'Spotify',\n",
       "   'Charts',\n",
       "   ',',\n",
       "   'Music',\n",
       "   'Data',\n",
       "   'Mining',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   '1',\n",
       "   'INTRODUCTION',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   'the',\n",
       "   'digital',\n",
       "   'era',\n",
       "   ',',\n",
       "   'streaming',\n",
       "   'services',\n",
       "   'have',\n",
       "   'revolutionized',\n",
       "   'the',\n",
       "   'way',\n",
       "   '\\n',\n",
       "   'people',\n",
       "   'consume',\n",
       "   'media',\n",
       "   'content',\n",
       "   ',',\n",
       "   'particularly',\n",
       "   'music',\n",
       "   '[',\n",
       "   '3',\n",
       "   ',',\n",
       "   '8',\n",
       "   ',',\n",
       "   '12',\n",
       "   ']',\n",
       "   '.',\n",
       "   'The',\n",
       "   '\\n',\n",
       "   'number',\n",
       "   'of',\n",
       "   'music',\n",
       "   'streaming',\n",
       "   'subscribers',\n",
       "   'worldwide',\n",
       "   'increased',\n",
       "   'from',\n",
       "   '\\n',\n",
       "   '616',\n",
       "   'million',\n",
       "   'at',\n",
       "   'the',\n",
       "   'end',\n",
       "   'of',\n",
       "   'the',\n",
       "   'second',\n",
       "   'quarter',\n",
       "   'of',\n",
       "   '2022',\n",
       "   'to',\n",
       "   '713',\n",
       "   'million',\n",
       "   '\\n',\n",
       "   'in',\n",
       "   'the',\n",
       "   'third',\n",
       "   'quarter',\n",
       "   'of',\n",
       "   '2023',\n",
       "   '[',\n",
       "   '26',\n",
       "   ']',\n",
       "   '.',\n",
       "   'As',\n",
       "   'a',\n",
       "   'result',\n",
       "   ',',\n",
       "   'paid',\n",
       "   'music',\n",
       "   'streaming',\n",
       "   '\\n',\n",
       "   'subscriptions',\n",
       "   'have',\n",
       "   'become',\n",
       "   'the',\n",
       "   'norm',\n",
       "   'for',\n",
       "   'many',\n",
       "   'music',\n",
       "   'fans',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'have',\n",
       "   'led',\n",
       "   'to',\n",
       "   'impressive',\n",
       "   'growth',\n",
       "   'in',\n",
       "   'subscriber',\n",
       "   'numbers',\n",
       "   'in',\n",
       "   'recent',\n",
       "   '\\n',\n",
       "   'years',\n",
       "   '.',\n",
       "   'This',\n",
       "   'surge',\n",
       "   'in',\n",
       "   'popularity',\n",
       "   'has',\n",
       "   'led',\n",
       "   'to',\n",
       "   'music',\n",
       "   'streaming',\n",
       "   'services',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   ':',\n",
       "   'Proceedings',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Web',\n",
       "   '(',\n",
       "   'WebMe',\n",
       "   '\\n',\n",
       "   'dia’2024',\n",
       "   ')',\n",
       "   '.',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '.',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   ':',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   ',',\n",
       "   '2024',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'SBC',\n",
       "   '–',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '-',\n",
       "   '2753',\n",
       "   '\\n\\n\\n',\n",
       "   'accounting',\n",
       "   'for',\n",
       "   'a',\n",
       "   'significant',\n",
       "   'portion',\n",
       "   'of',\n",
       "   'the',\n",
       "   'music',\n",
       "   'industry',\n",
       "   '’s',\n",
       "   'revenue',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'demonstrating',\n",
       "   'their',\n",
       "   'significant',\n",
       "   'influence',\n",
       "   'on',\n",
       "   'the',\n",
       "   'market',\n",
       "   'and',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'steady',\n",
       "   'global',\n",
       "   'expansion',\n",
       "   'of',\n",
       "   'these',\n",
       "   'platforms',\n",
       "   '[',\n",
       "   '4',\n",
       "   ',',\n",
       "   '8',\n",
       "   ',',\n",
       "   '36',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Among',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'various',\n",
       "   'music',\n",
       "   'streaming',\n",
       "   'services',\n",
       "   ',',\n",
       "   'Spotify',\n",
       "   'stands',\n",
       "   'out',\n",
       "   'as',\n",
       "   'a',\n",
       "   'profitable',\n",
       "   '\\n',\n",
       "   'player',\n",
       "   '.',\n",
       "   'In',\n",
       "   '2023',\n",
       "   ',',\n",
       "   'Spotify',\n",
       "   'paid',\n",
       "   'out',\n",
       "   'more',\n",
       "   'than',\n",
       "   '9',\n",
       "   'billion',\n",
       "   'dollars',\n",
       "   'to',\n",
       "   '\\n',\n",
       "   'artists',\n",
       "   ',',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'highest',\n",
       "   'annual',\n",
       "   'payouts',\n",
       "   'by',\n",
       "   'a',\n",
       "   'single',\n",
       "   'provider',\n",
       "   '\\n',\n",
       "   'in',\n",
       "   'history',\n",
       "   '[',\n",
       "   '1',\n",
       "   ']',\n",
       "   '.',\n",
       "   'What',\n",
       "   'factors',\n",
       "   'contribute',\n",
       "   'to',\n",
       "   'the',\n",
       "   'persistence',\n",
       "   'of',\n",
       "   'these',\n",
       "   '\\n',\n",
       "   'regional',\n",
       "   'patterns',\n",
       "   '?',\n",
       "   '\\n',\n",
       "   'In',\n",
       "   'parallel',\n",
       "   ',',\n",
       "   'the',\n",
       "   'music',\n",
       "   'industry',\n",
       "   'is',\n",
       "   'highly',\n",
       "   'competitive',\n",
       "   ',',\n",
       "   'and',\n",
       "   'maintaining',\n",
       "   'popularity',\n",
       "   'and',\n",
       "   'influence',\n",
       "   'is',\n",
       "   'crucial',\n",
       "   'for',\n",
       "   'artists',\n",
       "   ',',\n",
       "   'especially',\n",
       "   'for',\n",
       "   '\\n',\n",
       "   'independent',\n",
       "   'ones',\n",
       "   '[',\n",
       "   '32',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Expertise',\n",
       "   'in',\n",
       "   'production',\n",
       "   'and',\n",
       "   'promotion',\n",
       "   'offers',\n",
       "   'substantial',\n",
       "   'advantages',\n",
       "   'in',\n",
       "   'this',\n",
       "   'landscape',\n",
       "   '[',\n",
       "   '5',\n",
       "   ']',\n",
       "   '.',\n",
       "   'In',\n",
       "   'large',\n",
       "   'countries',\n",
       "   '\\n',\n",
       "   'such',\n",
       "   'as',\n",
       "   'Brazil',\n",
       "   ',',\n",
       "   'this',\n",
       "   'knowledge',\n",
       "   'is',\n",
       "   'even',\n",
       "   'more',\n",
       "   'important',\n",
       "   'due',\n",
       "   'to',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'great',\n",
       "   'geographic',\n",
       "   'and',\n",
       "   'cultural',\n",
       "   'diversity',\n",
       "   '[',\n",
       "   '45',\n",
       "   ']',\n",
       "   '.',\n",
       "   'A',\n",
       "   'music',\n",
       "   'popular',\n",
       "   'in',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'South',\n",
       "   'may',\n",
       "   'not',\n",
       "   'resonate',\n",
       "   'in',\n",
       "   'the',\n",
       "   'North',\n",
       "   ',',\n",
       "   'and',\n",
       "   'vice',\n",
       "   'versa',\n",
       "   '.',\n",
       "   'Thanks',\n",
       "   '\\n',\n",
       "   'to',\n",
       "   'extensive',\n",
       "   'data',\n",
       "   'from',\n",
       "   'streaming',\n",
       "   'platforms',\n",
       "   'such',\n",
       "   'as',\n",
       "   'Spotify',\n",
       "   ',',\n",
       "   'it',\n",
       "   'is',\n",
       "   '\\n',\n",
       "   'now',\n",
       "   'possible',\n",
       "   'to',\n",
       "   'understand',\n",
       "   'the',\n",
       "   'geographical',\n",
       "   'distribution',\n",
       "   'of',\n",
       "   'music',\n",
       "   'preferences',\n",
       "   '[',\n",
       "   '30',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Mapping',\n",
       "   'regional',\n",
       "   'popular',\n",
       "   'music',\n",
       "   'is',\n",
       "   'the',\n",
       "   'first',\n",
       "   '\\n',\n",
       "   'step',\n",
       "   'toward',\n",
       "   'identifying',\n",
       "   'the',\n",
       "   'profile',\n",
       "   'of',\n",
       "   'a',\n",
       "   'region',\n",
       "   'and',\n",
       "   'testing',\n",
       "   'relevant',\n",
       "   '\\n',\n",
       "   'marketing',\n",
       "   'strategies',\n",
       "   '[',\n",
       "   '45',\n",
       "   ']',\n",
       "   '.',\n",
       "   'This',\n",
       "   'profiling',\n",
       "   ',',\n",
       "   'carried',\n",
       "   'out',\n",
       "   'through',\n",
       "   'a',\n",
       "   'temporal',\n",
       "   'analysis',\n",
       "   ',',\n",
       "   'assesses',\n",
       "   'the',\n",
       "   'common',\n",
       "   'characteristics',\n",
       "   'associated',\n",
       "   'with',\n",
       "   '\\n',\n",
       "   'popular',\n",
       "   'music',\n",
       "   'and',\n",
       "   'their',\n",
       "   'cultural',\n",
       "   'significance',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Indeed',\n",
       "   ',',\n",
       "   'several',\n",
       "   'prior',\n",
       "   'efforts',\n",
       "   'have',\n",
       "   'concentrated',\n",
       "   'on',\n",
       "   'studying',\n",
       "   'music',\n",
       "   'popularity',\n",
       "   'within',\n",
       "   'groups',\n",
       "   '[',\n",
       "   '2',\n",
       "   ',',\n",
       "   '37',\n",
       "   ']',\n",
       "   ',',\n",
       "   'geographical',\n",
       "   'distribution',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'music',\n",
       "   'preferences',\n",
       "   '[',\n",
       "   '7',\n",
       "   ',',\n",
       "   '25',\n",
       "   ',',\n",
       "   '30',\n",
       "   ',',\n",
       "   '45',\n",
       "   ']',\n",
       "   ',',\n",
       "   'user',\n",
       "   'demographics',\n",
       "   'and',\n",
       "   'personality',\n",
       "   '[',\n",
       "   '6',\n",
       "   ',',\n",
       "   '43',\n",
       "   ']',\n",
       "   ',',\n",
       "   'and',\n",
       "   'music',\n",
       "   'diversity',\n",
       "   '[',\n",
       "   '20',\n",
       "   ',',\n",
       "   '31',\n",
       "   ',',\n",
       "   '46',\n",
       "   ']',\n",
       "   '.',\n",
       "   'However',\n",
       "   ',',\n",
       "   'these',\n",
       "   'studies',\n",
       "   '\\n',\n",
       "   'do',\n",
       "   'not',\n",
       "   'capture',\n",
       "   'the',\n",
       "   'regional',\n",
       "   'behaviors',\n",
       "   'of',\n",
       "   'these',\n",
       "   'groups',\n",
       "   'in',\n",
       "   'terms',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'genre',\n",
       "   'preferences',\n",
       "   ',',\n",
       "   'especially',\n",
       "   'in',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'context',\n",
       "   '.',\n",
       "   'Given',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'significant',\n",
       "   'role',\n",
       "   'that',\n",
       "   'music',\n",
       "   'streaming',\n",
       "   'platforms',\n",
       "   'play',\n",
       "   'in',\n",
       "   'the',\n",
       "   'distribution',\n",
       "   'and',\n",
       "   'consumption',\n",
       "   'of',\n",
       "   'music',\n",
       "   ',',\n",
       "   'it',\n",
       "   'is',\n",
       "   'important',\n",
       "   'to',\n",
       "   'understand',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'composition',\n",
       "   'of',\n",
       "   'musical',\n",
       "   'preferences',\n",
       "   'in',\n",
       "   'different',\n",
       "   'regions',\n",
       "   '[',\n",
       "   '6',\n",
       "   ',',\n",
       "   '15',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'In',\n",
       "   'Brazil',\n",
       "   ',',\n",
       "   'with',\n",
       "   'its',\n",
       "   'diverse',\n",
       "   'cultural',\n",
       "   'landscape',\n",
       "   ',',\n",
       "   'this',\n",
       "   'understanding',\n",
       "   ...],\n",
       "  'pos_tagger': '',\n",
       "  'lema': ['characterization',\n",
       "   'of',\n",
       "   'the',\n",
       "   'brazilian',\n",
       "   'musical',\n",
       "   'landscape',\n",
       "   'a',\n",
       "   'study',\n",
       "   'of',\n",
       "   'regional',\n",
       "   'preference',\n",
       "   'base',\n",
       "   'on',\n",
       "   'the',\n",
       "   'Spotify',\n",
       "   'chart',\n",
       "   'Filipe',\n",
       "   'a.',\n",
       "   'S.',\n",
       "   'moura',\n",
       "   'filipe.asm@aluno.ufop.edu.br',\n",
       "   'Departamento',\n",
       "   'de',\n",
       "   'Computação',\n",
       "   'e',\n",
       "   'Sistemas',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Ouro',\n",
       "   'Preto',\n",
       "   'ABSTRACT',\n",
       "   'Carlos',\n",
       "   'H.',\n",
       "   'G.',\n",
       "   'Ferreira',\n",
       "   'chgferreira@ufop.edu.br',\n",
       "   'Departamento',\n",
       "   'de',\n",
       "   'Computação',\n",
       "   'e',\n",
       "   'Sistemas',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Ouro',\n",
       "   'Preto',\n",
       "   'Helen',\n",
       "   'C.',\n",
       "   'S.',\n",
       "   'C.',\n",
       "   'Lima',\n",
       "   'helen@ufop.edu.br',\n",
       "   'Departamento',\n",
       "   'de',\n",
       "   'Computação',\n",
       "   'e',\n",
       "   'Sistemas',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Ouro',\n",
       "   'Preto',\n",
       "   'in',\n",
       "   'the',\n",
       "   'digital',\n",
       "   'age',\n",
       "   'streaming',\n",
       "   'service',\n",
       "   'such',\n",
       "   'as',\n",
       "   'Spotify',\n",
       "   'have',\n",
       "   'change',\n",
       "   'the',\n",
       "   'way',\n",
       "   'people',\n",
       "   'consume',\n",
       "   'music',\n",
       "   'highlight',\n",
       "   'the',\n",
       "   'enormous',\n",
       "   'influence',\n",
       "   'these',\n",
       "   'platform',\n",
       "   'have',\n",
       "   'on',\n",
       "   'the',\n",
       "   'market',\n",
       "   'in',\n",
       "   'the',\n",
       "   'highly',\n",
       "   'competitive',\n",
       "   'music',\n",
       "   'industry',\n",
       "   'it',\n",
       "   'be',\n",
       "   'crucial',\n",
       "   'for',\n",
       "   'independent',\n",
       "   'artist',\n",
       "   'in',\n",
       "   'particular',\n",
       "   'to',\n",
       "   'maintain',\n",
       "   'their',\n",
       "   'popularity',\n",
       "   'this',\n",
       "   'be',\n",
       "   'especially',\n",
       "   'true',\n",
       "   'in',\n",
       "   'country',\n",
       "   'like',\n",
       "   'Brazil',\n",
       "   'where',\n",
       "   'geographical',\n",
       "   'and',\n",
       "   'cultural',\n",
       "   'difference',\n",
       "   'influence',\n",
       "   'music',\n",
       "   'consumption',\n",
       "   'pattern',\n",
       "   'understand',\n",
       "   'these',\n",
       "   'pattern',\n",
       "   'be',\n",
       "   'essential',\n",
       "   'for',\n",
       "   'effective',\n",
       "   'marketing',\n",
       "   'and',\n",
       "   'production',\n",
       "   'strategy',\n",
       "   'despite',\n",
       "   'previous',\n",
       "   'research',\n",
       "   'on',\n",
       "   'music',\n",
       "   'consumption',\n",
       "   'genre',\n",
       "   'preference',\n",
       "   'and',\n",
       "   'user',\n",
       "   'behavior',\n",
       "   'there',\n",
       "   'be',\n",
       "   'a',\n",
       "   'lack',\n",
       "   'of',\n",
       "   'detailed',\n",
       "   'study',\n",
       "   'on',\n",
       "   'the',\n",
       "   'geographical',\n",
       "   'and',\n",
       "   'cultural',\n",
       "   'distribution',\n",
       "   'of',\n",
       "   'music',\n",
       "   'preference',\n",
       "   'in',\n",
       "   'Brazil',\n",
       "   'our',\n",
       "   'study',\n",
       "   'fill',\n",
       "   'this',\n",
       "   'gap',\n",
       "   'by',\n",
       "   'examine',\n",
       "   'musical',\n",
       "   'genre',\n",
       "   'preference',\n",
       "   'and',\n",
       "   'acoustic',\n",
       "   'feature',\n",
       "   'of',\n",
       "   'track',\n",
       "   'across',\n",
       "   'brazilian',\n",
       "   'region',\n",
       "   'over',\n",
       "   'two',\n",
       "   'year',\n",
       "   'we',\n",
       "   'collect',\n",
       "   'spotify',\n",
       "   'chart',\n",
       "   'datum',\n",
       "   'from',\n",
       "   '2022',\n",
       "   'and',\n",
       "   '2023',\n",
       "   'model',\n",
       "   'bipartite',\n",
       "   'genre',\n",
       "   'city',\n",
       "   'network',\n",
       "   'and',\n",
       "   'use',\n",
       "   'backbone',\n",
       "   'extraction',\n",
       "   'method',\n",
       "   'to',\n",
       "   'highlight',\n",
       "   'significant',\n",
       "   'genre',\n",
       "   'preference',\n",
       "   'temporal',\n",
       "   'analysis',\n",
       "   'reveal',\n",
       "   'pattern',\n",
       "   'and',\n",
       "   'persistence',\n",
       "   'of',\n",
       "   'musical',\n",
       "   'preference',\n",
       "   'across',\n",
       "   'city',\n",
       "   'while',\n",
       "   'cluster',\n",
       "   'technique',\n",
       "   'reveal',\n",
       "   'regional',\n",
       "   'and',\n",
       "   'cultural',\n",
       "   'difference',\n",
       "   'in',\n",
       "   'acoustic',\n",
       "   'feature',\n",
       "   'our',\n",
       "   'result',\n",
       "   'show',\n",
       "   'that',\n",
       "   'genre',\n",
       "   'preference',\n",
       "   'be',\n",
       "   'stable',\n",
       "   'across',\n",
       "   'brazilian',\n",
       "   'region',\n",
       "   'with',\n",
       "   'important',\n",
       "   'genre',\n",
       "   'emphasize',\n",
       "   'by',\n",
       "   'backbone',\n",
       "   'network',\n",
       "   'persistence',\n",
       "   'analysis',\n",
       "   'suggest',\n",
       "   'minimal',\n",
       "   'change',\n",
       "   'over',\n",
       "   'time',\n",
       "   'except',\n",
       "   'during',\n",
       "   'major',\n",
       "   'holiday',\n",
       "   'furthermore',\n",
       "   'brazilian',\n",
       "   'city',\n",
       "   'cluster',\n",
       "   'exhibit',\n",
       "   'distinct',\n",
       "   'acoustic',\n",
       "   'pattern',\n",
       "   'regardless',\n",
       "   'of',\n",
       "   'music',\n",
       "   'genre',\n",
       "   'with',\n",
       "   'notable',\n",
       "   'difference',\n",
       "   'in',\n",
       "   'feature',\n",
       "   'such',\n",
       "   'as',\n",
       "   'liveliness',\n",
       "   'speechiness',\n",
       "   'and',\n",
       "   'valence',\n",
       "   'this',\n",
       "   'research',\n",
       "   'provide',\n",
       "   'new',\n",
       "   'insight',\n",
       "   'into',\n",
       "   'regional',\n",
       "   'musical',\n",
       "   'diversity',\n",
       "   'in',\n",
       "   'Brazil',\n",
       "   'and',\n",
       "   'pave',\n",
       "   'the',\n",
       "   'way',\n",
       "   'for',\n",
       "   'future',\n",
       "   'study',\n",
       "   'on',\n",
       "   'cultural',\n",
       "   'and',\n",
       "   'geographical',\n",
       "   'influence',\n",
       "   'on',\n",
       "   'music',\n",
       "   'preference',\n",
       "   'keyword',\n",
       "   'Music',\n",
       "   'Preferences',\n",
       "   'Music',\n",
       "   'Genre',\n",
       "   'Networks',\n",
       "   'Regional',\n",
       "   'Analysis',\n",
       "   'spotify',\n",
       "   'Charts',\n",
       "   'Music',\n",
       "   'Data',\n",
       "   'Mining',\n",
       "   '1',\n",
       "   'introduction',\n",
       "   'in',\n",
       "   'the',\n",
       "   'digital',\n",
       "   'era',\n",
       "   'streaming',\n",
       "   'service',\n",
       "   'have',\n",
       "   'revolutionize',\n",
       "   'the',\n",
       "   'way',\n",
       "   'people',\n",
       "   'consume',\n",
       "   'medium',\n",
       "   'content',\n",
       "   'particularly',\n",
       "   'music',\n",
       "   '3',\n",
       "   '8',\n",
       "   '12',\n",
       "   'the',\n",
       "   'number',\n",
       "   'of',\n",
       "   'music',\n",
       "   'stream',\n",
       "   'subscriber',\n",
       "   'worldwide',\n",
       "   'increase',\n",
       "   'from',\n",
       "   '616',\n",
       "   'million',\n",
       "   'at',\n",
       "   'the',\n",
       "   'end',\n",
       "   'of',\n",
       "   'the',\n",
       "   'second',\n",
       "   'quarter',\n",
       "   'of',\n",
       "   '2022',\n",
       "   'to',\n",
       "   '713',\n",
       "   'million',\n",
       "   'in',\n",
       "   'the',\n",
       "   'third',\n",
       "   'quarter',\n",
       "   'of',\n",
       "   '2023',\n",
       "   '26',\n",
       "   'as',\n",
       "   'a',\n",
       "   'result',\n",
       "   'pay',\n",
       "   'music',\n",
       "   'stream',\n",
       "   'subscription',\n",
       "   'have',\n",
       "   'become',\n",
       "   'the',\n",
       "   'norm',\n",
       "   'for',\n",
       "   'many',\n",
       "   'music',\n",
       "   'fan',\n",
       "   'and',\n",
       "   'have',\n",
       "   'lead',\n",
       "   'to',\n",
       "   'impressive',\n",
       "   'growth',\n",
       "   'in',\n",
       "   'subscriber',\n",
       "   'number',\n",
       "   'in',\n",
       "   'recent',\n",
       "   'year',\n",
       "   'this',\n",
       "   'surge',\n",
       "   'in',\n",
       "   'popularity',\n",
       "   'have',\n",
       "   'lead',\n",
       "   'to',\n",
       "   'music',\n",
       "   'streaming',\n",
       "   'service',\n",
       "   'in',\n",
       "   'proceeding',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'web',\n",
       "   'WebMe',\n",
       "   'dia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   '2024',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'sbc',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '2753',\n",
       "   'accounting',\n",
       "   'for',\n",
       "   'a',\n",
       "   'significant',\n",
       "   'portion',\n",
       "   'of',\n",
       "   'the',\n",
       "   'music',\n",
       "   'industry',\n",
       "   '’s',\n",
       "   'revenue',\n",
       "   'demonstrate',\n",
       "   'their',\n",
       "   'significant',\n",
       "   'influence',\n",
       "   'on',\n",
       "   'the',\n",
       "   'market',\n",
       "   'and',\n",
       "   'the',\n",
       "   'steady',\n",
       "   'global',\n",
       "   'expansion',\n",
       "   'of',\n",
       "   'these',\n",
       "   'platform',\n",
       "   '4',\n",
       "   '8',\n",
       "   '36',\n",
       "   'among',\n",
       "   'the',\n",
       "   'various',\n",
       "   'music',\n",
       "   'streaming',\n",
       "   'service',\n",
       "   'spotify',\n",
       "   'stand',\n",
       "   'out',\n",
       "   'as',\n",
       "   'a',\n",
       "   'profitable',\n",
       "   'player',\n",
       "   'in',\n",
       "   '2023',\n",
       "   'spotify',\n",
       "   'pay',\n",
       "   'out',\n",
       "   'more',\n",
       "   'than',\n",
       "   '9',\n",
       "   'billion',\n",
       "   'dollar',\n",
       "   'to',\n",
       "   'artist',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'high',\n",
       "   'annual',\n",
       "   'payout',\n",
       "   'by',\n",
       "   'a',\n",
       "   'single',\n",
       "   'provider',\n",
       "   'in',\n",
       "   'history',\n",
       "   '1',\n",
       "   'what',\n",
       "   'factor',\n",
       "   'contribute',\n",
       "   'to',\n",
       "   'the',\n",
       "   'persistence',\n",
       "   'of',\n",
       "   'these',\n",
       "   'regional',\n",
       "   'pattern',\n",
       "   'in',\n",
       "   'parallel',\n",
       "   'the',\n",
       "   'music',\n",
       "   'industry',\n",
       "   'be',\n",
       "   'highly',\n",
       "   'competitive',\n",
       "   'and',\n",
       "   'maintain',\n",
       "   'popularity',\n",
       "   'and',\n",
       "   'influence',\n",
       "   'be',\n",
       "   'crucial',\n",
       "   'for',\n",
       "   'artist',\n",
       "   'especially',\n",
       "   'for',\n",
       "   'independent',\n",
       "   'one',\n",
       "   '32',\n",
       "   'expertise',\n",
       "   'in',\n",
       "   'production',\n",
       "   'and',\n",
       "   'promotion',\n",
       "   'offer',\n",
       "   'substantial',\n",
       "   'advantage',\n",
       "   'in',\n",
       "   'this',\n",
       "   'landscape',\n",
       "   '5',\n",
       "   'in',\n",
       "   'large',\n",
       "   'country',\n",
       "   'such',\n",
       "   'as',\n",
       "   'Brazil',\n",
       "   'this',\n",
       "   'knowledge',\n",
       "   'be',\n",
       "   'even',\n",
       "   'more',\n",
       "   'important',\n",
       "   'due',\n",
       "   'to',\n",
       "   'the',\n",
       "   'great',\n",
       "   'geographic',\n",
       "   'and',\n",
       "   'cultural',\n",
       "   'diversity',\n",
       "   '45',\n",
       "   'a',\n",
       "   'music',\n",
       "   'popular',\n",
       "   'in',\n",
       "   'the',\n",
       "   'South',\n",
       "   'may',\n",
       "   'not',\n",
       "   'resonate',\n",
       "   'in',\n",
       "   'the',\n",
       "   'North',\n",
       "   'and',\n",
       "   'vice',\n",
       "   'versa',\n",
       "   'thank',\n",
       "   'to',\n",
       "   'extensive',\n",
       "   'datum',\n",
       "   'from',\n",
       "   'stream',\n",
       "   'platform',\n",
       "   'such',\n",
       "   'as',\n",
       "   'Spotify',\n",
       "   'it',\n",
       "   'be',\n",
       "   'now',\n",
       "   'possible',\n",
       "   'to',\n",
       "   'understand',\n",
       "   'the',\n",
       "   'geographical',\n",
       "   'distribution',\n",
       "   'of',\n",
       "   'music',\n",
       "   'preference',\n",
       "   '30',\n",
       "   'mapping',\n",
       "   'regional',\n",
       "   'popular',\n",
       "   'music',\n",
       "   'be',\n",
       "   'the',\n",
       "   'first',\n",
       "   'step',\n",
       "   'toward',\n",
       "   'identify',\n",
       "   'the',\n",
       "   'profile',\n",
       "   'of',\n",
       "   'a',\n",
       "   'region',\n",
       "   'and',\n",
       "   'test',\n",
       "   'relevant',\n",
       "   'marketing',\n",
       "   'strategy',\n",
       "   '45',\n",
       "   'this',\n",
       "   'profiling',\n",
       "   'carry',\n",
       "   'out',\n",
       "   'through',\n",
       "   'a',\n",
       "   'temporal',\n",
       "   'analysis',\n",
       "   'assess',\n",
       "   'the',\n",
       "   'common',\n",
       "   'characteristic',\n",
       "   'associate',\n",
       "   'with',\n",
       "   'popular',\n",
       "   'music',\n",
       "   'and',\n",
       "   'their',\n",
       "   'cultural',\n",
       "   'significance',\n",
       "   'indeed',\n",
       "   'several',\n",
       "   'prior',\n",
       "   'effort',\n",
       "   'have',\n",
       "   'concentrate',\n",
       "   'on',\n",
       "   'study',\n",
       "   'music',\n",
       "   'popularity',\n",
       "   'within',\n",
       "   'group',\n",
       "   '2',\n",
       "   '37',\n",
       "   'geographical',\n",
       "   'distribution',\n",
       "   'of',\n",
       "   'music',\n",
       "   'preference',\n",
       "   '7',\n",
       "   '25',\n",
       "   '30',\n",
       "   '45',\n",
       "   'user',\n",
       "   'demographic',\n",
       "   'and',\n",
       "   'personality',\n",
       "   '6',\n",
       "   '43',\n",
       "   'and',\n",
       "   'music',\n",
       "   'diversity',\n",
       "   '20',\n",
       "   '31',\n",
       "   '46',\n",
       "   'however',\n",
       "   'these',\n",
       "   'study',\n",
       "   'do',\n",
       "   'not',\n",
       "   'capture',\n",
       "   'the',\n",
       "   'regional',\n",
       "   'behavior',\n",
       "   'of',\n",
       "   'these',\n",
       "   'group',\n",
       "   'in',\n",
       "   'term',\n",
       "   'of',\n",
       "   'genre',\n",
       "   'preference',\n",
       "   'especially',\n",
       "   'in',\n",
       "   'the',\n",
       "   'brazilian',\n",
       "   'context',\n",
       "   'give',\n",
       "   'the',\n",
       "   'significant',\n",
       "   'role',\n",
       "   'that',\n",
       "   'music',\n",
       "   'stream',\n",
       "   'platform',\n",
       "   'play',\n",
       "   'in',\n",
       "   'the',\n",
       "   'distribution',\n",
       "   'and',\n",
       "   'consumption',\n",
       "   'of',\n",
       "   'music',\n",
       "   'it',\n",
       "   'be',\n",
       "   'important',\n",
       "   'to',\n",
       "   'understand',\n",
       "   'the',\n",
       "   'composition',\n",
       "   'of',\n",
       "   'musical',\n",
       "   'preference',\n",
       "   'in',\n",
       "   'different',\n",
       "   'region',\n",
       "   '6',\n",
       "   '15',\n",
       "   'in',\n",
       "   'Brazil',\n",
       "   'with',\n",
       "   'its',\n",
       "   'diverse',\n",
       "   'cultural',\n",
       "   'landscape',\n",
       "   'this',\n",
       "   'understanding',\n",
       "   'be',\n",
       "   'particularly',\n",
       "   'important',\n",
       "   'for',\n",
       "   'artist',\n",
       "   'producer',\n",
       "   'and',\n",
       "   'marketer',\n",
       "   'who',\n",
       "   'want',\n",
       "   'to',\n",
       "   'tailor',\n",
       "   'their',\n",
       "   'strategy',\n",
       "   'to',\n",
       "   'regional',\n",
       "   'taste',\n",
       "   '13',\n",
       "   '17',\n",
       "   '47',\n",
       "   'with',\n",
       "   'this',\n",
       "   'in',\n",
       "   'mind',\n",
       "   'this',\n",
       "   'study',\n",
       "   'seek',\n",
       "   'to',\n",
       "   'fill',\n",
       "   'this',\n",
       "   'gap',\n",
       "   'by',\n",
       "   'examine',\n",
       "   'and',\n",
       "   'characterize',\n",
       "   'the',\n",
       "   'musical',\n",
       "   'genre',\n",
       "   'preference',\n",
       "   'and',\n",
       "   'audio',\n",
       "   'feature',\n",
       "   'of',\n",
       "   'track',\n",
       "   'in',\n",
       "   'different',\n",
       "   'region',\n",
       "   'of',\n",
       "   'Brazil',\n",
       "   'over',\n",
       "   'a',\n",
       "   'two',\n",
       "   'year',\n",
       "   'period',\n",
       "   'on',\n",
       "   'spotify',\n",
       "   'chart',\n",
       "   'in',\n",
       "   'this',\n",
       "   'way',\n",
       "   'we',\n",
       "   'aim',\n",
       "   'to',\n",
       "   'gain',\n",
       "   'valuable',\n",
       "   'insight',\n",
       "   'into',\n",
       "   'the',\n",
       "   'regional',\n",
       "   'musical',\n",
       "   'landscape',\n",
       "   'that',\n",
       "   'can',\n",
       "   'improve',\n",
       "   'marketing',\n",
       "   'strategy',\n",
       "   'production',\n",
       "   'decision',\n",
       "   'and',\n",
       "   'promotional',\n",
       "   'activity',\n",
       "   'our',\n",
       "   'study',\n",
       "   'aim',\n",
       "   'to',\n",
       "   'address',\n",
       "   'the',\n",
       "   'follow',\n",
       "   'research',\n",
       "   'question',\n",
       "   'RQ1',\n",
       "   'how',\n",
       "   'do',\n",
       "   'preference',\n",
       "   'for',\n",
       "   'musical',\n",
       "   'genre',\n",
       "   'differ',\n",
       "   'in',\n",
       "   'different',\n",
       "   'brazilian',\n",
       "   'city',\n",
       "   'over',\n",
       "   'the',\n",
       "   'year',\n",
       "   'what',\n",
       "   'factor',\n",
       "   'contribute',\n",
       "   'to',\n",
       "   'the',\n",
       "   'persistence',\n",
       "   'of',\n",
       "   'these',\n",
       "   'regional',\n",
       "   'pattern',\n",
       "   '1',\n",
       "   'spotify',\n",
       "   'royalty',\n",
       "   'datum',\n",
       "   '2023',\n",
       "   'https://apnews.com/article/spotify-loud-clear-report8ddab5a6e03f65233b0f9ed80eb99e0c',\n",
       "   '80',\n",
       "   'WebMedia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Filipe',\n",
       "   'a.',\n",
       "   'S.',\n",
       "   'Moura',\n",
       "   'Carlos',\n",
       "   'H.',\n",
       "   'G.',\n",
       "   'Ferreira',\n",
       "   'and',\n",
       "   'Helen',\n",
       "   'C.',\n",
       "   'S.',\n",
       "   'C.',\n",
       "   'Lima',\n",
       "   'RQ2',\n",
       "   'to',\n",
       "   'what',\n",
       "   'extent',\n",
       "   'do',\n",
       "   'the',\n",
       "   'acoustic',\n",
       "   'feature',\n",
       "   'of',\n",
       "   'the',\n",
       "   'track',\n",
       "   'reflect',\n",
       "   'regional',\n",
       "   'and',\n",
       "   'cultural',\n",
       "   'pattern',\n",
       "   'in',\n",
       "   'Brazil',\n",
       "   'independent',\n",
       "   'of',\n",
       "   'their',\n",
       "   'musical',\n",
       "   'genre',\n",
       "   'furthermore',\n",
       "   'be',\n",
       "   'there',\n",
       "   'acoustic',\n",
       "   'difference',\n",
       "   'within',\n",
       "   'the',\n",
       "   'same',\n",
       "   'musical',\n",
       "   'genre',\n",
       "   'across',\n",
       "   'different',\n",
       "   'region',\n",
       "   'to',\n",
       "   'tackle',\n",
       "   'these',\n",
       "   'research',\n",
       "   'question',\n",
       "   'we',\n",
       "   'collect',\n",
       "   'datum',\n",
       "   'from',\n",
       "   'spotify',\n",
       "   'chart',\n",
       "   'for',\n",
       "   'the',\n",
       "   'year',\n",
       "   '2022',\n",
       "   'and',\n",
       "   '2023',\n",
       "   'we',\n",
       "   'model',\n",
       "   'genre',\n",
       "   'city',\n",
       "   'bipartite',\n",
       "   'network',\n",
       "   'and',\n",
       "   'apply',\n",
       "   'a',\n",
       "   'state',\n",
       "   'of',\n",
       "   'the',\n",
       "   'art',\n",
       "   'backbone',\n",
       "   'extraction',\n",
       "   'method',\n",
       "   'to',\n",
       "   'highlight',\n",
       "   'significant',\n",
       "   'genre',\n",
       "   'preference',\n",
       "   'while',\n",
       "   'respect',\n",
       "   'the',\n",
       "   'heterogeneity',\n",
       "   'of',\n",
       "   'musical',\n",
       "   'taste',\n",
       "   'across',\n",
       "   'different',\n",
       "   'city',\n",
       "   'use',\n",
       "   'this',\n",
       "   'refined',\n",
       "   'topology',\n",
       "   'we',\n",
       "   'conduct',\n",
       "   'a',\n",
       "   'temporal',\n",
       "   'analysis',\n",
       "   'to',\n",
       "   'identify',\n",
       "   'pattern',\n",
       "   'and',\n",
       "   'persistence',\n",
       "   'in',\n",
       "   'musical',\n",
       "   'preference',\n",
       "   'address',\n",
       "   'rq1',\n",
       "   'for',\n",
       "   'RQ2',\n",
       "   'we',\n",
       "   'generate',\n",
       "   'feature',\n",
       "   'from',\n",
       "   ...]},\n",
       " {'titulo': 'Finding Fake News Websites in the Wild',\n",
       "  'informacoes_url': '',\n",
       "  'idioma': 'english',\n",
       "  'storage_key': '../articles/original/english/985-24756-1-10-20240923.pdf',\n",
       "  'author': 'Leandro Araujo; João M. M. Couto; Luiz Felipe Nery; Isadora Rodrigues; Jussara M. Almeida; Julio C. S. Reis; and Fabricio Benevenuto',\n",
       "  'data_publicacao': '11-09-2024',\n",
       "  'resumo': 'The battle against the spread of misinformation on the Internet is a daunting task faced by modern society. Fake news content is primarily distributed through digital platforms, with websites dedicated to producing and disseminating such content playing a pivotal role in this complex ecosystem. Therefore, these websites are of great interest to misinformation researchers. However, obtaining a comprehensive list of websites labeled as producers and/or spreaders of misinformation can be challenging, particularly in developing countries. In this study, we propose a novel methodology for identifying websites responsible for creating and disseminating misinformation content, which are closely linked to users who share confirmed instances of fake news on social media. We validate our approach on Twitter by examining various execution modes and contexts. Our findings demonstrate the effectiveness of the proposed methodology in identifying misinformation websites, which can aid in gaining a better understanding of this phenomenon and enabling competent entities to tackle the problem in various areas of society. ###',\n",
       "  'keywords': 'Fake News, Misinformation, Credibility, Websites, Social Media, Twitter, X',\n",
       "  'referencias': ['[1] Leandro Araújo, Luiz Felipe Nery, Isadora C Rodrigues, João MM Couto, Julio CS\\nReis, Ana PC Silva, Jussara M Almeida, and Fabrício Benevenuto. 2022. Identificando websites de desinformaçao no brasil. In *Brazilian Database Symposium*\\n*(SBBD)* . 355–360.',\n",
       "   '[2] Lutz Bornmann and Hans-Dieter Daniel. 2007. What do we know about the h\\nindex? *Journal of the American Society for Information Science and technology* 58,\\n9 (2007), 1381–1385.',\n",
       "   '[3] Alexandre Bovet and Hernán A Makse. 2019. Influence of fake news in Twitter\\nduring the 2016 US presidential election. *Nature Communicat.* 10, 1 (2019), 1–14.',\n",
       "   '[4] Lia Bozarth and Ceren Budak. 2020. Market forces: Quantifying the role of top\\ncredible ad servers in the fake news ecosystem. In *The Int’l Conference on Web*\\n*and Social Media (ICWSM)* .',\n",
       "   '[5] João MM Couto, Julio CS Reis, and Fabrício Benevenuto. 2024. Can computer\\nnetwork attributes be useful for identifying low-credibility websites? A case\\nstudy in Brazil. *Social Network Analysis and Mining* 14, 1 (2024), 153.',\n",
       "   '[6] João MM Couto, Julio CS Reis, Ítalo Cunha, Leandro Araújo, and Fabrício Benevenuto. 2022. Caracterizando websites de baixa credibilidade no brasil. In\\n*Brazilian Symposium on Computer Networks and Distributed Systems (SBRC)* .\\n503–516.',\n",
       "   '[7] Joao MM Couto, Julio CS Reis, Italo Cunha, Leandro Araujo, and Fabricio Benevenuto. 2022. Characterizing Low Credibility Websites in Brazil through Computer Networking Attributes. In *IEEE/ACM Int’l Conference on Advances in Social*\\n*Networks Analysis and Mining (ASONAM)* .',\n",
       "   '[8] Bárbara G. Ribeiro, Manoel Horta Ribeiro, Virgilio Almeida, and Wagner Meira Jr.\\n2022. Analyzing the “Sleeping Giants” Activism Model in Brazil. In *ACM Web*\\n*Science Conference (WebSci)* . 87–97.',\n",
       "   '[9] Venkata Rama Kiran Garimella and Ingmar Weber. 2017. A long-term analysis of\\npolarization on Twitter. In *Int’l Conference on Web and Social Media (ICWSM)* .',\n",
       "   '[10] Nir Grinberg, Kenneth Joseph, Lisa Friedland, Briony Swire-Thompson, and\\nDavid Lazer. 2019. Fake news on Twitter during the 2016 US presidential election.\\n*Science* 363, 6425 (2019), 374–378.',\n",
       "   '[11] Andrew Guess, Jonathan Nagler, and Joshua Tucker. 2019. Less than you think:\\nPrevalence and predictors of fake news dissemination on Facebook. *Science*\\n*advances* 5, 1 (2019), eaau4586.',\n",
       "   '[12] Mohamad Hoseini, Philipe Melo, Manoel Júnior, Fabrício Benevenuto, Balakrishnan Chandrasekaran, Anja Feldmann, and Savvas Zannettou. 2020. Demystifying\\nthe Messaging Platforms’ Ecosystem Through the Lens of Twitter. In *ACM Inter-*\\n*net Measurement Conference (IMC)* . 345–359.',\n",
       "   '[13] Eslam Hussein, Prerna Juneja, and Tanushree Mitra. 2020. Measuring misinformation in video search platforms: An audit study on YouTube. *Proc. of the ACM*\\n*on Human-Computer Interaction* 4, CSCW (2020), 1–27.',\n",
       "   '[14] Juhi Kulshrestha, Muhammad Zafar, Lisette Noboa, Krishna Gummadi, and Saptarshi Ghosh. 2015. Characterizing information diets of social media users. In\\n*Proc. of the Int’l AAAI Conference on Web and Social Media (ICWSM)* . 218–227.',\n",
       "   '[15] Sahil Loomba, Alexandre de Figueiredo, Simon J Piatek, Kristen de Graaf, and\\nHeidi J Larson. 2021. Measuring the impact of COVID-19 vaccine misinformation\\non vaccination intent in the UK and USA. *Nature human behaviour* 5, 3 (2021),\\n\\n337–348.',\n",
       "   '[16] Media Bias/Fact Check. 2015. https://mediabiasfactcheck.com/. Acces. May/2024.',\n",
       "   '[17] Julio CS Reis and Fabrício Benevenuto. 2021. Supervised learning for misinformation detection in whatsapp. In *Proc. of the Brazilian Symposium on Multimedia*\\n*and the Web (WebMedia)* . 245–252.',\n",
       "   '[18] Julio CS Reis, Philipe Melo, Fabiano Belém, Fabricio Murai, Jussara M Almeida,\\nand Fabricio Benevenuto. 2023. Helping Fact-Checkers Identify Fake News Stories\\nShared through Images on WhatsApp. In *Proc. of the Brazilian Symposium on*\\n*Multimedia and the Web (WebMedia)* . 159–167.',\n",
       "   '[19] Julio CS Reis, Philipe Melo, Márcio Silva, and Fabrício Benevenuto. 2023. Desinformação em plataformas digitais: Conceitos, abordagens tecnológicas e desafios.\\n*Sociedade Brasileira de Computação* (2023).',\n",
       "   '[20] Gustavo Resende, Philipe Melo, Hugo Sousa, Johnnatan Messias, Marisa Vasconcelos, Jussara Almeida, and Fabrício Benevenuto. 2019. (Mis)Information\\nDissemination in WhatsApp: Gathering, Analyzing and Countermeasures. In\\n*The Web Conference (WWW)* . 818–828.',\n",
       "   '[21] Filipe N Ribeiro, Koustuv Saha, Mahmoudreza Babaei, Lucas Henrique, Johnnatan\\nMessias, Fabricio Benevenuto, Oana Goga, Krishna P Gummadi, and Elissa M Redmiles. 2019. On microtargeting socially divisive ads: A case study of russia-linked\\nad campaigns on facebook. In *Proc. of the Conference on Fairness, Accountability,*\\n*and Transparency (FAT)* . 140–149.',\n",
       "   '[22] Manoel Horta Ribeiro, Raphael Ottoni, Robert West, Virgílio AF Almeida, and\\nWagner Meira Jr. 2020. Auditing radicalization pathways on YouTube. In *Proc. of*\\n*the Conference on Fairness, Accountability, and Transparency (FAT)* . 131–141.',\n",
       "   '[23] Vinay Setty and Erlend Rekve. 2020. Truth be Told: Fake News Detection Using\\nUser Reactions on Reddit. In *Proc. of the ACM Int’l Conference on Information &*\\n*Knowledge Management (CIKM)* . 3325–3328.',\n",
       "   '[24] Lisa Singh, Leticia Bode, Ceren Budak, Kornraphop Kawintiranon, Colton Padden,\\nand Emily Vraga. 2020. Understanding high-and low-quality URL Sharing on\\nCOVID-19 Twitter streams. *Journal of Computat. Social Science* (2020), 343–366.',\n",
       "   '[25] Kathie M d’I Treen, Hywel TP Williams, and Saffron J O’Neill. 2020. Online\\nmisinformation about climate change. *Wiley Interdisciplinary Reviews: Climate*\\n*Change* 11, 5 (2020), e665.',\n",
       "   '[26] Yash Vekaria, Rishab Nithyanand, and Zubair Shafiq. 2022. The Inventory is Dark\\nand Full of Misinformation: Understanding the Abuse of Ad Inventory Pooling\\nin the Ad-Tech Supply Chain. *arXiv preprint arXiv:2210.06654* (2022).',\n",
       "   '[27] Soroush Vosoughi, Deb Roy, and Sinan Aral. 2018. The spread of true and false\\nnews online. *science* 359, 6380 (2018), 1146–1151.',\n",
       "   '[28] Jevin D West and Carl T Bergstrom. 2021. Misinformation in and about science.\\n*Proceedings of the National Academy of Sciences* 118, 15 (2021), e1912444117.\\n\\n\\n178\\n\\n\\n-----'],\n",
       "  'text': '# **Finding Fake News Websites in the Wild**\\n\\n## Leandro Araujo\\n#### Universidade Federal de Minas Gerais Belo Horizonte, Brazil leandroaraujo@dcc.ufmg.br\\n## Isadora Rodrigues\\n#### Universidade Federal de Minas Gerais Belo Horizonte, Brazil isadorarodrigues@dcc.ufmg.br\\n### **ABSTRACT**\\n\\n## João M. M. Couto\\n#### Universidade Federal de Minas Gerais Belo Horizonte, Brazil joaocouto@dcc.ufmg.br\\n## Jussara M. Almeida\\n#### Universidade Federal de Minas Gerais Belo Horizonte, Brazil jussara@dcc.ufmg.br\\n## Fabricio Benevenuto\\n#### Universidade Federal de Minas Gerais Belo Horizonte, Brazil fabricio@dcc.ufmg.br\\n\\n## Luiz Felipe Nery\\n#### Universidade Federal de Minas Gerais Belo Horizonte, Brazil luiznery@dcc.ufmg.br\\n## Julio C. S. Reis\\n#### Universidade Federal de Viçosa Viçosa, Brazil jreis@ufv.br\\n\\n\\nThe battle against the spread of misinformation on the Internet\\nis a daunting task faced by modern society. Fake news content\\nis primarily distributed through digital platforms, with websites\\ndedicated to producing and disseminating such content playing a\\npivotal role in this complex ecosystem. Therefore, these websites\\nare of great interest to misinformation researchers. However, obtaining a comprehensive list of websites labeled as producers and/or\\nspreaders of misinformation can be challenging, particularly in developing countries. In this study, we propose a novel methodology\\nfor identifying websites responsible for creating and disseminating misinformation content, which are closely linked to users who\\nshare confirmed instances of fake news on social media. We validate\\nour approach on Twitter by examining various execution modes\\nand contexts. Our findings demonstrate the effectiveness of the proposed methodology in identifying misinformation websites, which\\ncan aid in gaining a better understanding of this phenomenon and\\nenabling competent entities to tackle the problem in various areas\\nof society.\\n### **KEYWORDS**\\n\\nFake News, Misinformation, Credibility, Websites, Social Media,\\nTwitter, X\\n### **1 INTRODUCTION**\\n\\nIn recent times, society has been faced with an unprecedented\\nscale of misinformation campaigns, covering highly sensitive topics including vaccines [ 15 ], climate change [ 25 ], scientific information [ 28 ], and politics [ 18 ]. The negative effects of misinformation\\ncampaigns are numerous, as they undermine the key processes used\\nto acquire and share information, posing a significant challenge for\\nsociety as a whole [ 19 ]. Addressing this issue has become part of\\nour daily lives, and must be tackled.\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n© 2024 SBC – Brazilian Computing Society.\\nISSN 2966-2753\\n\\n\\nIn the fight against misinformation, the complexity of the problem has emerged as one of the greatest challenges faced by modern\\nsociety. The issue manifests itself in any digital platform where\\nusers consume or exchange information, including video platforms\\n\\n[ 13 ], social networks [ 3, 10, 11 ], messaging applications [ 12, 17, 20 ],\\ndedicated websites, blogs, and forums [ 23 ]. The complexity of the\\nmisinformation ecosystem is further compounded by content recommendation algorithms employed by many of these platforms,\\nwhich often prioritize user engagement over the accuracy of the\\ninformation presented [ 14 ]. These factors can give rise to echo\\nchambers, leading to polarization [ 9 ] and even the radicalization of\\nusers [ 22 ]. Additionally, social platforms allow advertisers to target\\nusers based on detailed behavioral information, allowing misinformation campaigns to target specific and sometimes vulnerable\\nsegments of a population [21].\\nOne of the key features of this intricate ecosystem is the utilization of websites dedicated to the production and dissemination\\nof fabricated news content [ 5, 6 ]. These sites meticulously mimic\\nthe appearance and function of conventional and dependable news\\noutlets. When intertwined with misinformation campaigns, they\\nfrequently attempt to manipulate public opinion, propagating widespread suspicion and distrust of credible news sources. By positioning themselves as alternative, and claiming to be more trustworthy information sources, they contribute to the creation of an\\nalternative reality where a specific narrative and world view go\\nunchallenged. With such a strategy, misinformation campaigns can\\neffectively influence increasingly radicalized segments of society,\\nserving the interests of specific political entities.\\nIdentifying these websites and distinguishing them from their\\ncredible counterparts poses one of the most daunting challenges\\nto the misinformation research community. Despite its undeniable significance, obtaining lists of websites that are identified as\\nfake news sites, particularly for tackling misinformation in specific countries like Brazil, is far from a trivial task. This challenge\\nis partially explained by the fact that misinformation campaigns\\nare often orchestrated and supported by organizations with welldefined objectives. Those who propose to publish such lists are\\n\\n\\n171\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Araujo et al.\\n\\n\\nvulnerable to intimidation, whether by digital militias [1] or through\\nlegal harassment, involving vexatious litigation and other forms of\\ncostly legal action [2] .\\nThis study proposes a novel approach to detecting fake news\\nwebsites by leveraging user behavior rather than relying solely on\\nwebsite characteristics. Specifically, we hypothesize that users who\\nshare instances of fake news are likely to have shared additional\\nones. Our methodology identifies such users, ranks additional websites shared by them based on a specific criterion, and expands\\nthe search using articles pertaining to the newly identified websites. To validate our approach, we applied it to Twitter, currently\\nreferenced as X, and compared our findings with a curated list\\nof low-credibility websites published by an established American\\nfact-checking website. We further applied our methodology to the\\nBrazilian misinformation ecosystem, where we identified numerous\\npreviously unknown fake news websites. Our results demonstrate\\nthat our approach performs best when using a sorting criterion\\nthat accounts for both website impact and productivity within the\\nrelevant misinformation ecosystem and when initiated with a fake\\nnews URL checked by a recognized fact-checking entity such as the\\nInternational Fact-Checking Network (IFCN [3] ). Moreover, our study\\nshows that users identified through our methodology are indeed\\nmore likely to post instances of fake news, thereby reducing the\\nneed for manual evaluations per identification of a low-credibility\\nportal. We anticipate that our results will contribute to a better\\nunderstanding of this phenomenon and help competent entities to\\naddress the problem in various spheres of our society.\\nThe remainder of this paper is structured as follows: In Section\\n2, we provide a brief overview of previous approaches taken to\\naddress the issue of identifying low-credibility content and the\\nwebsites that disseminate them. Section 3 outlines the proposed\\nmethodology for identifying websites that spread misinformation.\\nWe then discuss the application of this methodology on Twitter in\\nthe American context, including details on the execution process, in\\nSection 4.3. Section 4 presents the findings of this execution, which\\nare compared to the ground truth provided by a renowned American\\nfact-checking website to assess the efficacy of our approach. In\\nSection 6, we perform a \"field test\" of the methodology in the context\\nof misinformation in Brazil. Finally, Section 7 concludes the paper\\nby highlighting its contributions and outlining future directions for\\nresearch.\\n### **2 RELATED WORK**\\n\\nIn recent times, there has been a significant body of literature that\\ndelves into various approaches to identifying websites that are\\ninvolved in creating and disseminating fake news. In this section\\nwe aim to summarize the research that is most pertinent to our\\nmethodology, with a particular focus on three key dimensions: (i)\\nthe dynamics underlying the spread of fake news; (ii) the monetization of fake news; and (iii) network aspects of domains associated\\nwith fake news.\\n\\n**Fake News Spreading Dynamics.** A number of noteworthy studies have been conducted on the dynamics of fake news on social\\n\\n1 https://www.latimes.com/91910540-132.html\\n2 https://www.abraji.org.br/entenda-o-que-e-assedio-judicial (in Portuguese)\\n3 https://www.poynter.org/ifcn/\\n\\n\\nmedia platforms. For instance, Vosoughi *et al.* [ 27 ] carried out a\\nseminal work by analyzing rumor cascades on Twitter from 2006 to\\n2017. Their findings reveal that fake news reached a wider audience\\nand spread more rapidly than accurate information. More recently,\\nSingh *et al.* [ 24 ] investigated the spread of URLs on Twitter during the COVID-19 pandemic. They classified URLs into different\\ncategories, such as high-quality health sources, traditional news\\nsources, and misinformation websites, which were identified as\\nsuch by the Media Bias/Fact Check (MBFC)[ 16 ] and other similar\\nsources. Their results indicated that the spread of news formed a\\nnetwork with a sub-network of high and low credible sources. The\\nstructure of the network showed that both high and low credible\\nsources were connected to traditional news sources, which played\\na critical role in bridging the two groups and facilitating the spread\\nof information from low to high quality.\\n\\n**Fake News Monetization.** Bozarth and Budak [ 4 ] have shown\\nthat a significant number of fake news websites, extracted from a\\ncarefully curated list, receive substantial support from reputable\\nad servers. This observation raises the possibility that leading ad\\nfirms could potentially help combat fake news by ceasing to provide\\nmonetization services to such websites. Meanwhile, Vekaria *et al.*\\n\\n[ 26 ] investigated how misinformation websites deceive ad server\\npolicies by pooling their ad inventory with unrelated sites in order\\nto circumvent brand safety policies. Using a curated list of misinformation websites, they showed that misinformation websites\\ndeceptively monetize their ad inventory by exploiting a complex ad\\nsupply chain. This finding suggests that monitoring ads on misinformation websites and exposing the brands that unwittingly fund\\nthem could be potential solutions. In this regard, a recent study\\nhas characterized the effectiveness of the Sleeping Giants Brazil\\ninitiative in demonetizing fake news websites [8].\\n\\n**Network Aspects of Fake News Web Domains.** Drawing on a\\ncurated list of websites and their corresponding credibility assessments, the study conducted by Couto *et al.* [ 7 ] leveraged computer\\nnetwork attributes to reveal that fake news websites exhibit a range\\nof content-agnostic characteristics that distinguish them from their\\ncredible counterparts. Specifically, they tend to be registered more\\nrecently, operate for shorter periods, and have certificates that expire more quickly. These findings suggest that fake news websites\\nin Brazil are often designed to be fleeting and ephemeral, allowing\\nthem to operate with greater impunity and evade detection by authorities and fact-checkers. The study also shows that fake news\\nwebsites are more likely to be hosted on foreign territories, suggesting a deliberate attempt to avoid scrutiny and regulation by local\\nauthorities. The use of computer network attributes provides an\\nefficient and scalable addition to the toolkit of researchers working\\nto combat the spread of misinformation.\\nDespite their contributions in understanding different aspects of\\nfake news websites, such as their dynamics of dissemination, monetization methods, and network characteristics, these studies provide\\nonly a narrow glimpse into the intricate ecosystem in which these\\nwebsites thrive. Moreover, the majority of these studies have mainly\\ntargeted websites from the United States, despite the worldwide\\nscope of the misinformation problem. Therefore, our research aims\\nto supplement existing literature by introducing a methodology\\n\\n\\n172\\n\\n\\n-----\\n\\nFinding Fake News Websites in the Wild WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\n**1** **2** **3**\\n\\nStarting Point User Identification URL Collection\\n\\n**5** **4**\\n\\n**Figure 1: Overview of the proposed methodology for identi-**\\n**fying fake news websites in the wild.**\\n\\nthat can be easily applied by researchers and practitioners in diverse regions and contexts to construct their own curated lists of\\nwebsites dedicated to producing and circulating fake news on the\\n\\nInternet.\\n### **3 PROPOSED METHODOLOGY**\\n\\nThis section outlines the proposed methodology for detecting fake\\nnews in the wild. The underlying hypothesis is that users who\\nhave shared news articles confirmed to be instances of fake news or\\nmisinformation, based on the verdict provided by an internationally\\nrecognized fact-checking agency, are more likely to have done so\\non other occasions compared to users who have not. While this\\nmay not always be the case, exploring the timelines of such users,\\nmoving from one user to another based on their mutual shared\\ncontent, is expected to effectively navigate the space of shared fake\\nnews articles in a given social network and identify websites closely\\nassociated with them. Our methodology consists of five main steps,\\nas illustrated in Figure 1.\\n\\n(1) **Starting point:** The proposed methodology begins by identifying a single \"seed\" news article URL that is associated\\nwith misinformation content, i.e., fake news. This can be\\naccomplished in two ways: (i) by identifying a fact-check\\nthat directly disproves information or claims made in the article, thereby establishing the article as a fake news instance,\\nor (ii) by determining that the article was published by a\\nlow credibility website. In both cases, the credibility label\\nmust have been produced by an internationally recognized\\nfact-checking agency such as IFCN to ensure the reliability\\nof the seed URL.\\n\\n(2) **User identification:** The next step is to identify users who\\nhave shared the seed news article on a social media platform\\nof choice. To validate the methodology, this study explores\\nTwitter, which is well-suited to the target phenomenon of\\nusers sharing news articles and offers an API [4] that enables\\neasy retrieval of these users’ timelines.\\n\\n(3) **URL collection:** All publicly available posts made by the\\nusers identified in Step 2 are collected. From these posts,\\nURLs are identified and extracted. The URLs are then filtered\\nby removing those that belong to websites known not to\\nhost news articles from external sources. Examples of such\\nwebsites include social networks and government websites.\\n\\n4 https://developer.twitter.com/\\n\\n\\n(4) **Ranking:** The websites hosting the filtered URLs are ranked\\naccording to a measure of relevance that captures their importance among the fake news-sharing users identified in\\nStep 2. In this study, the H-Index [ 2 ] is proposed as the metric of relevance. The websites are treated as authors, their\\nnews article URLs are treated as publications, and a user\\nsharing one of those publications is considered a citation.\\nThe URLs contained within each of these websites are also\\n\\nranked according to their importance, which is measured by\\na total count of shares by the users identified in Step 2.\\n\\n(5) **New seed selection:** The top-ranked news article URLs\\nassociated with the top-ranked websites are presented as\\ncandidates for addition as seeds. In this step, the URLs included in a given website’s H-Index set are presented in\\naccordance with the website standings. For instance, if the\\ntop-ranked website has an H-Index of 4, the candidates are\\nits top-4 shared URLs. Once the entity executing the methodology selects a new seed URL, Steps 2 through 5 are repeated.\\nThis loop is referred to as a *cycle* . Note that once a website’s\\nURL has been picked as a new seed on a given cycle, other\\nURLs from the same website are no longer considered for\\naddition in future cycles. This ensures that the methodology\\ndiscovers a distinct set of news websites with each cycle.\\n\\nDuring the execution of the proposed methodology, at the end of\\neach cycle, a list of new websites can be generated. This list is composed of the websites associated with the URLs selected as seeds\\nthroughout the execution cycles. This list of websites constitutes\\nthe final output of the methodology. It is important to note that the\\nauthors do not intend to release a public list of websites obtained\\nthrough this methodology. The purpose of this methodology is\\nnot to accuse any website of spreading misinformation, but rather\\nto provide a proven idea that enables researchers and competent\\nbodies to effectively navigate fake news ecosystems by identifying\\nwebsites that are closely associated with users who act as vectors\\nfor this type of content. Additionally, the proposed methodology facilitates novel research and misinformation prevention by enabling\\nresearchers to obtain their own lists of suspicious websites, which\\ncan then be evaluated for factual accuracy through fact-checks\\nconducted by internationally recognized agencies [5] or agencies that\\nassess the factuality of websites as a whole. In the following section,\\nwe present a strategy to validate the proposed methodology.\\n### **4 VALIDATION STRATEGY**\\n\\nTo the best of our knowledge, this methodology is unlike previously proposed approaches in the existing literature. To validate its\\ncapabilities, we investigate the effective of it at finding fake news\\nwebsites, validate premise that a user that has posted a fake news\\ninstance likely posts additional ones. Also, we analyze how well the\\nproposed methodology hold true as additional cycles are run and\\nlast, investigate if is H-Index capable of properly ranking suspicious\\nwebsites for analysis, as described next.\\n\\n5 https://www.poynter.org/ifcn/\\n\\n\\n173\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Araujo et al.\\n\\n### **4.1 Finding Ground Truth**\\n\\nThe validation process of our methodology poses a significant challenge, which is the establishment of a ground truth for comparison\\nagainst the obtained results. In Brazil, the absence of a curated list\\nof active fake news websites of sufficient size hinders the feasibility\\nof convincing analysis. Conversely, in the United States, the Media\\nBias/Fact Check [ 16 ] (henceforth, MBFC) offers a potential solution. MBFC is an autonomous website that assigns high, medium,\\nand low credibility labels, as well as political leanings, to a range\\nof news outlets operating in the US and beyond. MBFC identifies\\nas an independent online media outlet “devoted to educating the\\npublic on media bias and deceptive news practices”. Although its\\nassessments are less comprehensive than those of NewsGuard [6],\\nthe assigned attributes to each news source are publicly available.\\nIn this study, we utilize the factuality labels associated with the\\nindexed websites by MBFC.\\nThus, we compiled a dataset of websites whose credibility has\\nbeen evaluated by MBFC, where each website is assigned a credibility label. In this study, we analyzed MBFC’s methodology and\\ndefinitions in detail and considered websites with low and questionably low credibility labels as low credibility websites. All subsequent\\nanalyses presented in the forthcoming sections were carried out\\nbased on this definition. Our dataset contains evaluations of 3 *,* 510\\ndistinct websites. It is worth noting, however, that while the dataset\\ncovers a broad range of websites, it is limited by the dynamic nature\\nof the fake news ecosystem, which sees the continual emergence\\nof new fake news websites.\\n### **4.2 Setup**\\n\\nAs previously stated (refer to Section 3), the proposed methodology\\nnecessitates specific input parameters and definitions for its execution. Consequently, in this section, we present the configuration of\\nour validation strategy.\\n\\n*4.2.1* *Sets of initial seeds.* To validate the effectiveness of our\\nmethodology, it is crucial to examine its performance under various\\ninitial seed conditions. The choice of initial seed plays a critical\\nrole in determining the path of website discovery throughout the\\nexecution cycles. To this end, we conduct multiple executions of\\nthe proposed methodology, each time using a different initial seed.\\nWe consider three different sets of seeds for each execution: (i)\\nnews articles derived exclusively from high-credibility websites, (ii)\\nnews articles derived from an equal proportion of fake and highcredibility websites, and (iii) news articles derived exclusively from\\nfake news websites. Seeds from sets (i), (ii), and (iii) have 0%, 50%,\\nand 100% likelihood, respectively, of originating from fake news\\ninstances, as classified by MBFC. Through this approach, we can\\ncompare the effectiveness of our methodology in navigating the\\nsocial media URL sharing ecosystem when presented with actual\\nfake news instances versus when provided with high-credibility\\ninstances, while also validating several design decisions. The subsequent sections present the findings obtained from this setup.\\n\\n*4.2.2* *Automated execution and experimental setup.* In order to\\nassess the efficacy of our methodology, it is necessary to compare\\nit against alternative baseline approaches. For instance, we can\\n\\n6 https://www.newsguardtech.com/\\n\\n\\ncompare the ranking of websites by their total share against the\\nproposed H-Index method. This comparison allows us to measure\\nthe effect of the methodology’s design decisions on the purpose of\\nidentifying fake news websites.\\nHowever, generating enough data points to make a thorough\\nassessment of these effects can be costly. One of the main cost\\nfactors is Step 5 in the methodology (Figure 1), where a human\\nmust manually select a new URL to be used as seed for the next\\ncycle of execution. To address this issue, we propose an “automated”\\nexecution of the methodology: in this approach, the algorithm is fed\\nwith a random single initial seed from one of the three sets presented\\nin the previous section (0%, 50%, and 100% fake probability). From\\nthat point forward, the most shared URL from the top ranking\\nwebsite, as described in Step 4, is always the one added as new seed\\nfor the following steps. Although this automation is suboptimal, as\\nhumans are better equipped to identify actual fake news instances\\nin each cycle, it provides a lower bound for the quality of results\\nthat can be obtained in real-world usage.\\n### **4.3 Twitter Execution**\\n\\nIn order to assess the potential of the methodology in practical\\nsettings, we have applied it to Twitter within the experimental\\nframework presented previously. Through this application, we generated a dataset of results that allowed us to measure the methodol\\nogy’s ability to identify fake news websites, as well as to determine\\nits properties and behavior. Twitter is a widely adopted platform\\nfor discussions on a broad range of topics, and is notorious for\\nits use as a medium for the dissemination of misinformation cam\\npaigns [ 3, 10 ]. Although users on this platform interact with each\\nother in various ways (e.g., follow, retweet, comment), we only\\nconsidered tweets posted by users in their feeds in our work. The\\ndata was limited to tweets published in 2022. Notably, publications\\non Twitter may contain links to external news websites, which\\nis precisely the domain of news content explored in this effort.\\nThe steps taken for this execution are described below. It is worth\\nmentioning that many of these steps were only implemented to\\nfacilitate the execution of the methodology at scale, in accordance\\nwith the proposed validation strategy.\\n\\n*4.3.1* *Selection of initial seeds.* To begin the execution of our methodology (see Figure 1), the first step requires the identification of a\\nseed fake news article from which novel misinformation websites\\n\\nmay be discovered. To gather a sizable collection of initial seeds,\\nwe utilized Twitter’s search feature with the query \"lang:en\" which\\nis a way of filtering tweets associated exclusively with the English\\nlanguage. Using the query format \"max-dt:YYYY-MM-DD,\" we extracted a sample of tweets published between January 2022 and\\nDecember 2022. This process was carried out using a newly created account to prevent the results from being biased by any user\\nactivity or recommendation algorithms.\\nFrom the resulting dataset of tweets, we identified those containing URLs. Subsequently, we filtered the URLs to extract only\\nthe ones belonging to news websites labeled in MBFC, i.e., news\\narticles originating from sources that have a factuality label.\\nHenceforth, the methodology was executed strictly following\\nthe protocol outlined in Section 3. Specifically, the following steps\\nwere carried out: (i) identification of users who have tweeted the\\n\\n\\n174\\n\\n\\n-----\\n\\nFinding Fake News Websites in the Wild WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\ninitial seed, (ii) collection of additional URLs tweeted by these users,\\n(iii) ranking of websites associated with these URLs according to a\\nspecified criterion, and (iv) automatic addition of the most shared\\nlink of the top-ranked website to the list of ongoing seeds. It should\\nbe emphasized that, in each cycle, users who have shared previously\\nselected seeds were also considered for future H-index calculations.\\nTo generate a sufficient quantity of data points, the automated\\nmethodology was executed up to cycle 30, a total of 360 times\\nper website ranking criterion, each one under slightly different\\nconditions, to validate our design decisions, as discussed in the\\nfollowing sections.\\n### **5 EXPERIMENTAL RESULTS**\\n\\nTo evaluate the impact of the initial seed on the results of our\\nmethodology, we conducted an analysis of the ranking quality under\\ndifferent scenarios. Specifically, we investigated the effect of using\\ninitial seeds with 0%, 50%, and 100% probability of being fake, as\\nwell as replacing the H-Index with two alternative ranking criteria:\\n(i) the number of shares in the most shared URL and (ii) random\\nranking of websites. It is worth noting that regardless of the ranking\\ncriteria used, the most shared URL of the top-ranked website was\\nalways added to the ongoing set of seeds. By observing the pattern\\nof ranking quality across the different initial seed scenarios, we can\\nassess the impact of the credibility nature of the initial seed on the\\nmethodology’s performance.\\n### **5.1 Importance of the Initial Seed**\\n\\nThe subfigures depicted in Figure 2 present the quantity of known\\nfake news websites per MBFC observed in the top position up to\\neach cycle over a total of 30 cycles, each ranked by a specific ranking criterion and a varying initial set of seeds (0%, 50%, and 100%\\nfake). For each subfigure, each data point denotes the average of\\nthis quantity over 40 automated executions of each labeled scenario.\\nFor example, a data point on cycle 15 and average 10 on the 100%\\ncurve for the “Sorting Criteria: hindex” graph indicates that, on\\naverage, based on the 40 executions, among the first 15 websites observed in the top-1 spot, 10 of them were fake news websites. Figure\\n3a presents a juxtaposition of the 100% fake set execution curves\\nfrom the aforementioned individual average fake news websites\\namounts graphs, so that we may better compare the differences in\\nperformance between the ranking criteria.\\nFinally, we conducted 3 distinct runs of 40 executions up to cycle\\n30 for each scenario, which consists of a combination of one ranking methodology and type of initial seed, as described in Section\\n4.2. Note that each run used a unique initial seed derived from its\\nassigned set of seeds. Notably, we found that in every cycle, even\\nthe worst of the three 40-execution runs performed under the 100%\\ndataset yielded a better ranking than the best performance obtained\\nunder other initial seed datasets. These results suggest that, regardless of the ranking criteria, fake news websites are consistently\\nmore likely to be ranked in the top positions throughout the cycles\\nwhen the initial seed is more closely associated with misinformation. Figure 2, on “Sorting Criteria: hindex”, shows that the median\\nsubset performance under 0% probability would need 30 cycles so\\nthat 2.38 websites could be observed, against just 5 cycles under\\n100% probability. This finding highlights that our methodology is\\n\\n\\ncapable of navigating the URL sharing landscape effectively by\\nleveraging users with mutually shared fake news instances, resulting in the discovery of news portals of similar credibility nature to\\nthe seed.\\n### **5.2 Website Ranking Criteria**\\n\\nHaving demonstrated the significance of an adequate initial seed,\\nall subsequent analyses will be carried out assuming a 100% fake\\nnews seed dataset, as this more closely resembles the actual implementation of our methodology, where a human is responsible\\nfor determining which URLs are added to the ongoing set of seeds,\\nfrom which additional users are identified.\\nIn Figure 2, we have presented a side-by-side comparison of\\nthe performance of the three different ranking criteria used in our\\nstudy. The three lines in the graph represent the average number of\\nknown fake news websites observed in the top 1 position, plotted\\nover a total of 30 cycles, with each cycle representing an execution\\nof our methodology. As indicated in the previous paragraph, all\\nof these executions have been performed using a 100% fake set of\\nseeds.\\nOne important observation from this figure is the increasing\\ndetachment between the H-index curve and the other two curves\\n\\nas more cycles are completed. In other words, over time, H-index\\npresents a significantly better ability to steer the execution towards\\nportals that share content with a similar credibility nature as the\\nseed. This finding is consistent with our hypothesis that H-index\\nis a more effective metric for ranking websites in this context, as\\nit takes into account both the popularity of the website and the\\ndiversity of the sources that link to it. The other two criteria, by\\ncontrast, are less effective at distinguishing between credible and\\nnon-credible sources.\\n### **5.3 Fake News Website Discovery**\\n\\nIt is worth noting that, at the end of a cycle *𝑥*, the methodology can,\\nat most, have identified x fake news websites that were introduced\\nto the set of ongoing seeds. In this regard, Figure 3c displays the\\naverage percentage of optimal execution achieved over the course\\nof the cycles. Specifically, if a given cycle *𝑥* is linked with an average\\nof 0.7, it implies that on cycle *𝑥*, a total of 70% of the *𝑥* websites\\nranked at the top position throughout the execution were fake news\\nwebsites. Consequently, 0.7* *𝑥* fake news websites were discovered\\nuntil this cycle.\\nThe H-index curve hovering a median of approximately 50%\\nperformance level across the 30 cycles implies that in half of the\\ncycles, a human evaluator would have been able to identify a new\\nlow-credibility website by manually inspecting only the top-ranked\\nwebsite. It should be noted that this estimate represents a lower\\nlimit on the potential performance of our methodology because (1)\\nthe top-ranked website might be of low-credibility and not indexed\\nby MBFC, and (2) human evaluators are expected to outperform our\\nautomated executions, especially if they choose seeds that are more\\nstrongly associated with misinformation than just the most shared\\nURL from the top-ranked website. This difference arises from the\\nfact that our automated executions are designed to replicate the\\nreal-world execution of our methodology, as described in Section\\n\\n\\n175\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Araujo et al.\\n\\n\\n14\\n\\n12\\n\\n10\\n\\n8\\n\\n6\\n\\n4\\n\\n2\\n\\n0\\n\\n\\nSorting Criteria: random\\n\\n0 5 10 15 20 25 30\\n# cycles\\n\\n\\n14\\n\\n12\\n\\n10\\n\\n8\\n\\n6\\n\\n4\\n\\n2\\n\\n0\\n\\n\\nSorting Criteria: mostpop\\n\\n0 5 10 15 20 25 30\\n# cycles\\n\\n\\n14\\n\\n12\\n\\n10\\n\\n8\\n\\n6\\n\\n4\\n\\n2\\n\\n0\\n\\n\\nSorting Criteria: hindex\\n\\n0 5 10 15 20 25 30\\n# cycles\\n\\n\\n**Figure 2: Average rank 1 incidence of low credibility websites over 40 executions with varying ranking criteria and seed dataset.**\\n\\n\\n0 5 10 15 20 25 30\\n# cycles\\n\\n**(b)**\\n\\n\\n100\\n\\n90\\n\\n80\\n\\n70\\n\\n60\\n\\n50\\n\\n40\\n\\n30\\n\\n20\\n\\n10\\n\\n0\\n\\n\\nFake News Odds: 100%\\n\\n\\nFake News Odds: 100%\\n\\n\\n14\\n\\n12\\n\\n10\\n\\n8\\n\\n6\\n\\n4\\n\\n2\\n\\n0\\n\\n\\nFake News Odds: 100%\\n\\n0 5 10 15 20 25 30\\n# cycles\\n\\n**(a)**\\n\\n\\n100\\n\\n90\\n\\n80\\n\\n70\\n\\n60\\n\\n50\\n\\n40\\n\\n30\\n\\n20\\n\\n10\\n\\n0\\n\\n\\n0 5 10 15 20 25 30\\n# cycles\\n\\n**(c)**\\n\\n\\n**Figure 3: (a) Performance for different ranking criteria when seeds are URLs of known low-credibility websites; (b) Percentage**\\n**of successfully selected websites for different ranking criteria when seeds are URLs of known low-credibility websites, and; (c)**\\n**Recall for different ranking criteria normalized by the best possible scenario.**\\n\\n\\n4.2. Thus, it is reasonable to expect better results from human\\nevaluators in practice.\\nThe outcome demonstrates that our proposed methodology,\\nwhich incorporates H-index, commences with a low credibility seed,\\nand restricts each website to have a single seed included throughout the cycles, enables the detection of new fake news websites\\nwithout the necessity of manually verifying numerous websites\\nbefore finding a single discovery. Instead, it smartly navigates the\\nURL sharing ecosystem by leveraging the overlap of users sharing\\nURLs associated with various fake news websites and ranks them\\n\\nby a metric that considers the productivity and impact of these\\nwebsites. This approach results in a curated list of websites, among\\nwhich a human-in-the-loop can identify new fake news websites\\nwith significantly fewer manual inspections.\\n### **5.4 Dimishing Returns**\\n\\nFigure 3b shows the percentage of websites ranked at the top spot\\nthat are fake news, for each individual cycle, rather than the cumulative results shown in previous graphs. Our methodology’s ability\\nto navigate the URL sharing ecosystem is reflected in the initial\\ncycle, where the performance of the mostpop and h-index ranking\\ncriteria are similar, with both hovering around 70% in terms of the\\npercentage of websites on rank 1 that are fake news across the\\nindependent executions. However, as the cycles progress the performance of the different ranking criteria begins to diverge, with\\n\\n\\nthe h-index curve consistently outperforming the other criteria by\\na significant margin. By cycle 30, the performance of all ranking\\ncriteria converge to low 10’s percentage, indicating the diminishing\\nreturns of the methodology as the more readily reachable websites\\nare identified. As such, on automated executions context, it proves\\nefficient to stop the methodology after a certain number of cycles\\nand restart with a different set of initial seeds, highlighting the\\nimportance of a human-in-the-loop to provide feedback, keep the\\nseeds closely related to fake news instances through the cycles and\\nadjust the methodology to target the most promising areas for the\\ndiscovery of novel fake news websites.\\n### **5.5 Discovery of Impactful Fake News Websites**\\n\\nIn order to further evaluate the potential of our methodology, we\\naimed to assess its ability to discover the most relevant fake news\\nwebsites in a given time frame. To do so, we obtained the popularity\\nranking for each fake news website listed in MBFC by fetching\\ntheir corresponding Open Pagerank value from DomCop.org’s 10\\nmillion website dataset. The Open Pagerank score represents the\\nimportance and popularity of a website based on various factors\\nsuch as the number and quality of backlinks, social media mentions,\\nand overall web presence.\\nUsing the obtained Open Pagerank scores, we constructed a\\ncumulative distribution of the popularity rankings for the fake\\nnews websites discovered by our methodology. The resulting plot\\n\\n\\n176\\n\\n\\n-----\\n\\nFinding Fake News Websites in the Wild WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\n100\\n\\n\\nOdds: 100%\\n\\n\\n80\\n\\n60\\n\\n40\\n\\n20\\n\\n0 20 40 60 80 100\\nTop % Popular\\n\\n**Figure 4: Cumulative distribution function (CDF) of fake**\\n**news websites considering their popularity based on PageR-**\\n**ank.**\\n\\nis shown in Figure 4. As can be seen, 60% of the fake news websites\\ndiscovered by our methodology fall within the 15% most popular fake news websites indexed by MBFC. This indicates that our\\nmethodology by identifying and prioritizing the most influential\\nand widely disseminated fake news websites enables more targeted\\nand efficient interventions in the fight against spread of misinformation.\\n### **6 GATHERING FAKE NEWS WEBSITES IN** **BRAZIL**\\n\\nFinally, in order to assess the generalizability of our proposed\\nmethodology, we applied it to the Brazilian context, ultimately\\nidentifying 75 fake news websites. For this purpose, we adopted\\nthe same criteria used in our previous experiments to classify a\\nwebsite as a “fake news website”, namely, if the news published on\\nthe evaluated URL was fact-checked by an internationally recognized agency and deemed to be fake. Additionally, we collected a\\nlist of 99 news websites belonging to ANJ, the Brazilian national\\nnewspaper association responsible for defining rules and standards\\non news quality and factualness, in order to compare the identified\\nfake news websites with high-credible ones. The details of our application of the methodology in the Brazilian scenario, such as the\\nnews article used as a seed and the number of cycles, are presented\\nin another publication by our research group [1].\\n### **6.1** **Relevance of Identified Websites on Social** **Platforms**\\n\\nIt is noteworthy to mention that fake news websites rely heavily on\\ndigital platforms such as Twitter and Facebook to gain traction for\\ntheir publications. Consequently, it is common for them to establish\\nan official presence on these social networks. The practicality of\\nsharing a URL, which acts as a gateway to content hosted on an\\nexternal vehicle, rather than relying on that content being available\\non a third-party platform, greatly enhances the websites’ ability\\nto reach a larger portion of their target audiences and establishes\\nthem as misinformation vectors worth highlighting.\\nIn light of this, we conducted an investigation to assess the relevance of misinformation websites identified through our methodology in the context of Brazil. Specifically, we set out to find the\\ncorresponding Facebook pages of each website and measure their\\n\\n\\nspread. To accomplish this, we queried the name of each website on\\nFacebook’s search feature. As a result, we were able to identify 63\\nwebsites with a clearly corresponding Facebook page (e.g., sporting\\nthe exact same name and logo). For the remaining 13 websites,\\nwe were unable to establish a clear mapping between them and\\ncorresponding Facebook pages.\\nFinally, we employed CrowdTangle [7], a social media analytics\\ntool, to obtain information about the popularity of each Facebook\\npage over a period of 12 months (from December 2021 to November\\n2022). We were able to retrieve CrowdTangle data for 61 Facebook\\npages out of the 63 previously mentioned, taking into consideration\\n5 websites that are linked to two active pages each. Additionally,\\nwe retrieved data from 82 pages associated with ANJ in order to\\ncompare the results. An overview of these findings is presented in\\nTable 1.\\n\\nOverall, publications by the 61 Facebook pages received 23 *,* 580 *,* 129\\nshares and 160 *,* 387 *,* 038 reactions, indicating that the identified fake\\nnews websites were able to reach approximately 30 million users on\\nFacebook alone. It is important to note that this number represents\\na lower bound for the websites identified through our proposed\\nmethodology, as we were unable to fetch the Facebook page and\\nthe CrowdTangle information for all websites. Furthermore, CrowdTangle only makes available public information about the pages,\\nso content shared in private groups is not accounted for in our\\nmeasurement. However, this experiment suggests that the set of\\nwebsites discovered through the proposed methodology is highly\\nrelevant to the Brazilian fake news ecosystem.\\n### **7 CONCLUSION**\\n\\nIn this study, a novel methodology for identifying websites dedicated to the production and dissemination of fake news on the\\ninternet is proposed. The approach presented is designed to be easily applicable by research and competent authorities across various\\ngeographic locations. It is worth mentioning that the current work\\nrefrains from providing a ready-made list of such websites in the\\ninterest of avoiding potential legal repercussions. Accusations directed towards specific entities regarding their involvement in the\\ndissemination of fake news are avoided, and instead, the methodology is provided to enable research and government entities to\\nassemble their own lists of websites. With this approach, the risk\\nof judicial harassment is minimized, while the ability to identify\\nfake news websites is retained. Finally, some potential research\\ndirections in this context are discussed below.\\n\\n**Fake News in Different Contexts.** It is our hope that the present\\nresearch serves as a catalyst for future studies on the issue of fake\\nnews worldwide. Our proposed methodology offers a comprehensive framework that enables a deeper understanding of the dissemination of misinformation on digital platforms that extends\\nbeyond Twitter analysis, including messaging apps like WhatsApp\\nand Telegram. Our methodology takes a unique approach to this\\nissue by targeting a common vector across all digital platforms:\\nusers sharing external fake news websites. We believe that the incidental lists generated from our method provide an opportunity for\\ninvestigating various facets of fake news websites.\\n\\n7 https://www.crowdtangle.com/\\n\\n\\n177\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Araujo et al.\\n\\n**Table 1: Comparison of Facebook data for fake news websites that were found from the proposed methodology and credible**\\n**news outlets based on ANJ.**\\n\\n|son of Facebook data for fa d on ANJ. Feature|ake news websites that Fake News Websites|were found from the prop High-Credible Sources|posed methodo Total|\\n|---|---|---|---|\\n|Feature|Fake News Websites|High-Credible Sources|Total|\\n|All Reactions|160,387,038 (43.93%)|204,691,475 (56.07%)|365,078,513|\\n|Comments|36,188,143 (39.44%)|55,558,913 (60.56%)|91,747,056|\\n|Likes|129,658,929 (48.19%)|139,381,506 (51.81%)|269,040,435|\\n|Owned Post Views|770,778,110 (56.62%)|590,538,483 (43.38%)|1,361,316,593|\\n|Owned Total Views|818,999,144 (56.24%)|637,270,102 (43.76%)|1,456,269,246|\\n|Owned Views from Shares|48,221,034 (50.78%)|46,731,619 (49.22%)|94,952,653|\\n|Page Follower Growth|886,397 (46.67%)|1,013,009 (53.33%)|1,899,406|\\n|Page Followers|31,944,505 (34.09%)|61,764,855 (65.91%)|93,709,360|\\n|Page Likes|27,119,133 (32.09%)|57,402,458 (67.91%)|84,521,591|\\n|Shares|23,580,129 (62.61%)|14,084,141 (37.39%)|37,664,270|\\n|Total Interactions|220,155,313 (44.52%)|274,334,538 (55.48%)|494,489,851|\\n|Total Posts|225,756 (23.33%)|741,757 (76.67%)|967,513|\\n|Views on Shared Posts|27,241,954 (92.74%)|2,131,818 (7.26%)|29,373,772|\\n\\n\\n**Government Action.** Our proposed methodology presents a valuable tool for regulatory entities seeking to investigate the source\\nof funding and the beneficiaries of the publication of fake news\\nthrough websites. As part of our research efforts, our group is\\ncurrently engaged in a collaborative initiative with the Ministério\\nPúblico de Minas Gerais (MPMG) in Brazil, providing them with\\nsufficient material to warrant an investigation of these websites.\\n### **ACKNOWLEDGMENTS**\\n\\nThis work was partially supported by research grants from Ministério Público de Minas Gerais (MPMG), project Analytical Capabilities, CNPq, FAPEMIG, and FAPESP.\\n### **REFERENCES**\\n\\n[1] Leandro Araújo, Luiz Felipe Nery, Isadora C Rodrigues, João MM Couto, Julio CS\\nReis, Ana PC Silva, Jussara M Almeida, and Fabrício Benevenuto. 2022. Identificando websites de desinformaçao no brasil. In *Brazilian Database Symposium*\\n*(SBBD)* . 355–360.\\n\\n[2] Lutz Bornmann and Hans-Dieter Daniel. 2007. What do we know about the h\\nindex? *Journal of the American Society for Information Science and technology* 58,\\n9 (2007), 1381–1385.\\n\\n[3] Alexandre Bovet and Hernán A Makse. 2019. Influence of fake news in Twitter\\nduring the 2016 US presidential election. *Nature Communicat.* 10, 1 (2019), 1–14.\\n\\n[4] Lia Bozarth and Ceren Budak. 2020. Market forces: Quantifying the role of top\\ncredible ad servers in the fake news ecosystem. In *The Int’l Conference on Web*\\n*and Social Media (ICWSM)* .\\n\\n[5] João MM Couto, Julio CS Reis, and Fabrício Benevenuto. 2024. Can computer\\nnetwork attributes be useful for identifying low-credibility websites? A case\\nstudy in Brazil. *Social Network Analysis and Mining* 14, 1 (2024), 153.\\n\\n[6] João MM Couto, Julio CS Reis, Ítalo Cunha, Leandro Araújo, and Fabrício Benevenuto. 2022. Caracterizando websites de baixa credibilidade no brasil. In\\n*Brazilian Symposium on Computer Networks and Distributed Systems (SBRC)* .\\n503–516.\\n\\n[7] Joao MM Couto, Julio CS Reis, Italo Cunha, Leandro Araujo, and Fabricio Benevenuto. 2022. Characterizing Low Credibility Websites in Brazil through Computer Networking Attributes. In *IEEE/ACM Int’l Conference on Advances in Social*\\n*Networks Analysis and Mining (ASONAM)* .\\n\\n[8] Bárbara G. Ribeiro, Manoel Horta Ribeiro, Virgilio Almeida, and Wagner Meira Jr.\\n2022. Analyzing the “Sleeping Giants” Activism Model in Brazil. In *ACM Web*\\n*Science Conference (WebSci)* . 87–97.\\n\\n[9] Venkata Rama Kiran Garimella and Ingmar Weber. 2017. A long-term analysis of\\npolarization on Twitter. In *Int’l Conference on Web and Social Media (ICWSM)* .\\n\\n[10] Nir Grinberg, Kenneth Joseph, Lisa Friedland, Briony Swire-Thompson, and\\nDavid Lazer. 2019. Fake news on Twitter during the 2016 US presidential election.\\n*Science* 363, 6425 (2019), 374–378.\\n\\n[11] Andrew Guess, Jonathan Nagler, and Joshua Tucker. 2019. Less than you think:\\nPrevalence and predictors of fake news dissemination on Facebook. *Science*\\n*advances* 5, 1 (2019), eaau4586.\\n\\n\\n\\n[12] Mohamad Hoseini, Philipe Melo, Manoel Júnior, Fabrício Benevenuto, Balakrishnan Chandrasekaran, Anja Feldmann, and Savvas Zannettou. 2020. Demystifying\\nthe Messaging Platforms’ Ecosystem Through the Lens of Twitter. In *ACM Inter-*\\n*net Measurement Conference (IMC)* . 345–359.\\n\\n[13] Eslam Hussein, Prerna Juneja, and Tanushree Mitra. 2020. Measuring misinformation in video search platforms: An audit study on YouTube. *Proc. of the ACM*\\n*on Human-Computer Interaction* 4, CSCW (2020), 1–27.\\n\\n[14] Juhi Kulshrestha, Muhammad Zafar, Lisette Noboa, Krishna Gummadi, and Saptarshi Ghosh. 2015. Characterizing information diets of social media users. In\\n*Proc. of the Int’l AAAI Conference on Web and Social Media (ICWSM)* . 218–227.\\n\\n[15] Sahil Loomba, Alexandre de Figueiredo, Simon J Piatek, Kristen de Graaf, and\\nHeidi J Larson. 2021. Measuring the impact of COVID-19 vaccine misinformation\\non vaccination intent in the UK and USA. *Nature human behaviour* 5, 3 (2021),\\n\\n337–348.\\n\\n[16] Media Bias/Fact Check. 2015. https://mediabiasfactcheck.com/. Acces. May/2024.\\n\\n[17] Julio CS Reis and Fabrício Benevenuto. 2021. Supervised learning for misinformation detection in whatsapp. In *Proc. of the Brazilian Symposium on Multimedia*\\n*and the Web (WebMedia)* . 245–252.\\n\\n[18] Julio CS Reis, Philipe Melo, Fabiano Belém, Fabricio Murai, Jussara M Almeida,\\nand Fabricio Benevenuto. 2023. Helping Fact-Checkers Identify Fake News Stories\\nShared through Images on WhatsApp. In *Proc. of the Brazilian Symposium on*\\n*Multimedia and the Web (WebMedia)* . 159–167.\\n\\n[19] Julio CS Reis, Philipe Melo, Márcio Silva, and Fabrício Benevenuto. 2023. Desinformação em plataformas digitais: Conceitos, abordagens tecnológicas e desafios.\\n*Sociedade Brasileira de Computação* (2023).\\n\\n[20] Gustavo Resende, Philipe Melo, Hugo Sousa, Johnnatan Messias, Marisa Vasconcelos, Jussara Almeida, and Fabrício Benevenuto. 2019. (Mis)Information\\nDissemination in WhatsApp: Gathering, Analyzing and Countermeasures. In\\n*The Web Conference (WWW)* . 818–828.\\n\\n[21] Filipe N Ribeiro, Koustuv Saha, Mahmoudreza Babaei, Lucas Henrique, Johnnatan\\nMessias, Fabricio Benevenuto, Oana Goga, Krishna P Gummadi, and Elissa M Redmiles. 2019. On microtargeting socially divisive ads: A case study of russia-linked\\nad campaigns on facebook. In *Proc. of the Conference on Fairness, Accountability,*\\n*and Transparency (FAT)* . 140–149.\\n\\n[22] Manoel Horta Ribeiro, Raphael Ottoni, Robert West, Virgílio AF Almeida, and\\nWagner Meira Jr. 2020. Auditing radicalization pathways on YouTube. In *Proc. of*\\n*the Conference on Fairness, Accountability, and Transparency (FAT)* . 131–141.\\n\\n[23] Vinay Setty and Erlend Rekve. 2020. Truth be Told: Fake News Detection Using\\nUser Reactions on Reddit. In *Proc. of the ACM Int’l Conference on Information &*\\n*Knowledge Management (CIKM)* . 3325–3328.\\n\\n[24] Lisa Singh, Leticia Bode, Ceren Budak, Kornraphop Kawintiranon, Colton Padden,\\nand Emily Vraga. 2020. Understanding high-and low-quality URL Sharing on\\nCOVID-19 Twitter streams. *Journal of Computat. Social Science* (2020), 343–366.\\n\\n[25] Kathie M d’I Treen, Hywel TP Williams, and Saffron J O’Neill. 2020. Online\\nmisinformation about climate change. *Wiley Interdisciplinary Reviews: Climate*\\n*Change* 11, 5 (2020), e665.\\n\\n[26] Yash Vekaria, Rishab Nithyanand, and Zubair Shafiq. 2022. The Inventory is Dark\\nand Full of Misinformation: Understanding the Abuse of Ad Inventory Pooling\\nin the Ad-Tech Supply Chain. *arXiv preprint arXiv:2210.06654* (2022).\\n\\n[27] Soroush Vosoughi, Deb Roy, and Sinan Aral. 2018. The spread of true and false\\nnews online. *science* 359, 6380 (2018), 1146–1151.\\n\\n[28] Jevin D West and Carl T Bergstrom. 2021. Misinformation in and about science.\\n*Proceedings of the National Academy of Sciences* 118, 15 (2021), e1912444117.\\n\\n\\n178\\n\\n\\n-----\\n\\n',\n",
       "  'artigo_tokenizado': ['#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Finding',\n",
       "   'Fake',\n",
       "   'News',\n",
       "   'Websites',\n",
       "   'in',\n",
       "   'the',\n",
       "   'Wild',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Leandro',\n",
       "   'Araujo',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'Belo',\n",
       "   'Horizonte',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   'leandroaraujo@dcc.ufmg.br',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Isadora',\n",
       "   'Rodrigues',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'Belo',\n",
       "   'Horizonte',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   'isadorarodrigues@dcc.ufmg.br',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'ABSTRACT',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'João',\n",
       "   'M.',\n",
       "   'M.',\n",
       "   'Couto',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'Belo',\n",
       "   'Horizonte',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   'joaocouto@dcc.ufmg.br',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Jussara',\n",
       "   'M.',\n",
       "   'Almeida',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'Belo',\n",
       "   'Horizonte',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   'jussara@dcc.ufmg.br',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Fabricio',\n",
       "   'Benevenuto',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'Belo',\n",
       "   'Horizonte',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   'fabricio@dcc.ufmg.br',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Luiz',\n",
       "   'Felipe',\n",
       "   'Nery',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'Belo',\n",
       "   'Horizonte',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   'luiznery@dcc.ufmg.br',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Julio',\n",
       "   'C.',\n",
       "   'S.',\n",
       "   'Reis',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Viçosa',\n",
       "   'Viçosa',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   'jreis@ufv.br',\n",
       "   '\\n\\n\\n',\n",
       "   'The',\n",
       "   'battle',\n",
       "   'against',\n",
       "   'the',\n",
       "   'spread',\n",
       "   'of',\n",
       "   'misinformation',\n",
       "   'on',\n",
       "   'the',\n",
       "   'Internet',\n",
       "   '\\n',\n",
       "   'is',\n",
       "   'a',\n",
       "   'daunting',\n",
       "   'task',\n",
       "   'faced',\n",
       "   'by',\n",
       "   'modern',\n",
       "   'society',\n",
       "   '.',\n",
       "   'Fake',\n",
       "   'news',\n",
       "   'content',\n",
       "   '\\n',\n",
       "   'is',\n",
       "   'primarily',\n",
       "   'distributed',\n",
       "   'through',\n",
       "   'digital',\n",
       "   'platforms',\n",
       "   ',',\n",
       "   'with',\n",
       "   'websites',\n",
       "   '\\n',\n",
       "   'dedicated',\n",
       "   'to',\n",
       "   'producing',\n",
       "   'and',\n",
       "   'disseminating',\n",
       "   'such',\n",
       "   'content',\n",
       "   'playing',\n",
       "   'a',\n",
       "   '\\n',\n",
       "   'pivotal',\n",
       "   'role',\n",
       "   'in',\n",
       "   'this',\n",
       "   'complex',\n",
       "   'ecosystem',\n",
       "   '.',\n",
       "   'Therefore',\n",
       "   ',',\n",
       "   'these',\n",
       "   'websites',\n",
       "   '\\n',\n",
       "   'are',\n",
       "   'of',\n",
       "   'great',\n",
       "   'interest',\n",
       "   'to',\n",
       "   'misinformation',\n",
       "   'researchers',\n",
       "   '.',\n",
       "   'However',\n",
       "   ',',\n",
       "   'obtaining',\n",
       "   'a',\n",
       "   'comprehensive',\n",
       "   'list',\n",
       "   'of',\n",
       "   'websites',\n",
       "   'labeled',\n",
       "   'as',\n",
       "   'producers',\n",
       "   'and/or',\n",
       "   '\\n',\n",
       "   'spreaders',\n",
       "   'of',\n",
       "   'misinformation',\n",
       "   'can',\n",
       "   'be',\n",
       "   'challenging',\n",
       "   ',',\n",
       "   'particularly',\n",
       "   'in',\n",
       "   'developing',\n",
       "   'countries',\n",
       "   '.',\n",
       "   'In',\n",
       "   'this',\n",
       "   'study',\n",
       "   ',',\n",
       "   'we',\n",
       "   'propose',\n",
       "   'a',\n",
       "   'novel',\n",
       "   'methodology',\n",
       "   '\\n',\n",
       "   'for',\n",
       "   'identifying',\n",
       "   'websites',\n",
       "   'responsible',\n",
       "   'for',\n",
       "   'creating',\n",
       "   'and',\n",
       "   'disseminating',\n",
       "   'misinformation',\n",
       "   'content',\n",
       "   ',',\n",
       "   'which',\n",
       "   'are',\n",
       "   'closely',\n",
       "   'linked',\n",
       "   'to',\n",
       "   'users',\n",
       "   'who',\n",
       "   '\\n',\n",
       "   'share',\n",
       "   'confirmed',\n",
       "   'instances',\n",
       "   'of',\n",
       "   'fake',\n",
       "   'news',\n",
       "   'on',\n",
       "   'social',\n",
       "   'media',\n",
       "   '.',\n",
       "   'We',\n",
       "   'validate',\n",
       "   '\\n',\n",
       "   'our',\n",
       "   'approach',\n",
       "   'on',\n",
       "   'Twitter',\n",
       "   'by',\n",
       "   'examining',\n",
       "   'various',\n",
       "   'execution',\n",
       "   'modes',\n",
       "   '\\n',\n",
       "   'and',\n",
       "   'contexts',\n",
       "   '.',\n",
       "   'Our',\n",
       "   'findings',\n",
       "   'demonstrate',\n",
       "   'the',\n",
       "   'effectiveness',\n",
       "   'of',\n",
       "   'the',\n",
       "   'proposed',\n",
       "   'methodology',\n",
       "   'in',\n",
       "   'identifying',\n",
       "   'misinformation',\n",
       "   'websites',\n",
       "   ',',\n",
       "   'which',\n",
       "   '\\n',\n",
       "   'can',\n",
       "   'aid',\n",
       "   'in',\n",
       "   'gaining',\n",
       "   'a',\n",
       "   'better',\n",
       "   'understanding',\n",
       "   'of',\n",
       "   'this',\n",
       "   'phenomenon',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'enabling',\n",
       "   'competent',\n",
       "   'entities',\n",
       "   'to',\n",
       "   'tackle',\n",
       "   'the',\n",
       "   'problem',\n",
       "   'in',\n",
       "   'various',\n",
       "   'areas',\n",
       "   '\\n',\n",
       "   'of',\n",
       "   'society',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'KEYWORDS',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Fake',\n",
       "   'News',\n",
       "   ',',\n",
       "   'Misinformation',\n",
       "   ',',\n",
       "   'Credibility',\n",
       "   ',',\n",
       "   'Websites',\n",
       "   ',',\n",
       "   'Social',\n",
       "   'Media',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'Twitter',\n",
       "   ',',\n",
       "   'X',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   '1',\n",
       "   'INTRODUCTION',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   'recent',\n",
       "   'times',\n",
       "   ',',\n",
       "   'society',\n",
       "   'has',\n",
       "   'been',\n",
       "   'faced',\n",
       "   'with',\n",
       "   'an',\n",
       "   'unprecedented',\n",
       "   '\\n',\n",
       "   'scale',\n",
       "   'of',\n",
       "   'misinformation',\n",
       "   'campaigns',\n",
       "   ',',\n",
       "   'covering',\n",
       "   'highly',\n",
       "   'sensitive',\n",
       "   'topics',\n",
       "   'including',\n",
       "   'vaccines',\n",
       "   '[',\n",
       "   '15',\n",
       "   ']',\n",
       "   ',',\n",
       "   'climate',\n",
       "   'change',\n",
       "   '[',\n",
       "   '25',\n",
       "   ']',\n",
       "   ',',\n",
       "   'scientific',\n",
       "   'information',\n",
       "   '[',\n",
       "   '28',\n",
       "   ']',\n",
       "   ',',\n",
       "   'and',\n",
       "   'politics',\n",
       "   '[',\n",
       "   '18',\n",
       "   ']',\n",
       "   '.',\n",
       "   'The',\n",
       "   'negative',\n",
       "   'effects',\n",
       "   'of',\n",
       "   'misinformation',\n",
       "   '\\n',\n",
       "   'campaigns',\n",
       "   'are',\n",
       "   'numerous',\n",
       "   ',',\n",
       "   'as',\n",
       "   'they',\n",
       "   'undermine',\n",
       "   'the',\n",
       "   'key',\n",
       "   'processes',\n",
       "   'used',\n",
       "   '\\n',\n",
       "   'to',\n",
       "   'acquire',\n",
       "   'and',\n",
       "   'share',\n",
       "   'information',\n",
       "   ',',\n",
       "   'posing',\n",
       "   'a',\n",
       "   'significant',\n",
       "   'challenge',\n",
       "   'for',\n",
       "   '\\n',\n",
       "   'society',\n",
       "   'as',\n",
       "   'a',\n",
       "   'whole',\n",
       "   '[',\n",
       "   '19',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Addressing',\n",
       "   'this',\n",
       "   'issue',\n",
       "   'has',\n",
       "   'become',\n",
       "   'part',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'our',\n",
       "   'daily',\n",
       "   'lives',\n",
       "   ',',\n",
       "   'and',\n",
       "   'must',\n",
       "   'be',\n",
       "   'tackled',\n",
       "   '.',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   ':',\n",
       "   'Proceedings',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Web',\n",
       "   '(',\n",
       "   'WebMe',\n",
       "   '\\n',\n",
       "   'dia’2024',\n",
       "   ')',\n",
       "   '.',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '.',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   ':',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   ',',\n",
       "   '2024',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'SBC',\n",
       "   '–',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '-',\n",
       "   '2753',\n",
       "   '\\n\\n\\n',\n",
       "   'In',\n",
       "   'the',\n",
       "   'fight',\n",
       "   'against',\n",
       "   'misinformation',\n",
       "   ',',\n",
       "   'the',\n",
       "   'complexity',\n",
       "   'of',\n",
       "   'the',\n",
       "   'problem',\n",
       "   'has',\n",
       "   'emerged',\n",
       "   'as',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'greatest',\n",
       "   'challenges',\n",
       "   'faced',\n",
       "   'by',\n",
       "   'modern',\n",
       "   '\\n',\n",
       "   'society',\n",
       "   '.',\n",
       "   'The',\n",
       "   'issue',\n",
       "   'manifests',\n",
       "   'itself',\n",
       "   'in',\n",
       "   'any',\n",
       "   'digital',\n",
       "   'platform',\n",
       "   'where',\n",
       "   '\\n',\n",
       "   'users',\n",
       "   'consume',\n",
       "   'or',\n",
       "   'exchange',\n",
       "   'information',\n",
       "   ',',\n",
       "   'including',\n",
       "   'video',\n",
       "   'platforms',\n",
       "   '\\n\\n',\n",
       "   '[',\n",
       "   '13',\n",
       "   ']',\n",
       "   ',',\n",
       "   'social',\n",
       "   'networks',\n",
       "   '[',\n",
       "   '3',\n",
       "   ',',\n",
       "   '10',\n",
       "   ',',\n",
       "   '11',\n",
       "   ']',\n",
       "   ',',\n",
       "   'messaging',\n",
       "   'applications',\n",
       "   '[',\n",
       "   '12',\n",
       "   ',',\n",
       "   '17',\n",
       "   ',',\n",
       "   '20',\n",
       "   ']',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'dedicated',\n",
       "   'websites',\n",
       "   ',',\n",
       "   'blogs',\n",
       "   ',',\n",
       "   'and',\n",
       "   'forums',\n",
       "   '[',\n",
       "   '23',\n",
       "   ']',\n",
       "   '.',\n",
       "   'The',\n",
       "   'complexity',\n",
       "   'of',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'misinformation',\n",
       "   'ecosystem',\n",
       "   'is',\n",
       "   'further',\n",
       "   'compounded',\n",
       "   'by',\n",
       "   'content',\n",
       "   'recommendation',\n",
       "   'algorithms',\n",
       "   'employed',\n",
       "   'by',\n",
       "   'many',\n",
       "   'of',\n",
       "   'these',\n",
       "   'platforms',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'which',\n",
       "   'often',\n",
       "   'prioritize',\n",
       "   'user',\n",
       "   'engagement',\n",
       "   'over',\n",
       "   'the',\n",
       "   'accuracy',\n",
       "   'of',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'information',\n",
       "   'presented',\n",
       "   '[',\n",
       "   '14',\n",
       "   ']',\n",
       "   '.',\n",
       "   'These',\n",
       "   'factors',\n",
       "   'can',\n",
       "   'give',\n",
       "   'rise',\n",
       "   'to',\n",
       "   'echo',\n",
       "   '\\n',\n",
       "   'chambers',\n",
       "   ',',\n",
       "   'leading',\n",
       "   'to',\n",
       "   'polarization',\n",
       "   '[',\n",
       "   '9',\n",
       "   ']',\n",
       "   'and',\n",
       "   'even',\n",
       "   'the',\n",
       "   'radicalization',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'users',\n",
       "   '[',\n",
       "   '22',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Additionally',\n",
       "   ',',\n",
       "   'social',\n",
       "   'platforms',\n",
       "   'allow',\n",
       "   'advertisers',\n",
       "   'to',\n",
       "   'target',\n",
       "   '\\n',\n",
       "   'users',\n",
       "   'based',\n",
       "   'on',\n",
       "   'detailed',\n",
       "   'behavioral',\n",
       "   'information',\n",
       "   ',',\n",
       "   'allowing',\n",
       "   'misinformation',\n",
       "   'campaigns',\n",
       "   'to',\n",
       "   'target',\n",
       "   'specific',\n",
       "   'and',\n",
       "   'sometimes',\n",
       "   'vulnerable',\n",
       "   '\\n',\n",
       "   'segments',\n",
       "   'of',\n",
       "   'a',\n",
       "   'population',\n",
       "   '[',\n",
       "   '21',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'One',\n",
       "   'of',\n",
       "   'the',\n",
       "   'key',\n",
       "   'features',\n",
       "   'of',\n",
       "   'this',\n",
       "   'intricate',\n",
       "   'ecosystem',\n",
       "   'is',\n",
       "   'the',\n",
       "   'utilization',\n",
       "   'of',\n",
       "   'websites',\n",
       "   'dedicated',\n",
       "   'to',\n",
       "   'the',\n",
       "   'production',\n",
       "   'and',\n",
       "   'dissemination',\n",
       "   '\\n',\n",
       "   'of',\n",
       "   'fabricated',\n",
       "   'news',\n",
       "   'content',\n",
       "   '[',\n",
       "   '5',\n",
       "   ',',\n",
       "   '6',\n",
       "   ']',\n",
       "   '.',\n",
       "   'These',\n",
       "   'sites',\n",
       "   'meticulously',\n",
       "   'mimic',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'appearance',\n",
       "   'and',\n",
       "   'function',\n",
       "   'of',\n",
       "   'conventional',\n",
       "   'and',\n",
       "   'dependable',\n",
       "   'news',\n",
       "   '\\n',\n",
       "   'outlets',\n",
       "   '.',\n",
       "   'When',\n",
       "   'intertwined',\n",
       "   'with',\n",
       "   'misinformation',\n",
       "   'campaigns',\n",
       "   ',',\n",
       "   'they',\n",
       "   '\\n',\n",
       "   'frequently',\n",
       "   'attempt',\n",
       "   'to',\n",
       "   'manipulate',\n",
       "   'public',\n",
       "   'opinion',\n",
       "   ',',\n",
       "   'propagating',\n",
       "   'widespread',\n",
       "   'suspicion',\n",
       "   'and',\n",
       "   'distrust',\n",
       "   'of',\n",
       "   'credible',\n",
       "   'news',\n",
       "   'sources',\n",
       "   '.',\n",
       "   'By',\n",
       "   'positioning',\n",
       "   'themselves',\n",
       "   'as',\n",
       "   'alternative',\n",
       "   ',',\n",
       "   'and',\n",
       "   'claiming',\n",
       "   'to',\n",
       "   'be',\n",
       "   'more',\n",
       "   'trustworthy',\n",
       "   'information',\n",
       "   'sources',\n",
       "   ',',\n",
       "   'they',\n",
       "   'contribute',\n",
       "   'to',\n",
       "   'the',\n",
       "   'creation',\n",
       "   'of',\n",
       "   'an',\n",
       "   '\\n',\n",
       "   'alternative',\n",
       "   'reality',\n",
       "   'where',\n",
       "   'a',\n",
       "   'specific',\n",
       "   'narrative',\n",
       "   'and',\n",
       "   'world',\n",
       "   'view',\n",
       "   'go',\n",
       "   '\\n',\n",
       "   'unchallenged',\n",
       "   '.',\n",
       "   'With',\n",
       "   'such',\n",
       "   'a',\n",
       "   'strategy',\n",
       "   ',',\n",
       "   'misinformation',\n",
       "   'campaigns',\n",
       "   'can',\n",
       "   '\\n',\n",
       "   'effectively',\n",
       "   'influence',\n",
       "   'increasingly',\n",
       "   'radicalized',\n",
       "   'segments',\n",
       "   'of',\n",
       "   'society',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'serving',\n",
       "   'the',\n",
       "   'interests',\n",
       "   'of',\n",
       "   'specific',\n",
       "   'political',\n",
       "   'entities',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Identifying',\n",
       "   'these',\n",
       "   'websites',\n",
       "   'and',\n",
       "   'distinguishing',\n",
       "   'them',\n",
       "   'from',\n",
       "   'their',\n",
       "   '\\n',\n",
       "   'credible',\n",
       "   'counterparts',\n",
       "   'poses',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'most',\n",
       "   'daunting',\n",
       "   'challenges',\n",
       "   '\\n',\n",
       "   'to',\n",
       "   'the',\n",
       "   'misinformation',\n",
       "   'research',\n",
       "   'community',\n",
       "   '.',\n",
       "   'Despite',\n",
       "   'its',\n",
       "   'undeniable',\n",
       "   'significance',\n",
       "   ',',\n",
       "   'obtaining',\n",
       "   'lists',\n",
       "   'of',\n",
       "   'websites',\n",
       "   'that',\n",
       "   'are',\n",
       "   'identified',\n",
       "   'as',\n",
       "   '\\n',\n",
       "   'fake',\n",
       "   'news',\n",
       "   'sites',\n",
       "   ',',\n",
       "   'particularly',\n",
       "   'for',\n",
       "   'tackling',\n",
       "   'misinformation',\n",
       "   'in',\n",
       "   'specific',\n",
       "   'countries',\n",
       "   'like',\n",
       "   'Brazil',\n",
       "   ',',\n",
       "   'is',\n",
       "   'far',\n",
       "   'from',\n",
       "   'a',\n",
       "   'trivial',\n",
       "   'task',\n",
       "   '.',\n",
       "   'This',\n",
       "   'challenge',\n",
       "   '\\n',\n",
       "   'is',\n",
       "   'partially',\n",
       "   'explained',\n",
       "   'by',\n",
       "   'the',\n",
       "   'fact',\n",
       "   'that',\n",
       "   'misinformation',\n",
       "   'campaigns',\n",
       "   '\\n',\n",
       "   'are',\n",
       "   'often',\n",
       "   'orchestrated',\n",
       "   'and',\n",
       "   'supported',\n",
       "   'by',\n",
       "   'organizations',\n",
       "   'with',\n",
       "   'welldefined',\n",
       "   'objectives',\n",
       "   '.',\n",
       "   'Those',\n",
       "   'who',\n",
       "   'propose',\n",
       "   'to',\n",
       "   'publish',\n",
       "   'such',\n",
       "   'lists',\n",
       "   'are',\n",
       "   '\\n\\n\\n',\n",
       "   '171',\n",
       "   '\\n\\n\\n',\n",
       "   '-----',\n",
       "   '\\n\\n',\n",
       "   'WebMedia’2024',\n",
       "   ',',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   'Araujo',\n",
       "   'et',\n",
       "   'al',\n",
       "   '.',\n",
       "   '\\n\\n\\n',\n",
       "   'vulnerable',\n",
       "   'to',\n",
       "   'intimidation',\n",
       "   ',',\n",
       "   'whether',\n",
       "   'by',\n",
       "   'digital',\n",
       "   'militias',\n",
       "   '[',\n",
       "   '1',\n",
       "   ']',\n",
       "   'or',\n",
       "   'through',\n",
       "   '\\n',\n",
       "   'legal',\n",
       "   'harassment',\n",
       "   ',',\n",
       "   'involving',\n",
       "   'vexatious',\n",
       "   'litigation',\n",
       "   'and',\n",
       "   'other',\n",
       "   'forms',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'costly',\n",
       "   'legal',\n",
       "   'action',\n",
       "   '[',\n",
       "   '2',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'This',\n",
       "   'study',\n",
       "   ...],\n",
       "  'pos_tagger': '',\n",
       "  'lema': ['find',\n",
       "   'fake',\n",
       "   'News',\n",
       "   'Websites',\n",
       "   'in',\n",
       "   'the',\n",
       "   'Wild',\n",
       "   'Leandro',\n",
       "   'Araujo',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'Belo',\n",
       "   'Horizonte',\n",
       "   'Brazil',\n",
       "   'leandroaraujo@dcc.ufmg.br',\n",
       "   'Isadora',\n",
       "   'rodrigue',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'Belo',\n",
       "   'Horizonte',\n",
       "   'Brazil',\n",
       "   'isadorarodrigues@dcc.ufmg.br',\n",
       "   'ABSTRACT',\n",
       "   'João',\n",
       "   'M.',\n",
       "   'M.',\n",
       "   'Couto',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'Belo',\n",
       "   'Horizonte',\n",
       "   'Brazil',\n",
       "   'joaocouto@dcc.ufmg.br',\n",
       "   'Jussara',\n",
       "   'M.',\n",
       "   'Almeida',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'Belo',\n",
       "   'Horizonte',\n",
       "   'Brazil',\n",
       "   'jussara@dcc.ufmg.br',\n",
       "   'Fabricio',\n",
       "   'Benevenuto',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'Belo',\n",
       "   'Horizonte',\n",
       "   'Brazil',\n",
       "   'fabricio@dcc.ufmg.br',\n",
       "   'Luiz',\n",
       "   'Felipe',\n",
       "   'nery',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'Belo',\n",
       "   'Horizonte',\n",
       "   'Brazil',\n",
       "   'luiznery@dcc.ufmg.br',\n",
       "   'Julio',\n",
       "   'C.',\n",
       "   'S.',\n",
       "   'Reis',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Viçosa',\n",
       "   'Viçosa',\n",
       "   'Brazil',\n",
       "   'jreis@ufv.br',\n",
       "   'the',\n",
       "   'battle',\n",
       "   'against',\n",
       "   'the',\n",
       "   'spread',\n",
       "   'of',\n",
       "   'misinformation',\n",
       "   'on',\n",
       "   'the',\n",
       "   'internet',\n",
       "   'be',\n",
       "   'a',\n",
       "   'daunting',\n",
       "   'task',\n",
       "   'face',\n",
       "   'by',\n",
       "   'modern',\n",
       "   'society',\n",
       "   'fake',\n",
       "   'news',\n",
       "   'content',\n",
       "   'be',\n",
       "   'primarily',\n",
       "   'distribute',\n",
       "   'through',\n",
       "   'digital',\n",
       "   'platform',\n",
       "   'with',\n",
       "   'website',\n",
       "   'dedicate',\n",
       "   'to',\n",
       "   'produce',\n",
       "   'and',\n",
       "   'disseminate',\n",
       "   'such',\n",
       "   'content',\n",
       "   'play',\n",
       "   'a',\n",
       "   'pivotal',\n",
       "   'role',\n",
       "   'in',\n",
       "   'this',\n",
       "   'complex',\n",
       "   'ecosystem',\n",
       "   'therefore',\n",
       "   'these',\n",
       "   'website',\n",
       "   'be',\n",
       "   'of',\n",
       "   'great',\n",
       "   'interest',\n",
       "   'to',\n",
       "   'misinformation',\n",
       "   'researcher',\n",
       "   'however',\n",
       "   'obtain',\n",
       "   'a',\n",
       "   'comprehensive',\n",
       "   'list',\n",
       "   'of',\n",
       "   'website',\n",
       "   'label',\n",
       "   'as',\n",
       "   'producer',\n",
       "   'and/or',\n",
       "   'spreader',\n",
       "   'of',\n",
       "   'misinformation',\n",
       "   'can',\n",
       "   'be',\n",
       "   'challenge',\n",
       "   'particularly',\n",
       "   'in',\n",
       "   'develop',\n",
       "   'country',\n",
       "   'in',\n",
       "   'this',\n",
       "   'study',\n",
       "   'we',\n",
       "   'propose',\n",
       "   'a',\n",
       "   'novel',\n",
       "   'methodology',\n",
       "   'for',\n",
       "   'identify',\n",
       "   'website',\n",
       "   'responsible',\n",
       "   'for',\n",
       "   'create',\n",
       "   'and',\n",
       "   'disseminate',\n",
       "   'misinformation',\n",
       "   'content',\n",
       "   'which',\n",
       "   'be',\n",
       "   'closely',\n",
       "   'link',\n",
       "   'to',\n",
       "   'user',\n",
       "   'who',\n",
       "   'share',\n",
       "   'confirm',\n",
       "   'instance',\n",
       "   'of',\n",
       "   'fake',\n",
       "   'news',\n",
       "   'on',\n",
       "   'social',\n",
       "   'medium',\n",
       "   'we',\n",
       "   'validate',\n",
       "   'our',\n",
       "   'approach',\n",
       "   'on',\n",
       "   'Twitter',\n",
       "   'by',\n",
       "   'examine',\n",
       "   'various',\n",
       "   'execution',\n",
       "   'mode',\n",
       "   'and',\n",
       "   'contexts',\n",
       "   'our',\n",
       "   'finding',\n",
       "   'demonstrate',\n",
       "   'the',\n",
       "   'effectiveness',\n",
       "   'of',\n",
       "   'the',\n",
       "   'propose',\n",
       "   'methodology',\n",
       "   'in',\n",
       "   'identify',\n",
       "   'misinformation',\n",
       "   'website',\n",
       "   'which',\n",
       "   'can',\n",
       "   'aid',\n",
       "   'in',\n",
       "   'gain',\n",
       "   'a',\n",
       "   'well',\n",
       "   'understanding',\n",
       "   'of',\n",
       "   'this',\n",
       "   'phenomenon',\n",
       "   'and',\n",
       "   'enable',\n",
       "   'competent',\n",
       "   'entity',\n",
       "   'to',\n",
       "   'tackle',\n",
       "   'the',\n",
       "   'problem',\n",
       "   'in',\n",
       "   'various',\n",
       "   'area',\n",
       "   'of',\n",
       "   'society',\n",
       "   'keyword',\n",
       "   'Fake',\n",
       "   'News',\n",
       "   'Misinformation',\n",
       "   'Credibility',\n",
       "   'Websites',\n",
       "   'Social',\n",
       "   'Media',\n",
       "   'Twitter',\n",
       "   'x',\n",
       "   '1',\n",
       "   'introduction',\n",
       "   'in',\n",
       "   'recent',\n",
       "   'time',\n",
       "   'society',\n",
       "   'have',\n",
       "   'be',\n",
       "   'face',\n",
       "   'with',\n",
       "   'an',\n",
       "   'unprecedented',\n",
       "   'scale',\n",
       "   'of',\n",
       "   'misinformation',\n",
       "   'campaign',\n",
       "   'cover',\n",
       "   'highly',\n",
       "   'sensitive',\n",
       "   'topic',\n",
       "   'include',\n",
       "   'vaccine',\n",
       "   '15',\n",
       "   'climate',\n",
       "   'change',\n",
       "   '25',\n",
       "   'scientific',\n",
       "   'information',\n",
       "   '28',\n",
       "   'and',\n",
       "   'politic',\n",
       "   '18',\n",
       "   'the',\n",
       "   'negative',\n",
       "   'effect',\n",
       "   'of',\n",
       "   'misinformation',\n",
       "   'campaign',\n",
       "   'be',\n",
       "   'numerous',\n",
       "   'as',\n",
       "   'they',\n",
       "   'undermine',\n",
       "   'the',\n",
       "   'key',\n",
       "   'process',\n",
       "   'use',\n",
       "   'to',\n",
       "   'acquire',\n",
       "   'and',\n",
       "   'share',\n",
       "   'information',\n",
       "   'pose',\n",
       "   'a',\n",
       "   'significant',\n",
       "   'challenge',\n",
       "   'for',\n",
       "   'society',\n",
       "   'as',\n",
       "   'a',\n",
       "   'whole',\n",
       "   '19',\n",
       "   'address',\n",
       "   'this',\n",
       "   'issue',\n",
       "   'have',\n",
       "   'become',\n",
       "   'part',\n",
       "   'of',\n",
       "   'our',\n",
       "   'daily',\n",
       "   'life',\n",
       "   'and',\n",
       "   'must',\n",
       "   'be',\n",
       "   'tackle',\n",
       "   'in',\n",
       "   'proceeding',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'web',\n",
       "   'WebMe',\n",
       "   'dia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   '2024',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'sbc',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '2753',\n",
       "   'in',\n",
       "   'the',\n",
       "   'fight',\n",
       "   'against',\n",
       "   'misinformation',\n",
       "   'the',\n",
       "   'complexity',\n",
       "   'of',\n",
       "   'the',\n",
       "   'problem',\n",
       "   'have',\n",
       "   'emerge',\n",
       "   'as',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'great',\n",
       "   'challenge',\n",
       "   'face',\n",
       "   'by',\n",
       "   'modern',\n",
       "   'society',\n",
       "   'the',\n",
       "   'issue',\n",
       "   'manifest',\n",
       "   'itself',\n",
       "   'in',\n",
       "   'any',\n",
       "   'digital',\n",
       "   'platform',\n",
       "   'where',\n",
       "   'user',\n",
       "   'consume',\n",
       "   'or',\n",
       "   'exchange',\n",
       "   'information',\n",
       "   'include',\n",
       "   'video',\n",
       "   'platform',\n",
       "   '13',\n",
       "   'social',\n",
       "   'network',\n",
       "   '3',\n",
       "   '10',\n",
       "   '11',\n",
       "   'message',\n",
       "   'application',\n",
       "   '12',\n",
       "   '17',\n",
       "   '20',\n",
       "   'dedicated',\n",
       "   'website',\n",
       "   'blog',\n",
       "   'and',\n",
       "   'forum',\n",
       "   '23',\n",
       "   'the',\n",
       "   'complexity',\n",
       "   'of',\n",
       "   'the',\n",
       "   'misinformation',\n",
       "   'ecosystem',\n",
       "   'be',\n",
       "   'far',\n",
       "   'compound',\n",
       "   'by',\n",
       "   'content',\n",
       "   'recommendation',\n",
       "   'algorithm',\n",
       "   'employ',\n",
       "   'by',\n",
       "   'many',\n",
       "   'of',\n",
       "   'these',\n",
       "   'platform',\n",
       "   'which',\n",
       "   'often',\n",
       "   'prioritize',\n",
       "   'user',\n",
       "   'engagement',\n",
       "   'over',\n",
       "   'the',\n",
       "   'accuracy',\n",
       "   'of',\n",
       "   'the',\n",
       "   'information',\n",
       "   'present',\n",
       "   '14',\n",
       "   'these',\n",
       "   'factor',\n",
       "   'can',\n",
       "   'give',\n",
       "   'rise',\n",
       "   'to',\n",
       "   'echo',\n",
       "   'chamber',\n",
       "   'lead',\n",
       "   'to',\n",
       "   'polarization',\n",
       "   '9',\n",
       "   'and',\n",
       "   'even',\n",
       "   'the',\n",
       "   'radicalization',\n",
       "   'of',\n",
       "   'user',\n",
       "   '22',\n",
       "   'additionally',\n",
       "   'social',\n",
       "   'platform',\n",
       "   'allow',\n",
       "   'advertiser',\n",
       "   'to',\n",
       "   'target',\n",
       "   'user',\n",
       "   'base',\n",
       "   'on',\n",
       "   'detailed',\n",
       "   'behavioral',\n",
       "   'information',\n",
       "   'allow',\n",
       "   'misinformation',\n",
       "   'campaign',\n",
       "   'to',\n",
       "   'target',\n",
       "   'specific',\n",
       "   'and',\n",
       "   'sometimes',\n",
       "   'vulnerable',\n",
       "   'segment',\n",
       "   'of',\n",
       "   'a',\n",
       "   'population',\n",
       "   '21',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'key',\n",
       "   'feature',\n",
       "   'of',\n",
       "   'this',\n",
       "   'intricate',\n",
       "   'ecosystem',\n",
       "   'be',\n",
       "   'the',\n",
       "   'utilization',\n",
       "   'of',\n",
       "   'website',\n",
       "   'dedicate',\n",
       "   'to',\n",
       "   'the',\n",
       "   'production',\n",
       "   'and',\n",
       "   'dissemination',\n",
       "   'of',\n",
       "   'fabricate',\n",
       "   'news',\n",
       "   'content',\n",
       "   '5',\n",
       "   '6',\n",
       "   'these',\n",
       "   'site',\n",
       "   'meticulously',\n",
       "   'mimic',\n",
       "   'the',\n",
       "   'appearance',\n",
       "   'and',\n",
       "   'function',\n",
       "   'of',\n",
       "   'conventional',\n",
       "   'and',\n",
       "   'dependable',\n",
       "   'news',\n",
       "   'outlet',\n",
       "   'when',\n",
       "   'intertwine',\n",
       "   'with',\n",
       "   'misinformation',\n",
       "   'campaign',\n",
       "   'they',\n",
       "   'frequently',\n",
       "   'attempt',\n",
       "   'to',\n",
       "   'manipulate',\n",
       "   'public',\n",
       "   'opinion',\n",
       "   'propagate',\n",
       "   'widespread',\n",
       "   'suspicion',\n",
       "   'and',\n",
       "   'distrust',\n",
       "   'of',\n",
       "   'credible',\n",
       "   'news',\n",
       "   'source',\n",
       "   'by',\n",
       "   'position',\n",
       "   'themselves',\n",
       "   'as',\n",
       "   'alternative',\n",
       "   'and',\n",
       "   'claim',\n",
       "   'to',\n",
       "   'be',\n",
       "   'more',\n",
       "   'trustworthy',\n",
       "   'information',\n",
       "   'source',\n",
       "   'they',\n",
       "   'contribute',\n",
       "   'to',\n",
       "   'the',\n",
       "   'creation',\n",
       "   'of',\n",
       "   'an',\n",
       "   'alternative',\n",
       "   'reality',\n",
       "   'where',\n",
       "   'a',\n",
       "   'specific',\n",
       "   'narrative',\n",
       "   'and',\n",
       "   'world',\n",
       "   'view',\n",
       "   'go',\n",
       "   'unchallenged',\n",
       "   'with',\n",
       "   'such',\n",
       "   'a',\n",
       "   'strategy',\n",
       "   'misinformation',\n",
       "   'campaign',\n",
       "   'can',\n",
       "   'effectively',\n",
       "   'influence',\n",
       "   'increasingly',\n",
       "   'radicalize',\n",
       "   'segment',\n",
       "   'of',\n",
       "   'society',\n",
       "   'serve',\n",
       "   'the',\n",
       "   'interest',\n",
       "   'of',\n",
       "   'specific',\n",
       "   'political',\n",
       "   'entity',\n",
       "   'identify',\n",
       "   'these',\n",
       "   'website',\n",
       "   'and',\n",
       "   'distinguish',\n",
       "   'they',\n",
       "   'from',\n",
       "   'their',\n",
       "   'credible',\n",
       "   'counterpart',\n",
       "   'pose',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'most',\n",
       "   'daunting',\n",
       "   'challenge',\n",
       "   'to',\n",
       "   'the',\n",
       "   'misinformation',\n",
       "   'research',\n",
       "   'community',\n",
       "   'despite',\n",
       "   'its',\n",
       "   'undeniable',\n",
       "   'significance',\n",
       "   'obtain',\n",
       "   'list',\n",
       "   'of',\n",
       "   'website',\n",
       "   'that',\n",
       "   'be',\n",
       "   'identify',\n",
       "   'as',\n",
       "   'fake',\n",
       "   'news',\n",
       "   'site',\n",
       "   'particularly',\n",
       "   'for',\n",
       "   'tackle',\n",
       "   'misinformation',\n",
       "   'in',\n",
       "   'specific',\n",
       "   'country',\n",
       "   'like',\n",
       "   'Brazil',\n",
       "   'be',\n",
       "   'far',\n",
       "   'from',\n",
       "   'a',\n",
       "   'trivial',\n",
       "   'task',\n",
       "   'this',\n",
       "   'challenge',\n",
       "   'be',\n",
       "   'partially',\n",
       "   'explain',\n",
       "   'by',\n",
       "   'the',\n",
       "   'fact',\n",
       "   'that',\n",
       "   'misinformation',\n",
       "   'campaign',\n",
       "   'be',\n",
       "   'often',\n",
       "   'orchestrate',\n",
       "   'and',\n",
       "   'support',\n",
       "   'by',\n",
       "   'organization',\n",
       "   'with',\n",
       "   'welldefine',\n",
       "   'objective',\n",
       "   'those',\n",
       "   'who',\n",
       "   'propose',\n",
       "   'to',\n",
       "   'publish',\n",
       "   'such',\n",
       "   'list',\n",
       "   'be',\n",
       "   '171',\n",
       "   'WebMedia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Araujo',\n",
       "   'et',\n",
       "   'al',\n",
       "   'vulnerable',\n",
       "   'to',\n",
       "   'intimidation',\n",
       "   'whether',\n",
       "   'by',\n",
       "   'digital',\n",
       "   'militia',\n",
       "   '1',\n",
       "   'or',\n",
       "   'through',\n",
       "   'legal',\n",
       "   'harassment',\n",
       "   'involve',\n",
       "   'vexatious',\n",
       "   'litigation',\n",
       "   'and',\n",
       "   'other',\n",
       "   'form',\n",
       "   'of',\n",
       "   'costly',\n",
       "   'legal',\n",
       "   'action',\n",
       "   '2',\n",
       "   'this',\n",
       "   'study',\n",
       "   'propose',\n",
       "   'a',\n",
       "   'novel',\n",
       "   'approach',\n",
       "   'to',\n",
       "   'detect',\n",
       "   'fake',\n",
       "   'news',\n",
       "   'website',\n",
       "   'by',\n",
       "   'leverage',\n",
       "   'user',\n",
       "   'behavior',\n",
       "   'rather',\n",
       "   'than',\n",
       "   'rely',\n",
       "   'solely',\n",
       "   'on',\n",
       "   'website',\n",
       "   'characteristic',\n",
       "   'specifically',\n",
       "   'we',\n",
       "   'hypothesize',\n",
       "   'that',\n",
       "   'user',\n",
       "   'who',\n",
       "   'share',\n",
       "   'instance',\n",
       "   'of',\n",
       "   'fake',\n",
       "   'news',\n",
       "   'be',\n",
       "   'likely',\n",
       "   'to',\n",
       "   'have',\n",
       "   'share',\n",
       "   'additional',\n",
       "   'one',\n",
       "   'our',\n",
       "   'methodology',\n",
       "   'identify',\n",
       "   'such',\n",
       "   'user',\n",
       "   'rank',\n",
       "   'additional',\n",
       "   'website',\n",
       "   'share',\n",
       "   'by',\n",
       "   'they',\n",
       "   'base',\n",
       "   'on',\n",
       "   'a',\n",
       "   'specific',\n",
       "   'criterion',\n",
       "   'and',\n",
       "   'expand',\n",
       "   'the',\n",
       "   'search',\n",
       "   'use',\n",
       "   'article',\n",
       "   'pertain',\n",
       "   'to',\n",
       "   'the',\n",
       "   'newly',\n",
       "   'identify',\n",
       "   'website',\n",
       "   'to',\n",
       "   'validate',\n",
       "   'our',\n",
       "   'approach',\n",
       "   'we',\n",
       "   'apply',\n",
       "   'it',\n",
       "   'to',\n",
       "   'Twitter',\n",
       "   'currently',\n",
       "   'reference',\n",
       "   'as',\n",
       "   'x',\n",
       "   'and',\n",
       "   'compare',\n",
       "   'our',\n",
       "   'finding',\n",
       "   'with',\n",
       "   'a',\n",
       "   'curate',\n",
       "   'list',\n",
       "   'of',\n",
       "   'low',\n",
       "   'credibility',\n",
       "   'website',\n",
       "   'publish',\n",
       "   'by',\n",
       "   'an',\n",
       "   'establish',\n",
       "   'American',\n",
       "   'fact',\n",
       "   'check',\n",
       "   'website',\n",
       "   'we',\n",
       "   'far',\n",
       "   'apply',\n",
       "   'our',\n",
       "   'methodology',\n",
       "   'to',\n",
       "   'the',\n",
       "   'brazilian',\n",
       "   'misinformation',\n",
       "   'ecosystem',\n",
       "   'where',\n",
       "   'we',\n",
       "   'identify',\n",
       "   'numerous',\n",
       "   'previously',\n",
       "   'unknown',\n",
       "   'fake',\n",
       "   'news',\n",
       "   'website',\n",
       "   'our',\n",
       "   'result',\n",
       "   'demonstrate',\n",
       "   'that',\n",
       "   'our',\n",
       "   'approach',\n",
       "   'perform',\n",
       "   'good',\n",
       "   'when',\n",
       "   'use',\n",
       "   'a',\n",
       "   'sort',\n",
       "   'criterion',\n",
       "   'that',\n",
       "   'account',\n",
       "   'for',\n",
       "   'both',\n",
       "   'website',\n",
       "   'impact',\n",
       "   'and',\n",
       "   'productivity',\n",
       "   'within',\n",
       "   'the',\n",
       "   'relevant',\n",
       "   'misinformation',\n",
       "   'ecosystem',\n",
       "   'and',\n",
       "   'when',\n",
       "   'initiate',\n",
       "   'with',\n",
       "   'a',\n",
       "   'fake',\n",
       "   'news',\n",
       "   'url',\n",
       "   'check',\n",
       "   'by',\n",
       "   'a',\n",
       "   'recognize',\n",
       "   'fact',\n",
       "   'check',\n",
       "   'entity',\n",
       "   'such',\n",
       "   'as',\n",
       "   'the',\n",
       "   'International',\n",
       "   'Fact',\n",
       "   'Checking',\n",
       "   'Network',\n",
       "   'IFCN',\n",
       "   '3',\n",
       "   'moreover',\n",
       "   'our',\n",
       "   'study',\n",
       "   'show',\n",
       "   'that',\n",
       "   'user',\n",
       "   'identify',\n",
       "   'through',\n",
       "   'our',\n",
       "   'methodology',\n",
       "   'be',\n",
       "   'indeed',\n",
       "   'more',\n",
       "   'likely',\n",
       "   'to',\n",
       "   'post',\n",
       "   'instance',\n",
       "   'of',\n",
       "   'fake',\n",
       "   'news',\n",
       "   'thereby',\n",
       "   'reduce',\n",
       "   'the',\n",
       "   'need',\n",
       "   'for',\n",
       "   'manual',\n",
       "   'evaluation',\n",
       "   'per',\n",
       "   'identification',\n",
       "   'of',\n",
       "   'a',\n",
       "   'low',\n",
       "   'credibility',\n",
       "   'portal',\n",
       "   'we',\n",
       "   'anticipate',\n",
       "   'that',\n",
       "   'our',\n",
       "   'result',\n",
       "   'will',\n",
       "   'contribute',\n",
       "   'to',\n",
       "   'a',\n",
       "   'well',\n",
       "   'understanding',\n",
       "   'of',\n",
       "   'this',\n",
       "   'phenomenon',\n",
       "   'and',\n",
       "   'help',\n",
       "   'competent',\n",
       "   'entity',\n",
       "   'to',\n",
       "   'address',\n",
       "   'the',\n",
       "   'problem',\n",
       "   'in',\n",
       "   'various',\n",
       "   'sphere',\n",
       "   'of',\n",
       "   'our',\n",
       "   'society',\n",
       "   'the',\n",
       "   'remainder',\n",
       "   'of',\n",
       "   'this',\n",
       "   'paper',\n",
       "   'be',\n",
       "   'structure',\n",
       "   'as',\n",
       "   'follow',\n",
       "   'in',\n",
       "   'section',\n",
       "   '2',\n",
       "   'we',\n",
       "   'provide',\n",
       "   'a',\n",
       "   'brief',\n",
       "   'overview',\n",
       "   'of',\n",
       "   'previous',\n",
       "   'approach',\n",
       "   'take',\n",
       "   'to',\n",
       "   'address',\n",
       "   'the',\n",
       "   'issue',\n",
       "   'of',\n",
       "   'identify',\n",
       "   'low',\n",
       "   'credibility',\n",
       "   'content',\n",
       "   'and',\n",
       "   'the',\n",
       "   'website',\n",
       "   'that',\n",
       "   'disseminate',\n",
       "   'they',\n",
       "   'section',\n",
       "   '3',\n",
       "   'outline',\n",
       "   'the',\n",
       "   'propose',\n",
       "   'methodology',\n",
       "   ...]},\n",
       " {'titulo': 'Exploring Visual and Multimodal Interaction in NCL Authoring',\n",
       "  'informacoes_url': '',\n",
       "  'idioma': 'english',\n",
       "  'storage_key': '../articles/original/english/985-24754-1-10-20240923.pdf',\n",
       "  'author': 'Paulo Victor Borges; Daniel de S. Moraes; Joel dos Santos; Débora C Muchaluat-Saade; and Sérgio Colcher',\n",
       "  'data_publicacao': '11-09-2024',\n",
       "  'resumo': 'This paper introduces two innovative tools for enhancing interactive multimedia authoring using the Nested Context Language (NCL): (i) a visual extension that supports more traditional interactions with mouse and keyboard and (ii) a multimodal extension that incorporates gesture recognition and voice commands. These tools were implemented as Visual Studio Code extensions and aim to streamline the editing process, making it more intuitive and accessible. We present an evaluation of the usability and acceptance of both tools with developers in an experiment with three tasks for creating and manipulating spatial regions in hypermedia documents. By exploring the potential of multimodal interfaces, this work sets the stage for more efficient and user-friendly document editing. ###',\n",
       "  'keywords': 'Authoring, LLMs, NCL, Code Generation, Visual Studio Code',\n",
       "  'referencias': ['[1] NBR ABNT. [n. d.]. Digital Terrestrial Television-Data Coding and Transmission\\nSpecification for Digital Broadcasting-Part 2: Ginga-NCL for fixed and mobile\\nreceivers, Brazilian Standard 15606-2, Brazil, 2007.',\n",
       "   '[2] G Kumar Arora. 2017. *SOLID Principles Succinctly* . CreateSpace Independent\\nPublishing Platform. 1-4.',\n",
       "   '[3] Abel Avram. 2007. *Domain-driven design Quickly. 20-32* . Lulu.com.',\n",
       "   '[4] Aaron Bangor, Philip T Kortum, and James T Miller. 2008. An empirical evaluation\\nof the system usability scale. *Intl. Journal of Human–Computer Interaction* 24, 6\\n(2008), 574–594.',\n",
       "   '[5] Moniruzzaman Bhuiyan and Rich Picking. 2009. Gesture-controlled user interfaces, what have we done and what’s next. In *Proceedings of the fifth collaborative*\\n*research symposium on security, E-Learning, Internet and Networking (SEIN 2009),*\\n*Darmstadt, Germany* . Citeseer, 26–27.',\n",
       "   '[6] John Brooke et al . 1996. SUS-A quick and dirty usability scale. *Usability evaluation*\\n*in industry* 189, 194 (1996), 4–7.',\n",
       "   '[7] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde\\nDe Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\\nGreg Brockman, et al . 2021. Evaluating large language models trained on code.\\n*arXiv preprint arXiv:2107.03374* (2021).',\n",
       "   '[8] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2023. Teaching\\nlarge language models to self-debug. *arXiv preprint arXiv:2304.05128* (2023).',\n",
       "   '[9] Visual Studio Code. 2019. Visual studio code. *Recuperado el Octubre de* (2019).',\n",
       "   '[10] Douglas Paulo de Mattos and Débora C Muchaluat-Saade. 2018. Steve: A hypermedia authoring tool based on the simple interactive multimedia model. In\\n*Proceedings of the ACM Symposium on Document Engineering 2018* . 1–10.',\n",
       "   '[11] Joel André Ferreira Dos Santos and Débora Christina Muchaluat-Saade. 2012.\\nXTemplate 3.0: spatio-temporal semantics and structure reuse for hypermedia\\ncompositions. *Multimedia Tools and Applications* 61, 3 (2012), 645–673.',\n",
       "   '[12] Louie Giray. 2023. Prompt engineering with ChatGPT: a guide for academic\\nwriters. *Annals of biomedical engineering* 51, 12 (2023), 2629–2633.',\n",
       "   '[13] Moh Harris, Ali Suryaperdana Agoes, et al . 2021. Applying hand gesture recognition for user guide application using MediaPipe. In *2nd International Seminar*\\n*of Science and Applied Technology (ISSAT 2021)* . Atlantis Press, 101–108.',\n",
       "   '[14] Jhilmil Jain, Arnold Lund, and Dennis Wixon. 2011. The future of natural user\\ninterfaces. In *CHI’11 Extended Abstracts on Human Factors in Computing Systems* .\\n211–214.',\n",
       "   '[15] Ankur Joshi, Saket Kale, Satish Chandel, and D Kumar Pal. 2015. Likert scale:\\nExplored and explained. *British journal of applied science & technology* 7, 4 (2015),\\n396–403.',\n",
       "   '[16] Bipin Joshi and Bipin Joshi. 2016. Overview of SOLID Principles and Design\\nPatterns. *Beginning SOLID Principles and Design Patterns for ASP. NET Developers*\\n(2016), 1–44.',\n",
       "   '[17] Dr Manju Kaushik and Rashmi Jain. 2014. Natural user interfaces: Trend in\\nvirtual interaction. *arXiv preprint arXiv:1405.0101* (2014).',\n",
       "   '[18] James R Lewis. 2018. The system usability scale: past, present, and future. *Inter-*\\n*national Journal of Human–Computer Interaction* 34, 7 (2018), 577–590.',\n",
       "   '[19] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi\\nLeblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al . 2022.\\nCompetition-level code generation with alphacode. *Science* 378, 6624 (2022),\\n1092–1097.',\n",
       "   '[20] Scott Millett and Nick Tune. 2015. *Patterns, principles, and practices of domain-*\\n*driven design. 50-64* . John Wiley & Sons.',\n",
       "   '[21] Daniel de Sousa Moraes, Polyana Bezerra da Costa, Antonio JG Busson, José\\nMatheus Carvalho Boaro, Carlos de Salles Soares Neto, and Sergio Colcher. 2023.\\n\\n\\n160\\n\\n\\n-----\\n\\nExploring Visual and Multimodal Interaction in NCL Authoring WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\nOn the Challenges of Using Large Language Models for NCL Code Generation.\\nIn *Anais Estendidos do XXIX Simpósio Brasileiro de Sistemas Multimídia e Web* .\\nSBC, 151–156.',\n",
       "   '[22] Daniel de Sousa Moraes, André Luiz de B Damasceno, Antonio José G Busson, and\\nCarlos de Salles Soares Neto. 2016. Lua2NCL: framework for textual authoring\\nof NCL applications using Lua. In *Proceedings of the 22nd Brazilian Symposium*\\n*on Multimedia and the Web* . 47–54.',\n",
       "   '[23] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou,\\nSilvio Savarese, and Caiming Xiong. 2022. Codegen: An open large language\\nmodel for code with multi-turn program synthesis. *arXiv preprint arXiv:2203.13474*\\n(2022).',\n",
       "   '[24] Douglas Paulo de Mattos, Júlia Varanda da Silva, and Débora Christina MuchaluatSaade. 2013. NEXT: graphical editor for authoring NCL documents supporting\\ncomposite templates. In *Proceedings of the 11th european conference on Interactive*\\n*TV and video* . 89–98.',\n",
       "   '[25] Carlos de Salles Soares Neto Roberto Gerson de Albuquerque Azevedo, Mario\\nMeireles Teixeira. 2009. NCL Eclipse: Ambiente Integrado para o Desenvolvimento de Aplicações para TV Digital Interativa em Nested Context Language. In\\n*Salão de Ferramentas - SBRC 2009* . São Luís, MA, Brazil.',\n",
       "   '[26] Victor Hazin da Rocha. 2013. *DiTV–Arquitetura de desenvolvimento para aplicações*\\n*interativas distribuídas para TV digital* . Master’s thesis. Universidade Federal de\\nPernambuco.',\n",
       "   '[27] Harmeet Singh and Syed Imtiyaz Hassan. 2015. Effect of solid design principles\\non quality of software: An empirical assessment. *International Journal of Scientific*\\n*& Engineering Research* 6, 4 (2015), 1321–1324.',\n",
       "   '[28] Luiz Fernando Gomes Soares and Rogério Ferreira Rodrigues. 2006. Nested\\ncontext language 3.0 part 8–ncl digital tv profiles. *Monografias em Ciência da*\\n*Computação do Departamento de Informática da PUC-Rio* 1200, 35 (2006), 06.',\n",
       "   '[29] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert,\\nAshraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. 2023. A prompt\\npattern catalog to enhance prompt engineering with chatgpt. *arXiv preprint*\\n*arXiv:2302.11382* (2023).',\n",
       "   '[30] Anthony Zhang. 2017. SpeechRecognition 2.1.3. https://pypi.org/project/\\nSpeechRecognition/2.1.3/. Accessed: 2024-08-20.',\n",
       "   '[31] Fan Zhang, Valentin Bazarevsky, Andrey Vakunov, Andrei Tkachenka, George\\nSung, Chuo-Ling Chang, and Matthias Grundmann. 2020. Mediapipe hands:\\nOn-device real-time hand tracking. *arXiv preprint arXiv:2006.10214* (2020).\\n\\n\\n161\\n\\n\\n-----'],\n",
       "  'text': '# **Exploring Visual and Multimodal Interaction in NCL Authoring**\\n\\n## Paulo Victor Borges\\n#### Pontifical Catholic University of Rio de Janeiro Rio de Janeiro, Brazil pvborges@telemidia.puc-rio.br\\n\\n## Daniel de S. Moraes\\n#### Pontifical Catholic University of Rio de Janeiro Rio de Janeiro, Brazil danielmoraes@telemidia.puc-rio.br\\n\\n## Joel dos Santos\\n#### CEFET-RJ Rio de Janeiro, Brazil jsantos@eic.cefet-rj.br\\n\\n## Débora C Muchaluat-Saade\\n#### MídiaCom Lab Fluminense Federal University Nitéroi, Brazil debora@midiacom.uff.br\\n### **ABSTRACT**\\n\\nThis paper introduces two innovative tools for enhancing interactive multimedia authoring using the Nested Context Language\\n(NCL): (i) a visual extension that supports more traditional interactions with mouse and keyboard and (ii) a multimodal extension\\nthat incorporates gesture recognition and voice commands. These\\ntools were implemented as Visual Studio Code extensions and aim\\nto streamline the editing process, making it more intuitive and accessible. We present an evaluation of the usability and acceptance\\nof both tools with developers in an experiment with three tasks\\nfor creating and manipulating spatial regions in hypermedia documents. By exploring the potential of multimodal interfaces, this\\nwork sets the stage for more efficient and user-friendly document\\nediting.\\n### **KEYWORDS**\\n\\nAuthoring, LLMs, NCL, Code Generation, Visual Studio Code\\n### **1 INTRODUCTION**\\n\\nDigital TV (DTV) applications [ 26 ] have revolutionized content delivery and consumption, offering a rich multimedia experience that\\nintegrates text, images, audio, and video. These applications extend\\nbeyond passive viewing by enabling interactivity and creating a\\nmore engaging and immersive environment for the viewer.\\nDTV applications (as well as other interactive multimedia applications) can be developed using structured content, known as *mul-*\\n*timedia/hypermedia documents* . The structure of these documents\\nhas been the goal of standardization efforts in many instances, including the definition of declarative languages like the NCL (Nested\\nContext Language) [ 28 ], a language based on the NCM ( *Nested Con-*\\n*text Model* ), which facilitates the logical structuring of hypermedia\\ndocuments through compositions and specifies spatio-temporal\\nrelationships using connectors and links within an event-based\\nparadigm.\\nCurrent digital TV scenarios require simple and quick methodologies to create interactive multimedia applications. As in other\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n© 2024 SBC – Brazilian Computing Society.\\nISSN 2966-2753\\n\\n## Sérgio Colcher\\n#### Pontifical Catholic University of Rio de Janeiro Rio de Janeiro, Brazil colcher@inf.puc-rio.br\\n\\nscenarios, there is a growing need for functionalities that can assist\\nprogrammers in developing these applications efficiently. Thus,\\nauthors with varying skills might be able to create correct and\\nfunctional applications.\\nTherefore, creating and editing multimedia content, including\\nDTV applications, require precise and efficient tools that can effectively manage various elements within a digital interface while also\\nmaintaining ease of use. Traditional tools that have been relying on\\nmouse and keyboard interactions have proven their effectiveness,\\nbut there is still considerable room for improvement. Natural user\\ninterfaces (NUIs) [ 14, 17 ], such as body movements and gestures,\\nhave been used in many settings to operate machines, communicate\\nwith intelligent environments, and control smart home appliances\\n\\n[ 5 ] and could also be applied in a multimedia context to create more\\nintuitive and engaging authoring experiences.\\nLarge Language Models (LLMs), on their turn, have recently\\nrevolutionized various fields, enabling their use in developing a\\nwide range of applications, such as chatbots (like chatGPT and\\nGemini) that can handle queries in different contexts. These models\\nhave also shown good results in tasks related to programming and\\ncode synthesis [7, 8, 19, 23].\\nIn this sense, this work proposes two distinct tools to help improve the development of interactive applications for Digital TV,\\nspecifically in creating the layouts defined by the region base of\\na document. The first is a visual tool that allows the developer to\\nvisualize the regions defined in a document’s region base and relies\\non traditional mouse and keyboard interactions to create and manipulate regions graphically. The second tool incorporates gesture\\nrecognition and voice commands, processed by a language model to\\ngenerate editing actions (in addition to the visualization of the created regions). These tools are implemented as Visual Studio Code\\n(VS Code) extensions [ 9 ], taking advantage of its robust feature set,\\nversatility, and adaptability.\\nWe also present an evaluation of the usability and acceptance of\\nboth tools with developers in an experiment involving tasks of creation and manipulation of regions. This evaluation was conducted\\nthrough a custom usability questionnaire that was inspired by and\\nadapted from the principles of the System Usability Scale (SUS)\\n\\n[4, 6, 18] to fit our specific case.\\n\\n\\n153\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Borges et al.\\n\\n\\nThe remainder of this paper is structured as follows: Section 2\\nprovides a review of related work. Section 3 describes the methodology and implementation of the tools. Section 4 details the experimental setup, including the tasks and participants. Section 5\\npresents the results of the experiment and the usability questionnaire. Finally, Section 6 discusses the findings, implications, and\\npotential future work.\\n### **2 RELATED WORK**\\n\\nThe development of interactive multimedia applications for digital\\nTV has been significantly advanced by authoring tools that facilitate\\nboth textual and visual authoring approaches.\\nFor example, NEXT (NCL Editor Supporting XTemplate) [ 24 ] is\\na graphical editor developed to create NCL documents using hypermedia composite templates specified in the XTemplate 3.0 language\\n\\n[ 11 ], making the creation of interactive digital TV applications more\\naccessible to authors without in-depth NCL knowledge. NEXT offers functionalities such as creating and editing NCL documents in\\ndifferent views, utilizing an extensible template library that can be\\nadapted to different skill levels of authors. By providing a graphical\\ninterface and a set of plugins, NEXT simplifies the development\\nprocess and allows authors to focus on the content rather than the\\nintricacies of NCL programming. This tool represents a significant\\nstep forward in enabling a broader range of users to contribute to\\nthe development of interactive multimedia applications, enhancing\\nboth the usability and efficiency of the authoring process.\\nSTEVE (Spatio-Temporal View Editor) [ 10 ] allows users without\\nprior knowledge of authoring languages to create interactive multimedia applications for web and digital TV systems, presenting\\na graphical interface that visually exposes the elements and their\\nrelationships and exporting to HTML5 and NCL. These tools often\\nfeature user-friendly interfaces, enabling a broader audience to\\nengage in multimedia content creation without the need for extensive technical expertise. This inspires the development of a tool\\nthat can generate code through a visual interface while seamlessly\\nintegrating into popular development environments.\\nA notable contribution is the Lua2NCL framework [ 22 ], which\\naddresses the verbosity and complexity associated with NCL by\\nproviding a set of textual features that reduce the effort required\\nfor creating NCL applications. Lua2NCL leverages the Lua scripting language to enable the creation of applications with reduced\\ncode size and enhanced readability, making it accessible even to\\ndevelopers with limited experience in NCL. This framework has\\ndemonstrated considerable effectiveness in reducing the time and\\ncode length necessary for authoring digital TV applications, as\\nevidenced by experiments showing significant reductions in both\\nmetrics compared to traditional NCL coding.\\nNCL Eclipse [ 25 ] is a textual integrated development environment designed to assist in developing interactive multimedia applications in NCL. As a plugin for the Eclipse IDE, NCL Eclipse\\noffers user-centred features such as automatic and contextual code\\nsuggestion, validation of NCL documents, and syntax highlighting\\nfor XML elements and reserved words. By providing functionalities like automatic code formatting and error indication during the\\nauthoring process, NCL Eclipse significantly enhances developers’\\nproductivity.\\n\\n\\nThe development of authoring tools for interactive Digital TV\\n(iDTV) has focused on improving usability for content creators who\\nmay not be programmers. NCL presents a particular challenge for\\nnon-technical users due to its complexity. Recent studies, such as\\nthose of Moraes et al . [21], highlight the potential of using LLMs\\nto facilitate NCL code generation. However, their initial findings\\nindicate that current pre-trained LLMs struggle to generate highquality NCL code due to syntax and language rule challenges. This\\nunderscores the need for fine-tuning LLMs to more effectively process domain-specific languages like NCL, which could significantly\\nenhance the usability of multimedia authoring tools by allowing\\nusers to define application requirements in natural language.\\nIn that context, numerous authoring tools have introduced significant innovations to facilitate the creation of NCL documents\\nthrough graphical interfaces, reduced code verbosity, and enhanced\\ntextual authoring features such as automatic code suggestion and\\ndocument validation. Inspired by these advancements, our work\\nproposes a visual approach to create and edit applications’ layouts\\ndefined by the region base, allowing graphical manipulation of\\nthose elements using a mouse and keyboard. We also propose a\\nversion with interaction enriched by gesture recognition and voice\\ncommands, integrated with an LLM tool to generate the region base\\nNCL code. With these tools we aim for an intuitive and efficient\\napproach that can facilitate the creation and edition of regions in\\nan NCL application.\\n### **3 METHODOLOGY**\\n\\nTo create a tool available in IDEs and useful to users of different\\nexperience levels, we sought to develop extensions that facilitate\\nthe editing of NCL documents through intuitive visual interfaces.\\nThese extensions were designed for both beginners, who benefit\\nfrom the simplicity of visual manipulation, and advanced users,\\nwho need precise control over the document structure. Although\\ntechnologies exist to assist in the authoring of NCL code, there is\\nstill a need for a more modern solution that integrates with current\\ndevelopment environments. Our methodology, therefore, focuses\\non combining traditional and multimodal interaction techniques\\nto improve the user experience when creating and editing NCL\\n\\ncontent.\\n\\nIn this context, our methodology involves two main aspects: (i)\\nthe development of the two extensions for editing regions in an\\nNCL document and (ii) the implementation of an experiment to\\nevaluate these tools with developers with various experience levels.\\nThe overall system architecture is designed to provide real-time\\nfeedback and seamless interaction for multimedia content editing.\\nThe main components of the systems are presented in the following sections.\\n### **3.1 VS Code Extensions**\\n\\nTwo custom extensions were developed for VS Code using TypeScript: a Visual Extension and a Visual Multimodal Extension. The\\nVisual Extension is based on the NCL document being edited in the\\nIDE and opens a WebView interface that visually represents the\\nregions described in the <regionBase> element.\\nFigure 1 illustrates the graphical elements in Visual Studio Code,\\nwhere a window opens alongside the code editor, allowing regions\\n\\n\\n154\\n\\n\\n-----\\n\\nExploring Visual and Multimodal Interaction in NCL Authoring WebMedia’2024, Juiz de Fora, Brazil\\n\\nto be visually represented in the interface and ready to receive user\\n\\ninteractions.\\n\\n**Figure 1: Visual Extension Interface**\\n\\n\\nIn the Visual Extension, authors can create new regions, change\\nthe z-index, move, resize or delete them using the keyboard and\\nmouse, as illustrated in Figure 2. Any changes made in the graphical\\ninterface are reflected in the code, providing an intuitive way to\\nmanipulate the NCL document visually.\\n\\n**Figure 2: Visual Extension**\\n\\nThe Visual Multimodal Extension builds upon the capabilities\\nof the Visual Extension by incorporating gesture recognition and\\nvoice commands. As Figure 3 depicts, it communicates with an\\nexternal multimodal API via WebSocket, which is responsible for\\ncapturing gestures and voice commands. These inputs allow users\\nto interact with the regions in the same ways as with the Visual Extension—moving, resizing, deleting, and creating new regions—but\\nwith the added convenience of natural user interactions.\\n\\nBoth extensions ensure that the generated code of the region\\nelements adheres to the rules specified in the NCL standards [ 1 ],\\nsuch as parent regions, z-index, and more.\\n\\n\\n**Figure 3: Visual Multimodal Extension**\\n### **3.2 Multimodal API (MMA)**\\n\\nThe Multimodal API (MMA) serves as the central hub for processing and routing data, playing an important role in the seamless\\nintegration of multimodal interactions within the NCL authoring\\ntools. It receives messages from the Visual Studio Code extensions\\nand coordinates with external services for gesture classification\\nand speech recognition. Designed with SOLID principles [ 2, 16, 27 ],\\nthe MMA ensures modularity, maintainability, and scalability, allowing for easy updates and integration of new features without\\ncompromising the system’s integrity.\\nTo closely align the architecture with business requirements and\\ndomain logic, Domain-Driven Design (DDD) practices [ 3, 20, 20 ]\\nwere applied. This approach facilitated the creation of decoupled\\ncomponents within the MMA, enabling seamless integration with\\nvarious models and services for gesture and speech recognition,\\nas well as any large language models (LLMs). This design ensures\\nthat the system remains highly adaptable to future advancements,\\nallowing the incorporation of new technologies and methodologies\\nas they emerge.\\nFurthermore, the MMA’s architecture supports real-time processing and feedback, crucial for maintaining an interactive and\\nresponsive user experience. By adhering to SOLID principles and\\nDDD practices, the MMA meets current technical requirements and\\nalso provides a robust foundation for ongoing development and\\nenhancement. This strategic design choice underlines the commitment to creating a scalable, maintainable, and user-centric system\\nthat can evolve alongside the rapidly advancing field of multimodal\\ninteraction technologies.\\n\\n*3.2.1* *HandGesture Classifier Provider.* : This component is responsible for capturing video from the user’s webcam and applying the\\nclassification algorithm provided by the Mediapipe library [ 13, 31 ]\\nto identify specific hand gestures made by the user. It detects gestures such as Thumbs Up, open hand, and the hand’s position for\\n\\n\\n155\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Borges et al.\\n\\n\\nmovement purposes. The identified gestures are then sent to the\\nMultimodal API (MMA) via WebSocket. The provider sends detailed information about the current gesture, including the type of\\ngesture and the hand’s position, ensuring precise and responsive\\ninteraction with the NCL authoring tools.\\n\\n*3.2.2* *Speech Recognition Provider.* : This component is responsible\\nfor capturing audio from the user’s microphone and applying the\\nGoogle Speech Recognition API package in Python [ 30 ] to convert\\nit to text and identify voice commands. The process begins with\\nthe continuous capture of audio input, which is then processed in\\nreal-time to transcribe spoken words into text. The Google Speech\\nRecognition API is designed to handle various accents and speech\\npatterns, ensuring high accuracy in the transcription of commands.\\nBy breaking down the audio into phonetic units and comparing\\nthem against a comprehensive database of known speech patterns,\\nthe API generates an accurate textual representation of the spoken\\ncommands.\\n\\nOnce the speech is converted to text, the recognized commands\\nare parsed to identify specific keywords and phrases corresponding\\nto predefined actions within the NCL authoring tools. The parsed\\ncommands are then formatted into a structured message, including details such as the command type and relevant parameters.\\nThis structured message is sent to the Multimodal API (MMA) via\\nWebSocket, enabling real-time processing and interaction within\\nthe NCL authoring environment. The use of WebSocket technology ensures low-latency, bidirectional communication, allowing\\nfor seamless and responsive updates based on the user’s voice commands.\\n\\n*3.2.3* *Chat GPT API.* : The Multimodal API (MMA) sends HTTP\\nrequests to the Chat GPT API for processing recognized gestures\\nand voice commands using the GPT-3.5-turbo model. This language model generates specific editing actions based on the input\\nprovided. After each recognition by the speech recognizer, the recognized text is sent to the Chat GPT API along with a carefully\\ncrafted prompt to ensure the returned information is accurately\\nprocessed. This method guarantees that the response is correctly\\nformatted for seamless communication with the VS Code exten\\nsion, thereby eliminating the need for any additional cleanup of\\nthe spoken command.\\nAn example of the prompt used is:\\n\\nYou are an intelligent assistant that helps create\\nand manipulate\\nregions in a graphical interface.\\nThe user\\'s command is: \"{command}\"\\nYou are working with a screen resolution of 1920x1080\\npixels.\\nHere are the types of commands you can generate:\\n1. createRegion - Creates a new region based on the\\nprovided coordinates. Example: Create a region\\nof 100x100 pixels at position 200,200.\\n\\nYour job is to interpret the user\\'s command and\\ngenerate the appropriate response in correct format.\\n\\nPlease provide a structured response in the following\\n\\nformat:\\n\\n\\n\\n- Command: createRegion\\n\\n- left: <value>\\n\\n- top: <value>\\n\\n- width: <value>\\n\\n- height: <value>\\n\\n- id: <value>\\n\\n- title?: <value>\\n\\n- z-index?: <value>\\n\\nNow, interpret the user\\'s command and provide the\\nappropriate response, ensuring to use integer values for\\n\\ncoordinates and dimensions.\\n\\nIn this context, prompt engineering involves designing and optimizing input prompts to enhance LLM performance and effectiveness. This technique guides LLMs to produce accurate, relevant, and\\ncontextually appropriate responses [ 12 ]. Carefully crafted prompts\\nallow users to control the model’s output, ensuring alignment with\\nspecific requirements and objectives.\\nWe used a prompt to enable an LLM to assist in creating and\\nmanipulating regions within a graphical user interface (GUI). This\\nstructured approach, akin to software patterns, ensures clarity and\\nreusability. The goal is to program interactions between the user\\nand the LLM to efficiently generate NCL code for content editing\\ntasks, with a standard return format that the client application can\\nalways interpret correctly. This approach is informed by principles\\noutlined in the prompt pattern catalog by White et al. [29].\\nThis prompt has been adapted for various method calls to achieve\\ndifferent objectives, such as resizing, deleting, and moving regions.\\nEach adapted prompt ensures that recognized voice commands or\\ngestures are accurately interpreted and formatted for seamless communication with the VS Code extension. By tailoring the prompt to\\nhandle specific commands, the system efficiently processes a wide\\nrange of editing actions, enhancing flexibility and usability.\\n### **4 USER EXPERIMENT**\\n\\nA remote experiment was conducted to evaluate the effectiveness\\nand usability of the proposed systems for multimedia application\\nediting. This study explores whether visual and multimodal tools\\nfor NCL authoring can enhance the editing experience compared\\nto traditional methods.\\n\\nAfter the planning phase a pilot study was conducted, lasting 1\\nhour via call, to assess the validity of the questions, the quality of\\nthe instructions, and the material provided.\\nBased on the pilot study feedback, corrections and improvements\\nwere made, and then an extended experiment was carried out with a\\nlarger group of participants. The experiment involved programmers\\nof various experience levels from different universities performing\\nediting tasks using three different interaction methods: ( *a* ) using\\nonly the code editor; ( *b* ) using the visual extension traditional mouse\\nand keyboard interaction, ( *c* ) and the visual extension integrated\\nwith gesture and voice-based interface.\\n### **4.1 Participants**\\n\\nA total of 11 programmers participated in the study. The participants\\nwere selected based on their familiarity with multimedia content\\nediting and their experience with programming in VS Code.\\n\\n\\n156\\n\\n\\n-----\\n\\nExploring Visual and Multimodal Interaction in NCL Authoring WebMedia’2024, Juiz de Fora, Brazil\\n### **4.2 Tasks**\\n\\nThe participants were asked to complete a series of basic tasks\\ninvolving the creation and manipulation of regions within an NCL\\ndocument. These tasks included:\\n\\n(1) Creating 5 new regions by specifying dimensions and positions, following the example illustrated in Figure 4.\\n(2) Resizing the previously created regions to resemble the layout illustrated in Figure 5.\\n(3) Resizing and moving regions to different positions, as shown\\nin Figure 6.\\n\\n**Figure 6: Third Task - Move and resize the regions**\\n### **4.3 Procedure**\\n\\n\\n**Figure 4: First Task - Create 5 regions that reproduce the**\\n**defined layout**\\n\\n**Figure 5: Second Task - Resize existing regions**\\n\\nEach participant performed these tasks in three settings: once\\nusing only the VS Code code editor; once using the traditional\\nmouse and keyboard interaction with the visual interface; and\\nonce using the gesture and voice-based interaction alongside the\\nvisual interface. The experiment always started with the code editor\\nsetting, whilst the order of the two other settings was randomized\\nto mitigate any learning effects and ensure that the results were\\nnot biased by the sequence in which the tasks were performed.\\n\\n\\nThe experiment was conducted individually and remotely. Participants were given a script on how to install and use the extensions\\nbefore starting the tasks. The script also detailed each task and\\nexemplified the expected layouts, as shown in Figures 4, 5, 6. All\\nmaterials required for the implementation of the tasks were made\\navailable, including the incomplete NCL documents, which the\\nparticipants were meant to fill. After completing each experiment\\nsetting, participants were instructed to respond to a questionnaire\\nrelated to that specific setting.\\nAfter completing the tasks of a setting, participants were asked\\nto fill out a questionnaire assessing their experience and the usability of the interactions. The questionnaire included questions on\\nvarious aspects such as ease of use, efficiency, accuracy, user satisfaction, learning curve, preference, specific feedback, and suggestions\\nfor improvement. These questions aimed to gather comprehensive\\nfeedback on how easy and intuitive each interface was to use, how\\nquickly tasks could be completed, how accurately tasks were completed, overall user satisfaction, how easy it was to learn to use each\\ninterface, preferences between the two interfaces, specific likes or\\ndislikes, and any suggestions for improvement. The questionnaire\\nwas inspired by the System Usability Scale (SUS) [ 4, 18 ] and incorporates elements from it, although it does not strictly follow\\nthe standard SUS format. This adaptation allowed us to focus on\\nparticular aspects of usability that were crucial for our study, such\\nas the unique interaction methods of creating and manipulating\\nregions, and to gather more targeted feedback from participants.\\nAdditionally, participants were asked to respond to the following\\nstatements about their experience using a Likert Scale [ 15 ] from 1 to\\n7, where 1 represents Strongly Disagree and 7 represents Strongly\\nAgree:\\n\\n(1) *\"I think it is easy to use the tool to create and edit regions.\"*\\n(2) *\"I was able to create and edit regions using the tool.\"*\\n(3) *\"I think the regions created and edited with the tool are correct.\"*\\n(4) *\"I am satisfied with using the tool to create and edit regions.\"*\\n\\n\\n157\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Borges et al.\\n\\n\\nThese statements helped evaluate the creation and editing capabilities, the accuracy of the regions, and overall satisfaction with\\nthe tool.\\n### **4.4 Data Collection**\\n\\nThe data collection process involved timestamping each participant’s responses to track when the data was submitted. Participants’\\nemail addresses were the only personal information collected, to\\nensure the validity of responses and for follow-up purposes. Also,\\nparticipants’ experiences with NCL and VS Code were assessed to\\nunderstand the correlation between familiarity with these tools and\\ntheir performance and satisfaction with the interfaces.\\nParticipants preferred interface and their reasons for this preference were noted, providing valuable information on the usability\\nand practicality of the multimodal approach compared to traditional methods. Additionally, participants were asked about their\\nexperience with the VS Code IDE and their familiarity with NCL to\\nbetter understand how these factors influenced their preferences\\nand feedback.\\nAdditionally, participants were invited to submit the final version\\nof their NCL files and indicate if they would like to schedule a\\nmeeting for further discussion, allowing for a deeper understanding\\nof their experiences and suggestions for improvement.\\n### **5 RESULTS**\\n\\nThe collected data were analyzed to compare the performance of the\\nthree interaction methods: only using the VSCode text editor, the\\ntraditional mouse and keyboard interface (Visual Extension), and\\nthe gesture and voice-based interface (Multimodal Extension). Each\\nsession lasted approximately 50 minutes, during which participants\\ncompleted tasks and provided feedback.\\n\\n**Table 1: Summary of Results Comparing Textual, Visual Ex-**\\n**tension (VE), and Multimodal Extension (MME) Modalities**\\n\\n|Metric|Textual (median)|VE (median)|MME (median)|\\n|---|---|---|---|\\n|Ease of Use|5.0|7.0|5.0|\\n|Accuracy|6.0|7.0|6.0|\\n|Overall Satisfaction|5.0|6.0|5.0|\\n|Context of Use|General|General|Specifci Situations|\\n|Preference|0%|0.71%|29%|\\n\\n### **5.1 Participant Overview**\\n\\nA total of eleven participants took part in the study, with varying levels of experience with NCL and VS Code. However, only\\nseven participants completed the experiment and responded to all\\nsurvey sections. Therefore, we will use only these seven complete\\nresponses for the analysis.\\nTheir experiences ranged from beginners to advanced users,\\nproviding diverse perspectives. This diversity in experience levels\\nhelped ensure that the feedback covered a broad spectrum of potential users. Participants were primarily students, professionals, and\\nresearchers from various fields, ensuring a well-rounded evaluation\\nof the interfaces.\\n\\n### **5.2 Performance Metrics**\\n\\nParticipants rated the ease of use, efficiency, accuracy, and overall\\nsatisfaction for the textual, traditional mouse and keyboard interface (Visual Extension), and the gesture and voice-based interface\\n(Multimodal Extension) on a scale from 1 to 7. In this scale, 1 means\\n\"Strongly Disagree\" and 7 means \"Strongly Agree\". Additionally,\\nthe preference ratings reflect the number of votes each interface\\nreceived. The following observations were made, which are detailed\\nbelow and summarized in Table 1:\\n\\n  - Ease of Use: Participants had the perception that the Visual\\nExtension (VE) interface was generally easier to use, with a\\nmedian rating of 7.0, compared to 5.0 for both the Multimodal\\nExtension (MME) and the textual interface. This significant\\ndifference highlights the steep initial learning curve associated with the multimodal interface. Users thought the VE\\ninterface was more intuitive and straightforward, whereas\\nthe multimodal interface required more effort.\\n\\n  - Accuracy: Participants reported higher accuracy with the\\nVisual Extension, with a median rating of 7.0, compared to\\n6.0 for both the Multimodal Extension (MME) and the textual interface. The primary contributors to lower accuracy\\nin the multimodal interface were errors in gesture recognition and voice command interpretation. These inaccuracies\\noften resulted in unintended actions or the need for repeated\\nattempts to achieve the desired outcome, thereby affecting\\noverall task performance.\\n\\n  - Overall Satisfaction: The Visual Extension received higher\\noverall satisfaction scores, averaging 5.9, while the gesture\\nand voice-based interface received a score of 4.3 and the\\n\\ntextual interface scored 4.0. Participants appreciated the reliability and ease of use of the traditional interface. However,\\ndespite the lower satisfaction score for the multimodal interface, some users expressed excitement about its potential.\\nThey highlighted that with further refinements and improvements in gesture and voice recognition accuracy, the multimodal interface could become a powerful and efficient tool\\nfor NCL document editing, enhancing user engagement and\\n\\ninteraction.\\n### **5.3 Qualitative Feedback**\\n\\nParticipants provided detailed feedback on their experiences with\\nboth interfaces. Key points included:\\n\\n  - Learning Curve: Participants perceived that the gesture and\\nvoice-based interface had a steeper learning curve, particularly for those unaccustomed to such interaction methods.\\nTraining and practice were required to achieve proficiency.\\nBeginners found the traditional interface more intuitive and\\nless frustrating.\\n\\n  - Context of Use: Some participants suggested that the gesture and voice-based interface could be more effective in\\nspecific contexts where hands-free operation is beneficial,\\nsuch as during live presentations or when physical interaction with a keyboard and mouse is impractical. For example,\\nprofessionals working in dynamic environments or those\\nwith accessibility needs might find this interface particularly\\nuseful. The textual interface, while less flexible in dynamic\\n\\n\\n158\\n\\n\\n-----\\n\\nExploring Visual and Multimodal Interaction in NCL Authoring WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\ncontexts, was appreciated for its precision and suitability in\\ntraditional coding environments.\\n\\n  - Preference and Usability: Despite the potential of the Multimodal Extension, the majority of participants expressed a\\npreference for the traditional interface for most tasks due\\nto its familiarity and reliability. However, some found the\\nmultimodal interface more engaging and enjoyable for creative tasks, suggesting that it could complement rather than\\nreplace traditional methods.\\n\\n  - Improvements Needed: Feedback indicated that the gesture\\nrecognition accuracy and voice command responsiveness\\nrequire significant improvements for the multimodal interface to be viable for daily use.Participants highlighted the\\nneed for better error correction mechanisms, more intuitive\\ngesture sets, and the possibility was considered that some\\nissues might be caused by occasional erroneous responses\\nfrom the Language Model (LLM). Enhancements in these areas could make the interface more user-friendly and reduce\\nthe occurrence of errors.\\n\\n  - Task Suitability: Participants noted that simple tasks were\\neasier and faster with the Visual Extension, while more complex tasks involving multiple regions and dynamic interactions could benefit from the multimodal interface once the\\ninitial learning curve is overcome. This indicates that each\\ninterface has strengths in different areas of task complexity.\\n### **5.4 Participant Preferences**\\n\\nWhen asked to choose their preferred tool for creating and editing\\nregions, 5 out of 7 respondents indicated a preference for the Visual\\nExtension. The primary reasons for this preference included greater\\naccuracy, ease of use, and reliability. Conversely, the remaining\\nparticipants who favored the gesture and voice-based interface\\nhighlighted the novelty of the interaction and the potential for more\\nnatural and intuitive controls once the learning curve is overcome.\\nComments such as \"the multimodal interface feels more futuristic\\nand engaging\" were common among this group, reflecting their\\nenthusiasm for the innovative approach.\\n### **5.5 Additional Insights**\\n\\nThe study also revealed that the participants experience levels significantly influenced their preferences and performance. Advanced\\nusers were more likely to appreciate the efficiency gains from the\\nmultimodal interface, while beginners found the Visual Extension\\nmore accessible and straightforward. This suggests that the Multimodal Extension could be more beneficial as an auxiliary tool\\nfor experienced developers rather than a complete replacement for\\ntraditional methods.\\n### **5.6 Detailed Task Analysis**\\n\\nFurther analysis of specific tasks showed that:\\n\\n  - Region Creation: Participants found that creating new regions was more intuitive with the multimodal interface once\\nthey became proficient, thanks to the ability to use voice\\ncommands for specifying dimensions and positions. It can\\nbe inferred that initially the visual version brings greater\\n\\n\\nsatisfaction, but after learning how to use the tool, the multimodal extension can be very useful.\\n\\n  - Region Editing: Editing existing regions, such as resizing and\\nmoving, was more accurate with the visual interface. The\\nprecision required for these tasks often led to frustration\\nwith gesture-based controls.\\n\\n  - Complex Interactions: Tasks involving multiple regions and\\ndynamic adjustments highlighted the potential of the multimodal interface. Participants who mastered the gestures and\\nvoice commands found these tasks, such as creating multiple regions in a specific layout, easier and more efficient\\ncompared to traditional methods.\\n### **5.7 Interview Feedback**\\n\\nIn addition to the quantitative data collected, we conducted interviews with participants who accepted an interview to obtain deeper\\nfeedback on their experiences and preferences. One participant provided particularly insightful feedback about the suitability of each\\ninterface for different types of tasks.\\nThe participant reported that for simpler applications, the Visual\\nExtension is more useful due to its straightforward and familiar\\nnature. They felt that the traditional interface allows for quick and\\nprecise interactions without the need for extensive adjustments or\\ncorrections. This preference aligns with the higher ease of use and\\naccuracy ratings observed in the quantitative data. Another interviewee highlighted the ease of using the Visual Extension to quickly\\nunderstand where their content would be displayed, emphasizing\\nits effectiveness for straightforward visualization tasks.\\nHowever, the participant noted a significant advantage of the\\nmultimodal interface for more complex tasks that involve managing\\nmany regions. They highlighted that the ability to use voice commands and gestures for creating and manipulating regions made\\nthese tasks less cumbersome and more efficient. Specifically, for\\ntasks requiring multiple adjustments and dynamic interactions, the\\nmultimodal interface’s intuitive control mechanisms reduced the\\n\\ncognitive load and made the process more engaging.\\nThis feedback underscores the potential for a hybrid approach\\nwhere the traditional interface could be used for simpler, more\\nprecise tasks, while the multimodal interface could be leveraged\\nfor handling complexity and enhancing user engagement in more\\ndemanding scenarios. It suggests that offering users the flexibility to\\nswitch between interfaces based on the task at hand could maximize\\n\\nproductivity and satisfaction.\\nThe participant’s insights reinforce the need for further development and refinement of the multimodal interface to fully realize its\\npotential for complex interactions while maintaining the reliability\\nand precision of the traditional interface for simpler tasks.\\nFurthermore, there are threats to the validity of this study, such\\nas the small number of participants and the variability in their\\nexperience with NCL and VS Code, which may limit the generalizability of the results. The reliance on voice and gesture recognition\\ntechnologies, which are prone to errors, may also have influenced\\nusers perceptions of the effectiveness of the Multimodal Extension.\\nTherefore, future research should consider a larger and more homogeneous sample of users and improvements in the supporting\\ntechnologies to ensure more robust and representative results.\\n\\n\\n159\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Borges et al.\\n\\n### **6 CONCLUSION**\\n\\nThis paper presents two approaches to facilitate the creation and\\nmanipulation of NCL regions graphically. The first approach uses\\ntraditional mouse and keyboard interfaces, while the second integrates gesture recognition and voice commands into a VS Code\\nextension to enhance the editing experience for NCL documents.\\nWe experimented to evaluate the usability and developers’ preferences for both tools. Our initial results indicate that, while the\\ntraditional mouse and keyboard interface remains superior in terms\\nof user satisfaction and perceived accuracy, the multimodal interface shows promise by significantly reducing task completion time\\nand the number of errors.\\n\\nThe integration of these new tools that help author NCL documents visually into VS Code for editing can represent a step towards\\nmore practical and easy-to-use development environments. While\\nthere are challenges to address, the potential benefits in terms of\\nefficiency, availability, and user satisfaction are substantial. By focusing on continuous improvement and user-centred design, we\\nanticipate that this approach can positively influence the future\\ndevelopment of multimedia applications\\nFuture work will focus on refining the tasks performed using the\\nmultimodal interface to ensure they match or exceed the efficiency\\nand accuracy of manual text editing. This includes improving gesture accuracy, enhancing the responsiveness of voice commands,\\nand expanding the range of supported editing actions. Understanding the potential of using LLMs to generate code snippets, we\\nbelieve a multi-agent approach could efficiently resolve multiple\\nblocks within an NCL document. By providing more comprehensive\\nsupport for generating entire NCL code blocks, we aim to offer a\\ntool that is not only innovative but also practical for everyday use\\nby developers.\\nFurthermore, exploring the integration of artificial intelligence\\nand machine learning techniques to enhance the accuracy of gesture and voice recognition could lead to even more robust and\\nintelligent systems. AI-driven predictive models could anticipate\\nuser actions and offer contextual assistance, thereby improving\\noverall efficiency. Future work could explore the possibility of allowing users to select from various Large Language Models (LLMs)\\nbased on their specific needs or preferences. This customization\\nwould enable users to optimize the tool’s performance for different\\ntasks or domains, enhancing the overall flexibility and utility of the\\nsystem. Additionally, enabling users to submit their LLMs could\\nextend the tool’s capabilities, allowing for tailored solutions that\\nleverage specialized knowledge or proprietary data.\\nFinally, conducting extensive user studies with a larger and more\\ndiverse participant pool will be crucial. This will help validate the\\nfindings of our initial experiments and ensure that the multimodal\\ninterface meets the needs of a broad range of users. Gathering detailed feedback on usability, effectiveness, and user satisfaction will\\nprovide valuable insights that can guide the continuous improvement of the tool.\\n### **ETHICS STATEMENT**\\n\\nIn this work, we adhere to ethical guidelines throughout our experiments involving volunteers. All participants were provided with\\nan Informed Consent Form before their involvement, ensuring they\\n\\n\\nwere fully informed about the research scope, procedures, and their\\nrights. We ensure the protection of participants’ data by exclusively\\nusing non-sensitive anonymized data, minimizing risks in data\\nhandling.\\nIn addition, a comprehensive document detailing all aspects of\\nthe research, including objectives, methodology, potential risks,\\nand benefits, was submitted to the Ethics Committee of the Pontifical Catholic University of Rio de Janeiro (PUC-Rio) for review\\nand approval, demonstrating our commitment to ethical research\\npractices.\\n### **ACKNOWLEDGMENTS**\\n\\nThe authors would like to acknowledge RNP (Rede Nacional de\\nEnsino e Pesquisa) and CAPES (Coordenação de Aperfeiçoamento\\nde Pessoal de Nível Superior) for the financial support.\\n### **REFERENCES**\\n\\n[1] NBR ABNT. [n. d.]. Digital Terrestrial Television-Data Coding and Transmission\\nSpecification for Digital Broadcasting-Part 2: Ginga-NCL for fixed and mobile\\nreceivers, Brazilian Standard 15606-2, Brazil, 2007.\\n\\n[2] G Kumar Arora. 2017. *SOLID Principles Succinctly* . CreateSpace Independent\\nPublishing Platform. 1-4.\\n\\n[3] Abel Avram. 2007. *Domain-driven design Quickly. 20-32* . Lulu.com.\\n\\n[4] Aaron Bangor, Philip T Kortum, and James T Miller. 2008. An empirical evaluation\\nof the system usability scale. *Intl. Journal of Human–Computer Interaction* 24, 6\\n(2008), 574–594.\\n\\n[5] Moniruzzaman Bhuiyan and Rich Picking. 2009. Gesture-controlled user interfaces, what have we done and what’s next. In *Proceedings of the fifth collaborative*\\n*research symposium on security, E-Learning, Internet and Networking (SEIN 2009),*\\n*Darmstadt, Germany* . Citeseer, 26–27.\\n\\n[6] John Brooke et al . 1996. SUS-A quick and dirty usability scale. *Usability evaluation*\\n*in industry* 189, 194 (1996), 4–7.\\n\\n[7] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde\\nDe Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\\nGreg Brockman, et al . 2021. Evaluating large language models trained on code.\\n*arXiv preprint arXiv:2107.03374* (2021).\\n\\n[8] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2023. Teaching\\nlarge language models to self-debug. *arXiv preprint arXiv:2304.05128* (2023).\\n\\n[9] Visual Studio Code. 2019. Visual studio code. *Recuperado el Octubre de* (2019).\\n\\n[10] Douglas Paulo de Mattos and Débora C Muchaluat-Saade. 2018. Steve: A hypermedia authoring tool based on the simple interactive multimedia model. In\\n*Proceedings of the ACM Symposium on Document Engineering 2018* . 1–10.\\n\\n[11] Joel André Ferreira Dos Santos and Débora Christina Muchaluat-Saade. 2012.\\nXTemplate 3.0: spatio-temporal semantics and structure reuse for hypermedia\\ncompositions. *Multimedia Tools and Applications* 61, 3 (2012), 645–673.\\n\\n[12] Louie Giray. 2023. Prompt engineering with ChatGPT: a guide for academic\\nwriters. *Annals of biomedical engineering* 51, 12 (2023), 2629–2633.\\n\\n[13] Moh Harris, Ali Suryaperdana Agoes, et al . 2021. Applying hand gesture recognition for user guide application using MediaPipe. In *2nd International Seminar*\\n*of Science and Applied Technology (ISSAT 2021)* . Atlantis Press, 101–108.\\n\\n[14] Jhilmil Jain, Arnold Lund, and Dennis Wixon. 2011. The future of natural user\\ninterfaces. In *CHI’11 Extended Abstracts on Human Factors in Computing Systems* .\\n211–214.\\n\\n[15] Ankur Joshi, Saket Kale, Satish Chandel, and D Kumar Pal. 2015. Likert scale:\\nExplored and explained. *British journal of applied science & technology* 7, 4 (2015),\\n396–403.\\n\\n[16] Bipin Joshi and Bipin Joshi. 2016. Overview of SOLID Principles and Design\\nPatterns. *Beginning SOLID Principles and Design Patterns for ASP. NET Developers*\\n(2016), 1–44.\\n\\n[17] Dr Manju Kaushik and Rashmi Jain. 2014. Natural user interfaces: Trend in\\nvirtual interaction. *arXiv preprint arXiv:1405.0101* (2014).\\n\\n[18] James R Lewis. 2018. The system usability scale: past, present, and future. *Inter-*\\n*national Journal of Human–Computer Interaction* 34, 7 (2018), 577–590.\\n\\n[19] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi\\nLeblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al . 2022.\\nCompetition-level code generation with alphacode. *Science* 378, 6624 (2022),\\n1092–1097.\\n\\n[20] Scott Millett and Nick Tune. 2015. *Patterns, principles, and practices of domain-*\\n*driven design. 50-64* . John Wiley & Sons.\\n\\n[21] Daniel de Sousa Moraes, Polyana Bezerra da Costa, Antonio JG Busson, José\\nMatheus Carvalho Boaro, Carlos de Salles Soares Neto, and Sergio Colcher. 2023.\\n\\n\\n160\\n\\n\\n-----\\n\\nExploring Visual and Multimodal Interaction in NCL Authoring WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\nOn the Challenges of Using Large Language Models for NCL Code Generation.\\nIn *Anais Estendidos do XXIX Simpósio Brasileiro de Sistemas Multimídia e Web* .\\nSBC, 151–156.\\n\\n[22] Daniel de Sousa Moraes, André Luiz de B Damasceno, Antonio José G Busson, and\\nCarlos de Salles Soares Neto. 2016. Lua2NCL: framework for textual authoring\\nof NCL applications using Lua. In *Proceedings of the 22nd Brazilian Symposium*\\n*on Multimedia and the Web* . 47–54.\\n\\n[23] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou,\\nSilvio Savarese, and Caiming Xiong. 2022. Codegen: An open large language\\nmodel for code with multi-turn program synthesis. *arXiv preprint arXiv:2203.13474*\\n(2022).\\n\\n[24] Douglas Paulo de Mattos, Júlia Varanda da Silva, and Débora Christina MuchaluatSaade. 2013. NEXT: graphical editor for authoring NCL documents supporting\\ncomposite templates. In *Proceedings of the 11th european conference on Interactive*\\n*TV and video* . 89–98.\\n\\n[25] Carlos de Salles Soares Neto Roberto Gerson de Albuquerque Azevedo, Mario\\nMeireles Teixeira. 2009. NCL Eclipse: Ambiente Integrado para o Desenvolvimento de Aplicações para TV Digital Interativa em Nested Context Language. In\\n*Salão de Ferramentas - SBRC 2009* . São Luís, MA, Brazil.\\n\\n\\n\\n[26] Victor Hazin da Rocha. 2013. *DiTV–Arquitetura de desenvolvimento para aplicações*\\n*interativas distribuídas para TV digital* . Master’s thesis. Universidade Federal de\\nPernambuco.\\n\\n[27] Harmeet Singh and Syed Imtiyaz Hassan. 2015. Effect of solid design principles\\non quality of software: An empirical assessment. *International Journal of Scientific*\\n*& Engineering Research* 6, 4 (2015), 1321–1324.\\n\\n[28] Luiz Fernando Gomes Soares and Rogério Ferreira Rodrigues. 2006. Nested\\ncontext language 3.0 part 8–ncl digital tv profiles. *Monografias em Ciência da*\\n*Computação do Departamento de Informática da PUC-Rio* 1200, 35 (2006), 06.\\n\\n[29] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert,\\nAshraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. 2023. A prompt\\npattern catalog to enhance prompt engineering with chatgpt. *arXiv preprint*\\n*arXiv:2302.11382* (2023).\\n\\n[30] Anthony Zhang. 2017. SpeechRecognition 2.1.3. https://pypi.org/project/\\nSpeechRecognition/2.1.3/. Accessed: 2024-08-20.\\n\\n[31] Fan Zhang, Valentin Bazarevsky, Andrey Vakunov, Andrei Tkachenka, George\\nSung, Chuo-Ling Chang, and Matthias Grundmann. 2020. Mediapipe hands:\\nOn-device real-time hand tracking. *arXiv preprint arXiv:2006.10214* (2020).\\n\\n\\n161\\n\\n\\n-----\\n\\n',\n",
       "  'artigo_tokenizado': ['#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Exploring',\n",
       "   'Visual',\n",
       "   'and',\n",
       "   'Multimodal',\n",
       "   'Interaction',\n",
       "   'in',\n",
       "   'NCL',\n",
       "   'Authoring',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Paulo',\n",
       "   'Victor',\n",
       "   'Borges',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Pontifical',\n",
       "   'Catholic',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Rio',\n",
       "   'de',\n",
       "   'Janeiro',\n",
       "   'Rio',\n",
       "   'de',\n",
       "   'Janeiro',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   'pvborges@telemidia.puc-rio.br',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Daniel',\n",
       "   'de',\n",
       "   'S.',\n",
       "   'Moraes',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Pontifical',\n",
       "   'Catholic',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Rio',\n",
       "   'de',\n",
       "   'Janeiro',\n",
       "   'Rio',\n",
       "   'de',\n",
       "   'Janeiro',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   'danielmoraes@telemidia.puc-rio.br',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Joel',\n",
       "   'dos',\n",
       "   'Santos',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'CEFET',\n",
       "   '-',\n",
       "   'RJ',\n",
       "   'Rio',\n",
       "   'de',\n",
       "   'Janeiro',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   'jsantos@eic.cefet-rj.br',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Débora',\n",
       "   'C',\n",
       "   'Muchaluat',\n",
       "   '-',\n",
       "   'Saade',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'MídiaCom',\n",
       "   'Lab',\n",
       "   'Fluminense',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'Nitéroi',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   'debora@midiacom.uff.br',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'ABSTRACT',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'This',\n",
       "   'paper',\n",
       "   'introduces',\n",
       "   'two',\n",
       "   'innovative',\n",
       "   'tools',\n",
       "   'for',\n",
       "   'enhancing',\n",
       "   'interactive',\n",
       "   'multimedia',\n",
       "   'authoring',\n",
       "   'using',\n",
       "   'the',\n",
       "   'Nested',\n",
       "   'Context',\n",
       "   'Language',\n",
       "   '\\n',\n",
       "   '(',\n",
       "   'NCL',\n",
       "   '):',\n",
       "   '(',\n",
       "   'i',\n",
       "   ')',\n",
       "   'a',\n",
       "   'visual',\n",
       "   'extension',\n",
       "   'that',\n",
       "   'supports',\n",
       "   'more',\n",
       "   'traditional',\n",
       "   'interactions',\n",
       "   'with',\n",
       "   'mouse',\n",
       "   'and',\n",
       "   'keyboard',\n",
       "   'and',\n",
       "   '(',\n",
       "   'ii',\n",
       "   ')',\n",
       "   'a',\n",
       "   'multimodal',\n",
       "   'extension',\n",
       "   '\\n',\n",
       "   'that',\n",
       "   'incorporates',\n",
       "   'gesture',\n",
       "   'recognition',\n",
       "   'and',\n",
       "   'voice',\n",
       "   'commands',\n",
       "   '.',\n",
       "   'These',\n",
       "   '\\n',\n",
       "   'tools',\n",
       "   'were',\n",
       "   'implemented',\n",
       "   'as',\n",
       "   'Visual',\n",
       "   'Studio',\n",
       "   'Code',\n",
       "   'extensions',\n",
       "   'and',\n",
       "   'aim',\n",
       "   '\\n',\n",
       "   'to',\n",
       "   'streamline',\n",
       "   'the',\n",
       "   'editing',\n",
       "   'process',\n",
       "   ',',\n",
       "   'making',\n",
       "   'it',\n",
       "   'more',\n",
       "   'intuitive',\n",
       "   'and',\n",
       "   'accessible',\n",
       "   '.',\n",
       "   'We',\n",
       "   'present',\n",
       "   'an',\n",
       "   'evaluation',\n",
       "   'of',\n",
       "   'the',\n",
       "   'usability',\n",
       "   'and',\n",
       "   'acceptance',\n",
       "   '\\n',\n",
       "   'of',\n",
       "   'both',\n",
       "   'tools',\n",
       "   'with',\n",
       "   'developers',\n",
       "   'in',\n",
       "   'an',\n",
       "   'experiment',\n",
       "   'with',\n",
       "   'three',\n",
       "   'tasks',\n",
       "   '\\n',\n",
       "   'for',\n",
       "   'creating',\n",
       "   'and',\n",
       "   'manipulating',\n",
       "   'spatial',\n",
       "   'regions',\n",
       "   'in',\n",
       "   'hypermedia',\n",
       "   'documents',\n",
       "   '.',\n",
       "   'By',\n",
       "   'exploring',\n",
       "   'the',\n",
       "   'potential',\n",
       "   'of',\n",
       "   'multimodal',\n",
       "   'interfaces',\n",
       "   ',',\n",
       "   'this',\n",
       "   '\\n',\n",
       "   'work',\n",
       "   'sets',\n",
       "   'the',\n",
       "   'stage',\n",
       "   'for',\n",
       "   'more',\n",
       "   'efficient',\n",
       "   'and',\n",
       "   'user',\n",
       "   '-',\n",
       "   'friendly',\n",
       "   'document',\n",
       "   '\\n',\n",
       "   'editing',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'KEYWORDS',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Authoring',\n",
       "   ',',\n",
       "   'LLMs',\n",
       "   ',',\n",
       "   'NCL',\n",
       "   ',',\n",
       "   'Code',\n",
       "   'Generation',\n",
       "   ',',\n",
       "   'Visual',\n",
       "   'Studio',\n",
       "   'Code',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   '1',\n",
       "   'INTRODUCTION',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Digital',\n",
       "   'TV',\n",
       "   '(',\n",
       "   'DTV',\n",
       "   ')',\n",
       "   'applications',\n",
       "   '[',\n",
       "   '26',\n",
       "   ']',\n",
       "   'have',\n",
       "   'revolutionized',\n",
       "   'content',\n",
       "   'delivery',\n",
       "   'and',\n",
       "   'consumption',\n",
       "   ',',\n",
       "   'offering',\n",
       "   'a',\n",
       "   'rich',\n",
       "   'multimedia',\n",
       "   'experience',\n",
       "   'that',\n",
       "   '\\n',\n",
       "   'integrates',\n",
       "   'text',\n",
       "   ',',\n",
       "   'images',\n",
       "   ',',\n",
       "   'audio',\n",
       "   ',',\n",
       "   'and',\n",
       "   'video',\n",
       "   '.',\n",
       "   'These',\n",
       "   'applications',\n",
       "   'extend',\n",
       "   '\\n',\n",
       "   'beyond',\n",
       "   'passive',\n",
       "   'viewing',\n",
       "   'by',\n",
       "   'enabling',\n",
       "   'interactivity',\n",
       "   'and',\n",
       "   'creating',\n",
       "   'a',\n",
       "   '\\n',\n",
       "   'more',\n",
       "   'engaging',\n",
       "   'and',\n",
       "   'immersive',\n",
       "   'environment',\n",
       "   'for',\n",
       "   'the',\n",
       "   'viewer',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'DTV',\n",
       "   'applications',\n",
       "   '(',\n",
       "   'as',\n",
       "   'well',\n",
       "   'as',\n",
       "   'other',\n",
       "   'interactive',\n",
       "   'multimedia',\n",
       "   'applications',\n",
       "   ')',\n",
       "   'can',\n",
       "   'be',\n",
       "   'developed',\n",
       "   'using',\n",
       "   'structured',\n",
       "   'content',\n",
       "   ',',\n",
       "   'known',\n",
       "   'as',\n",
       "   '*',\n",
       "   'mul-',\n",
       "   '*',\n",
       "   '\\n',\n",
       "   '*',\n",
       "   'timedia',\n",
       "   '/',\n",
       "   'hypermedia',\n",
       "   'documents',\n",
       "   '*',\n",
       "   '.',\n",
       "   'The',\n",
       "   'structure',\n",
       "   'of',\n",
       "   'these',\n",
       "   'documents',\n",
       "   '\\n',\n",
       "   'has',\n",
       "   'been',\n",
       "   'the',\n",
       "   'goal',\n",
       "   'of',\n",
       "   'standardization',\n",
       "   'efforts',\n",
       "   'in',\n",
       "   'many',\n",
       "   'instances',\n",
       "   ',',\n",
       "   'including',\n",
       "   'the',\n",
       "   'definition',\n",
       "   'of',\n",
       "   'declarative',\n",
       "   'languages',\n",
       "   'like',\n",
       "   'the',\n",
       "   'NCL',\n",
       "   '(',\n",
       "   'Nested',\n",
       "   '\\n',\n",
       "   'Context',\n",
       "   'Language',\n",
       "   ')',\n",
       "   '[',\n",
       "   '28',\n",
       "   ']',\n",
       "   ',',\n",
       "   'a',\n",
       "   'language',\n",
       "   'based',\n",
       "   'on',\n",
       "   'the',\n",
       "   'NCM',\n",
       "   '(',\n",
       "   '*',\n",
       "   'Nested',\n",
       "   'Con-',\n",
       "   '*',\n",
       "   '\\n',\n",
       "   '*',\n",
       "   'text',\n",
       "   'Model',\n",
       "   '*',\n",
       "   ')',\n",
       "   ',',\n",
       "   'which',\n",
       "   'facilitates',\n",
       "   'the',\n",
       "   'logical',\n",
       "   'structuring',\n",
       "   'of',\n",
       "   'hypermedia',\n",
       "   '\\n',\n",
       "   'documents',\n",
       "   'through',\n",
       "   'compositions',\n",
       "   'and',\n",
       "   'specifies',\n",
       "   'spatio',\n",
       "   '-',\n",
       "   'temporal',\n",
       "   '\\n',\n",
       "   'relationships',\n",
       "   'using',\n",
       "   'connectors',\n",
       "   'and',\n",
       "   'links',\n",
       "   'within',\n",
       "   'an',\n",
       "   'event',\n",
       "   '-',\n",
       "   'based',\n",
       "   '\\n',\n",
       "   'paradigm',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Current',\n",
       "   'digital',\n",
       "   'TV',\n",
       "   'scenarios',\n",
       "   'require',\n",
       "   'simple',\n",
       "   'and',\n",
       "   'quick',\n",
       "   'methodologies',\n",
       "   'to',\n",
       "   'create',\n",
       "   'interactive',\n",
       "   'multimedia',\n",
       "   'applications',\n",
       "   '.',\n",
       "   'As',\n",
       "   'in',\n",
       "   'other',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   ':',\n",
       "   'Proceedings',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Web',\n",
       "   '(',\n",
       "   'WebMe',\n",
       "   '\\n',\n",
       "   'dia’2024',\n",
       "   ')',\n",
       "   '.',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '.',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   ':',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   ',',\n",
       "   '2024',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'SBC',\n",
       "   '–',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '-',\n",
       "   '2753',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Sérgio',\n",
       "   'Colcher',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Pontifical',\n",
       "   'Catholic',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Rio',\n",
       "   'de',\n",
       "   'Janeiro',\n",
       "   'Rio',\n",
       "   'de',\n",
       "   'Janeiro',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   'colcher@inf.puc-rio.br',\n",
       "   '\\n\\n',\n",
       "   'scenarios',\n",
       "   ',',\n",
       "   'there',\n",
       "   'is',\n",
       "   'a',\n",
       "   'growing',\n",
       "   'need',\n",
       "   'for',\n",
       "   'functionalities',\n",
       "   'that',\n",
       "   'can',\n",
       "   'assist',\n",
       "   '\\n',\n",
       "   'programmers',\n",
       "   'in',\n",
       "   'developing',\n",
       "   'these',\n",
       "   'applications',\n",
       "   'efficiently',\n",
       "   '.',\n",
       "   'Thus',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'authors',\n",
       "   'with',\n",
       "   'varying',\n",
       "   'skills',\n",
       "   'might',\n",
       "   'be',\n",
       "   'able',\n",
       "   'to',\n",
       "   'create',\n",
       "   'correct',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'functional',\n",
       "   'applications',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Therefore',\n",
       "   ',',\n",
       "   'creating',\n",
       "   'and',\n",
       "   'editing',\n",
       "   'multimedia',\n",
       "   'content',\n",
       "   ',',\n",
       "   'including',\n",
       "   '\\n',\n",
       "   'DTV',\n",
       "   'applications',\n",
       "   ',',\n",
       "   'require',\n",
       "   'precise',\n",
       "   'and',\n",
       "   'efficient',\n",
       "   'tools',\n",
       "   'that',\n",
       "   'can',\n",
       "   'effectively',\n",
       "   'manage',\n",
       "   'various',\n",
       "   'elements',\n",
       "   'within',\n",
       "   'a',\n",
       "   'digital',\n",
       "   'interface',\n",
       "   'while',\n",
       "   'also',\n",
       "   '\\n',\n",
       "   'maintaining',\n",
       "   'ease',\n",
       "   'of',\n",
       "   'use',\n",
       "   '.',\n",
       "   'Traditional',\n",
       "   'tools',\n",
       "   'that',\n",
       "   'have',\n",
       "   'been',\n",
       "   'relying',\n",
       "   'on',\n",
       "   '\\n',\n",
       "   'mouse',\n",
       "   'and',\n",
       "   'keyboard',\n",
       "   'interactions',\n",
       "   'have',\n",
       "   'proven',\n",
       "   'their',\n",
       "   'effectiveness',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'but',\n",
       "   'there',\n",
       "   'is',\n",
       "   'still',\n",
       "   'considerable',\n",
       "   'room',\n",
       "   'for',\n",
       "   'improvement',\n",
       "   '.',\n",
       "   'Natural',\n",
       "   'user',\n",
       "   '\\n',\n",
       "   'interfaces',\n",
       "   '(',\n",
       "   'NUIs',\n",
       "   ')',\n",
       "   '[',\n",
       "   '14',\n",
       "   ',',\n",
       "   '17',\n",
       "   ']',\n",
       "   ',',\n",
       "   'such',\n",
       "   'as',\n",
       "   'body',\n",
       "   'movements',\n",
       "   'and',\n",
       "   'gestures',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'have',\n",
       "   'been',\n",
       "   'used',\n",
       "   'in',\n",
       "   'many',\n",
       "   'settings',\n",
       "   'to',\n",
       "   'operate',\n",
       "   'machines',\n",
       "   ',',\n",
       "   'communicate',\n",
       "   '\\n',\n",
       "   'with',\n",
       "   'intelligent',\n",
       "   'environments',\n",
       "   ',',\n",
       "   'and',\n",
       "   'control',\n",
       "   'smart',\n",
       "   'home',\n",
       "   'appliances',\n",
       "   '\\n\\n',\n",
       "   '[',\n",
       "   '5',\n",
       "   ']',\n",
       "   'and',\n",
       "   'could',\n",
       "   'also',\n",
       "   'be',\n",
       "   'applied',\n",
       "   'in',\n",
       "   'a',\n",
       "   'multimedia',\n",
       "   'context',\n",
       "   'to',\n",
       "   'create',\n",
       "   'more',\n",
       "   '\\n',\n",
       "   'intuitive',\n",
       "   'and',\n",
       "   'engaging',\n",
       "   'authoring',\n",
       "   'experiences',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Large',\n",
       "   'Language',\n",
       "   'Models',\n",
       "   '(',\n",
       "   'LLMs',\n",
       "   ')',\n",
       "   ',',\n",
       "   'on',\n",
       "   'their',\n",
       "   'turn',\n",
       "   ',',\n",
       "   'have',\n",
       "   'recently',\n",
       "   '\\n',\n",
       "   'revolutionized',\n",
       "   'various',\n",
       "   'fields',\n",
       "   ',',\n",
       "   'enabling',\n",
       "   'their',\n",
       "   'use',\n",
       "   'in',\n",
       "   'developing',\n",
       "   'a',\n",
       "   '\\n',\n",
       "   'wide',\n",
       "   'range',\n",
       "   'of',\n",
       "   'applications',\n",
       "   ',',\n",
       "   'such',\n",
       "   'as',\n",
       "   'chatbots',\n",
       "   '(',\n",
       "   'like',\n",
       "   'chatGPT',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'Gemini',\n",
       "   ')',\n",
       "   'that',\n",
       "   'can',\n",
       "   'handle',\n",
       "   'queries',\n",
       "   'in',\n",
       "   'different',\n",
       "   'contexts',\n",
       "   '.',\n",
       "   'These',\n",
       "   'models',\n",
       "   '\\n',\n",
       "   'have',\n",
       "   'also',\n",
       "   'shown',\n",
       "   'good',\n",
       "   'results',\n",
       "   'in',\n",
       "   'tasks',\n",
       "   'related',\n",
       "   'to',\n",
       "   'programming',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'code',\n",
       "   'synthesis',\n",
       "   '[',\n",
       "   '7',\n",
       "   ',',\n",
       "   '8',\n",
       "   ',',\n",
       "   '19',\n",
       "   ',',\n",
       "   '23',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'In',\n",
       "   'this',\n",
       "   'sense',\n",
       "   ',',\n",
       "   'this',\n",
       "   'work',\n",
       "   'proposes',\n",
       "   'two',\n",
       "   'distinct',\n",
       "   'tools',\n",
       "   'to',\n",
       "   'help',\n",
       "   'improve',\n",
       "   'the',\n",
       "   'development',\n",
       "   'of',\n",
       "   'interactive',\n",
       "   'applications',\n",
       "   'for',\n",
       "   'Digital',\n",
       "   'TV',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'specifically',\n",
       "   'in',\n",
       "   'creating',\n",
       "   'the',\n",
       "   'layouts',\n",
       "   'defined',\n",
       "   'by',\n",
       "   'the',\n",
       "   'region',\n",
       "   'base',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'a',\n",
       "   'document',\n",
       "   '.',\n",
       "   'The',\n",
       "   'first',\n",
       "   'is',\n",
       "   'a',\n",
       "   'visual',\n",
       "   'tool',\n",
       "   'that',\n",
       "   'allows',\n",
       "   'the',\n",
       "   'developer',\n",
       "   'to',\n",
       "   '\\n',\n",
       "   'visualize',\n",
       "   'the',\n",
       "   'regions',\n",
       "   'defined',\n",
       "   'in',\n",
       "   'a',\n",
       "   'document',\n",
       "   '’s',\n",
       "   'region',\n",
       "   'base',\n",
       "   'and',\n",
       "   'relies',\n",
       "   '\\n',\n",
       "   'on',\n",
       "   'traditional',\n",
       "   'mouse',\n",
       "   'and',\n",
       "   'keyboard',\n",
       "   'interactions',\n",
       "   'to',\n",
       "   'create',\n",
       "   'and',\n",
       "   'manipulate',\n",
       "   'regions',\n",
       "   'graphically',\n",
       "   '.',\n",
       "   'The',\n",
       "   'second',\n",
       "   'tool',\n",
       "   'incorporates',\n",
       "   'gesture',\n",
       "   '\\n',\n",
       "   'recognition',\n",
       "   'and',\n",
       "   'voice',\n",
       "   'commands',\n",
       "   ',',\n",
       "   'processed',\n",
       "   'by',\n",
       "   'a',\n",
       "   'language',\n",
       "   'model',\n",
       "   'to',\n",
       "   '\\n',\n",
       "   'generate',\n",
       "   'editing',\n",
       "   'actions',\n",
       "   '(',\n",
       "   'in',\n",
       "   'addition',\n",
       "   'to',\n",
       "   'the',\n",
       "   'visualization',\n",
       "   'of',\n",
       "   'the',\n",
       "   'created',\n",
       "   'regions',\n",
       "   ')',\n",
       "   '.',\n",
       "   'These',\n",
       "   'tools',\n",
       "   'are',\n",
       "   'implemented',\n",
       "   'as',\n",
       "   'Visual',\n",
       "   'Studio',\n",
       "   'Code',\n",
       "   '\\n',\n",
       "   '(',\n",
       "   'VS',\n",
       "   'Code',\n",
       "   ')',\n",
       "   'extensions',\n",
       "   '[',\n",
       "   '9',\n",
       "   ']',\n",
       "   ',',\n",
       "   'taking',\n",
       "   'advantage',\n",
       "   'of',\n",
       "   'its',\n",
       "   'robust',\n",
       "   'feature',\n",
       "   'set',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'versatility',\n",
       "   ',',\n",
       "   'and',\n",
       "   'adaptability',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'We',\n",
       "   'also',\n",
       "   'present',\n",
       "   'an',\n",
       "   'evaluation',\n",
       "   'of',\n",
       "   'the',\n",
       "   'usability',\n",
       "   'and',\n",
       "   'acceptance',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'both',\n",
       "   'tools',\n",
       "   'with',\n",
       "   'developers',\n",
       "   'in',\n",
       "   'an',\n",
       "   'experiment',\n",
       "   'involving',\n",
       "   'tasks',\n",
       "   'of',\n",
       "   'creation',\n",
       "   'and',\n",
       "   'manipulation',\n",
       "   'of',\n",
       "   'regions',\n",
       "   '.',\n",
       "   'This',\n",
       "   'evaluation',\n",
       "   'was',\n",
       "   'conducted',\n",
       "   '\\n',\n",
       "   'through',\n",
       "   'a',\n",
       "   'custom',\n",
       "   'usability',\n",
       "   'questionnaire',\n",
       "   'that',\n",
       "   'was',\n",
       "   'inspired',\n",
       "   'by',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'adapted',\n",
       "   'from',\n",
       "   'the',\n",
       "   'principles',\n",
       "   'of',\n",
       "   'the',\n",
       "   'System',\n",
       "   'Usability',\n",
       "   'Scale',\n",
       "   '(',\n",
       "   'SUS',\n",
       "   ')',\n",
       "   '\\n\\n',\n",
       "   '[',\n",
       "   '4',\n",
       "   ',',\n",
       "   '6',\n",
       "   ',',\n",
       "   '18',\n",
       "   ']',\n",
       "   'to',\n",
       "   'fit',\n",
       "   'our',\n",
       "   'specific',\n",
       "   'case',\n",
       "   '.',\n",
       "   ...],\n",
       "  'pos_tagger': '',\n",
       "  'lema': ['explore',\n",
       "   'visual',\n",
       "   'and',\n",
       "   'Multimodal',\n",
       "   'Interaction',\n",
       "   'in',\n",
       "   'NCL',\n",
       "   'Authoring',\n",
       "   'Paulo',\n",
       "   'Victor',\n",
       "   'Borges',\n",
       "   'Pontifical',\n",
       "   'Catholic',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Rio',\n",
       "   'de',\n",
       "   'Janeiro',\n",
       "   'Rio',\n",
       "   'de',\n",
       "   'Janeiro',\n",
       "   'Brazil',\n",
       "   'pvborges@telemidia.puc-rio.br',\n",
       "   'Daniel',\n",
       "   'de',\n",
       "   'S.',\n",
       "   'Moraes',\n",
       "   'Pontifical',\n",
       "   'Catholic',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Rio',\n",
       "   'de',\n",
       "   'Janeiro',\n",
       "   'Rio',\n",
       "   'de',\n",
       "   'Janeiro',\n",
       "   'Brazil',\n",
       "   'danielmoraes@telemidia.puc-rio.br',\n",
       "   'Joel',\n",
       "   'do',\n",
       "   'santos',\n",
       "   'CEFET',\n",
       "   'RJ',\n",
       "   'Rio',\n",
       "   'de',\n",
       "   'Janeiro',\n",
       "   'Brazil',\n",
       "   'jsantos@eic.cefet-rj.br',\n",
       "   'Débora',\n",
       "   'C',\n",
       "   'Muchaluat',\n",
       "   'Saade',\n",
       "   'MídiaCom',\n",
       "   'Lab',\n",
       "   'Fluminense',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'Nitéroi',\n",
       "   'Brazil',\n",
       "   'debora@midiacom.uff.br',\n",
       "   'ABSTRACT',\n",
       "   'this',\n",
       "   'paper',\n",
       "   'introduce',\n",
       "   'two',\n",
       "   'innovative',\n",
       "   'tool',\n",
       "   'for',\n",
       "   'enhance',\n",
       "   'interactive',\n",
       "   'multimedia',\n",
       "   'author',\n",
       "   'use',\n",
       "   'the',\n",
       "   'Nested',\n",
       "   'Context',\n",
       "   'Language',\n",
       "   'NCL',\n",
       "   'i',\n",
       "   'a',\n",
       "   'visual',\n",
       "   'extension',\n",
       "   'that',\n",
       "   'support',\n",
       "   'more',\n",
       "   'traditional',\n",
       "   'interaction',\n",
       "   'with',\n",
       "   'mouse',\n",
       "   'and',\n",
       "   'keyboard',\n",
       "   'and',\n",
       "   'ii',\n",
       "   'a',\n",
       "   'multimodal',\n",
       "   'extension',\n",
       "   'that',\n",
       "   'incorporate',\n",
       "   'gesture',\n",
       "   'recognition',\n",
       "   'and',\n",
       "   'voice',\n",
       "   'command',\n",
       "   'these',\n",
       "   'tool',\n",
       "   'be',\n",
       "   'implement',\n",
       "   'as',\n",
       "   'Visual',\n",
       "   'Studio',\n",
       "   'Code',\n",
       "   'extension',\n",
       "   'and',\n",
       "   'aim',\n",
       "   'to',\n",
       "   'streamline',\n",
       "   'the',\n",
       "   'editing',\n",
       "   'process',\n",
       "   'make',\n",
       "   'it',\n",
       "   'more',\n",
       "   'intuitive',\n",
       "   'and',\n",
       "   'accessible',\n",
       "   'we',\n",
       "   'present',\n",
       "   'an',\n",
       "   'evaluation',\n",
       "   'of',\n",
       "   'the',\n",
       "   'usability',\n",
       "   'and',\n",
       "   'acceptance',\n",
       "   'of',\n",
       "   'both',\n",
       "   'tool',\n",
       "   'with',\n",
       "   'developer',\n",
       "   'in',\n",
       "   'an',\n",
       "   'experiment',\n",
       "   'with',\n",
       "   'three',\n",
       "   'task',\n",
       "   'for',\n",
       "   'create',\n",
       "   'and',\n",
       "   'manipulate',\n",
       "   'spatial',\n",
       "   'region',\n",
       "   'in',\n",
       "   'hypermedia',\n",
       "   'document',\n",
       "   'by',\n",
       "   'explore',\n",
       "   'the',\n",
       "   'potential',\n",
       "   'of',\n",
       "   'multimodal',\n",
       "   'interface',\n",
       "   'this',\n",
       "   'work',\n",
       "   'set',\n",
       "   'the',\n",
       "   'stage',\n",
       "   'for',\n",
       "   'more',\n",
       "   'efficient',\n",
       "   'and',\n",
       "   'user',\n",
       "   'friendly',\n",
       "   'document',\n",
       "   'editing',\n",
       "   'keyword',\n",
       "   'authoring',\n",
       "   'LLMs',\n",
       "   'NCL',\n",
       "   'Code',\n",
       "   'Generation',\n",
       "   'Visual',\n",
       "   'Studio',\n",
       "   'Code',\n",
       "   '1',\n",
       "   'introduction',\n",
       "   'Digital',\n",
       "   'TV',\n",
       "   'DTV',\n",
       "   'application',\n",
       "   '26',\n",
       "   'have',\n",
       "   'revolutionize',\n",
       "   'content',\n",
       "   'delivery',\n",
       "   'and',\n",
       "   'consumption',\n",
       "   'offer',\n",
       "   'a',\n",
       "   'rich',\n",
       "   'multimedia',\n",
       "   'experience',\n",
       "   'that',\n",
       "   'integrate',\n",
       "   'text',\n",
       "   'image',\n",
       "   'audio',\n",
       "   'and',\n",
       "   'video',\n",
       "   'these',\n",
       "   'application',\n",
       "   'extend',\n",
       "   'beyond',\n",
       "   'passive',\n",
       "   'viewing',\n",
       "   'by',\n",
       "   'enable',\n",
       "   'interactivity',\n",
       "   'and',\n",
       "   'create',\n",
       "   'a',\n",
       "   'more',\n",
       "   'engaging',\n",
       "   'and',\n",
       "   'immersive',\n",
       "   'environment',\n",
       "   'for',\n",
       "   'the',\n",
       "   'viewer',\n",
       "   'DTV',\n",
       "   'application',\n",
       "   'as',\n",
       "   'well',\n",
       "   'as',\n",
       "   'other',\n",
       "   'interactive',\n",
       "   'multimedia',\n",
       "   'application',\n",
       "   'can',\n",
       "   'be',\n",
       "   'develop',\n",
       "   'use',\n",
       "   'structured',\n",
       "   'content',\n",
       "   'know',\n",
       "   'as',\n",
       "   'mul-',\n",
       "   'timedia',\n",
       "   'hypermedia',\n",
       "   'document',\n",
       "   'the',\n",
       "   'structure',\n",
       "   'of',\n",
       "   'these',\n",
       "   'document',\n",
       "   'have',\n",
       "   'be',\n",
       "   'the',\n",
       "   'goal',\n",
       "   'of',\n",
       "   'standardization',\n",
       "   'effort',\n",
       "   'in',\n",
       "   'many',\n",
       "   'instance',\n",
       "   'include',\n",
       "   'the',\n",
       "   'definition',\n",
       "   'of',\n",
       "   'declarative',\n",
       "   'language',\n",
       "   'like',\n",
       "   'the',\n",
       "   'NCL',\n",
       "   'nest',\n",
       "   'Context',\n",
       "   'Language',\n",
       "   '28',\n",
       "   'a',\n",
       "   'language',\n",
       "   'base',\n",
       "   'on',\n",
       "   'the',\n",
       "   'NCM',\n",
       "   'Nested',\n",
       "   'Con-',\n",
       "   'text',\n",
       "   'Model',\n",
       "   'which',\n",
       "   'facilitate',\n",
       "   'the',\n",
       "   'logical',\n",
       "   'structuring',\n",
       "   'of',\n",
       "   'hypermedia',\n",
       "   'document',\n",
       "   'through',\n",
       "   'composition',\n",
       "   'and',\n",
       "   'specifie',\n",
       "   'spatio',\n",
       "   'temporal',\n",
       "   'relationship',\n",
       "   'use',\n",
       "   'connector',\n",
       "   'and',\n",
       "   'link',\n",
       "   'within',\n",
       "   'an',\n",
       "   'event',\n",
       "   'base',\n",
       "   'paradigm',\n",
       "   'current',\n",
       "   'digital',\n",
       "   'tv',\n",
       "   'scenario',\n",
       "   'require',\n",
       "   'simple',\n",
       "   'and',\n",
       "   'quick',\n",
       "   'methodology',\n",
       "   'to',\n",
       "   'create',\n",
       "   'interactive',\n",
       "   'multimedia',\n",
       "   'application',\n",
       "   'as',\n",
       "   'in',\n",
       "   'other',\n",
       "   'in',\n",
       "   'proceeding',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'web',\n",
       "   'WebMe',\n",
       "   'dia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   '2024',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'sbc',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '2753',\n",
       "   'Sérgio',\n",
       "   'Colcher',\n",
       "   'Pontifical',\n",
       "   'Catholic',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Rio',\n",
       "   'de',\n",
       "   'Janeiro',\n",
       "   'Rio',\n",
       "   'de',\n",
       "   'Janeiro',\n",
       "   'Brazil',\n",
       "   'colcher@inf.puc-rio.br',\n",
       "   'scenario',\n",
       "   'there',\n",
       "   'be',\n",
       "   'a',\n",
       "   'grow',\n",
       "   'need',\n",
       "   'for',\n",
       "   'functionality',\n",
       "   'that',\n",
       "   'can',\n",
       "   'assist',\n",
       "   'programmer',\n",
       "   'in',\n",
       "   'develop',\n",
       "   'these',\n",
       "   'application',\n",
       "   'efficiently',\n",
       "   'thus',\n",
       "   'author',\n",
       "   'with',\n",
       "   'varying',\n",
       "   'skill',\n",
       "   'might',\n",
       "   'be',\n",
       "   'able',\n",
       "   'to',\n",
       "   'create',\n",
       "   'correct',\n",
       "   'and',\n",
       "   'functional',\n",
       "   'application',\n",
       "   'therefore',\n",
       "   'create',\n",
       "   'and',\n",
       "   'edit',\n",
       "   'multimedia',\n",
       "   'content',\n",
       "   'include',\n",
       "   'DTV',\n",
       "   'application',\n",
       "   'require',\n",
       "   'precise',\n",
       "   'and',\n",
       "   'efficient',\n",
       "   'tool',\n",
       "   'that',\n",
       "   'can',\n",
       "   'effectively',\n",
       "   'manage',\n",
       "   'various',\n",
       "   'element',\n",
       "   'within',\n",
       "   'a',\n",
       "   'digital',\n",
       "   'interface',\n",
       "   'while',\n",
       "   'also',\n",
       "   'maintain',\n",
       "   'ease',\n",
       "   'of',\n",
       "   'use',\n",
       "   'traditional',\n",
       "   'tool',\n",
       "   'that',\n",
       "   'have',\n",
       "   'be',\n",
       "   'rely',\n",
       "   'on',\n",
       "   'mouse',\n",
       "   'and',\n",
       "   'keyboard',\n",
       "   'interaction',\n",
       "   'have',\n",
       "   'prove',\n",
       "   'their',\n",
       "   'effectiveness',\n",
       "   'but',\n",
       "   'there',\n",
       "   'be',\n",
       "   'still',\n",
       "   'considerable',\n",
       "   'room',\n",
       "   'for',\n",
       "   'improvement',\n",
       "   'natural',\n",
       "   'user',\n",
       "   'interface',\n",
       "   'NUIs',\n",
       "   '14',\n",
       "   '17',\n",
       "   'such',\n",
       "   'as',\n",
       "   'body',\n",
       "   'movement',\n",
       "   'and',\n",
       "   'gesture',\n",
       "   'have',\n",
       "   'be',\n",
       "   'use',\n",
       "   'in',\n",
       "   'many',\n",
       "   'setting',\n",
       "   'to',\n",
       "   'operate',\n",
       "   'machine',\n",
       "   'communicate',\n",
       "   'with',\n",
       "   'intelligent',\n",
       "   'environment',\n",
       "   'and',\n",
       "   'control',\n",
       "   'smart',\n",
       "   'home',\n",
       "   'appliance',\n",
       "   '5',\n",
       "   'and',\n",
       "   'could',\n",
       "   'also',\n",
       "   'be',\n",
       "   'apply',\n",
       "   'in',\n",
       "   'a',\n",
       "   'multimedia',\n",
       "   'context',\n",
       "   'to',\n",
       "   'create',\n",
       "   'more',\n",
       "   'intuitive',\n",
       "   'and',\n",
       "   'engaging',\n",
       "   'author',\n",
       "   'experience',\n",
       "   'large',\n",
       "   'Language',\n",
       "   'Models',\n",
       "   'LLMs',\n",
       "   'on',\n",
       "   'their',\n",
       "   'turn',\n",
       "   'have',\n",
       "   'recently',\n",
       "   'revolutionize',\n",
       "   'various',\n",
       "   'field',\n",
       "   'enable',\n",
       "   'their',\n",
       "   'use',\n",
       "   'in',\n",
       "   'develop',\n",
       "   'a',\n",
       "   'wide',\n",
       "   'range',\n",
       "   'of',\n",
       "   'application',\n",
       "   'such',\n",
       "   'as',\n",
       "   'chatbot',\n",
       "   'like',\n",
       "   'chatGPT',\n",
       "   'and',\n",
       "   'Gemini',\n",
       "   'that',\n",
       "   'can',\n",
       "   'handle',\n",
       "   'query',\n",
       "   'in',\n",
       "   'different',\n",
       "   'contexts',\n",
       "   'these',\n",
       "   'model',\n",
       "   'have',\n",
       "   'also',\n",
       "   'show',\n",
       "   'good',\n",
       "   'result',\n",
       "   'in',\n",
       "   'task',\n",
       "   'relate',\n",
       "   'to',\n",
       "   'program',\n",
       "   'and',\n",
       "   'code',\n",
       "   'synthesis',\n",
       "   '7',\n",
       "   '8',\n",
       "   '19',\n",
       "   '23',\n",
       "   'in',\n",
       "   'this',\n",
       "   'sense',\n",
       "   'this',\n",
       "   'work',\n",
       "   'propose',\n",
       "   'two',\n",
       "   'distinct',\n",
       "   'tool',\n",
       "   'to',\n",
       "   'help',\n",
       "   'improve',\n",
       "   'the',\n",
       "   'development',\n",
       "   'of',\n",
       "   'interactive',\n",
       "   'application',\n",
       "   'for',\n",
       "   'Digital',\n",
       "   'TV',\n",
       "   'specifically',\n",
       "   'in',\n",
       "   'create',\n",
       "   'the',\n",
       "   'layout',\n",
       "   'define',\n",
       "   'by',\n",
       "   'the',\n",
       "   'region',\n",
       "   'base',\n",
       "   'of',\n",
       "   'a',\n",
       "   'document',\n",
       "   'the',\n",
       "   'first',\n",
       "   'be',\n",
       "   'a',\n",
       "   'visual',\n",
       "   'tool',\n",
       "   'that',\n",
       "   'allow',\n",
       "   'the',\n",
       "   'developer',\n",
       "   'to',\n",
       "   'visualize',\n",
       "   'the',\n",
       "   'region',\n",
       "   'define',\n",
       "   'in',\n",
       "   'a',\n",
       "   'document',\n",
       "   '’s',\n",
       "   'region',\n",
       "   'base',\n",
       "   'and',\n",
       "   'rely',\n",
       "   'on',\n",
       "   'traditional',\n",
       "   'mouse',\n",
       "   'and',\n",
       "   'keyboard',\n",
       "   'interaction',\n",
       "   'to',\n",
       "   'create',\n",
       "   'and',\n",
       "   'manipulate',\n",
       "   'region',\n",
       "   'graphically',\n",
       "   'the',\n",
       "   'second',\n",
       "   'tool',\n",
       "   'incorporate',\n",
       "   'gesture',\n",
       "   'recognition',\n",
       "   'and',\n",
       "   'voice',\n",
       "   'command',\n",
       "   'process',\n",
       "   'by',\n",
       "   'a',\n",
       "   'language',\n",
       "   'model',\n",
       "   'to',\n",
       "   'generate',\n",
       "   'editing',\n",
       "   'action',\n",
       "   'in',\n",
       "   'addition',\n",
       "   'to',\n",
       "   'the',\n",
       "   'visualization',\n",
       "   'of',\n",
       "   'the',\n",
       "   'create',\n",
       "   'region',\n",
       "   'these',\n",
       "   'tool',\n",
       "   'be',\n",
       "   'implement',\n",
       "   'as',\n",
       "   'Visual',\n",
       "   'Studio',\n",
       "   'Code',\n",
       "   'VS',\n",
       "   'Code',\n",
       "   'extension',\n",
       "   '9',\n",
       "   'take',\n",
       "   'advantage',\n",
       "   'of',\n",
       "   'its',\n",
       "   'robust',\n",
       "   'feature',\n",
       "   'set',\n",
       "   'versatility',\n",
       "   'and',\n",
       "   'adaptability',\n",
       "   'we',\n",
       "   'also',\n",
       "   'present',\n",
       "   'an',\n",
       "   'evaluation',\n",
       "   'of',\n",
       "   'the',\n",
       "   'usability',\n",
       "   'and',\n",
       "   'acceptance',\n",
       "   'of',\n",
       "   'both',\n",
       "   'tool',\n",
       "   'with',\n",
       "   'developer',\n",
       "   'in',\n",
       "   'an',\n",
       "   'experiment',\n",
       "   'involve',\n",
       "   'task',\n",
       "   'of',\n",
       "   'creation',\n",
       "   'and',\n",
       "   'manipulation',\n",
       "   'of',\n",
       "   'region',\n",
       "   'this',\n",
       "   'evaluation',\n",
       "   'be',\n",
       "   'conduct',\n",
       "   'through',\n",
       "   'a',\n",
       "   'custom',\n",
       "   'usability',\n",
       "   'questionnaire',\n",
       "   'that',\n",
       "   'be',\n",
       "   'inspire',\n",
       "   'by',\n",
       "   'and',\n",
       "   'adapt',\n",
       "   'from',\n",
       "   'the',\n",
       "   'principle',\n",
       "   'of',\n",
       "   'the',\n",
       "   'System',\n",
       "   'Usability',\n",
       "   'Scale',\n",
       "   'SUS',\n",
       "   '4',\n",
       "   '6',\n",
       "   '18',\n",
       "   'to',\n",
       "   'fit',\n",
       "   'our',\n",
       "   'specific',\n",
       "   'case',\n",
       "   '153',\n",
       "   'WebMedia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Borges',\n",
       "   'et',\n",
       "   'al',\n",
       "   'the',\n",
       "   'remainder',\n",
       "   'of',\n",
       "   'this',\n",
       "   'paper',\n",
       "   'be',\n",
       "   'structure',\n",
       "   'as',\n",
       "   'follow',\n",
       "   'section',\n",
       "   '2',\n",
       "   'provide',\n",
       "   'a',\n",
       "   'review',\n",
       "   'of',\n",
       "   'related',\n",
       "   'work',\n",
       "   'section',\n",
       "   '3',\n",
       "   'describe',\n",
       "   'the',\n",
       "   'methodology',\n",
       "   'and',\n",
       "   'implementation',\n",
       "   'of',\n",
       "   'the',\n",
       "   'tool',\n",
       "   'section',\n",
       "   '4',\n",
       "   'detail',\n",
       "   'the',\n",
       "   'experimental',\n",
       "   'setup',\n",
       "   'include',\n",
       "   'the',\n",
       "   'task',\n",
       "   'and',\n",
       "   'participant',\n",
       "   'section',\n",
       "   '5',\n",
       "   'present',\n",
       "   'the',\n",
       "   'result',\n",
       "   'of',\n",
       "   'the',\n",
       "   'experiment',\n",
       "   'and',\n",
       "   'the',\n",
       "   'usability',\n",
       "   'questionnaire',\n",
       "   'finally',\n",
       "   'Section',\n",
       "   '6',\n",
       "   'discuss',\n",
       "   'the',\n",
       "   'finding',\n",
       "   'implication',\n",
       "   'and',\n",
       "   'potential',\n",
       "   'future',\n",
       "   'work',\n",
       "   '2',\n",
       "   'relate',\n",
       "   'work',\n",
       "   'the',\n",
       "   'development',\n",
       "   'of',\n",
       "   'interactive',\n",
       "   'multimedia',\n",
       "   'application',\n",
       "   'for',\n",
       "   'digital',\n",
       "   'tv',\n",
       "   'have',\n",
       "   'be',\n",
       "   'significantly',\n",
       "   'advance',\n",
       "   'by',\n",
       "   'author',\n",
       "   'tool',\n",
       "   'that',\n",
       "   'facilitate',\n",
       "   'both',\n",
       "   'textual',\n",
       "   'and',\n",
       "   'visual',\n",
       "   'authoring',\n",
       "   'approach',\n",
       "   'for',\n",
       "   'example',\n",
       "   'NEXT',\n",
       "   'NCL',\n",
       "   'Editor',\n",
       "   'Supporting',\n",
       "   'XTemplate',\n",
       "   '24',\n",
       "   'be',\n",
       "   'a',\n",
       "   'graphical',\n",
       "   'editor',\n",
       "   'develop',\n",
       "   'to',\n",
       "   'create',\n",
       "   'NCL',\n",
       "   'document',\n",
       "   'use',\n",
       "   'hypermedia',\n",
       "   'composite',\n",
       "   'template',\n",
       "   'specify',\n",
       "   'in',\n",
       "   'the',\n",
       "   'XTemplate',\n",
       "   '3.0',\n",
       "   'language',\n",
       "   '11',\n",
       "   'make',\n",
       "   'the',\n",
       "   'creation',\n",
       "   'of',\n",
       "   'interactive',\n",
       "   'digital',\n",
       "   'tv',\n",
       "   'application',\n",
       "   'more',\n",
       "   'accessible',\n",
       "   'to',\n",
       "   'author',\n",
       "   'without',\n",
       "   'in',\n",
       "   'depth',\n",
       "   'NCL',\n",
       "   'knowledge',\n",
       "   'NEXT',\n",
       "   'offer',\n",
       "   'functionality',\n",
       "   'such',\n",
       "   'as',\n",
       "   'create',\n",
       "   'and',\n",
       "   'edit',\n",
       "   'NCL',\n",
       "   'document',\n",
       "   'in',\n",
       "   'different',\n",
       "   'view',\n",
       "   'utilize',\n",
       "   'an',\n",
       "   'extensible',\n",
       "   'template',\n",
       "   'library',\n",
       "   'that',\n",
       "   'can',\n",
       "   'be',\n",
       "   'adapt',\n",
       "   'to',\n",
       "   'different',\n",
       "   'skill',\n",
       "   'level',\n",
       "   'of',\n",
       "   'author',\n",
       "   'by',\n",
       "   'provide',\n",
       "   'a',\n",
       "   'graphical',\n",
       "   'interface',\n",
       "   'and',\n",
       "   'a',\n",
       "   'set',\n",
       "   'of',\n",
       "   'plugin',\n",
       "   'next',\n",
       "   'simplify',\n",
       "   'the',\n",
       "   'development',\n",
       "   'process',\n",
       "   'and',\n",
       "   'allow',\n",
       "   'author',\n",
       "   'to',\n",
       "   'focus',\n",
       "   'on',\n",
       "   'the',\n",
       "   'content',\n",
       "   'rather',\n",
       "   'than',\n",
       "   'the',\n",
       "   'intricacy',\n",
       "   'of',\n",
       "   'NCL',\n",
       "   'programming',\n",
       "   'this',\n",
       "   'tool',\n",
       "   'represent',\n",
       "   'a',\n",
       "   'significant',\n",
       "   'step',\n",
       "   'forward',\n",
       "   'in',\n",
       "   'enable',\n",
       "   'a',\n",
       "   'broad',\n",
       "   'range',\n",
       "   'of',\n",
       "   'user',\n",
       "   'to',\n",
       "   'contribute',\n",
       "   'to',\n",
       "   'the',\n",
       "   'development',\n",
       "   'of',\n",
       "   'interactive',\n",
       "   'multimedia',\n",
       "   'application',\n",
       "   'enhance',\n",
       "   'both',\n",
       "   'the',\n",
       "   'usability',\n",
       "   'and',\n",
       "   'efficiency',\n",
       "   'of',\n",
       "   'the',\n",
       "   'author',\n",
       "   'process',\n",
       "   'STEVE',\n",
       "   'spatio',\n",
       "   'Temporal',\n",
       "   'View',\n",
       "   'Editor',\n",
       "   '10',\n",
       "   'allow',\n",
       "   'user',\n",
       "   'without',\n",
       "   'prior',\n",
       "   'knowledge',\n",
       "   'of',\n",
       "   'author',\n",
       "   'language',\n",
       "   'to',\n",
       "   'create',\n",
       "   'interactive',\n",
       "   'multimedia',\n",
       "   'application',\n",
       "   'for',\n",
       "   'web',\n",
       "   'and',\n",
       "   'digital',\n",
       "   'tv',\n",
       "   'system',\n",
       "   'present',\n",
       "   'a',\n",
       "   'graphical',\n",
       "   'interface',\n",
       "   'that',\n",
       "   ...]},\n",
       " {'titulo': 'Why Ignore Content? A Guideline for Intrinsic Evaluation of Item Embeddings for Collaborative Filtering',\n",
       "  'informacoes_url': '',\n",
       "  'idioma': 'english',\n",
       "  'storage_key': '../articles/original/english/985-24776-1-10-20240923.pdf',\n",
       "  'author': 'Pedro R. Pires; Bruno B. Rizzi; and Tiago A. Almeida',\n",
       "  'data_publicacao': '11-09-2024',\n",
       "  'resumo': 'With the constant growth in available information and the popularization of technology, recommender systems have to deal with an increasing number of users and items. This leads to two problems in representing items: scalability and sparsity. Therefore, many recommender systems aim to generate low-dimensional dense representations of items. Matrix factorization techniques are popular, but models based on neural embeddings have recently been proposed and are gaining ground in the literature. Their main goal is to learn dense representations with intrinsic meaning. However, most studies proposing embeddings for recommender systems ignore this property and focus only on extrinsic evaluations. This study presents a guideline for assessing the intrinsic quality of matrix factorization and neural-based embedding models for collaborative filtering, comparing the results with a traditional extrinsic evaluation. To enrich the evaluation pipeline, we suggest adapting an intrinsic evaluation task commonly employed in the Natural Language Processing literature, and we propose a novel strategy for evaluating the learned representation compared to a content-based scenario. Finally, every mentioned technique is analyzed over established recommender models, and the results show how vector representations that do not yield good recommendations can still be useful in other tasks that demand intrinsic knowledge, highlighting the potential of this perspective of evaluation. ###',\n",
       "  'keywords': 'embeddings, intrinsic evaluation, qualitative evaluation, recommender systems, similarity tables, intruder detection, autotagging',\n",
       "  'referencias': ['[1] Gediminas Adomavicius and Alexander Tuzhilin. 2005. Toward the Next Generation of Recommender Systems: A Survey of the State-of-the-Art and Possible\\nExtensions. *IEEE Transactions on Knowledge and Data Engineering* 17, 6 (2005),\\n734–749. https://doi.org/10.1109/TKDE.2005.99',\n",
       "   '[2] Oren Barkan and Noam Koenigstein. 2016. Item2Vec: Neural Item Embedding\\nFor Collaborative Filtering. In *IEEE 26th International Workshop on Machine*\\n*Learning for Signal Processing (MLSP 2016)* . IEEE, Vietri sul Mare, Italy, 1–6.\\nhttps://doi.org/10.1109/MLSP.2016.7738886',\n",
       "   '[3] Marco Baroni, Georgiana Dinu, and Germán Kruszewski. 2014. Don’t count,\\npredict! A systematic comparison of context-counting vs. context-predicting\\nsemantic vectors. In *Proceedings of the 52nd Annual Meeting of the Association for*\\n*Computational Linguistics (ACL ‘14)* . Association for Computational Linguistics,\\nBaltimore, MD, USA, 238–247. https://doi.org/10.3115/v1/P14-1023',\n",
       "   '[4] J. Bobadilla, F. Ortega, A. Hernando, and A. Gutiérrez. 2013. Recommender\\nsystems survey. *Knowledge-Based Systems* 46 (2013), 109–132. https://doi.org/10.\\n1016/j.knosys.2013.03.012',\n",
       "   '[5] Hugo Caselles-Duprés, Florian Lesaint, and Jimena Royo-Letelier. 2018. Word2vec\\napplied to recommendation: hyperparameters matter. In *Proceedings of the 12th*\\n*ACM Conference on Recommender Systems (RecSys ‘18)* . Association for Computing Machinery, Vancouver, Canada, 352–356. https://doi.org/10.1145/3240323.\\n3240377',\n",
       "   '[6] Chao Chang, Junming Zhou, Yu Weng, Xiangwei Zeng, Zhengyang Wu, ChangDong Wang, and Yong Tang. 2023. KGTN: Knowledge Graph Transformer\\nNetwork for explainable multi-category item recommendation. *Knowledge-Based*\\n*Systems* 278 (2023), 110854. https://doi.org/10.1016/j.knosys.2023.110854',\n",
       "   '[7] Hao Chen, Zefan Wang, Feiran Huang, Xiao Huang, Yue Xu, Yishi Lin, Peng He,\\nand Zhoujun Li. 2022. Generative Adversarial Framework for Cold-Start Item\\nRecommendation. In *Proceedings of the 45th International ACM SIGIR Conference*\\n*on Research and Development in Information Retrieval (SIGIR ’22)* . Association\\nfor Computing Machinery, Anchorage, AK, USA, 2565–2571. https://doi.org/10.\\n1145/3477495.3531897',\n",
       "   '[8] Gabriel de Souza P. Moreira, Dietmar Jannach, and Adilson Marques da Cunha.\\n2019. On the Importance of News Content Representation in Hybrid Neural\\nSession-based Recommender Systems. *IEEE Access* 7 (2019), 169185–169203.\\nhttps://doi.org/10.1109/ACCESS.2019.2954957',\n",
       "   '[9] Janez Demšar. 2006. Statistical Comparisons of Classifiers over Multiple Data\\nSets. *The Journal of Machine Learning Research* 7 (2006), 1–30. https://doi.org/10.\\n5555/1248547.1248548',\n",
       "   '[10] Chengxin Ding, Zhongying Zhao, Chao Li, Yanwei Yu, and Qingtian Zeng. 2023.\\nSession-based recommendation with hypergraph convolutional networks and\\nsequential information embeddings. *Expert Systems with Applications* 223, 119875\\n(2023), 1–11. https://doi.org/10.1016/j.eswa.2023.119875',\n",
       "   '[11] Douglas Eck, Paul Lamere, Thierry Bertin-Mahieux, and Stephen Green. 2007.\\nAutomatic generation of social tags for music recommendation. In *Proceedings of*\\n*the 20th International Conference on Neural Information Processing Systems (NIPS*\\n*2007)* . Curran Associates Inc., Vancouver, Canada, 385–392. https://doi.org/10.\\n5555/2981562.2981611',\n",
       "   '[12] Manaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi, and Chris Dyer. 2016.\\nProblems With Evaluation of Word Embeddings Using Word Similarity Tasks.\\nIn *Proceedings of the 1st Workshop on Evaluating Vector Space Representations*\\n*for NLP* . Association for Computational Linguistics, Berlin, Germany, 30–35.\\nhttps://doi.org/10.18653/v1/W16-2506',\n",
       "   '[13] Ralph José Rassweiler Filho, Jônatas Wehrmann, and Rodrigo C. Barros. 2017.\\nLeveraging Deep Visual Features for Content-based Movie Recommender Systems. In *Proceedings of the 2017 International Joint Conference on Neural Networks*\\n*(IJCNN 2017)* . IEEE, Anchorage, AK, USA, 604–611. https://doi.org/10.1109/\\nIJCNN.2017.7965908',\n",
       "   '[14] Claudiu S. Firan, Wolfgang Nejdl, and Raluca Paiu. 2007. The Benefit of Using TagBased Profiles. In *Proceedings of the 5th Latin American Web Conference (LA-WEB*\\n*‘07)* . IEEE Computer Society, Santiago, Chile, 32–41. https://doi.org/10.1109/LAWEB.2007.24',\n",
       "   '[15] Peng FU, Jiang hua LV, Shi long MA, and Bing jie LI. 2017. Attr2vec: A Neural\\nNetwork Based Item Embedding Method. In *Proceedings of the 2nd International*\\n*Conference on Computer, Mechatronics and Electronic Engineering (CMEE 2017)* .\\nDEStech Publications, Xiamen, China, 300–307. https://doi.org/10.12783/dtcse/\\ncmee2017/19993',\n",
       "   '[16] Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei\\nZhang. 2023. Chat-REC: Towards Interactive and Explainable LLMs-Augmented\\nRecommender System. *arXiv:* 2303.14524 (2023), 1–17. https://doi.org/10.48550/\\narXiv.2303.14524',\n",
       "   '[17] Anna Gladkova and Aleksandr Drozd. 2016. Intrinsic Evaluations of Word\\nEmbeddings: What Can We Do Better?. In *Proceedings of the 1st Workshop on*\\n*Evaluating Vector Space Representations for NLP* . Association for Computational\\nLinguistics, Berlin, Germany, 36–42. https://doi.org/10.18653/v1/W16-2507',\n",
       "   '[18] Mihajlo Grbovic, Vladan Radosavljevic, Nemanja Djuric, Narayan Bhamidipati,\\nJaikit Savla, Varun Bhagwan, and Doug Sharp. 2015. E-commerce in Your Inbox:\\n\\n\\nProduct Recommendations at Scale. In *Proceedings of the 21th ACM SIGKDD*\\n*International Conference on Knowledge Discovery and Data Mining (KDD ‘15)* .\\nAssociation for Computing Machinery, Sydney, Australia, 1809–1818. https:\\n//doi.org/10.1145/2783258.2788627',\n",
       "   '[19] Asnat Greenstein-Messica, Lior Rokach, and Michael Friedman. 2017. SessionBased Recommendations Using Item Embedding. In *Proceedings of the 22nd*\\n*International Conference on Intelligent User Interfaces (IUI ‘17)* . Association for\\nComputing Machinery, Limassol, Cyprus, 629–633. https://doi.org/10.1145/\\n3025171.3025197',\n",
       "   '[20] S. Hasanzadeh, S. M. Fakhrahmad, and M. Taheri. 2020. Review-Based Recommender Systems: A Proposed Rating Prediction Scheme Using Word Embedding Representation of Reviews. *Comput. J.* bxaa044, ; (2020), 1–10. https:\\n//doi.org/10.1093/comjnl/bxaa044',\n",
       "   '[21] Antonio Hernando, JesÚs Bobadilla, and Fernando Ortega. 2016. A Non Negative\\nMatrix Factorization for Collaborative Filtering Recommender Systems Based on\\na Bayesian Probabilistic Model. *Knowledge-Based Systems* 97, C (2016), 188—-202.\\nhttps://doi.org/10.1016/j.knosys.2015.12.018',\n",
       "   '[22] Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.\\n2016. Session-Based Recommendations with Recurrent Neural Networks. In\\n*Proceedings of the International Conference on Learning Representations (ICLR*\\n*2016)* . OpenReview, San Juan, Puerto Rico, 1–10.',\n",
       "   '[23] Yifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Collaborative Filtering for\\nImplicit Feedback Datasets. In *Proceedings of the 8th IEEE International Conference*\\n*on Data Mining (ICDM ‘08)* . IEEE Computer Society, Pisa, Italy, 263–272. https:\\n//doi.org/10.1109/ICDM.2008.22',\n",
       "   '[24] Salmo M.S. Júnior and Marcelo G. Manzato. 2015. Collaborative Filtering Based on\\nSemantic Distance Among Items. In *Proceedings of the 21st Brazilian Symposium on*\\n*Multimedia and the Web (WebMedia ’15)* . Association for Computing Machinery,\\nManaus, Brazil, 53–56. https://doi.org/10.1145/2820426.2820466',\n",
       "   '[25] Shan Khsuro, Zafar Ali, and Irfan Ullah. 2016. Recommender Systems: Issues, Challenges, and Research Opportunities. In *Proceedings of the 7th In-*\\n*ternational Conference on Information Science and Applications (ICISA 2016)* .\\nSpringer Science+Business Media, Ho Chi Minh, Vietnam, 1179–1189. https:\\n//doi.org/10.1007/978-981-10-0557-2_112',\n",
       "   '[26] Jooeun Kim, Jinri Kim, Kwangeun Yeo, Eungi Kim, Kyoung-Woon On, Jonghwan\\nMun, and Joonseok Lee. 2024. General Item Representation Learning for Coldstart Content Recommendations. *arXiv:* 2404.13808 (2024), 1–14. https://doi.org/\\n10.48550/arXiv.2404.13808',\n",
       "   '[27] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix Factorization\\nTechniques For Recommender Systems. *Computer* 42, 8 (2009), 30–37. https:\\n//doi.org/10.1109/MC.2009.263',\n",
       "   '[28] Quoc Le and Tomas Mikolov. 2014. Distributed Representations of Sentences\\nand Documents. In *Proceedings of the 31st International Conference on Machine*\\n*Learning (ICML 2014)* . JMLR.org, Beijing, China, 1188–1196. https://doi.org/10.\\n5555/3044805.3045025',\n",
       "   '[29] Pasquale Lisena, Albert Meroño-Peñuela, and Raphaëla Troncy. 2022. MIDI2vec:\\nLearning MIDI embeddings for reliable prediction of symbolic music metadata.\\n*Semantic Web* 13, 3 (2022), 357–377. https://doi.org/10.3233/SW-210446',\n",
       "   '[30] Junling Liu, Chao Liu, Peilin Zhou, Qichen Ye, Dading Chong, Kang Zhou,\\nYueqi Xie, Yuwei Cao, Shoujin Wang, Chenyu You, and Philip S.Yu. 2023. LLMRec: Benchmarking Large Language Models on Recommendation Task. *arXiv:*\\n2308.12241 (2023), 1–13. https://doi.org/10.48550/arXiv.2308.12241',\n",
       "   '[31] Jie Lu, Dianshuang Wu, Mingsong Mao, Wei Wang, and Guangquan Zhang. 2015.\\nRecommender system application developments: A survey. *Decision Support*\\n*Systems* 74 (2015), 12–32. https://doi.org/10.1016/j.dss.2015.03.008',\n",
       "   '[32] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Conrado, and Jeffrey Dan. 2013.\\nDistributed Representations of Words and Phrases and their Compositionality. In\\n*Proceedings of the 26th International Conference on Neural Information Processing*\\n*Systems (NIPS 2013)* . Curran Associates Inc., Stateline, NV, USA, 3111–3119. https:\\n//doi.org/10.5555/2999792.2999959',\n",
       "   '[33] Cataldo Musto, Pasquale Lops, Marco de Gemmis, and Giovanni Semeraro. 2017.\\nSemantics-aware Recommender Systems exploiting Linked Open Data and graphbased features. *Knowledge-Based Systems* 136 (2017), 1–14. https://doi.org/10.\\n1016/j.knosys.2017.08.015',\n",
       "   '[34] Makbule Gulcin Ozsoy. 2016. From Word Embeddings to Item Recommendation.\\n*arXiv:* 1601.01356 (2016), 1–8. https://doi.org/10.48550/arXiv.1601.01356',\n",
       "   '[35] Yuanyuan Qiu, Hongzheng Li, Shen Li, Yingdi Jiang, Renfen Hu, and Lijiao Yang.\\n2018. Revisiting Correlations between Intrinsic and Extrinsic Evaluations of\\nWord Embeddings. In *Chinese Computational Linguistics and Natural Language*\\n*Processing Based on Naturally Annotated Big Data (CCL 2018)* . Springer, Changsha,\\nChina, 209–221. https://doi.org/10.1007/978-3-030-01716-3_18',\n",
       "   '[36] Radim Řehůřek and Petr Sojka. 2010. Software Framework for Topic Modelling\\nwith Large Corpora. In *Proceedings of the LREC 2010 Workshop on New Challenges*\\n*for NLP Frameworks (LREC 2010)* . European Language Resources Association\\n(ELRA), Valletta, Malta, 45–50. https://doi.org/10.13140/2.1.2393.1847',\n",
       "   '[37] Stefen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\\n2009. BPR: Bayesian Personalized Ranking from Implicit Feedback. In *Proceedings*\\n\\n\\n353\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Pedro R. Pires, Bruno B. Rizzi, and Tiago A. Almeida\\n\\n\\n*of the 25th Conference on Uncertainty in Artificial Intelligence (UAI ‘09)* . AUAI\\nPress, Montreal, Canada, 452–461. https://doi.org/10.5555/1795114.1795167',\n",
       "   '[38] Steffen Rendle, Walid Krichene, Li Zhang, and John Anderson. 2020. Neural\\nCollaborative Filtering vs. Matrix Factorization Revisited. In *Proceedings of the*\\n*14th ACM Conference on Recommender Systems (RecSys ’20)* . Association for\\nComputing Machinery, Virtual Event, Brazil, 240–248. https://doi.org/10.1145/\\n3383313.3412488',\n",
       "   '[39] Steffen Rendle, Walid Krichene, Li Zhang, and Yehuda Koren. 2022. Revisiting\\nthe Performance of iALS on Item Recommendation Benchmarks. In *Proceedings*\\n*of the 16th ACM Conference on Recommender Systems (RecSys ’22)* . Association\\nfor Computing Machinery, Seattle, WA, USA, 427–435. https://doi.org/10.1145/\\n3523227.3548486',\n",
       "   '[40] Badrul M. Sarwar, George Karypis, Joseph A. Konstan, and John T. Riedl. 2000.\\nApplication of Dimensionality Reduction in Recommender System - A Case\\nStudy. In *Proceedings of the 9th WebKDD Workshop on Web Mining for e-commerce*\\n*(WebKDD ‘00)* . Association for Computing Machinery, Boston, Massachusetts,\\nUSA, 1–12. https://doi.org/10.21236/ada439541',\n",
       "   '[41] Tobias Schnabel, Igor Labutov, David Mimno, and Thorsten Joachims. 2015.\\nEvaluation methods for unsupervised word embeddings. In *Proceedings of the*\\n*2015 Conference on Empirical Methods in Natural Language Processing (EMNLP*\\n*2015)* . Association for Computational Linguistics, Lisbon, Portugal, 298–307.\\nhttps://doi.org/10.18653/v1/D15-1036',\n",
       "   '[42] Guy Shani and Asela Gunawardana. 2011. Evaluating Recommendation Systems.\\nIn *Recommender Systems Handbook*, Francesco Ricci, Lior Rokach, Bracha Shapira,\\nand Paul B. Kantor (Eds.). Springer US, New York, NY, USA, Chapter 8, 257–259.\\nhttps://doi.org/10.1007/978-0-387-85820-3',\n",
       "   '[43] Sumit Sidana, Mikhail Trofimov, Oleh Horodnytskyi, Charlotte Laclau, Yury\\nMaximov, and Massih-Reza Amini. 2021. User preference and embedding learning\\nwith implicit feedback for recommender systems. *Data Mining and Knowledge*\\n*Discovery* 35 (2021), 568–592. https://doi.org/10.1007/s10618-020-00730-8',\n",
       "   '[44] Abe Vallerian Siswanto, Lilian Tjong, and Yordan Saputra. 2018. Simple Vector\\nRepresentations of E-commerce Products. In *2018 International Conference on*\\n*Asian Language Processing (IALP 2018)* . IEEE, Bandung, Indonesia, 368–372. https:\\n//doi.org/10.1109/IALP.2018.8629245',\n",
       "   '[45] Yang Song, Lu Zhang, and Clyde Lee Giles. 2011. Automatic tag recommendation\\nalgorithms for social recommender systems. *ACM Transactions on the Web* 4, 1\\n(2011), 4:1–4:31. https://doi.org/10.1145/1921591.1921595',\n",
       "   '[46] Jiaxi Tang and Ke Wang. 2018. Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding. In *Proceedings of the 11th ACM*\\n*International Conference on Web Search and Data Mining (WSDM ‘18)* . Association for Computing Machinery, Marina Del Rey, CA, USA, 565–573. https:\\n//doi.org/10.1145/2939672.2939673',\n",
       "   '[47] Flavian Vasile, Elena Smirnova, and Alexis Conneau. 2016. Meta-Prod2Vec:\\nProduct Embeddings Using Side-Information for Recommendation. In *Proceedings*\\n*of the 10th ACM Conference on Recommender Systems (RecSys ‘16)* . Association\\nfor Computing Machinery, Boston, Massachusetts, USA, 225–232. https://doi.\\norg/10.1145/2959100.2959160',\n",
       "   '[48] Dongjing Wang, Guandong Xu, and Shuiguang Deng. 2017. Music recommendation via heterogeneous information graph embedding. In *Proceedings of the 2017*\\n*International Joint Conference on Neural Networks (IJCNN 2017)* . IEEE, Anchorage,\\nAK, USA, 596–603. https://doi.org/10.1109/IJCNN.2017.7965907',\n",
       "   '[49] Jiaqi Wang and Jing Lv. 2020. Tag-informed collaborative topic modeling for\\ncross domain recommendations. *Knowledge-Based Systems* 203 (2020), 106119.\\nhttps://doi.org/10.1016/j.knosys.2020.106119',\n",
       "   '[50] Qinyong Wang, Hongzhi Yin, Hao Wang, Quoc Viet Hung Nguyen, Zi Huang,\\nand Lizhen Cui. 2019. Enhancing Collaborative Filtering with Generative Augmentation. In *Proceedings of the 25th ACM SIGKDD International Conference on*\\n*Knowledge Discovery and Data Mining (KDD ’19)* . Association for Computing Machinery, Anchorage, AK, USA, 548–556. https://doi.org/10.1145/3292500.3330873',\n",
       "   '[51] Tian Wang, Yuri M. Brovman, and Sriganesh Madhvanath. 2021. Personalized\\nEmbedding-based e-Commerce Recommendations at eBay. *arXiv:* 2102.06156\\n(2021), 1–9. https://doi.org/10.48550/arXiv.2102.06156',\n",
       "   '[52] Heitor Werneck, Nícollas Silva, Matheus Carvalho Viana, Fernando Mour ao,\\nAdriano C. M. Pereira, and Leonardo Rocha. 2020. A Survey on Point-of-Interest\\nRecommendation in Location-based Social Networks. In *Proceedings of the Brazil-*\\n*ian Symposium on Multimedia and the Web (WebMedia ’20)* . Association for Computing Machinery, São Luís, Brazil, 185–192. https://doi.org/10.1145/3428658.\\n3430970',\n",
       "   '[53] Dongqiang Yang, Ning Li, Li Zou, and Hongwei Ma. 2022. Lexical semantics\\nenhanced neural word embeddings. *Knowledge-Based Systems* 252 (2022), 109298.\\nhttps://doi.org/10.1016/j.knosys.2022.109298',\n",
       "   '[54] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Jundong Li, and Zi Huang. 2024.\\nSelf-Supervised Learning for Recommender Systems: A Survey. *IEEE Transactions*\\n*on Knowledge and Data Engineering* 36 (2024), 335–355. https://doi.org/10.1109/\\nTKDE.2023.3282907',\n",
       "   '[55] Hafed Zarzour, Ziad A. Al-Sharif, and Yaser Jararweh. 2019. RecDNNing:\\na recommender system using deep neural network with user and item embeddings. In *Proceedings of the 10th International Conference on Information*\\n\\n\\n*and Communication Systems (ICICS 2019)* . IEEE, Irbid, Jordan, 99–103. https:\\n//doi.org/10.1109/IACS.2019.8809156',\n",
       "   '[56] Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, and Wei-Ying Ma.\\n2016. Collaborative Knowledge Base Embedding for Recommender Systems.\\nIn *Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge*\\n*Discovery and Data Mining (KDD ‘16)* . Association for Computing Machinery,\\nSan Francisco, CA, USA, 353–362. https://doi.org/10.1145/2939672.2939673',\n",
       "   '[57] Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019. Deep Learning Based\\nRecommender System: A Survey and New Perspectives. *ACM Comput. Surv.* 52,\\n1 (2019), 5:1–5:35. https://doi.org/10.1145/3285029',\n",
       "   '[58] Xiangyu Zhao, Maolin Wang, Xinjian Zhao, Jiansheng Li, Shucheng Zhou, Dawei\\nYin, Qing Li, Jiliang Tang, and Ruocheng Guo. 2023. Embedding in Recommender\\nSystems: A Survey. *arXiv:* 2310.18608 (2023), 1–42. https://doi.org/10.48550/\\narXiv.2310.18608',\n",
       "   '[59] Lütfi Kerem Şenel, İhsan Utlu, Veysel Yücesoy, Aykut Koç, and Tolga Çukur.\\n2018. Semantic Structure and Interpretability of Word Embeddings. *IEEE/ACM*\\n*Transactions on Audio, Speech, and Language Processing* 26, 10 (2018), 1769–1779.\\nhttps://doi.org/10.1109/TASLP.2018.2837384\\n\\n\\n354\\n\\n\\n-----'],\n",
       "  'text': '# **Why Ignore Content? A Guideline for Intrinsic Evaluation of Item** **Embeddings for Collaborative Filtering**\\n\\n## Pedro R. Pires\\n#### pedro.pires@dcomp.sor.ufscar.br Universidade Federal de São Carlos São Carlos, SP\\n\\n## Bruno B. Rizzi\\n#### brunosora@hotmail.com BTG Pactual São Paulo, SP\\n\\n## Tiago A. Almeida\\n#### talmeida@ufscar.br Universidade Federal de São Carlos Sorocaba, SP\\n\\n### **ABSTRACT**\\n\\nWith the constant growth in available information and the popularization of technology, recommender systems have to deal with an\\nincreasing number of users and items. This leads to two problems\\nin representing items: scalability and sparsity. Therefore, many\\nrecommender systems aim to generate low-dimensional dense representations of items. Matrix factorization techniques are popular,\\nbut models based on neural embeddings have recently been proposed and are gaining ground in the literature. Their main goal is to\\nlearn dense representations with intrinsic meaning. However, most\\nstudies proposing embeddings for recommender systems ignore\\nthis property and focus only on extrinsic evaluations. This study\\npresents a guideline for assessing the intrinsic quality of matrix\\nfactorization and neural-based embedding models for collaborative\\nfiltering, comparing the results with a traditional extrinsic evaluation. To enrich the evaluation pipeline, we suggest adapting an\\nintrinsic evaluation task commonly employed in the Natural Language Processing literature, and we propose a novel strategy for\\nevaluating the learned representation compared to a content-based\\nscenario. Finally, every mentioned technique is analyzed over established recommender models, and the results show how vector\\nrepresentations that do not yield good recommendations can still be\\nuseful in other tasks that demand intrinsic knowledge, highlighting\\nthe potential of this perspective of evaluation.\\n### **KEYWORDS**\\n\\nembeddings, intrinsic evaluation, qualitative evaluation, recommender systems, similarity tables, intruder detection, autotagging\\n### **1 INTRODUCTION**\\n\\nRecommender systems are tools commonly used by companies for\\nenhancing the experience users have when utilizing their services\\nby filtering and recommending particularly relevant information [ 1 ].\\nAmong different types of recommender systems, collaborative filtering (CF) is one of the most popular [4].\\nPioneer CF recommender systems represented items as sparse\\nvectors of consumption. However, due to the accelerated growth in\\nthe number of users and items, this form of representation started\\nfacing limitations related to: *(i)* sparsity, since modern recommender\\nsystems must deal with a number of possible interactions that follows a power law according to the number of users and items,\\nyielding an often highly sparse interactions matrix; and *(ii)* scalability, given that the vectors used to represent users and items can\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n© 2024 SBC – Brazilian Computing Society.\\nISSN 2966-2753\\n\\n\\nbecome quite large, increasing the demand for greater storage and\\nprocessing capacity.\\nTo circumvent these problems, efforts are being made to represent users and items in a much smaller dimensional space [ 40 ]. In\\nthis context, two main techniques gained ground with the literature:\\nmatrix factorization [ 27 ] and neural networks [ 57 ]. In the latter,\\nneural embedding models inspired by Natural Language Processing\\n(NLP) have recently gained traction [ 34 ]. A significant advantage\\nof embeddings in NLP is their ability to carry intrinsic meaning, i.e.,\\nthe knowledge encapsulated within the representation beyond the\\ninformation used for training. In recommender systems, this translates to the potential of leveraging item embeddings for various\\ntasks beyond mere recommendation.\\nDespite the promising nature of embeddings, most existing research has focused primarily on extrinsic evaluations, neglecting\\nthe intrinsic qualities of the learned representations. It is well\\nknown that the performance of embeddings in downstream applications does not always correlate with their intrinsic quality [41].\\nWhile the primary goal of a recommender system is to provide\\nhigh-quality recommendations, the vector representations of items\\nand users can also be applied to other tasks such as automatic feature prediction, knowledge discovery, and clustering [ 21, 31 ]. These\\napplications can only be significantly improved if the intrinsic quality of the embeddings is adequately assessed. Therefore, evaluating\\nthe intrinsic quality of matrix factorization and neural embedding\\nmodels is crucial for their successful application in diverse contexts.\\nTo address this gap, we present various methods for using item\\nmetadata to intrinsically evaluate vector representations of items\\nin a CF recommender system. We introduce a commonly used\\nevaluation technique in recommender system literature and adapt\\nan intrinsic evaluation task from NLP to our context. Given the\\ntime-consuming and expertise-dependent nature of subjective analyses, we propose a new quantitative, non-subjective strategy using content-based data to evaluate the intrinsic ranking quality of\\nembeddings. We illustrate our proposed pipeline with an extrinsic evaluation, assessing the quality of vector representations in\\ngenerating recommendations, and compare these results with our\\nintrinsic evaluation tasks. Our findings demonstrate that embedding models can perform very differently across tasks, with some\\nmodels that perform poorly in recommendation tasks excelling in\\nintrinsic evaluations, underscoring the necessity of comprehensive\\nanalysis when developing new representation models. With these\\nconsiderations, the primary objectives of this study are:\\n\\n(1) Introduce new methods for evaluating item embeddings using strategies derived from Natural Language Processing,\\nsuch as *intruder detection*, and from content-based recommendation, such as *content-based ranking comparison* ;\\n(2) Present a collection of techniques for intrinsically evaluating\\nitem embeddings in both subjective and objective manners;\\n\\n\\n345\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Pedro R. Pires, Bruno B. Rizzi, and Tiago A. Almeida\\n\\n\\n(3) Compare traditional embedding-based recommender models in extrinsic and intrinsic tasks to illustrate the varied\\nperformance of embeddings across different applications.\\n### **2 RELATED WORK**\\n\\nThe earliest collaborative filtering recommender systems employed\\nneighborhood-based methods to compute recommendations. However, as the number of users and items increased, these techniques\\nencountered significant challenges related to sparsity and scalability [ 25 ]. Consequently, the use of embeddings for representing users\\nand items gained popularity, i.e., low-dimensional vector representations that carry intrinsic meaning. Initially, matrix factorization\\nmodels were applied for this task [ 27 ], reducing memory consumption and processing demand while generating valuable insights.\\nOver the years, numerous methods have been proposed to generate embeddings, employing diverse strategies such as graph-based\\nalgorithms and quantization techniques [58].\\nInspired by state-of-the-art Natural Language Processing (NLP)\\ntechniques [ 32, 34 ], recent methods aim to learn neural embeddings\\nfor items and users. The first studies in the area were Item2Vec [ 2 ]\\nand Prod2Vec [ 18 ], neural networks heavily inspired by the Skipgram architecture [ 32 ], and User2Vec [ 18 ], inspired by Paragraph\\nVector [ 28 ]. Subsequent studies have built upon these foundational\\nmodels, incorporating various techniques to enhance performance,\\ne.g., consuming item metadata [ 15, 47 ], leveraging content information to enrich embeddings [19, 51, 56].\\nBeyond incorporating item content, more complex neural models have been employed, including deep learning [ 43, 55 ], recurrent\\nneural networks [ 22, 51 ], convolutional neural networks [ 10, 46 ],\\nGANs [ 7, 50 ], and transformers [ 26, 54 ]. Another notable approach\\nis training NLP models on textual data from items, users, or interactions [ 20, 44 ], with Large Language Models (LLMs) gaining\\nattention in recent years [16, 30].\\nA particularly intriguing aspect of neural embeddings is their\\nintrinsic meaning [ 17 ], though few studies have thoroughly investigated this property. The most common approach for evaluating item\\nembeddings is using similarity tables [ 2, 15, 18 ], but this method\\nheavily relies on subjective opinions, which can be misleading.\\nOther methods, such as genre plotting [ 2, 15, 44 ] and sample clustering [ 26, 51 ], also depend on human judgment or are difficult to\\napply across different domains, such as analogy analysis [19].\\nIn NLP, several established methods exist for intrinsic evaluation\\nof word embeddings [ 3, 17, 35, 53 ], including: (i) comparing human\\njudgment of word similarity with embedding space similarity; (ii)\\npredicting word analogies through vector arithmetic; (iii) clustering\\nword embeddings and evaluating cluster quality; and (iv) detecting\\nsynonyms or “intruders” in groups of similar embeddings. However,\\nmany of these methods are challenging to adapt to recommender\\nsystems due to their reliance on external semantic datasets.\\nIn many areas of recommender systems, the accuracy of the\\nrecommendation is prioritized over other characteristics such as\\nquality [ 52 ], and although item embeddings are often measured\\nin downstream applications such as recommendations, this does\\nnot guarantee intrinsic quality [ 41 ]. High-quality embeddings can\\nboost various tasks in a recommendation scenario, such as automatic feature prediction, user and item clustering, and knowledge\\ndiscovery [ 21, 31 ], or enhance semantic-based recommendation\\nengines, that use intrinsic knowledge as the main strategy for filtering items [ 24 ]. Embeddings with strong intrinsic value can even\\n\\n\\nassist in uncovering item metadata for systems relying on categorical features [ 6, 49 ], which are often incomplete or inaccurate, and\\nenhance traditional collaborative filtering models [33].\\nTo the best of our knowledge, no work on the recommender system literature has focused on studying the intrinsic aspects of item\\nembeddings. Studies proposing novel embedding-based models often perform simple forms of intrinsic evaluation, normally based\\non subjective approaches that can add human bias and problems\\nto the conclusions. Neglecting or improperly conducting intrinsic\\nevaluations can result in the loss of valuable information that could\\nsupport related tasks and improve representation models.\\nIn this context, we introduce alternative methods for intrinsically\\nevaluating recommender system embeddings. Besides presenting\\ncommonly used evaluation techniques, we adapt an NLP evaluation\\ntask to the recommender context and propose a novel quantitative\\nmetric for intrinsic quality using a content-based ranking comparison approach. We conduct both extrinsic and intrinsic evaluations,\\ncomparing results across different tasks to illustrate the varied\\nperformance of embedding models.\\n### **3 SUBJECTIVE METHODS FOR INTRINSIC** **EVALUATION**\\n\\nThe intrinsic evaluation of embeddings in both NLP and recommender systems often relies on subjective approaches, where the\\nquality of the representation is assessed based on human opinions.\\nIn this section, we present two subjective tasks: similarity tables\\nand intruder detection.\\n### **3.1 Similarity Tables**\\n\\nThe similarity table evaluation strategy involves training different\\nmodels on the same datasets and selecting a known item as a seed.\\nThe distances between this seed item and all other items are calculated for each representation using a measure of angular similarity.\\nThe top- *𝑁* nearest items for each representation are then displayed\\nside-by-side in a table. Human evaluators can subjectively assess\\neach group of nearest items to determine how well they match the\\ntarget item. This task can be repeated with multiple known items,\\nproviding a broader perspective on the representations’ behavior.\\nSimilarity tables are the most straightforward and intuitive way\\nto check for intrinsic meaning in the evaluated representations.\\nThis ease of use makes it one of the most commonly employed\\nschemes [ 2, 15, 18 ]. However, it heavily depends on human interpretation based on subjective analysis [17].\\n### **3.2 Intruder Detection**\\n\\nIn NLP, intruder detection, also known as outlier detection, involves\\nidentifying \"intruder\" words in a set [ 41 ]. We propose adapting\\nthis task for the recommender system context by treating items\\nas words. To do this, we first select a few target items (which can\\nbe randomly drawn or, preferably, human-selected) and, using the\\nembedding-vector space, find the top- *𝑁* nearest items to each target.\\nA random item from the representation space is then added to the\\nneighboring items, and Human evaluators must then determine the\\n“intruder” items (i.e., the randomly selected ones). The accuracy of\\nthese guesses can be computed and used to compare the methods.\\nAlthough the task’s outcome is quantitative, subjectivity is reduced but not eliminated. It still relies on human interpretation and\\ncan be time- and cost-intensive, requiring one or more individuals\\n\\n\\n346\\n\\n\\n-----\\n\\nWhy Ignore Content? A Guideline for Intrinsic Evaluation of Item Embeddings for Collaborative Filtering WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\nto analyze each chosen example. However, the need for multiple\\nevaluators can be mitigated using crowdsourcing platforms, such as\\nAmazon Mechanical Turk [1] and Prolific [2], which allows researchers\\nto hire participants from diverse demographics.\\n### **3.3 Shortcomings of subjective evaluation**\\n\\nThe weak point of using the aforementioned subjective strategies\\nresides in their high dependence on human opinions, which may be\\nheavily biased or too shallow for a particular domain [ 12 ]. Human\\nopinions can unfairly penalize embeddings when the grouping\\ncriteria selected by the algorithm differs from the one preferred by\\nthe evaluator [59].\\nTo avoid human bias, the experiments must be conducted with\\na vast range of people, which is more time-demanding and will\\nmost likely result in cost increases. To overcome subjectivity in the\\nexisting evaluation schemes, it is recommended to use objective\\napproaches, i.e., the ones that consume additional data to quantify\\nthe intrinsic quality and are calculated automatically.\\n### **4 OBJECTIVE METHODS FOR INTRINSIC** **EVALUATION**\\n\\nTo avoid bias problems that subjective evaluation suffers, here we\\nsuggest the use of two different metrics: automatic feature prediction and a novel metric of content-based ranking comparison.\\n### **4.1 Automatic Feature Prediction**\\n\\nAutomatically discovering item features involves predicting a set\\nof unknown tags for a target item *𝑖* based on its most similar items.\\nAlso known as auto-tagging, it is a specific problem in the fields\\nof recommender systems and knowledge discovery, with many\\nmethods proposed exclusively for this [8, 29, 45].\\nWe can employ a neighborhood-based automatic feature prediction approach to assess the intrinsic value of a vector representation [ 2, 15 ]. For each item *𝑖* in the entire catalog, its *𝑘* nearest items\\nare selected, considering the embedding-vector space. Next, the\\nattributes of the target item are discovered through some voting\\nprocess, such as a simple majority vote.We can then compare the\\noriginal item’s attributes with the ones predicted by the neighbors\\nand quantify the intrinsic quality of the representation using traditional classification metrics, such as precision, recall, or F1-score.\\n### **4.2 Content-based Ranking Comparison**\\n\\nAs an additional objective and automatic method for intrinsically\\nevaluating vector representations of items, we propose a novel metric that compares the neighborhood generated by the embeddings\\nin the vector space with a neighborhood constructed using contentbased information about the items. The quality of the comparison\\ncan then be quantified using a ranking comparison metric.\\nTo properly evaluate the spatial distribution generated by the\\nlearned embeddings, we first assume that there is a correct order\\nfor the neighborhood of a given item when filtering its most similar\\nitems on the embeddings vector space. In this study, we constructed\\nthe target ordering using the similarities between the high-level features of the items: the item’s category, genre, or tags. As we aim to\\n\\n1 Amazon Web Services. *Amazon Mechanical Turk* . Available at: https://www.mturk.\\ncom/.\\n2 Prolific. *Prolific: Definitive human data to deliver world-leading research and AI* . Available at: https://www.prolific.com/.\\n\\n\\napproximate items with similar features, we can compare this neighborhood with the one generated by the embeddings. Moreover, it is\\npossible to use more complex and domain-specific techniques for\\nranking, e.g., low-level visual features for movies [ 13 ] and context\\nand metadata graph embeddings for music [48].\\nUsing those general features to describe the items, we can represent an item *𝑖* through a bag-of-words encoding, i.e., an array\\nof attributes *𝑖* [�] . With this representation, we can build a similarity\\nmatrix C of dimensions | *𝐼* | × | *𝐼* |, in which | *𝐼* | represents the number\\nof items in the catalog. Thus, for a given pair of items *𝑖* and *𝑗*, C *𝑖,𝑗*\\nstores their similarity when using the content-based representation, i.e., vectors *𝑖* [�] and *𝑗* [�], calculated using a metric such as cosine\\nsimilarity. Similarly, we construct the similarity matrix E, which\\nstores the similarity of the items’ dense embeddings.\\nAfterward, for each item *𝑖* ∈ *𝐼*, we can construct two neighborhoods, N *𝑖* [C] and N *𝑖* [E] [. The former corresponds to the subset of]\\nitems most similar to *𝑖* considering the similarity matrix C, i.e., the\\nitems that share the most related content-based features. The latter\\nrepresents the same neighborhood concept but uses the similarity\\nvalues stored in matrix E . Both N *𝑖* [C] and N *𝑖* [E] may be limited to a\\nrestricted number of neighbors, defined by a hyperparameter *𝑘*, to\\nreduce memory consumption when calculating the ranking.\\nWith both similarity matrices constructed, we can compare them\\nusing different metrics and approaches, e.g., traditional or utilitybased ranking measures, and sample sets’ similarity metrics. Regardless of the adopted strategy, all of them are maximized according\\nto the same ordering, i.e., the one created using the content-based\\nrepresentation, with the differences being in how the neighborhoods are used to calculate the final score. In the following, we\\noffer metrics for each one of those approaches.\\n\\n*4.2.1* *Rank correlation metrics.* When considering the order of the\\nitems, we can calculate metrics designed to compare the correlation\\nof rankings when you have a reference ranking, such as Spearman’s rank correlation coefficient *𝜌*, Kendall’s *𝜏* coefficient, or the\\nNormalized Distance-based Performance Measure (NDPM).\\nFor comparing the rankings using Spearman’s *𝜌*, we can calculate\\nthe correlation coefficient for each item and average the results, as\\nshown in Equation 1. To calculate *𝜌* *𝑖*, we use Equation 2, in which\\n*𝑑* corresponds to the difference among ranking positions when\\nsorting the items according to N *𝑖* [C] and N *𝑖* [E] [.]\\n\\n*𝜌* = � *𝑖* | ∈ *𝐼* *𝐼* | *[𝜌]* *𝑖* (1) *𝜌* *𝑖* = 1 − | *𝐼* 6|(| [�] *𝐼* *𝑗* | ∈ [2] *𝐼* − *[𝑑]* 1 [2] *𝑗* ) (2)\\n\\n*4.2.2* *Set similarity metrics.* If we limit the neighborhoods to the\\ntop- *𝐾* similar items, we can treat them both as sample sets and\\ncalculate metrics designed to compare the similarity and diversity\\nof sets, such as the Jaccard Index or the Sørensen–Dice coefficient.\\nFor the Jaccard Index *𝐽*, we must first calculate the Jaccard Index\\n*𝐽* *𝑖* for each item *𝑖*, and then average the results, as shown in Equation 3. For *𝐽* *𝑖*, we must build two sets, *𝑆* *𝑖* [C] [and] *[ 𝑆]* *𝑖* [E] [, consisting of the]\\ntop- *𝐾* most similar items from both N *𝑖* [C] and N *𝑖* [E] [, respectively, and]\\nthen calculate the set similarity using Equation 4:\\n\\n\\n∈ *𝐼* *[𝐽]* *𝑖* (3) *𝐽* *𝑖* = | *𝑆* *𝑖* [C] [∩] *[𝑆]* *𝑖* [E] [|]\\n\\n| *𝐼* | | *𝑆* [C] [∪] *[𝑆]* [E] [|]\\n\\n\\n*𝐽* = � *𝑖* ∈ *𝐼* *[𝐽]* *𝑖*\\n\\n\\n*𝑖* *𝑖* (4)\\n\\n| *𝑆* *𝑖* [C] [∪] *[𝑆]* *𝑖* [E] [|]\\n\\n\\n347\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Pedro R. Pires, Bruno B. Rizzi, and Tiago A. Almeida\\n\\n\\n*4.2.3* *Utility-based ranking scores.* Finally, we can also adapt utilitybased ranking metrics, using the content-based similarities to quantify the utility of the embeddings ranking.\\nHere, we explain how the Normalized Discounted Cumulative\\nGain (NDCG) can be adapted for this type of evaluation. First, as it\\nis commonly calculated for the metric, we define the overall NDCG\\nas the average of the NDCG for each item *𝑖* (NDCG *𝑖* ), as shown in\\nEquation 5. NDCG *𝑖*, on the other hand, is defined as the Discounted\\nCumulative Gain of the item (DCG *𝑖* ) divided by its Ideal Discounted\\nCumulative Gain (CB-IDCG *𝑖* ), normalizing the final value to a 0–1\\nrange, as shown in Equation 6.\\n\\n\\nNDCG = [1]\\n\\n| *𝐼* |\\n\\n\\nrange, as shown in Equation 6.\\n\\n*𝑖* ∈ *𝐼*\\n\\n∑︁\\n\\n\\n(NDCG *𝑖* ) (5) NDCG *𝑖* = [DCG] *[𝑖]*\\n\\nIDCG\\n\\n*𝑖* ∈ *𝐼*\\n\\n∑︁\\n\\n\\n(6)\\nIDCG *𝑖*\\n\\n### **5.2 Embedding-based Algorithms**\\n\\nWe have implemented two well-known methods for matrix factorization, Alternating Least Squares (ALS) [ 23 ] and Bayesian Personalized Ranking (BPR) [ 37 ], using the implicit [8] library. Moreover,\\nconsidering that a good intrinsic meaning is commonly achieved by\\ncontext-window models [ 32 ], we have also implemented two contextual neural embeddings-based recommenders, Item2Vec (I2V) [ 2 ]\\nand User2Vec (U2V) [ 18 ], using the gensim [ 36 ] library. All models\\nwere selected based on their popularity in recent studies [ 38, 39 ],\\nease of replication, and the fact that they do not rely on item metadata, showing the power those methods can have for figuring out\\nknowledge about the items without consuming this information.\\nWe fine-tuned the methods through a grid search holdout maximizing NDCG@15 [ 42 ], with a rate of 8:1:1 for training, validation, and test sets, respectively. For ALS and BPR, we tested hidden factors of sizes *𝑓* = {50, 100, 300}, regularization factor *𝜆* =\\n{0.01, 0.1, 1} and learning rate *𝛼* = {0.0025, 0.025, 0.25}, using 100\\nepochs for training. For the embedding models, we varied the number of epochs *𝑛* = {50, 100, 200}, sub-sampling rate of frequent\\nitems *𝑡* = { 10 [−][5] *,* 10 [−][4] *,* 10 [−][3] }, and exponent to shape the negative\\nsampling distribution *𝛼* = {-1.0, -0.5, 0.5, 1.0}, as recommended\\nby Caselles-Duprés et al . [5] . For any unmentioned parameter, we\\nused the library default values.\\n### **6 EXTRINSIC RESULTS**\\n\\nTo assess the representation models’ ability to provide good recommendations, i.e., to evaluate the models extrinsically, we conducted\\na top- *𝑁* ranking task, calculating the NDCG for multiple values of\\n*𝑁* . Results are shown in Table 2, in which darker-toned cells correspond to better results, with the best result for each combination\\nof dataset and the value for *𝑁* highlighted in **bold** .\\nThe results indicate a similar performance between ALS, BPR,\\nand Item2Vec. On the contrary, User2Vec was the worst, obtaining\\nlow results for every dataset and threshold. ALS and BPR tend to\\npresent better results for small values of *𝑁*, with their worst results\\nbeing achieved when *𝑁* = 20. The same is not true for Item2Vec\\nand User2Vec, which benefit from bigger values of *𝑁* .\\nTo properly analyze the models, we conducted a non-parametric\\nFriedman test to verify if there is a statistically significant difference\\nbetween them [ 9 ], using a ranking constructed with the results.\\nThe test indicates that the models differ, with 99% confidence ( *𝑋* *𝐹* [2] [=]\\n29 *.* 96). We then conducted a Nemenyi test to compare them to each\\nother [ 9 ]. With 95% confidence and considering a critical difference\\nof 1 *.* 21, there is no statistical evidence of superiority among ALS,\\nBPR, and Item2Vec. Additionally, we have that the three models are\\nstatistically superior to User2Vec, considering that the differences\\nin the average rankings were superior to the critical difference [ 9 ].\\nResults of the extrinsic experiment show that ALS, BPR, and\\nItem2Vec achieve very similar results and that User2Vec is the most\\nunsuitable method for this task compared to the others. However,\\nfollowing insights for the NLP area [ 41 ], this behavior may not\\nnecessarily repeat when we apply the same representation vectors to intrinsic tasks. Therefore, we conducted different intrinsic\\nevaluation strategies to assess this particularity.\\n\\n8 Ben Frederickson. 2017. *Implicit: Fast Python Collaborative Filtering for Implicit*\\n*Datasets* . Available at: https://github.com/benfred/implicit\\n\\n\\n∈\\n\\nThe main differences arise when calculating the DCG *𝑖* and the\\nIDCG *𝑖* . For both scores, we consider that the “gain” of a given\\nitem *𝑗* in the neighborhood of *𝑖* is given by the values of matrix C,\\ni.e., the content-based similarity matrix. The ideal gain, IDCG *𝑖*, is\\nretrieved using the top- *𝑘* items of the content-based neighborhood,\\nN [C]\\n*𝑖* [, while the obtained gain, DCG] *[𝑖]* [uses the embedding-based]\\nneighborhood, N *𝑖* [E] [, as presented in Equations 7 and 8, respectively.]\\n\\n\\n*𝑘*\\n\\n*𝑛* =1\\n\\n∑︁\\n\\n\\n*𝑛* =1\\n\\n\\nIDCG *𝑖* =\\n\\n\\n*𝑘*\\n\\n*𝑛* =1\\n\\n∑︁\\n\\n\\n*𝑛* =1\\n\\n\\n*𝐶* *𝑖,* N *𝑖𝑛* C (7) DCG *𝑖* =\\nlog 2 ( *𝑛* + 1)\\n\\n\\n*𝐶* *𝑖,* N *𝑖𝑛* E (8)\\nlog 2 ( *𝑛* + 1)\\n\\n\\nFor the ideal score (Equation 7), we defined the gain for the *𝑛* *𝑛*\\n*𝑛* th item in the neighborhood as *𝐶* *𝑖,* N *𝑖𝑛* C [, which corresponds to the]\\n\\ncontent-based similarity stored in C between the target item *𝑖* and\\nthe *𝑛* th item of *𝑖* ’s neighborhood in N [C] . For the generated score\\n(Equation 8), the gain is calculated similarly. It is represented by\\n*𝐶* *𝑖,* N *𝑖𝑛* E [, corresponding to the similarity stored in] [ C] [, but between]\\n\\nthe target item *𝑖* and the *𝑛* th item of *𝑖* ’s neighborhood in N [E] .\\n### **5 EXPERIMENTAL SETUP**\\n\\nThis section details the experimental setup. First, we present the\\ndatasets used in the experiments, then we describe the benchmark\\nalgorithms and the fine-tuning phase.\\n### **5.1 Datasets and Data Preprocessing**\\n\\nTable 1 presents the datasets used in the experiments. They are\\npublicly available, used in past research or challenges, and provide item metadata. The features describing the items can be of two\\ntypes: *(i)* categories, attributes inherent to the item, informed by the\\nsystem owner; or *(ii)* tags, values informed by users without moderation. Since user-informed tags are liable to noise and inconsistency,\\nwe opt to use only the top-100 most recurring tags by dataset, as\\nperformed in studies of tag-based recommender systems [11, 14].\\n\\n3 Anime Recommendations dataset. Available at: www.kaggle.com/datasets/\\nCooperUnion/anime-recommendations-database\\n4 Data Mining Hackathon on Big Data (7GB). Available at: www.kaggle.com/c/acm-sfchapter-hackathon-big\\n5 DeliciousBookmarks. Available at: www.grouplens.org/datasets/hetrec-2011/\\n6 Last.FM. Available at: www.grouplens.org/datasets/hetrec-2011/\\n7 MovieLens 25M. Available at: www.grouplens.org/datasets/movielens/\\n\\n\\n348\\n\\n\\n-----\\n\\nWhy Ignore Content? A Guideline for Intrinsic Evaluation of Item Embeddings for Collaborative Filtering WebMedia’2024, Juiz de Fora, Brazil\\n\\n**Dataset** ***Users*** ***Items*** ***Interactions*** ***Sparsity*** ***Categories*** ***Tags***\\n\\n**Anime** [3] 73,514 11,200 7,813,733 99.05% 43 N/A\\n**BestBuy** [4] 1,268,702 69,858 1,865,269 99.99% 1,540 N/A\\n**Delicious** [5] 1,867 69,223 104,799 99.92% N/A 14,346\\n**Last.FM** [6] 1,892 17,632 92,834 99.72% N/A 9,718\\n**MovieLens** [7] 162,541 59,047 25,000,095 99.74% 20 65,464\\n\\n**Table 1: Description of each dataset used in the experiments. N/A is used for datasets without categories or tags.**\\n\\n\\n**Re** **p** **resentation Model**\\n**Dataset** *𝑁*\\n**ALS** **BPR** **I2V** **U2V**\\n\\n**Table 2: NDCG achieved by each algorithm in each dataset in**\\n**a top-** *𝑁* **recommendation task with different values of** *𝑁* **.**\\n### **7 INTRINSIC RESULTS**\\n\\nTo intrinsically evaluate the vector representation, we performed\\nthe tasks presented in Sections 3 and 4, discussing the main differences of each experiment. The results show how the same representation model can perform differently according to the task,\\nespecially when comparing subjective to objective strategies.\\n### **7.1 Similarity Table**\\n\\nWe built two similarity tables using popular items from datasets of\\nwidely known domains: Last.FM (Table 3) and MovieLens (Table 4),\\nalong with their top-3 neighbors in each representation.\\nIn Table 3, all methods found a neighborhood with similar items\\nto the target. In most cases, the bands and artists have completely\\nvaried for each representation model, with only a few exceptions\\nsuch as *Jay-Z*, *30 Seconds to Mars*, and *Beyoncé*, that were present\\non three of the algorithms. Even with the selection of different\\nartists, the music genres were normally very related to the target.\\nSome methods behave in a more conservative manner, such as BPR\\nreturning only hip-hop and rap artists for *Eminem*, while others\\nreturned relevant items, but deviating from the tags, such as *Ke$ha*\\nand *P!nk* in the I2V neighborhood for *Eminem* . Even so, we can say\\nthat every method achieved some pertinent neighborhood. ALS\\n\\n\\nwas the worst due to certain tag contradictions in its results, such\\nas the learned neighborhood for *The Beatles*, which instead of other\\nrock or 60s bands, contains Arabic and baroque artists.\\nWe can not say the same for Table 4. BPR and User2Vec, especially, found some very related items, such as the sequels for *X-Men* .\\nHowever, they also found some odd neighbors, such as *Jack-O* or\\n*Men in Black*, a horror and sci-fi movie, respectively, for *Toy Story*,\\na children’s animation. For some movies, such as *Titanic*, all methods performed poorly when comparing the genres between target\\nand neighbor items. On the other hand, all neighboring movies are\\nconsidered classic films, implying the representations discovered a\\npattern. Due to these conflicting results, it is hard to select a superior\\nrepresentation without relying on human personal opinions.\\nAs mentioned, this evaluation method is heavily influenced by\\nhuman subjectivity. For instance, in Table 3, among all neighborhoods of *Shakira*, the ones composed by *Rihanna*, *Britney Spears*,\\n*Katy Perry*, *Mariah Carey* and *Beyoncé* would be the most similar if\\nwe consider that they are all world-famous pop artists. However,\\n*Thalía*, *Fanny Lu* and *Juanes* are all Latin pop artists; hence, they\\nare strongly connected with the target, *Shakira* . Therefore, deciding\\nwhat is more similar is complex, and our opinions and backgrounds\\ncan strongly skew our guesses.\\n### **7.2 Intruder Detection**\\n\\nWe conducted the task using Anime, MovieLens, and Last.FM datasets,\\nas they are from well-known domains and contain well-curated\\nadditional information, e.g., genre and release year. For each dataset,\\nwe selected 15 items to use as seeds, of which 10 were popular items,\\nand 5 were completely random. We then built five questionnaires,\\neach with 15 items, alternating between the representation models.\\nFinally, we asked a group of 10 individuals to discover the intruder.\\nEach representation model received 30 votes, and the accuracy for\\neach model is shown in Table 5. Results are shown in three different\\nviews: the general accuracy, considering all of the 15 items, the\\naccuracy for only the 10 popular items, and the accuracy for the 5\\nrandom (and probably unknown) items.\\nBPR and User2Vec performed better at building a good quality\\nneighborhood, as they usually presented the highest number of correct answers per dataset. Item2Vec also constructed a satisfactory\\nneighborhood, being a close second in almost every case.\\nBPR generally achieved the best results in the scenario considering all items, being the best model for MovieLens and Last.FM, and\\nthe second-best for Anime. User2Vec presented promising results\\nfor the Anime dataset and reached 100% accuracy in the scenario\\nwhere items were randomly selected, showing that it could generate a relevant neighborhood even in cases where there is little\\nknowledge about the item. This result is exciting, considering the\\nscores obtained by the model in the extrinsic evaluation (Section 6).\\n\\n\\n349\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Pedro R. Pires, Bruno B. Rizzi, and Tiago A. Almeida\\n\\n**Re** **p** **resentation Model**\\n**Target item**\\n**ALS** **BPR** **I2V** **U2V**\\n\\n\\nThe Beatles\\n\\n\\nRicky Nelson Beach Boys David Bowie The Kinks\\n*rock* *60s* *rock* *60s*\\n\\n\\n*classic rock* Souad Massi John Lennon Radiohead Rolling Stones\\n*female, arabic* *classic rock* *alternative* *classic rock*\\n\\nAndrés Segovia Ringo Starr Led Zeppelin Velvet Underground\\n*baroque* *classic rock* *hard rock, rock* *psychedelic*\\n\\n\\nEminem\\n\\n\\nIce Cube Jay-Z Ke$ha Akon\\n*hip-hop, rap* *hip-hop, rap* *pop, dance* *hip-hop, rap*\\n\\n\\n*hip-hop, rap* Bizarre 50cent P!nk Nelly\\n*hip-hop, rap* *hip-hop, rap* *pop, female* *hip-hop, rap*\\n\\nXzibit Kanye West Jay-Z Jason Derulo\\n*hip-hop, rap* *hip-hop, rap* *hip-hop, rap* *pop, rnb*\\n\\n\\nShakira\\n\\n\\nJuanes Beyoncé Rihanna Katy Perry\\n*latin, pop* *rnb, pop* *pop, rnb* *pop, female*\\n\\n\\n*female, pop* Fanny Lu Marilyn Monroe Beyoncé Mariah Carey\\n*latin, pop* *jazz, female* *rnb, pop* *rnb, pop*\\n\\nThalía Rihanna Britney Spears Beyoncé\\n*latin, pop* *pop, rnb* *pop, dance* *rnb, pop*\\n\\n**Table 3: Similarity table of five popular artists from the Last.FM dataset**\\n\\n**Re** **p** **resentation Model**\\n**Target item**\\n**ALS** **BPR** **I2V** **U2V**\\n\\n\\nFriday the 13th\\n\\n\\nA View to Kill Nigthmare in Elm Street Gremlins 2 Friday the 13th 2\\n*action* *horror* *comedy, horror* *horror*\\n\\n\\n*horror, thriller* Child’s Play Friday the 13th 3 Texas Chainsaw Massacre Halloween II\\n*horror, thriller* *horror* *horror* *horror*\\n\\nPet Sematary Children of the Corn Halloween Child’s Play\\n*horror* *horror* *horror* *horror, thriller*\\n\\n\\nTitanic\\n\\n\\nGroundhog Day Truman Show Gd. Will Hnt. Jurassic Park\\n*comedy* *comedy* *drama* *action, sci-fi*\\n\\n\\n*drama* Truman Show Catch Me If You Can Men in Black Truman Show\\n*comedy* *crime, drama* *action, sci-fi* *comedy*\\n\\nChristmas Do-Over My Best Friend’s Wedding Saving Private Ryan Men in Black\\n*comedy* *comedy* *drama, war* *action, sci-fi*\\n\\n\\nToy Story\\n\\n\\nAverage Italian Muppet Treasure Island Braveheart Lion King\\n*comedy* *children* *drama, war* *children*\\n\\n\\n*children* The Pride & The Passion Babe 12 Monkeys Toy Story 2\\n*war, action* *children* *sci-fi, thriller* *children*\\n\\nBarbie Jack-O The Usual Suspects Men in Black\\n*animation* *horror* *crime* *action, sci-fi*\\n\\n**Table 4: Similarity table of five popular movies from the MovieLens dataset**\\n\\n\\nIt is important to highlight that the conducted experiment does\\nnot have strong statistical rigor and may not represent an accurate\\nevaluation of the embedding models. The interview was conducted\\nwith only ten participants without concern about selecting persons\\nwith different backgrounds and belonging to contrasting demographic groups. Nonetheless, this is one of the main drawbacks of\\nthis evaluation scheme. Conducting a proper intruder detection\\ntask is very time and resource-demanding. Even so, the obtained\\nresults can provide some valuable insights about the models.\\n\\n### **7.3 Automatic Feature Prediction**\\n\\nIn every evaluated dataset, each item is described with a single\\nfeature related to the domain of the problem, such as genres for\\nmovies and styles for music artists. For each item in the datasets,\\nwe predicted their features using the most recurrent features of\\nother *𝑘* nearest items, with *𝑘* ranging between 10 *,* 15 and 20. For\\neach prediction, we checked if the selected feature was correct, and\\nusing the average multiclass precision and recall, we computed the\\nF1-score for each model, selecting the value for *𝑘* that resulted in\\nthe best F1-score. Table 6 shows the results.\\nWhen comparing the results, it is challenging to indicate a superior model. However, User2Vec performed best on BestBuy and for\\n\\n\\n350\\n\\n\\n-----\\n\\nWhy Ignore Content? A Guideline for Intrinsic Evaluation of Item Embeddings for Collaborative Filtering WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\n**Re** **p** **resentation Model**\\n**Seed items** **Dataset**\\n**ALS** **BPR** **I2V** **U2V**\\n\\n**Table 5: Accuracy for the intruder detection task**\\n\\nsome values of *𝑘* for the Delicious dataset, also being a close second\\non Last.FM. This indicates that the neural model discovered the\\n\\nmost about the intrinsic content of all evaluated methods for these\\nspecific datasets. This outcome is interesting when we compare\\nthe results of the feature prediction task with those of the extrinsic\\nexperiment. In the latter, User2Vec was the worst representation\\nmodel for every value of *𝑁* and dataset. For the BestBuy dataset,\\nthe NDCG when *𝑁* = 10 was more than five times worse than that\\nof BPR. This behavior highlights how the results of extrinsic and\\nintrinsic tasks can drastically differ.\\n\\n**Re** **p** **resentation Model**\\n**Dataset** *𝑘*\\n**ALS** **BPR** **I2V** **U2V**\\n\\n**Table 6: F1-score in an automatic feature prediction task with**\\n**different values of** *𝑘* **.**\\n\\nIn addition, we can see those methods that reached better scores\\nin Table 6 may differ from those of the intruder detection task. In\\nthe former, ALS was the most accurate model for datasets Last.FM\\nand Delicious when *𝑘* = 10, while the latter was the least accurate\\nfor every dataset, including the aforementioned ones. This shows\\nhow even different intrinsic metrics can achieve varying results,\\n\\n\\nespecially when comparing subjective approaches to objective ones\\nsince the former is more prone to human bias.\\n### **7.4 Content-based Ranking Comparison**\\n\\nLastly, we have calculated the three metrics for assessing the item\\nembeddings’ intrinsic quality using a content-based ranking comparison, as detailed in Section 4.2. Results for the Spearman correlation coefficient are shown in Table 7, for the Jaccard Index in\\nTable 8, and NDCG in Table 9. For both the Jaccard Index and the\\nNDCG, we used a neighborhood size *𝑘* ranging from {10 *,* 15 *,* 20}.\\nLike the automatic feature prediction task, the performance varied widely according to the metric and dataset, with each model\\nscoring higher in a specific case. For Spearman’s *𝜌*, Item2Vec was\\nthe best model for Anime and BestBuy datasets, contrary to what\\nhappened on the intruder detection and automatic feature prediction, in which the model was surpassed by BPR and User2Vec,\\ndepending on the dataset and task. User2Vec achieved the best\\nresults for Delicious and Last.FM, which is also impressive since\\nits behavior on the intruder detection task for dataset Last.FM was\\npoorly, being the worst or second-worst model.\\nWhen limiting the observed neighborhood to a subset of items,\\nas it is performed on the Jaccard Index and NDCG, the scores\\nwere vastly different from Spearman’s *𝜌* . For the Jaccard Index,\\nItem2Vec achieved the worst results for the Anime dataset and the\\nbest for Delicious and Last.FM. User2Vec presented the highest\\nscores for BestBuy and competitive performance in the Anime\\ndataset. Although ALS achieved some promising results for the\\nLast.FM dataset, it was surpassed by every other model for every\\ndataset. Finally, for the NDCG, the results were similar to the ones\\nobtained in the automatic feature prediction (Table 6), which is\\nexpected given that both metrics limit the observed neighborhood\\nand weigh their scores according to the content information.\\nThe experiment shows how different metrics of ranking comparison can achieve different results for the same representation model\\nand dataset. The use of a specific metric can vary according to the\\nanalysis’s objective and the content-based representation’s characteristics. When we want to evaluate the entire ranking, considering\\nonly the relative position of items, rank correlation metrics, such as\\nSpearman’s *𝜌*, are more well-suited. In cases where only the quality\\nof the neighborhood is important, disregarding the intensity of the\\nitem’s similarity, metrics of set similarity are more recommended,\\nsuch as the Jaccard Index. Lastly, when we are only interested in\\nthe neighborhood’s items but want to weigh the results according to their similarity scores and rank positions, we can calculate\\nutility-based metrics such as NDCG.\\nWhen comparing the content-based ranking metrics and the\\nachieved results for the extrinsic evaluation, it is clear how representation models not useful for generating recommendations may\\nstill have value when used in intrinsic tasks. User2Vec, statistically\\nproven as the worst method for the top- *𝑁* recommendation problem, presented excellent results for some datasets on the contentbased analysis, especially when calculating the NDCG. Additionally,\\nthe differences in the obtained results with the intruder detection’s\\naccuracy show how subjective approaches can lead to contrasting conclusions about the model’s quality. This entire comparison\\ndemonstrates how a thoroughly performed analysis can lead to\\nmore knowledge about the behavior of representation models.\\n\\n\\n351\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Pedro R. Pires, Bruno B. Rizzi, and Tiago A. Almeida\\n\\n\\n**Re** **p** **resentation Model**\\n**Dataset**\\n**ALS** **BPR** **I2V** **U2V**\\n\\n**Table 7: Spearman’s rank correlation coefficient** *𝜌* **of the**\\n**content-based ranking comparison.**\\n\\n**Re** **p** **resentation Model**\\n**Dataset** *𝑘*\\n**ALS** **BPR** **I2V** **U2V**\\n\\n**Table 8: Jaccard Index** @ *𝑘* **of the content-based ranking com-**\\n**parison, with different values of** *𝑘* **.**\\n### **8 CONCLUSION**\\n\\nEmbeddings with strong intrinsic meaning can benefit many tasks\\nbeyond recommendation. While intrinsic evaluation has gained attention in NLP, it is poised to become a focal point in recommender\\nsystems. However, intrinsic evaluations of vector representations\\nfor recommender systems are rarely conducted, with most studies focusing solely on extrinsic assessments. Even when intrinsic\\nevaluations are performed, they often rely on human interaction,\\nwhich can be time-consuming and susceptible to human bias.\\nThis study presented approaches to assess the intrinsic quality of\\nitem embeddings. We first detailed a well-known evaluation method\\nand adapted an NLP evaluation task for recommender systems.\\nSince both methods are subjective and rely on human opinions,\\nwe also introduced two evaluation schemes based on objective\\nmetrics: a feature prediction task and a novel strategy for obtaining\\na quantitative score through content-based ranking comparison.\\nFor the latter, we provided various metrics to assess ranking quality.\\nWe compared four models that learn item embeddings across these\\nevaluation approaches and conducted an extrinsic evaluation via\\ntraditional top- *𝑁* ranking recommendation.\\n\\n\\n**Re** **p** **resentation Model**\\n**Dataset** *𝑘*\\n**ALS** **BPR** **I2V** **U2V**\\n\\n**Table 9: NDCG** @ *𝑘* **of the content-based ranking comparison,**\\n**with different values of** *𝑘* **.**\\n\\nThe extrinsic evaluation revealed similarities between the two\\n\\nmatrix factorization methods and Item2Vec. Each model excelled\\non specific datasets, making it difficult to declare a superior model.\\nConversely, User2Vec performed poorly across all datasets in the\\nextrinsic evaluation, emerging as the worst method. Intriguingly,\\nthe intrinsic tasks were the opposite. User2Vec excelled in generating representations with intrinsic value, ranking first or second\\nfor most datasets for both subjective and objective approaches. Our\\nfindings highlight the necessity of careful intrinsic evaluation to\\navoid misleading impressions of a model’s capabilities.\\nConsidering the presented metrics assume that metadata accurately describes the item, which may only sometimes be the case,\\nfor future research, we recommend a detailed study of contentbased representation and item-sorting methods to improve the proposed strategy’s quality. Utilizing more domain-specific datasets\\nwith extended feature sets and descriptive attributes can enrich\\nthe results. We also plan to analyze why each algorithm performed\\nbetter in each task and dataset. Finding which characteristics favor\\nspecific evaluation scenarios can help future researchers develop\\ntask-specific recommenders. Additionally, a similar study focusing\\non user embeddings is suggested, using demographic information\\nto overcome the problem of data scarcity users commonly have.\\nUltimately, future studies on item embeddings should heed their\\nintrinsic quality. An in-depth analysis can offer a comprehensive\\nview of the models, useful for tasks beyond recommendation and\\npotentially accelerating the development of new methods.\\n### **ACKNOWLEDGMENTS**\\n\\nThis study was financed by the Brazilian Coordination for the\\nImprovement of Higher Education Personnel (CAPES), Finance\\nCode 88882.426978/2019-01; the Brazilian National Council for\\nScientific and Technological Development (CNPq); and The São\\nPaulo Research Foundation (FAPESP), grants #2019/15393-4 and\\n#2021/14591-7.\\n\\n\\n352\\n\\n\\n-----\\n\\nWhy Ignore Content? A Guideline for Intrinsic Evaluation of Item Embeddings for Collaborative Filtering WebMedia’2024, Juiz de Fora, Brazil\\n\\n### **REFERENCES**\\n\\n[1] Gediminas Adomavicius and Alexander Tuzhilin. 2005. Toward the Next Generation of Recommender Systems: A Survey of the State-of-the-Art and Possible\\nExtensions. *IEEE Transactions on Knowledge and Data Engineering* 17, 6 (2005),\\n734–749. https://doi.org/10.1109/TKDE.2005.99\\n\\n[2] Oren Barkan and Noam Koenigstein. 2016. Item2Vec: Neural Item Embedding\\nFor Collaborative Filtering. In *IEEE 26th International Workshop on Machine*\\n*Learning for Signal Processing (MLSP 2016)* . IEEE, Vietri sul Mare, Italy, 1–6.\\nhttps://doi.org/10.1109/MLSP.2016.7738886\\n\\n[3] Marco Baroni, Georgiana Dinu, and Germán Kruszewski. 2014. Don’t count,\\npredict! A systematic comparison of context-counting vs. context-predicting\\nsemantic vectors. In *Proceedings of the 52nd Annual Meeting of the Association for*\\n*Computational Linguistics (ACL ‘14)* . Association for Computational Linguistics,\\nBaltimore, MD, USA, 238–247. https://doi.org/10.3115/v1/P14-1023\\n\\n[4] J. Bobadilla, F. Ortega, A. Hernando, and A. Gutiérrez. 2013. Recommender\\nsystems survey. *Knowledge-Based Systems* 46 (2013), 109–132. https://doi.org/10.\\n1016/j.knosys.2013.03.012\\n\\n[5] Hugo Caselles-Duprés, Florian Lesaint, and Jimena Royo-Letelier. 2018. Word2vec\\napplied to recommendation: hyperparameters matter. In *Proceedings of the 12th*\\n*ACM Conference on Recommender Systems (RecSys ‘18)* . Association for Computing Machinery, Vancouver, Canada, 352–356. https://doi.org/10.1145/3240323.\\n3240377\\n\\n[6] Chao Chang, Junming Zhou, Yu Weng, Xiangwei Zeng, Zhengyang Wu, ChangDong Wang, and Yong Tang. 2023. KGTN: Knowledge Graph Transformer\\nNetwork for explainable multi-category item recommendation. *Knowledge-Based*\\n*Systems* 278 (2023), 110854. https://doi.org/10.1016/j.knosys.2023.110854\\n\\n[7] Hao Chen, Zefan Wang, Feiran Huang, Xiao Huang, Yue Xu, Yishi Lin, Peng He,\\nand Zhoujun Li. 2022. Generative Adversarial Framework for Cold-Start Item\\nRecommendation. In *Proceedings of the 45th International ACM SIGIR Conference*\\n*on Research and Development in Information Retrieval (SIGIR ’22)* . Association\\nfor Computing Machinery, Anchorage, AK, USA, 2565–2571. https://doi.org/10.\\n1145/3477495.3531897\\n\\n[8] Gabriel de Souza P. Moreira, Dietmar Jannach, and Adilson Marques da Cunha.\\n2019. On the Importance of News Content Representation in Hybrid Neural\\nSession-based Recommender Systems. *IEEE Access* 7 (2019), 169185–169203.\\nhttps://doi.org/10.1109/ACCESS.2019.2954957\\n\\n[9] Janez Demšar. 2006. Statistical Comparisons of Classifiers over Multiple Data\\nSets. *The Journal of Machine Learning Research* 7 (2006), 1–30. https://doi.org/10.\\n5555/1248547.1248548\\n\\n[10] Chengxin Ding, Zhongying Zhao, Chao Li, Yanwei Yu, and Qingtian Zeng. 2023.\\nSession-based recommendation with hypergraph convolutional networks and\\nsequential information embeddings. *Expert Systems with Applications* 223, 119875\\n(2023), 1–11. https://doi.org/10.1016/j.eswa.2023.119875\\n\\n[11] Douglas Eck, Paul Lamere, Thierry Bertin-Mahieux, and Stephen Green. 2007.\\nAutomatic generation of social tags for music recommendation. In *Proceedings of*\\n*the 20th International Conference on Neural Information Processing Systems (NIPS*\\n*2007)* . Curran Associates Inc., Vancouver, Canada, 385–392. https://doi.org/10.\\n5555/2981562.2981611\\n\\n[12] Manaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi, and Chris Dyer. 2016.\\nProblems With Evaluation of Word Embeddings Using Word Similarity Tasks.\\nIn *Proceedings of the 1st Workshop on Evaluating Vector Space Representations*\\n*for NLP* . Association for Computational Linguistics, Berlin, Germany, 30–35.\\nhttps://doi.org/10.18653/v1/W16-2506\\n\\n[13] Ralph José Rassweiler Filho, Jônatas Wehrmann, and Rodrigo C. Barros. 2017.\\nLeveraging Deep Visual Features for Content-based Movie Recommender Systems. In *Proceedings of the 2017 International Joint Conference on Neural Networks*\\n*(IJCNN 2017)* . IEEE, Anchorage, AK, USA, 604–611. https://doi.org/10.1109/\\nIJCNN.2017.7965908\\n\\n[14] Claudiu S. Firan, Wolfgang Nejdl, and Raluca Paiu. 2007. The Benefit of Using TagBased Profiles. In *Proceedings of the 5th Latin American Web Conference (LA-WEB*\\n*‘07)* . IEEE Computer Society, Santiago, Chile, 32–41. https://doi.org/10.1109/LAWEB.2007.24\\n\\n[15] Peng FU, Jiang hua LV, Shi long MA, and Bing jie LI. 2017. Attr2vec: A Neural\\nNetwork Based Item Embedding Method. In *Proceedings of the 2nd International*\\n*Conference on Computer, Mechatronics and Electronic Engineering (CMEE 2017)* .\\nDEStech Publications, Xiamen, China, 300–307. https://doi.org/10.12783/dtcse/\\ncmee2017/19993\\n\\n[16] Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei\\nZhang. 2023. Chat-REC: Towards Interactive and Explainable LLMs-Augmented\\nRecommender System. *arXiv:* 2303.14524 (2023), 1–17. https://doi.org/10.48550/\\narXiv.2303.14524\\n\\n[17] Anna Gladkova and Aleksandr Drozd. 2016. Intrinsic Evaluations of Word\\nEmbeddings: What Can We Do Better?. In *Proceedings of the 1st Workshop on*\\n*Evaluating Vector Space Representations for NLP* . Association for Computational\\nLinguistics, Berlin, Germany, 36–42. https://doi.org/10.18653/v1/W16-2507\\n\\n[18] Mihajlo Grbovic, Vladan Radosavljevic, Nemanja Djuric, Narayan Bhamidipati,\\nJaikit Savla, Varun Bhagwan, and Doug Sharp. 2015. E-commerce in Your Inbox:\\n\\n\\nProduct Recommendations at Scale. In *Proceedings of the 21th ACM SIGKDD*\\n*International Conference on Knowledge Discovery and Data Mining (KDD ‘15)* .\\nAssociation for Computing Machinery, Sydney, Australia, 1809–1818. https:\\n//doi.org/10.1145/2783258.2788627\\n\\n[19] Asnat Greenstein-Messica, Lior Rokach, and Michael Friedman. 2017. SessionBased Recommendations Using Item Embedding. In *Proceedings of the 22nd*\\n*International Conference on Intelligent User Interfaces (IUI ‘17)* . Association for\\nComputing Machinery, Limassol, Cyprus, 629–633. https://doi.org/10.1145/\\n3025171.3025197\\n\\n[20] S. Hasanzadeh, S. M. Fakhrahmad, and M. Taheri. 2020. Review-Based Recommender Systems: A Proposed Rating Prediction Scheme Using Word Embedding Representation of Reviews. *Comput. J.* bxaa044, ; (2020), 1–10. https:\\n//doi.org/10.1093/comjnl/bxaa044\\n\\n[21] Antonio Hernando, JesÚs Bobadilla, and Fernando Ortega. 2016. A Non Negative\\nMatrix Factorization for Collaborative Filtering Recommender Systems Based on\\na Bayesian Probabilistic Model. *Knowledge-Based Systems* 97, C (2016), 188—-202.\\nhttps://doi.org/10.1016/j.knosys.2015.12.018\\n\\n[22] Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.\\n2016. Session-Based Recommendations with Recurrent Neural Networks. In\\n*Proceedings of the International Conference on Learning Representations (ICLR*\\n*2016)* . OpenReview, San Juan, Puerto Rico, 1–10.\\n\\n[23] Yifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Collaborative Filtering for\\nImplicit Feedback Datasets. In *Proceedings of the 8th IEEE International Conference*\\n*on Data Mining (ICDM ‘08)* . IEEE Computer Society, Pisa, Italy, 263–272. https:\\n//doi.org/10.1109/ICDM.2008.22\\n\\n[24] Salmo M.S. Júnior and Marcelo G. Manzato. 2015. Collaborative Filtering Based on\\nSemantic Distance Among Items. In *Proceedings of the 21st Brazilian Symposium on*\\n*Multimedia and the Web (WebMedia ’15)* . Association for Computing Machinery,\\nManaus, Brazil, 53–56. https://doi.org/10.1145/2820426.2820466\\n\\n[25] Shan Khsuro, Zafar Ali, and Irfan Ullah. 2016. Recommender Systems: Issues, Challenges, and Research Opportunities. In *Proceedings of the 7th In-*\\n*ternational Conference on Information Science and Applications (ICISA 2016)* .\\nSpringer Science+Business Media, Ho Chi Minh, Vietnam, 1179–1189. https:\\n//doi.org/10.1007/978-981-10-0557-2_112\\n\\n[26] Jooeun Kim, Jinri Kim, Kwangeun Yeo, Eungi Kim, Kyoung-Woon On, Jonghwan\\nMun, and Joonseok Lee. 2024. General Item Representation Learning for Coldstart Content Recommendations. *arXiv:* 2404.13808 (2024), 1–14. https://doi.org/\\n10.48550/arXiv.2404.13808\\n\\n[27] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix Factorization\\nTechniques For Recommender Systems. *Computer* 42, 8 (2009), 30–37. https:\\n//doi.org/10.1109/MC.2009.263\\n\\n[28] Quoc Le and Tomas Mikolov. 2014. Distributed Representations of Sentences\\nand Documents. In *Proceedings of the 31st International Conference on Machine*\\n*Learning (ICML 2014)* . JMLR.org, Beijing, China, 1188–1196. https://doi.org/10.\\n5555/3044805.3045025\\n\\n[29] Pasquale Lisena, Albert Meroño-Peñuela, and Raphaëla Troncy. 2022. MIDI2vec:\\nLearning MIDI embeddings for reliable prediction of symbolic music metadata.\\n*Semantic Web* 13, 3 (2022), 357–377. https://doi.org/10.3233/SW-210446\\n\\n[30] Junling Liu, Chao Liu, Peilin Zhou, Qichen Ye, Dading Chong, Kang Zhou,\\nYueqi Xie, Yuwei Cao, Shoujin Wang, Chenyu You, and Philip S.Yu. 2023. LLMRec: Benchmarking Large Language Models on Recommendation Task. *arXiv:*\\n2308.12241 (2023), 1–13. https://doi.org/10.48550/arXiv.2308.12241\\n\\n[31] Jie Lu, Dianshuang Wu, Mingsong Mao, Wei Wang, and Guangquan Zhang. 2015.\\nRecommender system application developments: A survey. *Decision Support*\\n*Systems* 74 (2015), 12–32. https://doi.org/10.1016/j.dss.2015.03.008\\n\\n[32] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Conrado, and Jeffrey Dan. 2013.\\nDistributed Representations of Words and Phrases and their Compositionality. In\\n*Proceedings of the 26th International Conference on Neural Information Processing*\\n*Systems (NIPS 2013)* . Curran Associates Inc., Stateline, NV, USA, 3111–3119. https:\\n//doi.org/10.5555/2999792.2999959\\n\\n[33] Cataldo Musto, Pasquale Lops, Marco de Gemmis, and Giovanni Semeraro. 2017.\\nSemantics-aware Recommender Systems exploiting Linked Open Data and graphbased features. *Knowledge-Based Systems* 136 (2017), 1–14. https://doi.org/10.\\n1016/j.knosys.2017.08.015\\n\\n[34] Makbule Gulcin Ozsoy. 2016. From Word Embeddings to Item Recommendation.\\n*arXiv:* 1601.01356 (2016), 1–8. https://doi.org/10.48550/arXiv.1601.01356\\n\\n[35] Yuanyuan Qiu, Hongzheng Li, Shen Li, Yingdi Jiang, Renfen Hu, and Lijiao Yang.\\n2018. Revisiting Correlations between Intrinsic and Extrinsic Evaluations of\\nWord Embeddings. In *Chinese Computational Linguistics and Natural Language*\\n*Processing Based on Naturally Annotated Big Data (CCL 2018)* . Springer, Changsha,\\nChina, 209–221. https://doi.org/10.1007/978-3-030-01716-3_18\\n\\n[36] Radim Řehůřek and Petr Sojka. 2010. Software Framework for Topic Modelling\\nwith Large Corpora. In *Proceedings of the LREC 2010 Workshop on New Challenges*\\n*for NLP Frameworks (LREC 2010)* . European Language Resources Association\\n(ELRA), Valletta, Malta, 45–50. https://doi.org/10.13140/2.1.2393.1847\\n\\n[37] Stefen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\\n2009. BPR: Bayesian Personalized Ranking from Implicit Feedback. In *Proceedings*\\n\\n\\n353\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Pedro R. Pires, Bruno B. Rizzi, and Tiago A. Almeida\\n\\n\\n*of the 25th Conference on Uncertainty in Artificial Intelligence (UAI ‘09)* . AUAI\\nPress, Montreal, Canada, 452–461. https://doi.org/10.5555/1795114.1795167\\n\\n[38] Steffen Rendle, Walid Krichene, Li Zhang, and John Anderson. 2020. Neural\\nCollaborative Filtering vs. Matrix Factorization Revisited. In *Proceedings of the*\\n*14th ACM Conference on Recommender Systems (RecSys ’20)* . Association for\\nComputing Machinery, Virtual Event, Brazil, 240–248. https://doi.org/10.1145/\\n3383313.3412488\\n\\n[39] Steffen Rendle, Walid Krichene, Li Zhang, and Yehuda Koren. 2022. Revisiting\\nthe Performance of iALS on Item Recommendation Benchmarks. In *Proceedings*\\n*of the 16th ACM Conference on Recommender Systems (RecSys ’22)* . Association\\nfor Computing Machinery, Seattle, WA, USA, 427–435. https://doi.org/10.1145/\\n3523227.3548486\\n\\n[40] Badrul M. Sarwar, George Karypis, Joseph A. Konstan, and John T. Riedl. 2000.\\nApplication of Dimensionality Reduction in Recommender System - A Case\\nStudy. In *Proceedings of the 9th WebKDD Workshop on Web Mining for e-commerce*\\n*(WebKDD ‘00)* . Association for Computing Machinery, Boston, Massachusetts,\\nUSA, 1–12. https://doi.org/10.21236/ada439541\\n\\n[41] Tobias Schnabel, Igor Labutov, David Mimno, and Thorsten Joachims. 2015.\\nEvaluation methods for unsupervised word embeddings. In *Proceedings of the*\\n*2015 Conference on Empirical Methods in Natural Language Processing (EMNLP*\\n*2015)* . Association for Computational Linguistics, Lisbon, Portugal, 298–307.\\nhttps://doi.org/10.18653/v1/D15-1036\\n\\n[42] Guy Shani and Asela Gunawardana. 2011. Evaluating Recommendation Systems.\\nIn *Recommender Systems Handbook*, Francesco Ricci, Lior Rokach, Bracha Shapira,\\nand Paul B. Kantor (Eds.). Springer US, New York, NY, USA, Chapter 8, 257–259.\\nhttps://doi.org/10.1007/978-0-387-85820-3\\n\\n[43] Sumit Sidana, Mikhail Trofimov, Oleh Horodnytskyi, Charlotte Laclau, Yury\\nMaximov, and Massih-Reza Amini. 2021. User preference and embedding learning\\nwith implicit feedback for recommender systems. *Data Mining and Knowledge*\\n*Discovery* 35 (2021), 568–592. https://doi.org/10.1007/s10618-020-00730-8\\n\\n[44] Abe Vallerian Siswanto, Lilian Tjong, and Yordan Saputra. 2018. Simple Vector\\nRepresentations of E-commerce Products. In *2018 International Conference on*\\n*Asian Language Processing (IALP 2018)* . IEEE, Bandung, Indonesia, 368–372. https:\\n//doi.org/10.1109/IALP.2018.8629245\\n\\n[45] Yang Song, Lu Zhang, and Clyde Lee Giles. 2011. Automatic tag recommendation\\nalgorithms for social recommender systems. *ACM Transactions on the Web* 4, 1\\n(2011), 4:1–4:31. https://doi.org/10.1145/1921591.1921595\\n\\n[46] Jiaxi Tang and Ke Wang. 2018. Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding. In *Proceedings of the 11th ACM*\\n*International Conference on Web Search and Data Mining (WSDM ‘18)* . Association for Computing Machinery, Marina Del Rey, CA, USA, 565–573. https:\\n//doi.org/10.1145/2939672.2939673\\n\\n[47] Flavian Vasile, Elena Smirnova, and Alexis Conneau. 2016. Meta-Prod2Vec:\\nProduct Embeddings Using Side-Information for Recommendation. In *Proceedings*\\n*of the 10th ACM Conference on Recommender Systems (RecSys ‘16)* . Association\\nfor Computing Machinery, Boston, Massachusetts, USA, 225–232. https://doi.\\norg/10.1145/2959100.2959160\\n\\n[48] Dongjing Wang, Guandong Xu, and Shuiguang Deng. 2017. Music recommendation via heterogeneous information graph embedding. In *Proceedings of the 2017*\\n*International Joint Conference on Neural Networks (IJCNN 2017)* . IEEE, Anchorage,\\nAK, USA, 596–603. https://doi.org/10.1109/IJCNN.2017.7965907\\n\\n[49] Jiaqi Wang and Jing Lv. 2020. Tag-informed collaborative topic modeling for\\ncross domain recommendations. *Knowledge-Based Systems* 203 (2020), 106119.\\nhttps://doi.org/10.1016/j.knosys.2020.106119\\n\\n[50] Qinyong Wang, Hongzhi Yin, Hao Wang, Quoc Viet Hung Nguyen, Zi Huang,\\nand Lizhen Cui. 2019. Enhancing Collaborative Filtering with Generative Augmentation. In *Proceedings of the 25th ACM SIGKDD International Conference on*\\n*Knowledge Discovery and Data Mining (KDD ’19)* . Association for Computing Machinery, Anchorage, AK, USA, 548–556. https://doi.org/10.1145/3292500.3330873\\n\\n[51] Tian Wang, Yuri M. Brovman, and Sriganesh Madhvanath. 2021. Personalized\\nEmbedding-based e-Commerce Recommendations at eBay. *arXiv:* 2102.06156\\n(2021), 1–9. https://doi.org/10.48550/arXiv.2102.06156\\n\\n[52] Heitor Werneck, Nícollas Silva, Matheus Carvalho Viana, Fernando Mour ao,\\nAdriano C. M. Pereira, and Leonardo Rocha. 2020. A Survey on Point-of-Interest\\nRecommendation in Location-based Social Networks. In *Proceedings of the Brazil-*\\n*ian Symposium on Multimedia and the Web (WebMedia ’20)* . Association for Computing Machinery, São Luís, Brazil, 185–192. https://doi.org/10.1145/3428658.\\n3430970\\n\\n[53] Dongqiang Yang, Ning Li, Li Zou, and Hongwei Ma. 2022. Lexical semantics\\nenhanced neural word embeddings. *Knowledge-Based Systems* 252 (2022), 109298.\\nhttps://doi.org/10.1016/j.knosys.2022.109298\\n\\n[54] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Jundong Li, and Zi Huang. 2024.\\nSelf-Supervised Learning for Recommender Systems: A Survey. *IEEE Transactions*\\n*on Knowledge and Data Engineering* 36 (2024), 335–355. https://doi.org/10.1109/\\nTKDE.2023.3282907\\n\\n[55] Hafed Zarzour, Ziad A. Al-Sharif, and Yaser Jararweh. 2019. RecDNNing:\\na recommender system using deep neural network with user and item embeddings. In *Proceedings of the 10th International Conference on Information*\\n\\n\\n*and Communication Systems (ICICS 2019)* . IEEE, Irbid, Jordan, 99–103. https:\\n//doi.org/10.1109/IACS.2019.8809156\\n\\n[56] Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, and Wei-Ying Ma.\\n2016. Collaborative Knowledge Base Embedding for Recommender Systems.\\nIn *Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge*\\n*Discovery and Data Mining (KDD ‘16)* . Association for Computing Machinery,\\nSan Francisco, CA, USA, 353–362. https://doi.org/10.1145/2939672.2939673\\n\\n[57] Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019. Deep Learning Based\\nRecommender System: A Survey and New Perspectives. *ACM Comput. Surv.* 52,\\n1 (2019), 5:1–5:35. https://doi.org/10.1145/3285029\\n\\n[58] Xiangyu Zhao, Maolin Wang, Xinjian Zhao, Jiansheng Li, Shucheng Zhou, Dawei\\nYin, Qing Li, Jiliang Tang, and Ruocheng Guo. 2023. Embedding in Recommender\\nSystems: A Survey. *arXiv:* 2310.18608 (2023), 1–42. https://doi.org/10.48550/\\narXiv.2310.18608\\n\\n[59] Lütfi Kerem Şenel, İhsan Utlu, Veysel Yücesoy, Aykut Koç, and Tolga Çukur.\\n2018. Semantic Structure and Interpretability of Word Embeddings. *IEEE/ACM*\\n*Transactions on Audio, Speech, and Language Processing* 26, 10 (2018), 1769–1779.\\nhttps://doi.org/10.1109/TASLP.2018.2837384\\n\\n\\n354\\n\\n\\n-----\\n\\n',\n",
       "  'artigo_tokenizado': ['#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Why',\n",
       "   'Ignore',\n",
       "   'Content',\n",
       "   '?',\n",
       "   'A',\n",
       "   'Guideline',\n",
       "   'for',\n",
       "   'Intrinsic',\n",
       "   'Evaluation',\n",
       "   'of',\n",
       "   'Item',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Embeddings',\n",
       "   'for',\n",
       "   'Collaborative',\n",
       "   'Filtering',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Pedro',\n",
       "   'R.',\n",
       "   'Pires',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'pedro.pires@dcomp.sor.ufscar.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'São',\n",
       "   'Carlos',\n",
       "   'São',\n",
       "   'Carlos',\n",
       "   ',',\n",
       "   'SP',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Bruno',\n",
       "   'B.',\n",
       "   'Rizzi',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'brunosora@hotmail.com',\n",
       "   'BTG',\n",
       "   'Pactual',\n",
       "   'São',\n",
       "   'Paulo',\n",
       "   ',',\n",
       "   'SP',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Tiago',\n",
       "   'A.',\n",
       "   'Almeida',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'talmeida@ufscar.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'São',\n",
       "   'Carlos',\n",
       "   'Sorocaba',\n",
       "   ',',\n",
       "   'SP',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'ABSTRACT',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'With',\n",
       "   'the',\n",
       "   'constant',\n",
       "   'growth',\n",
       "   'in',\n",
       "   'available',\n",
       "   'information',\n",
       "   'and',\n",
       "   'the',\n",
       "   'popularization',\n",
       "   'of',\n",
       "   'technology',\n",
       "   ',',\n",
       "   'recommender',\n",
       "   'systems',\n",
       "   'have',\n",
       "   'to',\n",
       "   'deal',\n",
       "   'with',\n",
       "   'an',\n",
       "   '\\n',\n",
       "   'increasing',\n",
       "   'number',\n",
       "   'of',\n",
       "   'users',\n",
       "   'and',\n",
       "   'items',\n",
       "   '.',\n",
       "   'This',\n",
       "   'leads',\n",
       "   'to',\n",
       "   'two',\n",
       "   'problems',\n",
       "   '\\n',\n",
       "   'in',\n",
       "   'representing',\n",
       "   'items',\n",
       "   ':',\n",
       "   'scalability',\n",
       "   'and',\n",
       "   'sparsity',\n",
       "   '.',\n",
       "   'Therefore',\n",
       "   ',',\n",
       "   'many',\n",
       "   '\\n',\n",
       "   'recommender',\n",
       "   'systems',\n",
       "   'aim',\n",
       "   'to',\n",
       "   'generate',\n",
       "   'low',\n",
       "   '-',\n",
       "   'dimensional',\n",
       "   'dense',\n",
       "   'representations',\n",
       "   'of',\n",
       "   'items',\n",
       "   '.',\n",
       "   'Matrix',\n",
       "   'factorization',\n",
       "   'techniques',\n",
       "   'are',\n",
       "   'popular',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'but',\n",
       "   'models',\n",
       "   'based',\n",
       "   'on',\n",
       "   'neural',\n",
       "   'embeddings',\n",
       "   'have',\n",
       "   'recently',\n",
       "   'been',\n",
       "   'proposed',\n",
       "   'and',\n",
       "   'are',\n",
       "   'gaining',\n",
       "   'ground',\n",
       "   'in',\n",
       "   'the',\n",
       "   'literature',\n",
       "   '.',\n",
       "   'Their',\n",
       "   'main',\n",
       "   'goal',\n",
       "   'is',\n",
       "   'to',\n",
       "   '\\n',\n",
       "   'learn',\n",
       "   'dense',\n",
       "   'representations',\n",
       "   'with',\n",
       "   'intrinsic',\n",
       "   'meaning',\n",
       "   '.',\n",
       "   'However',\n",
       "   ',',\n",
       "   'most',\n",
       "   '\\n',\n",
       "   'studies',\n",
       "   'proposing',\n",
       "   'embeddings',\n",
       "   'for',\n",
       "   'recommender',\n",
       "   'systems',\n",
       "   'ignore',\n",
       "   '\\n',\n",
       "   'this',\n",
       "   'property',\n",
       "   'and',\n",
       "   'focus',\n",
       "   'only',\n",
       "   'on',\n",
       "   'extrinsic',\n",
       "   'evaluations',\n",
       "   '.',\n",
       "   'This',\n",
       "   'study',\n",
       "   '\\n',\n",
       "   'presents',\n",
       "   'a',\n",
       "   'guideline',\n",
       "   'for',\n",
       "   'assessing',\n",
       "   'the',\n",
       "   'intrinsic',\n",
       "   'quality',\n",
       "   'of',\n",
       "   'matrix',\n",
       "   '\\n',\n",
       "   'factorization',\n",
       "   'and',\n",
       "   'neural',\n",
       "   '-',\n",
       "   'based',\n",
       "   'embedding',\n",
       "   'models',\n",
       "   'for',\n",
       "   'collaborative',\n",
       "   '\\n',\n",
       "   'filtering',\n",
       "   ',',\n",
       "   'comparing',\n",
       "   'the',\n",
       "   'results',\n",
       "   'with',\n",
       "   'a',\n",
       "   'traditional',\n",
       "   'extrinsic',\n",
       "   'evaluation',\n",
       "   '.',\n",
       "   'To',\n",
       "   'enrich',\n",
       "   'the',\n",
       "   'evaluation',\n",
       "   'pipeline',\n",
       "   ',',\n",
       "   'we',\n",
       "   'suggest',\n",
       "   'adapting',\n",
       "   'an',\n",
       "   '\\n',\n",
       "   'intrinsic',\n",
       "   'evaluation',\n",
       "   'task',\n",
       "   'commonly',\n",
       "   'employed',\n",
       "   'in',\n",
       "   'the',\n",
       "   'Natural',\n",
       "   'Language',\n",
       "   'Processing',\n",
       "   'literature',\n",
       "   ',',\n",
       "   'and',\n",
       "   'we',\n",
       "   'propose',\n",
       "   'a',\n",
       "   'novel',\n",
       "   'strategy',\n",
       "   'for',\n",
       "   '\\n',\n",
       "   'evaluating',\n",
       "   'the',\n",
       "   'learned',\n",
       "   'representation',\n",
       "   'compared',\n",
       "   'to',\n",
       "   'a',\n",
       "   'content',\n",
       "   '-',\n",
       "   'based',\n",
       "   '\\n',\n",
       "   'scenario',\n",
       "   '.',\n",
       "   'Finally',\n",
       "   ',',\n",
       "   'every',\n",
       "   'mentioned',\n",
       "   'technique',\n",
       "   'is',\n",
       "   'analyzed',\n",
       "   'over',\n",
       "   'established',\n",
       "   'recommender',\n",
       "   'models',\n",
       "   ',',\n",
       "   'and',\n",
       "   'the',\n",
       "   'results',\n",
       "   'show',\n",
       "   'how',\n",
       "   'vector',\n",
       "   '\\n',\n",
       "   'representations',\n",
       "   'that',\n",
       "   'do',\n",
       "   'not',\n",
       "   'yield',\n",
       "   'good',\n",
       "   'recommendations',\n",
       "   'can',\n",
       "   'still',\n",
       "   'be',\n",
       "   '\\n',\n",
       "   'useful',\n",
       "   'in',\n",
       "   'other',\n",
       "   'tasks',\n",
       "   'that',\n",
       "   'demand',\n",
       "   'intrinsic',\n",
       "   'knowledge',\n",
       "   ',',\n",
       "   'highlighting',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'potential',\n",
       "   'of',\n",
       "   'this',\n",
       "   'perspective',\n",
       "   'of',\n",
       "   'evaluation',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'KEYWORDS',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'embeddings',\n",
       "   ',',\n",
       "   'intrinsic',\n",
       "   'evaluation',\n",
       "   ',',\n",
       "   'qualitative',\n",
       "   'evaluation',\n",
       "   ',',\n",
       "   'recommender',\n",
       "   'systems',\n",
       "   ',',\n",
       "   'similarity',\n",
       "   'tables',\n",
       "   ',',\n",
       "   'intruder',\n",
       "   'detection',\n",
       "   ',',\n",
       "   'autotagging',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   '1',\n",
       "   'INTRODUCTION',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Recommender',\n",
       "   'systems',\n",
       "   'are',\n",
       "   'tools',\n",
       "   'commonly',\n",
       "   'used',\n",
       "   'by',\n",
       "   'companies',\n",
       "   'for',\n",
       "   '\\n',\n",
       "   'enhancing',\n",
       "   'the',\n",
       "   'experience',\n",
       "   'users',\n",
       "   'have',\n",
       "   'when',\n",
       "   'utilizing',\n",
       "   'their',\n",
       "   'services',\n",
       "   '\\n',\n",
       "   'by',\n",
       "   'filtering',\n",
       "   'and',\n",
       "   'recommending',\n",
       "   'particularly',\n",
       "   'relevant',\n",
       "   'information',\n",
       "   '[',\n",
       "   '1',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Among',\n",
       "   'different',\n",
       "   'types',\n",
       "   'of',\n",
       "   'recommender',\n",
       "   'systems',\n",
       "   ',',\n",
       "   'collaborative',\n",
       "   'filtering',\n",
       "   '(',\n",
       "   'CF',\n",
       "   ')',\n",
       "   'is',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'most',\n",
       "   'popular',\n",
       "   '[',\n",
       "   '4',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Pioneer',\n",
       "   'CF',\n",
       "   'recommender',\n",
       "   'systems',\n",
       "   'represented',\n",
       "   'items',\n",
       "   'as',\n",
       "   'sparse',\n",
       "   '\\n',\n",
       "   'vectors',\n",
       "   'of',\n",
       "   'consumption',\n",
       "   '.',\n",
       "   'However',\n",
       "   ',',\n",
       "   'due',\n",
       "   'to',\n",
       "   'the',\n",
       "   'accelerated',\n",
       "   'growth',\n",
       "   'in',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'number',\n",
       "   'of',\n",
       "   'users',\n",
       "   'and',\n",
       "   'items',\n",
       "   ',',\n",
       "   'this',\n",
       "   'form',\n",
       "   'of',\n",
       "   'representation',\n",
       "   'started',\n",
       "   '\\n',\n",
       "   'facing',\n",
       "   'limitations',\n",
       "   'related',\n",
       "   'to',\n",
       "   ':',\n",
       "   '*',\n",
       "   '(',\n",
       "   'i',\n",
       "   ')',\n",
       "   '*',\n",
       "   'sparsity',\n",
       "   ',',\n",
       "   'since',\n",
       "   'modern',\n",
       "   'recommender',\n",
       "   '\\n',\n",
       "   'systems',\n",
       "   'must',\n",
       "   'deal',\n",
       "   'with',\n",
       "   'a',\n",
       "   'number',\n",
       "   'of',\n",
       "   'possible',\n",
       "   'interactions',\n",
       "   'that',\n",
       "   'follows',\n",
       "   'a',\n",
       "   'power',\n",
       "   'law',\n",
       "   'according',\n",
       "   'to',\n",
       "   'the',\n",
       "   'number',\n",
       "   'of',\n",
       "   'users',\n",
       "   'and',\n",
       "   'items',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'yielding',\n",
       "   'an',\n",
       "   'often',\n",
       "   'highly',\n",
       "   'sparse',\n",
       "   'interactions',\n",
       "   'matrix',\n",
       "   ';',\n",
       "   'and',\n",
       "   '*',\n",
       "   '(',\n",
       "   'ii',\n",
       "   ')',\n",
       "   '*',\n",
       "   'scalability',\n",
       "   ',',\n",
       "   'given',\n",
       "   'that',\n",
       "   'the',\n",
       "   'vectors',\n",
       "   'used',\n",
       "   'to',\n",
       "   'represent',\n",
       "   'users',\n",
       "   'and',\n",
       "   'items',\n",
       "   'can',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   ':',\n",
       "   'Proceedings',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Web',\n",
       "   '(',\n",
       "   'WebMe',\n",
       "   '\\n',\n",
       "   'dia’2024',\n",
       "   ')',\n",
       "   '.',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '.',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   ':',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   ',',\n",
       "   '2024',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'SBC',\n",
       "   '–',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '-',\n",
       "   '2753',\n",
       "   '\\n\\n\\n',\n",
       "   'become',\n",
       "   'quite',\n",
       "   'large',\n",
       "   ',',\n",
       "   'increasing',\n",
       "   'the',\n",
       "   'demand',\n",
       "   'for',\n",
       "   'greater',\n",
       "   'storage',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'processing',\n",
       "   'capacity',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'To',\n",
       "   'circumvent',\n",
       "   'these',\n",
       "   'problems',\n",
       "   ',',\n",
       "   'efforts',\n",
       "   'are',\n",
       "   'being',\n",
       "   'made',\n",
       "   'to',\n",
       "   'represent',\n",
       "   'users',\n",
       "   'and',\n",
       "   'items',\n",
       "   'in',\n",
       "   'a',\n",
       "   'much',\n",
       "   'smaller',\n",
       "   'dimensional',\n",
       "   'space',\n",
       "   '[',\n",
       "   '40',\n",
       "   ']',\n",
       "   '.',\n",
       "   'In',\n",
       "   '\\n',\n",
       "   'this',\n",
       "   'context',\n",
       "   ',',\n",
       "   'two',\n",
       "   'main',\n",
       "   'techniques',\n",
       "   'gained',\n",
       "   'ground',\n",
       "   'with',\n",
       "   'the',\n",
       "   'literature',\n",
       "   ':',\n",
       "   '\\n',\n",
       "   'matrix',\n",
       "   'factorization',\n",
       "   '[',\n",
       "   '27',\n",
       "   ']',\n",
       "   'and',\n",
       "   'neural',\n",
       "   'networks',\n",
       "   '[',\n",
       "   '57',\n",
       "   ']',\n",
       "   '.',\n",
       "   'In',\n",
       "   'the',\n",
       "   'latter',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'neural',\n",
       "   'embedding',\n",
       "   'models',\n",
       "   'inspired',\n",
       "   'by',\n",
       "   'Natural',\n",
       "   'Language',\n",
       "   'Processing',\n",
       "   '\\n',\n",
       "   '(',\n",
       "   'NLP',\n",
       "   ')',\n",
       "   'have',\n",
       "   'recently',\n",
       "   'gained',\n",
       "   'traction',\n",
       "   '[',\n",
       "   '34',\n",
       "   ']',\n",
       "   '.',\n",
       "   'A',\n",
       "   'significant',\n",
       "   'advantage',\n",
       "   '\\n',\n",
       "   'of',\n",
       "   'embeddings',\n",
       "   'in',\n",
       "   'NLP',\n",
       "   'is',\n",
       "   'their',\n",
       "   'ability',\n",
       "   'to',\n",
       "   'carry',\n",
       "   'intrinsic',\n",
       "   'meaning',\n",
       "   ',',\n",
       "   'i.e.',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'knowledge',\n",
       "   'encapsulated',\n",
       "   'within',\n",
       "   'the',\n",
       "   'representation',\n",
       "   'beyond',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'information',\n",
       "   'used',\n",
       "   'for',\n",
       "   'training',\n",
       "   '.',\n",
       "   'In',\n",
       "   'recommender',\n",
       "   'systems',\n",
       "   ',',\n",
       "   'this',\n",
       "   'translates',\n",
       "   'to',\n",
       "   'the',\n",
       "   'potential',\n",
       "   'of',\n",
       "   'leveraging',\n",
       "   'item',\n",
       "   'embeddings',\n",
       "   'for',\n",
       "   'various',\n",
       "   '\\n',\n",
       "   'tasks',\n",
       "   'beyond',\n",
       "   'mere',\n",
       "   'recommendation',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Despite',\n",
       "   'the',\n",
       "   'promising',\n",
       "   'nature',\n",
       "   'of',\n",
       "   'embeddings',\n",
       "   ',',\n",
       "   'most',\n",
       "   'existing',\n",
       "   'research',\n",
       "   'has',\n",
       "   'focused',\n",
       "   'primarily',\n",
       "   'on',\n",
       "   'extrinsic',\n",
       "   'evaluations',\n",
       "   ',',\n",
       "   'neglecting',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'intrinsic',\n",
       "   'qualities',\n",
       "   'of',\n",
       "   'the',\n",
       "   'learned',\n",
       "   'representations',\n",
       "   '.',\n",
       "   'It',\n",
       "   'is',\n",
       "   'well',\n",
       "   '\\n',\n",
       "   'known',\n",
       "   'that',\n",
       "   'the',\n",
       "   'performance',\n",
       "   'of',\n",
       "   'embeddings',\n",
       "   'in',\n",
       "   'downstream',\n",
       "   'applications',\n",
       "   'does',\n",
       "   'not',\n",
       "   'always',\n",
       "   'correlate',\n",
       "   'with',\n",
       "   'their',\n",
       "   'intrinsic',\n",
       "   'quality',\n",
       "   '[',\n",
       "   '41',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'While',\n",
       "   'the',\n",
       "   'primary',\n",
       "   'goal',\n",
       "   'of',\n",
       "   'a',\n",
       "   'recommender',\n",
       "   'system',\n",
       "   'is',\n",
       "   'to',\n",
       "   'provide',\n",
       "   '\\n',\n",
       "   'high',\n",
       "   '-',\n",
       "   'quality',\n",
       "   'recommendations',\n",
       "   ',',\n",
       "   'the',\n",
       "   'vector',\n",
       "   'representations',\n",
       "   'of',\n",
       "   'items',\n",
       "   '\\n',\n",
       "   'and',\n",
       "   'users',\n",
       "   'can',\n",
       "   'also',\n",
       "   'be',\n",
       "   'applied',\n",
       "   'to',\n",
       "   'other',\n",
       "   'tasks',\n",
       "   'such',\n",
       "   'as',\n",
       "   'automatic',\n",
       "   'feature',\n",
       "   'prediction',\n",
       "   ',',\n",
       "   'knowledge',\n",
       "   'discovery',\n",
       "   ',',\n",
       "   'and',\n",
       "   'clustering',\n",
       "   '[',\n",
       "   '21',\n",
       "   ',',\n",
       "   '31',\n",
       "   ']',\n",
       "   '.',\n",
       "   'These',\n",
       "   '\\n',\n",
       "   'applications',\n",
       "   'can',\n",
       "   'only',\n",
       "   'be',\n",
       "   'significantly',\n",
       "   'improved',\n",
       "   'if',\n",
       "   'the',\n",
       "   'intrinsic',\n",
       "   'quality',\n",
       "   'of',\n",
       "   'the',\n",
       "   'embeddings',\n",
       "   'is',\n",
       "   'adequately',\n",
       "   'assessed',\n",
       "   '.',\n",
       "   'Therefore',\n",
       "   ',',\n",
       "   'evaluating',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'intrinsic',\n",
       "   'quality',\n",
       "   'of',\n",
       "   'matrix',\n",
       "   'factorization',\n",
       "   'and',\n",
       "   'neural',\n",
       "   'embedding',\n",
       "   '\\n',\n",
       "   'models',\n",
       "   'is',\n",
       "   'crucial',\n",
       "   'for',\n",
       "   'their',\n",
       "   'successful',\n",
       "   'application',\n",
       "   'in',\n",
       "   'diverse',\n",
       "   'contexts',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'To',\n",
       "   'address',\n",
       "   'this',\n",
       "   'gap',\n",
       "   ',',\n",
       "   'we',\n",
       "   'present',\n",
       "   'various',\n",
       "   'methods',\n",
       "   'for',\n",
       "   'using',\n",
       "   'item',\n",
       "   '\\n',\n",
       "   'metadata',\n",
       "   'to',\n",
       "   'intrinsically',\n",
       "   'evaluate',\n",
       "   'vector',\n",
       "   'representations',\n",
       "   'of',\n",
       "   'items',\n",
       "   '\\n',\n",
       "   'in',\n",
       "   'a',\n",
       "   'CF',\n",
       "   'recommender',\n",
       "   'system',\n",
       "   '.',\n",
       "   'We',\n",
       "   'introduce',\n",
       "   'a',\n",
       "   'commonly',\n",
       "   'used',\n",
       "   '\\n',\n",
       "   'evaluation',\n",
       "   'technique',\n",
       "   'in',\n",
       "   'recommender',\n",
       "   'system',\n",
       "   'literature',\n",
       "   'and',\n",
       "   'adapt',\n",
       "   '\\n',\n",
       "   'an',\n",
       "   'intrinsic',\n",
       "   'evaluation',\n",
       "   'task',\n",
       "   'from',\n",
       "   'NLP',\n",
       "   'to',\n",
       "   'our',\n",
       "   'context',\n",
       "   '.',\n",
       "   'Given',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'time',\n",
       "   '-',\n",
       "   'consuming',\n",
       "   'and',\n",
       "   'expertise',\n",
       "   '-',\n",
       "   'dependent',\n",
       "   'nature',\n",
       "   'of',\n",
       "   'subjective',\n",
       "   'analyses',\n",
       "   ',',\n",
       "   'we',\n",
       "   'propose',\n",
       "   'a',\n",
       "   'new',\n",
       "   'quantitative',\n",
       "   ',',\n",
       "   'non',\n",
       "   '-',\n",
       "   'subjective',\n",
       "   'strategy',\n",
       "   'using',\n",
       "   'content',\n",
       "   '-',\n",
       "   'based',\n",
       "   'data',\n",
       "   'to',\n",
       "   'evaluate',\n",
       "   'the',\n",
       "   'intrinsic',\n",
       "   'ranking',\n",
       "   'quality',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'embeddings',\n",
       "   '.',\n",
       "   'We',\n",
       "   'illustrate',\n",
       "   'our',\n",
       "   'proposed',\n",
       "   'pipeline',\n",
       "   'with',\n",
       "   'an',\n",
       "   'extrinsic',\n",
       "   'evaluation',\n",
       "   ',',\n",
       "   'assessing',\n",
       "   'the',\n",
       "   'quality',\n",
       "   'of',\n",
       "   'vector',\n",
       "   'representations',\n",
       "   'in',\n",
       "   '\\n',\n",
       "   'generating',\n",
       "   'recommendations',\n",
       "   ',',\n",
       "   'and',\n",
       "   'compare',\n",
       "   'these',\n",
       "   'results',\n",
       "   'with',\n",
       "   'our',\n",
       "   '\\n',\n",
       "   'intrinsic',\n",
       "   ...],\n",
       "  'pos_tagger': '',\n",
       "  'lema': ['why',\n",
       "   'Ignore',\n",
       "   'Content',\n",
       "   'a',\n",
       "   'Guideline',\n",
       "   'for',\n",
       "   'Intrinsic',\n",
       "   'Evaluation',\n",
       "   'of',\n",
       "   'Item',\n",
       "   'embedding',\n",
       "   'for',\n",
       "   'Collaborative',\n",
       "   'Filtering',\n",
       "   'Pedro',\n",
       "   'R.',\n",
       "   'Pires',\n",
       "   'pedro.pires@dcomp.sor.ufscar.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'São',\n",
       "   'Carlos',\n",
       "   'São',\n",
       "   'Carlos',\n",
       "   'SP',\n",
       "   'Bruno',\n",
       "   'B.',\n",
       "   'Rizzi',\n",
       "   'brunosora@hotmail.com',\n",
       "   'BTG',\n",
       "   'Pactual',\n",
       "   'São',\n",
       "   'Paulo',\n",
       "   'sp',\n",
       "   'Tiago',\n",
       "   'a.',\n",
       "   'Almeida',\n",
       "   'talmeida@ufscar.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'São',\n",
       "   'Carlos',\n",
       "   'Sorocaba',\n",
       "   'sp',\n",
       "   'ABSTRACT',\n",
       "   'with',\n",
       "   'the',\n",
       "   'constant',\n",
       "   'growth',\n",
       "   'in',\n",
       "   'available',\n",
       "   'information',\n",
       "   'and',\n",
       "   'the',\n",
       "   'popularization',\n",
       "   'of',\n",
       "   'technology',\n",
       "   'recommender',\n",
       "   'system',\n",
       "   'have',\n",
       "   'to',\n",
       "   'deal',\n",
       "   'with',\n",
       "   'an',\n",
       "   'increase',\n",
       "   'number',\n",
       "   'of',\n",
       "   'user',\n",
       "   'and',\n",
       "   'item',\n",
       "   'this',\n",
       "   'lead',\n",
       "   'to',\n",
       "   'two',\n",
       "   'problem',\n",
       "   'in',\n",
       "   'represent',\n",
       "   'item',\n",
       "   'scalability',\n",
       "   'and',\n",
       "   'sparsity',\n",
       "   'therefore',\n",
       "   'many',\n",
       "   'recommender',\n",
       "   'system',\n",
       "   'aim',\n",
       "   'to',\n",
       "   'generate',\n",
       "   'low',\n",
       "   'dimensional',\n",
       "   'dense',\n",
       "   'representation',\n",
       "   'of',\n",
       "   'item',\n",
       "   'matrix',\n",
       "   'factorization',\n",
       "   'technique',\n",
       "   'be',\n",
       "   'popular',\n",
       "   'but',\n",
       "   'model',\n",
       "   'base',\n",
       "   'on',\n",
       "   'neural',\n",
       "   'embedding',\n",
       "   'have',\n",
       "   'recently',\n",
       "   'be',\n",
       "   'propose',\n",
       "   'and',\n",
       "   'be',\n",
       "   'gain',\n",
       "   'ground',\n",
       "   'in',\n",
       "   'the',\n",
       "   'literature',\n",
       "   'their',\n",
       "   'main',\n",
       "   'goal',\n",
       "   'be',\n",
       "   'to',\n",
       "   'learn',\n",
       "   'dense',\n",
       "   'representation',\n",
       "   'with',\n",
       "   'intrinsic',\n",
       "   'meaning',\n",
       "   'however',\n",
       "   'most',\n",
       "   'study',\n",
       "   'propose',\n",
       "   'embedding',\n",
       "   'for',\n",
       "   'recommender',\n",
       "   'system',\n",
       "   'ignore',\n",
       "   'this',\n",
       "   'property',\n",
       "   'and',\n",
       "   'focus',\n",
       "   'only',\n",
       "   'on',\n",
       "   'extrinsic',\n",
       "   'evaluation',\n",
       "   'this',\n",
       "   'study',\n",
       "   'present',\n",
       "   'a',\n",
       "   'guideline',\n",
       "   'for',\n",
       "   'assess',\n",
       "   'the',\n",
       "   'intrinsic',\n",
       "   'quality',\n",
       "   'of',\n",
       "   'matrix',\n",
       "   'factorization',\n",
       "   'and',\n",
       "   'neural',\n",
       "   'base',\n",
       "   'embed',\n",
       "   'model',\n",
       "   'for',\n",
       "   'collaborative',\n",
       "   'filtering',\n",
       "   'compare',\n",
       "   'the',\n",
       "   'result',\n",
       "   'with',\n",
       "   'a',\n",
       "   'traditional',\n",
       "   'extrinsic',\n",
       "   'evaluation',\n",
       "   'to',\n",
       "   'enrich',\n",
       "   'the',\n",
       "   'evaluation',\n",
       "   'pipeline',\n",
       "   'we',\n",
       "   'suggest',\n",
       "   'adapt',\n",
       "   'an',\n",
       "   'intrinsic',\n",
       "   'evaluation',\n",
       "   'task',\n",
       "   'commonly',\n",
       "   'employ',\n",
       "   'in',\n",
       "   'the',\n",
       "   'Natural',\n",
       "   'Language',\n",
       "   'processing',\n",
       "   'literature',\n",
       "   'and',\n",
       "   'we',\n",
       "   'propose',\n",
       "   'a',\n",
       "   'novel',\n",
       "   'strategy',\n",
       "   'for',\n",
       "   'evaluate',\n",
       "   'the',\n",
       "   'learn',\n",
       "   'representation',\n",
       "   'compare',\n",
       "   'to',\n",
       "   'a',\n",
       "   'content',\n",
       "   'base',\n",
       "   'scenario',\n",
       "   'finally',\n",
       "   'every',\n",
       "   'mention',\n",
       "   'technique',\n",
       "   'be',\n",
       "   'analyze',\n",
       "   'over',\n",
       "   'establish',\n",
       "   'recommender',\n",
       "   'model',\n",
       "   'and',\n",
       "   'the',\n",
       "   'result',\n",
       "   'show',\n",
       "   'how',\n",
       "   'vector',\n",
       "   'representation',\n",
       "   'that',\n",
       "   'do',\n",
       "   'not',\n",
       "   'yield',\n",
       "   'good',\n",
       "   'recommendation',\n",
       "   'can',\n",
       "   'still',\n",
       "   'be',\n",
       "   'useful',\n",
       "   'in',\n",
       "   'other',\n",
       "   'task',\n",
       "   'that',\n",
       "   'demand',\n",
       "   'intrinsic',\n",
       "   'knowledge',\n",
       "   'highlight',\n",
       "   'the',\n",
       "   'potential',\n",
       "   'of',\n",
       "   'this',\n",
       "   'perspective',\n",
       "   'of',\n",
       "   'evaluation',\n",
       "   'keyword',\n",
       "   'embedding',\n",
       "   'intrinsic',\n",
       "   'evaluation',\n",
       "   'qualitative',\n",
       "   'evaluation',\n",
       "   'recommender',\n",
       "   'system',\n",
       "   'similarity',\n",
       "   'table',\n",
       "   'intruder',\n",
       "   'detection',\n",
       "   'autotagge',\n",
       "   '1',\n",
       "   'introduction',\n",
       "   'Recommender',\n",
       "   'system',\n",
       "   'be',\n",
       "   'tool',\n",
       "   'commonly',\n",
       "   'use',\n",
       "   'by',\n",
       "   'company',\n",
       "   'for',\n",
       "   'enhance',\n",
       "   'the',\n",
       "   'experience',\n",
       "   'user',\n",
       "   'have',\n",
       "   'when',\n",
       "   'utilize',\n",
       "   'their',\n",
       "   'service',\n",
       "   'by',\n",
       "   'filter',\n",
       "   'and',\n",
       "   'recommend',\n",
       "   'particularly',\n",
       "   'relevant',\n",
       "   'information',\n",
       "   '1',\n",
       "   'among',\n",
       "   'different',\n",
       "   'type',\n",
       "   'of',\n",
       "   'recommender',\n",
       "   'system',\n",
       "   'collaborative',\n",
       "   'filtering',\n",
       "   'CF',\n",
       "   'be',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'most',\n",
       "   'popular',\n",
       "   '4',\n",
       "   'Pioneer',\n",
       "   'CF',\n",
       "   'recommender',\n",
       "   'system',\n",
       "   'represent',\n",
       "   'item',\n",
       "   'as',\n",
       "   'sparse',\n",
       "   'vector',\n",
       "   'of',\n",
       "   'consumption',\n",
       "   'however',\n",
       "   'due',\n",
       "   'to',\n",
       "   'the',\n",
       "   'accelerated',\n",
       "   'growth',\n",
       "   'in',\n",
       "   'the',\n",
       "   'number',\n",
       "   'of',\n",
       "   'user',\n",
       "   'and',\n",
       "   'item',\n",
       "   'this',\n",
       "   'form',\n",
       "   'of',\n",
       "   'representation',\n",
       "   'start',\n",
       "   'face',\n",
       "   'limitation',\n",
       "   'relate',\n",
       "   'to',\n",
       "   'i',\n",
       "   'sparsity',\n",
       "   'since',\n",
       "   'modern',\n",
       "   'recommender',\n",
       "   'system',\n",
       "   'must',\n",
       "   'deal',\n",
       "   'with',\n",
       "   'a',\n",
       "   'number',\n",
       "   'of',\n",
       "   'possible',\n",
       "   'interaction',\n",
       "   'that',\n",
       "   'follow',\n",
       "   'a',\n",
       "   'power',\n",
       "   'law',\n",
       "   'accord',\n",
       "   'to',\n",
       "   'the',\n",
       "   'number',\n",
       "   'of',\n",
       "   'user',\n",
       "   'and',\n",
       "   'item',\n",
       "   'yield',\n",
       "   'an',\n",
       "   'often',\n",
       "   'highly',\n",
       "   'sparse',\n",
       "   'interaction',\n",
       "   'matrix',\n",
       "   'and',\n",
       "   'ii',\n",
       "   'scalability',\n",
       "   'give',\n",
       "   'that',\n",
       "   'the',\n",
       "   'vector',\n",
       "   'use',\n",
       "   'to',\n",
       "   'represent',\n",
       "   'user',\n",
       "   'and',\n",
       "   'item',\n",
       "   'can',\n",
       "   'in',\n",
       "   'proceeding',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'web',\n",
       "   'WebMe',\n",
       "   'dia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   '2024',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'sbc',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '2753',\n",
       "   'become',\n",
       "   'quite',\n",
       "   'large',\n",
       "   'increase',\n",
       "   'the',\n",
       "   'demand',\n",
       "   'for',\n",
       "   'great',\n",
       "   'storage',\n",
       "   'and',\n",
       "   'processing',\n",
       "   'capacity',\n",
       "   'to',\n",
       "   'circumvent',\n",
       "   'these',\n",
       "   'problem',\n",
       "   'effort',\n",
       "   'be',\n",
       "   'be',\n",
       "   'make',\n",
       "   'to',\n",
       "   'represent',\n",
       "   'user',\n",
       "   'and',\n",
       "   'item',\n",
       "   'in',\n",
       "   'a',\n",
       "   'much',\n",
       "   'small',\n",
       "   'dimensional',\n",
       "   'space',\n",
       "   '40',\n",
       "   'in',\n",
       "   'this',\n",
       "   'context',\n",
       "   'two',\n",
       "   'main',\n",
       "   'technique',\n",
       "   'gain',\n",
       "   'ground',\n",
       "   'with',\n",
       "   'the',\n",
       "   'literature',\n",
       "   'matrix',\n",
       "   'factorization',\n",
       "   '27',\n",
       "   'and',\n",
       "   'neural',\n",
       "   'network',\n",
       "   '57',\n",
       "   'in',\n",
       "   'the',\n",
       "   'latter',\n",
       "   'neural',\n",
       "   'embed',\n",
       "   'model',\n",
       "   'inspire',\n",
       "   'by',\n",
       "   'Natural',\n",
       "   'Language',\n",
       "   'Processing',\n",
       "   'NLP',\n",
       "   'have',\n",
       "   'recently',\n",
       "   'gain',\n",
       "   'traction',\n",
       "   '34',\n",
       "   'a',\n",
       "   'significant',\n",
       "   'advantage',\n",
       "   'of',\n",
       "   'embedding',\n",
       "   'in',\n",
       "   'NLP',\n",
       "   'be',\n",
       "   'their',\n",
       "   'ability',\n",
       "   'to',\n",
       "   'carry',\n",
       "   'intrinsic',\n",
       "   'meaning',\n",
       "   'i.e.',\n",
       "   'the',\n",
       "   'knowledge',\n",
       "   'encapsulate',\n",
       "   'within',\n",
       "   'the',\n",
       "   'representation',\n",
       "   'beyond',\n",
       "   'the',\n",
       "   'information',\n",
       "   'use',\n",
       "   'for',\n",
       "   'training',\n",
       "   'in',\n",
       "   'recommender',\n",
       "   'system',\n",
       "   'this',\n",
       "   'translate',\n",
       "   'to',\n",
       "   'the',\n",
       "   'potential',\n",
       "   'of',\n",
       "   'leverage',\n",
       "   'item',\n",
       "   'embedding',\n",
       "   'for',\n",
       "   'various',\n",
       "   'task',\n",
       "   'beyond',\n",
       "   'mere',\n",
       "   'recommendation',\n",
       "   'despite',\n",
       "   'the',\n",
       "   'promising',\n",
       "   'nature',\n",
       "   'of',\n",
       "   'embedding',\n",
       "   'most',\n",
       "   'exist',\n",
       "   'research',\n",
       "   'have',\n",
       "   'focus',\n",
       "   'primarily',\n",
       "   'on',\n",
       "   'extrinsic',\n",
       "   'evaluation',\n",
       "   'neglect',\n",
       "   'the',\n",
       "   'intrinsic',\n",
       "   'quality',\n",
       "   'of',\n",
       "   'the',\n",
       "   'learn',\n",
       "   'representation',\n",
       "   'it',\n",
       "   'be',\n",
       "   'well',\n",
       "   'know',\n",
       "   'that',\n",
       "   'the',\n",
       "   'performance',\n",
       "   'of',\n",
       "   'embedding',\n",
       "   'in',\n",
       "   'downstream',\n",
       "   'application',\n",
       "   'do',\n",
       "   'not',\n",
       "   'always',\n",
       "   'correlate',\n",
       "   'with',\n",
       "   'their',\n",
       "   'intrinsic',\n",
       "   'quality',\n",
       "   '41',\n",
       "   'while',\n",
       "   'the',\n",
       "   'primary',\n",
       "   'goal',\n",
       "   'of',\n",
       "   'a',\n",
       "   'recommender',\n",
       "   'system',\n",
       "   'be',\n",
       "   'to',\n",
       "   'provide',\n",
       "   'high',\n",
       "   'quality',\n",
       "   'recommendation',\n",
       "   'the',\n",
       "   'vector',\n",
       "   'representation',\n",
       "   'of',\n",
       "   'item',\n",
       "   'and',\n",
       "   'user',\n",
       "   'can',\n",
       "   'also',\n",
       "   'be',\n",
       "   'apply',\n",
       "   'to',\n",
       "   'other',\n",
       "   'task',\n",
       "   'such',\n",
       "   'as',\n",
       "   'automatic',\n",
       "   'feature',\n",
       "   'prediction',\n",
       "   'knowledge',\n",
       "   'discovery',\n",
       "   'and',\n",
       "   'clustering',\n",
       "   '21',\n",
       "   '31',\n",
       "   'these',\n",
       "   'application',\n",
       "   'can',\n",
       "   'only',\n",
       "   'be',\n",
       "   'significantly',\n",
       "   'improve',\n",
       "   'if',\n",
       "   'the',\n",
       "   'intrinsic',\n",
       "   'quality',\n",
       "   'of',\n",
       "   'the',\n",
       "   'embedding',\n",
       "   'be',\n",
       "   'adequately',\n",
       "   'assess',\n",
       "   'therefore',\n",
       "   'evaluate',\n",
       "   'the',\n",
       "   'intrinsic',\n",
       "   'quality',\n",
       "   'of',\n",
       "   'matrix',\n",
       "   'factorization',\n",
       "   'and',\n",
       "   'neural',\n",
       "   'embed',\n",
       "   'model',\n",
       "   'be',\n",
       "   'crucial',\n",
       "   'for',\n",
       "   'their',\n",
       "   'successful',\n",
       "   'application',\n",
       "   'in',\n",
       "   'diverse',\n",
       "   'contexts',\n",
       "   'to',\n",
       "   'address',\n",
       "   'this',\n",
       "   'gap',\n",
       "   'we',\n",
       "   'present',\n",
       "   'various',\n",
       "   'method',\n",
       "   'for',\n",
       "   'use',\n",
       "   'item',\n",
       "   'metadata',\n",
       "   'to',\n",
       "   'intrinsically',\n",
       "   'evaluate',\n",
       "   'vector',\n",
       "   'representation',\n",
       "   'of',\n",
       "   'item',\n",
       "   'in',\n",
       "   'a',\n",
       "   'CF',\n",
       "   'recommender',\n",
       "   'system',\n",
       "   'we',\n",
       "   'introduce',\n",
       "   'a',\n",
       "   'commonly',\n",
       "   'use',\n",
       "   'evaluation',\n",
       "   'technique',\n",
       "   'in',\n",
       "   'recommender',\n",
       "   'system',\n",
       "   'literature',\n",
       "   'and',\n",
       "   'adapt',\n",
       "   'an',\n",
       "   'intrinsic',\n",
       "   'evaluation',\n",
       "   'task',\n",
       "   'from',\n",
       "   'NLP',\n",
       "   'to',\n",
       "   'our',\n",
       "   'context',\n",
       "   'give',\n",
       "   'the',\n",
       "   'time',\n",
       "   'consume',\n",
       "   'and',\n",
       "   'expertise',\n",
       "   'dependent',\n",
       "   'nature',\n",
       "   'of',\n",
       "   'subjective',\n",
       "   'analysis',\n",
       "   'we',\n",
       "   'propose',\n",
       "   'a',\n",
       "   'new',\n",
       "   'quantitative',\n",
       "   'non',\n",
       "   'subjective',\n",
       "   'strategy',\n",
       "   'use',\n",
       "   'content',\n",
       "   'base',\n",
       "   'datum',\n",
       "   'to',\n",
       "   'evaluate',\n",
       "   'the',\n",
       "   'intrinsic',\n",
       "   'ranking',\n",
       "   'quality',\n",
       "   'of',\n",
       "   'embedding',\n",
       "   'we',\n",
       "   'illustrate',\n",
       "   'our',\n",
       "   'propose',\n",
       "   'pipeline',\n",
       "   'with',\n",
       "   'an',\n",
       "   'extrinsic',\n",
       "   'evaluation',\n",
       "   'assess',\n",
       "   'the',\n",
       "   'quality',\n",
       "   'of',\n",
       "   'vector',\n",
       "   'representation',\n",
       "   'in',\n",
       "   'generating',\n",
       "   'recommendation',\n",
       "   'and',\n",
       "   'compare',\n",
       "   'these',\n",
       "   'result',\n",
       "   'with',\n",
       "   'our',\n",
       "   'intrinsic',\n",
       "   'evaluation',\n",
       "   'task',\n",
       "   'our',\n",
       "   'finding',\n",
       "   'demonstrate',\n",
       "   'that',\n",
       "   'embed',\n",
       "   'model',\n",
       "   'can',\n",
       "   'perform',\n",
       "   'very',\n",
       "   'differently',\n",
       "   'across',\n",
       "   'task',\n",
       "   'with',\n",
       "   'some',\n",
       "   'model',\n",
       "   'that',\n",
       "   'perform',\n",
       "   'poorly',\n",
       "   'in',\n",
       "   'recommendation',\n",
       "   'task',\n",
       "   'excel',\n",
       "   'in',\n",
       "   'intrinsic',\n",
       "   'evaluation',\n",
       "   'underscore',\n",
       "   'the',\n",
       "   'necessity',\n",
       "   'of',\n",
       "   'comprehensive',\n",
       "   'analysis',\n",
       "   'when',\n",
       "   'develop',\n",
       "   'new',\n",
       "   'representation',\n",
       "   'model',\n",
       "   'with',\n",
       "   'these',\n",
       "   'consideration',\n",
       "   'the',\n",
       "   'primary',\n",
       "   'objective',\n",
       "   'of',\n",
       "   'this',\n",
       "   'study',\n",
       "   'be',\n",
       "   '1',\n",
       "   'introduce',\n",
       "   'new',\n",
       "   'method',\n",
       "   'for',\n",
       "   'evaluate',\n",
       "   'item',\n",
       "   'embedding',\n",
       "   'use',\n",
       "   'strategy',\n",
       "   'derive',\n",
       "   'from',\n",
       "   'Natural',\n",
       "   'Language',\n",
       "   'Processing',\n",
       "   'such',\n",
       "   'as',\n",
       "   'intruder',\n",
       "   'detection',\n",
       "   'and',\n",
       "   'from',\n",
       "   'content',\n",
       "   'base',\n",
       "   'recommendation',\n",
       "   'such',\n",
       "   'as',\n",
       "   'content',\n",
       "   'base',\n",
       "   'ranking',\n",
       "   'comparison',\n",
       "   '2',\n",
       "   'present',\n",
       "   'a',\n",
       "   'collection',\n",
       "   'of',\n",
       "   'technique',\n",
       "   'for',\n",
       "   'intrinsically',\n",
       "   'evaluate',\n",
       "   'item',\n",
       "   'embedding',\n",
       "   'in',\n",
       "   'both',\n",
       "   'subjective',\n",
       "   'and',\n",
       "   'objective',\n",
       "   'manner',\n",
       "   '345',\n",
       "   'WebMedia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Pedro',\n",
       "   'R.',\n",
       "   'Pires',\n",
       "   'Bruno',\n",
       "   'B.',\n",
       "   'Rizzi',\n",
       "   'and',\n",
       "   'Tiago',\n",
       "   'A.',\n",
       "   'Almeida',\n",
       "   '3',\n",
       "   'compare',\n",
       "   'traditional',\n",
       "   'embed',\n",
       "   'base',\n",
       "   'recommender',\n",
       "   'model',\n",
       "   'in',\n",
       "   'extrinsic',\n",
       "   'and',\n",
       "   'intrinsic',\n",
       "   'task',\n",
       "   'to',\n",
       "   'illustrate',\n",
       "   'the',\n",
       "   'varied',\n",
       "   'performance',\n",
       "   'of',\n",
       "   'embedding',\n",
       "   'across',\n",
       "   'different',\n",
       "   'application',\n",
       "   '2',\n",
       "   'relate',\n",
       "   'work',\n",
       "   'the',\n",
       "   'early',\n",
       "   'collaborative',\n",
       "   'filtering',\n",
       "   'recommender',\n",
       "   'system',\n",
       "   'employ',\n",
       "   'neighborhood',\n",
       "   'base',\n",
       "   'method',\n",
       "   'to',\n",
       "   'compute',\n",
       "   'recommendation',\n",
       "   'however',\n",
       "   'as',\n",
       "   'the',\n",
       "   'number',\n",
       "   'of',\n",
       "   'user',\n",
       "   'and',\n",
       "   'item',\n",
       "   'increase',\n",
       "   'these',\n",
       "   'technique',\n",
       "   'encounter',\n",
       "   'significant',\n",
       "   'challenge',\n",
       "   'relate',\n",
       "   'to',\n",
       "   'sparsity',\n",
       "   'and',\n",
       "   'scalability',\n",
       "   '25',\n",
       "   'consequently',\n",
       "   'the',\n",
       "   'use',\n",
       "   'of',\n",
       "   'embedding',\n",
       "   'for',\n",
       "   'represent',\n",
       "   'user',\n",
       "   'and',\n",
       "   'item',\n",
       "   'gain',\n",
       "   'popularity',\n",
       "   'i.e.',\n",
       "   'low',\n",
       "   'dimensional',\n",
       "   'vector',\n",
       "   'representation',\n",
       "   'that',\n",
       "   'carry',\n",
       "   'intrinsic',\n",
       "   'meaning',\n",
       "   'initially',\n",
       "   'matrix',\n",
       "   'factorization',\n",
       "   'model',\n",
       "   'be',\n",
       "   'apply',\n",
       "   'for',\n",
       "   'this',\n",
       "   'task',\n",
       "   '27',\n",
       "   'reduce',\n",
       "   'memory',\n",
       "   'consumption',\n",
       "   'and',\n",
       "   'processing',\n",
       "   'demand',\n",
       "   'while',\n",
       "   'generate',\n",
       "   'valuable',\n",
       "   'insight',\n",
       "   'over',\n",
       "   'the',\n",
       "   'year',\n",
       "   'numerous',\n",
       "   'method',\n",
       "   'have',\n",
       "   'be',\n",
       "   'propose',\n",
       "   'to',\n",
       "   'generate',\n",
       "   'embedding',\n",
       "   'employ',\n",
       "   'diverse',\n",
       "   'strategy',\n",
       "   'such',\n",
       "   'as',\n",
       "   'graph',\n",
       "   'base',\n",
       "   'algorithm',\n",
       "   'and',\n",
       "   'quantization',\n",
       "   'technique',\n",
       "   '58',\n",
       "   'inspire',\n",
       "   'by',\n",
       "   'state',\n",
       "   'of',\n",
       "   'the',\n",
       "   'art',\n",
       "   'Natural',\n",
       "   'Language',\n",
       "   ...]},\n",
       " {'titulo': \"Investigating User's Attentional Focus in Computational Environments\",\n",
       "  'informacoes_url': '',\n",
       "  'idioma': 'english',\n",
       "  'storage_key': '../articles/original/english/985-24759-1-10-20240923.pdf',\n",
       "  'author': 'Cassiano da Silva Souza; Milene Selbach Silveira; and Isabel Harb Manssour',\n",
       "  'data_publicacao': '11-09-2024',\n",
       "  'resumo': 'Maintaining the user’s attentional focus has become a recurring concern in recent years. This is due to the consolidation of remote and hybrid models for study and work, which were widely experienced during the social distancing caused by COVID-19. This paper presents a review of works that address this problem by analyzing webcam data, a promising device for behavioral studies. The literature review from 2013 to 2023 was carried out using a hybrid search strategy, through which we selected and analyzed 57 papers. The summary of this study is presented in an interactive visual survey format called the *AttentionVis Browser* tool. As additional contributions, we provide a list of lessons learned, a list of work limitations, and possibilities for future research. ###',\n",
       "  'keywords': 'attention monitoring, webcam, data analysis, literature review.',\n",
       "  'referencias': ['[1] Andrea F. Abate, Lucia Cascone, Michele Nappi, Fabio Narducci, and Ignazio\\nPassero. 2021. Attention monitoring for synchronous distance learning. *Future*\\n*Generation Computer Systems* 125 (2021), 774–784. https://doi.org/10.1016/j.\\nfuture.2021.07.026',\n",
       "   '[2] Cevat Giray Aksoy, Jose Maria Barrero, Nicholas Bloom, Steven J Davis, Mathias\\nDolls, and Pablo Zarate. 2023. *Working from home around the globe: 2023 Report* .\\nTechnical Report. EconPol Policy Brief.',\n",
       "   '[3] Nese Alyuz, Eda Okur, Utku Genc, Sinem Aslan, Cagri Tanriover, and Asli Arslan\\nEsme. 2017. An unobtrusive and multimodal approach for behavioral engagement detection of students. In *Proceedings of the 1st International Workshop*\\n*on Multimodal Interaction for Education* . ACM, Glasgow, UK, 26–32. https:\\n//doi.org/10.1145/3139513.3139521',\n",
       "   '[4] Sinem Aslan, Nese Alyuz, Cagri Tanriover, Sinem E. Mete, Eda Okur, Sidney K.\\nD’Mello, and Asli Arslan Esme. 2019. Investigating the Impact of a Real-time,\\nMultimodal Student Engagement Analytics Technology in Authentic Classrooms.\\nIn *Proceedings of the CHI Conference on Human Factors in Computing Systems* .\\nACM, Glasgow, Uk, 1–12. https://doi.org/10.1145/3290605.3300534',\n",
       "   '[5] Yousef Atoum, Liping Chen, Alex X. Liu, Stephen D. H. Hsu, and Xiaoming Liu.\\n2017. Automated Online Exam Proctoring. *IEEE Transactions on Multimedia* 19,\\n7 (July 2017), 1609–1624. https://doi.org/10.1109/TMM.2017.2656064',\n",
       "   '[6] Charles E. Bailey. 2007. Cognitive Accuracy and Intelligent Executive Function\\nin the Brain and in Business. *Annals of the New York Academy of Sciences* 1118, 1\\n(2007), 122–141. https://doi.org/10.1196/annals.1412.011',\n",
       "   '[7] Jose Maria Barrero, Nicholas Bloom, and Steven J Davis. 2021. *Internet access*\\n*and its implications for productivity, inequality, and resilience* . Technical Report.\\nNational Bureau of Economic Research.',\n",
       "   '[8] Guido Borghi, Marco Venturelli, Roberto Vezzani, and Rita Cucchiara. 2017.\\nPoseidon: Face-from-depth for driver pose estimation. In *Proceedings of the IEEE*\\n*Conference on Computer Vision and Pattern Recognition* . IEEE Computer Society,\\nLos Alamitos, US, 5494–5503. https://doi.org/10.48550/arXiv.1611.10195',\n",
       "   '[9] Benjamin T. Carter and Steven G. Luke. 2020. Best practices in eye tracking\\nresearch. *International Journal of Psychophysiology* 155 (2020), 49–62. https:\\n//doi.org/10.1016/j.ijpsycho.2020.05.010',\n",
       "   '[10] Comitê Gestor da Internet no Brasil. 2022. *Pesquisa sobre o uso das tecnologias*\\n*de informação e comunicação nas escolas brasileiras - TIC Educação 2021* (1 ed.).\\nNúcleo de Informação e Coordenação do Ponto BR, São Paulo, BR.',\n",
       "   '[11] Melissa Cote, Frederic Jean, Alexandra Branzan Albu, and David Capson. 2016.\\nVideo summarization for remote invigilation of online exams. In *Proceedings of*\\n*the IEEE Winter Conference on Applications of Computer Vision* . IEEE, Lake Placid,\\nUS, 1–9. https://doi.org/10.1109/WACV.2016.7477704',\n",
       "   '[12] Adele Diamond and Kathleen Lee. 2011. Interventions Shown to Aid Executive\\nFunction Development in Children 4 to 12 Years Old. *Science* 333, 6045 (2011),\\n959–964. https://doi.org/10.1126/science.1204529',\n",
       "   '[13] Fahmid Morshed Fahid, Seung Lee, Bradford Mott, Jessica Vandenberg, Halim\\nAcosta, Thomas Brush, Krista Glazewski, Cindy Hmelo-Silver, and James Lester.\\n2023. Effects of Modalities in Detecting Behavioral Engagement in Collaborative\\nGame-Based Learning. In *Proceedings of the 13th International Learning Analytics*\\n*and Knowledge Conference* . ACM, Arlington, US, 208–218. https://doi.org/10.\\n1145/3576050.3576079',\n",
       "   '[14] Gabriele Fanelli, Thibaut Weise, Juergen Gall, and Luc Van Gool. 2011. Real Time\\nHead Pose Estimation from Consumer Depth Cameras. *Pattern Recognition* 6835\\n(2011), 101–110. https://doi.org/10.1109/3DV.2014.54',\n",
       "   '[15] Ian J Goodfellow, Dumitru Erhan, Pierre Luc Carrier, Aaron Courville, Mehdi\\nMirza, Ben Hamner, Will Cukierski, Yichuan Tang, David Thaler, Dong-Hyun Lee,\\nYingbo Zhou, Chetan Ramaiah, Fangxiang Feng, Rui Li, Xiaojie Wang, Dimitris\\nAthanasakis, John Shawe-Taylor, Maximilian Milakov, John Park, Radu Ionescu,\\nMarius Popescu, Cristian Grozea, James Bergstra, Jingjing Xie, Lukasz Romaszko,\\nBing Xu, Zhang Chuang, and Yoshua Bengio. 2015. Challenges in representation\\nlearning: A report on three machine learning contests. *Neural Networks* 64 (2015),\\n59–63. https://doi.org/10.1016/j.neunet.2014.09.005',\n",
       "   '[16] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren\\nJansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan\\nSeybold, et al . 2017. CNN architectures for large-scale audio classification. In\\n*Proceedings of the 2017 IEEE International Conference on Acoustics, Speech and*\\n*Signal Processing* . IEEE, IEEE Press, New Orleans, US, 131–135. https://doi.org/\\n10.48550/arXiv.1609.09430',\n",
       "   '[17] Basavaraj N Hiremath, Anushree Mitra, Aman Thapa, S Amoolya, and A Tameem.\\n2023. Proctoring using Artificial Intelligence. *SSRN Electronic Journal* (2023).\\nhttps://doi.org/10.2139/ssrn.4411332',\n",
       "   '[18] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. 2007. *La-*\\n*beled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained*\\n*Environments* . Technical Report 07-49. University of Massachusetts, Amherst.',\n",
       "   '[19] Instituto Semesp. 2023. *Mapa do Ensino Superior no Brasil* . Semesp, São Paulo,\\nBR. https://www.semesp.org.br/mapa/edicao-13/download/ 13 [a] ed.',\n",
       "   '[20] Mohamed Irfan, Mohammed Aslam, Ziyan Maraikar, Upul Jayasinghe, and Mohamed Fawzan. 2021. Ensuring Academic Integrity of Online Examinations. In\\n*Proceedings of the IEEE 16th International Conference on Industrial and Information*\\n*Systems* . IEEE, Kandy, LK, 295–300. https://doi.org/10.1109/ICIIS53135.2021.\\n9660737',\n",
       "   '[21] Amr Jadi. 2021. New Detection Cheating Method of Online-Exams during COVID19 Pandemic. *International Journal of Computer Science and Network Security* 21,\\n4 (April 2021), 123–130. https://doi.org/10.22937/IJCSNS.2021.21.4.17',\n",
       "   '[22] Marcos Kalinowski, Tatiana Escovedo, Hugo Villamizar, and Hélio Lopes. 2023.\\n*Engenharia de Software para Ciência de Dados: Um guia de boas práticas com*\\n*ênfase na construção de sistemas de Machine Learning em Python* . Casa do Código,\\nSão Paulo, BR.',\n",
       "   '[23] Virgínia Kastrup. 2007. O funcionamento da atenção no trabalho do cartógrafo.\\n*Psicologia e Sociedade* 19, 1 (Jan 2007), 15–22. https://doi.org/10.1590/S010271822007000100003',\n",
       "   '[24] Amanjot Kaur, Aamir Mustafa, Love Mehta, and Abhinav Dhall. 2018. Prediction\\nand Localization of Student Engagement in the Wild. In *Proceedings of the Digital*\\n*Image Computing: Techniques and Applications* . IEEE, Canberra, AU, 1–8. https:\\n//doi.org/10.48550/arXiv.1804.00858',\n",
       "   '[25] S. Keshav. 2007. How to Read a Paper. *Special Interest Group on Data Com-*\\n*munication da Association for Computing Machinery* 37, 3 (jul 2007), 83–84.\\nhttps://doi.org/10.1145/1273445.1273458',\n",
       "   '[26] Ahsan Raza Khan, Sara Khosravi, Sajjad Hussain, Rami Ghannam, Ahmed Zoha,\\nand Muhammad Ali Imran. 2022. EXECUTE: Exploring Eye Tracking to Support\\nE-learning. In *Proceedings of the IEEE Global Engineering Education Conference* .\\nIEEE, Tunis, TN, 670–676. https://doi.org/10.1109/EDUCON52537.2022.9766506',\n",
       "   '[27] Barbara Kitchenham and Stuart Charters. 2007. *Guidelines for performing system-*\\n*atic literature reviews in software engineering* . Technical Report EBSE 2007-001.\\nUniversity of Durham.',\n",
       "   '[28] Kostiantyn Kucher and Andreas Kerren. 2015. Text visualization techniques:\\nTaxonomy, visual survey, and community insights. In *Proceedings of the IEEE*\\n*Pacific Visualization Symposium* . IEEE, Hangzhou, CN, 117–121. https://doi.org/\\n10.1109/PACIFICVIS.2015.7156366',\n",
       "   '[29] Taeckyung Lee, Dain Kim, Sooyoung Park, Dongwhi Kim, and Sung-Ju Lee. 2022.\\nPredicting Mind-Wandering with Facial Videos in Online Lectures. In *Proceedings*\\n*of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops* .\\nIEEE, New Orleans, US, 2103–2112. https://doi.org/10.1109/CVPRW56347.2022.\\n\\n\\n204\\n\\n\\n-----\\n\\nInvestigating User’s Attentional Focus in Computational Environments WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\n00228',\n",
       "   '[30] Haotian Li, Min Xu, Yong Wang, Huan Wei, and Huamin Qu. 2021. A visual\\nanalytics approach to facilitate the proctoring of online exams. In *Proceedings of*\\n*the CHI Conference on Human Factors in Computing Systems* . ACM, New York,\\nUS, 1–17. https://doi.org/10.1145/3411764.3445294',\n",
       "   '[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva\\nRamanan, Piotr Dollár, and C. Lawrence Zitnick. 2014. Microsoft COCO: Common\\nObjects in Context. In *Proceedings of the 2014 Conference on Computer Vision*,\\nVol. 8693. Springer, Zurich, CH, 740–755. https://doi.org/10.48550/arXiv.1405.\\n0312',\n",
       "   '[32] Jens Madsen, Sara U. Júlio, Pawel J. Gucik, Richard Steinberg, and Lucas C.\\nParra. 2021. Synchronized eye movements predict test scores in online video\\neducation. *Proceedings of the National Academy of Sciences* 118, 5 (2021). https:\\n//doi.org/10.1073/pnas.2016980118',\n",
       "   '[33] Rachna Maithani. 2023. Securelens: Enhancing Trustworthiness in Remote Exams\\nwith Opencv Protective Eye. *Journal of Modernization in Engineering Technology*\\n*and Science* 05, 6 (2023), 1857–1861.',\n",
       "   '[34] Margaret W Matlin. 2004. *Psicologia cognitiva, 5* *[a]* *ed* . LTC, Rio de Janeiro, BR.',\n",
       "   '[35] Samuel Monteiro, Rutuja Bhate, Lav Sharma, and Phiroj Shaikh. 2022. Proct-Xam\\n– AI Based Proctoring. In *Proceedings of the 2nd Asian Conference on Innovation*\\n*in Technology* . IEEE, Ravet, IN, 1–6. https://doi.org/10.1109/ASIANCON55314.\\n2022.9908817',\n",
       "   '[36] Erica Mourão, Marcos Kalinowski, Leonardo Murta, Emilia Mendes, and Claes\\nWohlin. 2017. Investigating the Use of a Hybrid Search Strategy for Systematic Reviews. In *Proceedings of the 2017 ACM/IEEE International Symposium on*\\n*Empirical Software Engineering and Measurement* . IEEE, Toronto, CA, 193–198.\\nhttps://doi.org/10.1109/ESEM.2017.30',\n",
       "   '[37] Makoto Nakayama and Charlie C Chen. 2022. Digital Distractions and Remote\\nWork: A Balancing Act at Home. *Information Resources Management Journal* 35,\\n1 (2022), 1–17. https://doi.org/10.4018/IRMJ.308675',\n",
       "   '[38] Azmi Can Ozgen, Mahiye Uluyagmur Ozturk, and Umut Bayraktar. 2021. Cheating Detection Pipeline for Online Interviews and Exams. *ArXiv* abs/2106.14483\\n(2021), 4. https://doi.org/10.48550/arXiv.2106.14483',\n",
       "   '[39] Azmi Can Ozgen, Mahiye Uluyagmur Ozturk, Orkun Torun, Jianguo Yang, and\\nMehmet Zahit Alparslan. 2021. Cheating Detection Pipeline for Online Interviews.\\nIn *Proceedings of the 29th Signal Processing and Communications Applications Con-*\\n*ference* . IEEE, Istanbul, TR, 1–4. https://doi.org/10.1109/SIU53274.2021.9477950',\n",
       "   '[40] Kim Parker. 2023. About a third of U.S. workers who can work from home now do\\nso all the time. https://www.pewresearch.org/short-reads/2023/03/30/about-athird-of-us-workers-who-can-work-from-home-do-so-all-the-time Accessed:\\n\\n2024-02-07.',\n",
       "   '[41] Leandro Persona, Fernando Meloni, and Alessandra Alaniz Macedo. 2023. An\\naccurate real-time method to detect the smile facial expression. In *Proceedings of*\\n*the 29th Brazilian Symposium on Multimedia and the Web* (Ribeirão Preto, Brazil)\\n*(WebMedia ’23)* . Association for Computing Machinery, New York, USA, 46–55.\\nhttps://doi.org/10.1145/3617023.3617031',\n",
       "   '[42] Michael I Posner. 1980. Orienting of attention. *Quarterly journal of experimental*\\n*psychology* 32, 1 (1980), 3–25. https://doi.org/10.1080/00335558008248231',\n",
       "   '[43] Tejaswi Potluri, Venkatarama Phani Kumar, and K Venkata Krishna Kishore. 2023.\\nAn automated online proctoring system using attentive-net to assess student\\nmischievous behavior. *Multimedia Tools and Applications* 82, 20 (Aug. 2023),\\n30375–30404. https://doi.org/10.1007/s11042-023-14604-w',\n",
       "   '[44] Tejaswi Potluri, Venkatarama Phani Kumar Sistla, and Drkv Krishna. 2023. Detecting autism of examinee in automated online proctoring using eye-tracking.\\n*Journal of Theoretical and Applied Information Technology* 101, 3 (2023).',\n",
       "   '[45] Swathi Prathish, Athi Narayanan S., and Kamal Bijlani. 2016. An intelligent system for online exam monitoring. In *Proceedings of the International Conference on*\\n*Information Science* . IEEE, Kochi, IN, 138–143. https://doi.org/10.1109/INFOSCI.\\n2016.7845315',\n",
       "   '[46] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,\\nZhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C.\\nBerg, and Li Fei-Fei. 2015. ImageNet Large Scale Visual Recognition Challenge.\\n*International Journal of Computer Vision* 115, 3 (2015), 211–252. https://doi.org/\\n10.1007/s11263-015-0816-y',\n",
       "   '[47] Andrew Sanders, Bradley Boswell, Andrew Allen, Gursimran Singh Walia, and\\nMd Shakil Hossain. 2022. Development and Field-Testing of a Non-intrusive\\nClassroom Attention Tracking System (NiCATS) for Tracking Student Attention\\nin CS Classrooms. In *Proceedings of the IEEE Frontiers in Education Conference* .\\nIEEE, Uppsala, SE, 1–9. https://doi.org/10.1109/FIE56618.2022.9962447',\n",
       "   '[48] Andrew Sanders, Bradley Boswell, Gursimran Singh Walia, and Andrew Allen.\\n2021. Non-Intrusive Classroom Attention Tracking System (NiCATS). In *Pro-*\\n*ceedings of the IEEE Frontiers in Education Conference* . IEEE, Lincoln, US, 1–9.\\nhttps://doi.org/10.1109/FIE49875.2021.9637411',\n",
       "   '[49] Steven Seiden, Long Huang, and Chen Wang. 2023. Poster: Snooping Online\\nForm Choice Privacy in Video Calls. In *Proceedings of the IEEE Symposium on*\\n*Security and Privacy* . IEEE, San Francisco, US, 1–2.',\n",
       "   '[50] Abdelrahman Shata, Zineddine N. Haitaamar, and Abdsamad Benkrid. 2023.\\nBaseline Estimation in Face Detection for AI Proctored Examinations through\\n\\n\\nConvoluted Neural Networks. In *Proceedings of the International Conference*\\n*on IT Innovation and Knowledge Discovery* . IEEE, Manama, BH, 1–5. https:\\n//doi.org/10.1109/ITIKD56332.2023.10100328',\n",
       "   '[51] Merzoug Soltane and Mohamed Ridda Laouar. 2021. A Smart System to Detect\\nCheating in the Online Exam. In *Proceedings of the International Conference on*\\n*Information Systems and Advanced Technologies* . IEEE, Tebessa, DZ, 1–5. https:\\n//doi.org/10.1109/ICISAT54145.2021.9678418',\n",
       "   '[52] Robert J Sternberg. 2008. *Psicologia cognitiva* (4 [a] ed.). artmed, Rio de Janeiro, BR.',\n",
       "   '[53] P. Tejaswi, S. Venkatramaphanikumar, and K. Venkata Krishna Kishore. 2023.\\nProctor net: An AI framework for suspicious activity detection in online proctored\\nexaminations. *Measurement* 206 (Jan. 2023), 112266. https://doi.org/10.1016/j.\\nmeasurement.2022.112266',\n",
       "   '[54] Shogo Terai, Shizuka Shirai, Mehrasa Alizadeh, Ryosuke Kawamura, Noriko\\nTakemura, Yuki Uranishi, Haruo Takemura, and Hajime Nagahara. 2020. Detecting Learner Drowsiness Based on Facial Expressions and Head Movements in\\nOnline Courses. In *Proceedings of the 25th International Conference on Intelligent*\\n*User Interfaces Companion* . ACM, Cagliari, IT, 124–125. https://doi.org/10.1145/\\n3379336.3381500',\n",
       "   '[55] Jian-Wei Tzeng, Cheng-Yu Hsueh, Chia-An Lee, and Wei-Yun Shih. 2023. Identifying the Correlation Between Online Exam Answer Trajectory and Test Behavior\\nBased on Artificial Intelligence and Eye Movement Detection Technology. In *Pro-*\\n*ceedings of the International Conference on Consumer Electronics* . IEEE, PingTung,\\nTW, 503–504. https://doi.org/10.1109/ICCE-Taiwan58799.2023.10226745',\n",
       "   '[56] Puru Verma, Neil Malhotra, Ram Suri, and Rajesh Kumar. 2024. Automated\\nsmart artificial intelligence-based proctoring system using deep learning. *Soft*\\n*Computing* 28, 4 (Feb. 2024), 3479–3489. https://doi.org/10.1007/s00500-02308696-7',\n",
       "   '[57] Chenghao Wang et al . 2022. Comprehensively summarizing what distracts\\nstudents from online learning: A literature review. *Human Behavior and Emerging*\\n*Technologies* 2022, 1 (2022), 1483531. https://doi.org/10.1155/2022/1483531',\n",
       "   '[58] Jacob Whitehill, Zewelanji Serpell, Yi-Ching Lin, Aysha Foster, and Javier R.\\nMovellan. 2014. The Faces of Engagement: Automatic Recognition of Student\\nEngagementfrom Facial Expressions. *IEEE Transactions on Affective Computing*\\n5, 1 (Jan. 2014), 86–98. https://doi.org/10.1109/TAFFC.2014.2316163',\n",
       "   '[59] Claes Wohlin. 2014. Guidelines for Snowballing in Systematic Literature Studies\\nand a Replication in Software Engineering. In *Proceedings of the 18th Interna-*\\n*tional Conference on Evaluation and Assessment in Software Engineering* (London,\\nEngland, United Kingdom) *(EASE ’14)* . Association for Computing Machinery,\\nNew York, US, 10 pages. https://doi.org/10.1145/2601248.2601268',\n",
       "   '[60] Claes Wohlin, Marcos Kalinowski, Katia Romero Felizardo, and Emilia Mendes.\\n2022. Successful combination of database search and snowballing for identification of primary studies in systematic literature studies. *Information and Software*\\n*Technology* 147 (2022), 106908. https://doi.org/10.1016/j.infsof.2022.106908',\n",
       "   '[61] Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. 2014. Learning face representation from scratch. *arXiv preprint* 1411.7923 (2014), 9 pages. https:\\n//doi.org/10.48550/arXiv.1411.7923',\n",
       "   '[62] Intan Nurma Yulita, Fauzan Akmal Hariz, Ino Suryana, and Anton Satria Prabuwono. 2023. Educational Innovation Faced with COVID-19: Deep Learning\\nfor Online Exam Cheating Detection. *Education Sciences* 13, 2 (Feb. 2023), 194.\\nhttps://doi.org/10.3390/educsci13020194',\n",
       "   '[63] Cheng Zhang, Cheng Chang, Lei Chen, and Yang Liu. 2018. Online PrivacySafe Engagement Tracking System. In *Proceedings of the 20th ACM International*\\n*Conference on Multimodal Interaction* . ACM, Boulder, US, 553–554. https://doi.\\norg/10.1145/3242969.3266295',\n",
       "   '[64] Xiangyu Zhu, Zhen Lei, Xiaoming Liu, Hailin Shi, and Stan Z Li. 2016. Face\\nalignment across large poses: A 3D solution. In *Proceedings of the IEEE Conference*\\n*on Computer Vision and Pattern Recognition* . IEEE, Las Vegas, US, 146–155. https:\\n//doi.org/10.1109/CVPR.2016.23\\n\\n\\n205\\n\\n\\n-----'],\n",
       "  'text': '# **Investigating User’s Attentional Focus in Computational** **Environments**\\n## A Literature Review with Emphasis on Webcam Data\\n\\n## Cassiano da Silva Souza\\n#### cassiano.souza@edu.pucrs.br Pontifical Catholic University of Rio Grande do Sul and Federal Institute Sul-rio-grandense Porto Alegre, Brazil\\n\\n## Milene Selbach Silveira\\n#### milene.silveira@pucrs.br Pontifical Catholic University of Rio Grande do Sul Porto Alegre, Brazil\\n\\n## Isabel Harb Manssour\\n#### isabel.manssour@pucrs.br Pontifical Catholic University of Rio Grande do Sul Porto Alegre, Brazil\\n\\n\\n**Figure 1:** ***AttentionVis Browser*** **: a web-based user interface of our visual survey, which is composed of (A) the interaction panel,**\\n**including the search field and filters by category, and (B) the main panel - thumbnails representing each paper.**\\n\\n### **ABSTRACT**\\n\\nMaintaining the user’s attentional focus has become a recurring\\nconcern in recent years. This is due to the consolidation of remote\\nand hybrid models for study and work, which were widely experienced during the social distancing caused by COVID-19. This paper\\npresents a review of works that address this problem by analyzing webcam data, a promising device for behavioral studies. The\\nliterature review from 2013 to 2023 was carried out using a hybrid\\nsearch strategy, through which we selected and analyzed 57 papers.\\nThe summary of this study is presented in an interactive visual\\nsurvey format called the *AttentionVis Browser* tool. As additional\\ncontributions, we provide a list of lessons learned, a list of work\\nlimitations, and possibilities for future research.\\n### **KEYWORDS**\\n\\nattention monitoring, webcam, data analysis, literature review.\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n© 2024 SBC – Sociedade Brasileira de Computação.\\nISSN 2966-2753\\n\\n### **1 INTRODUCTION**\\n\\nThe advancement of information and communication technologies\\n(ICT) has significantly impacted our daily lives. According to the\\nInternet Steering Committee in Brazil [10], with the advent of the\\nCOVID-19 pandemic and given the barriers imposed by social isolation to contain the spread of the virus, the demands for such\\nresources proved to be fundamental for the continuity of work,\\neducation, and social interactions. In this context, remote and hybrid models stand out, from temporary solutions to consolidated\\npractices in the post-pandemic scenario.\\nRecent surveys on the work environment show a growing adoption of remote work, with significantly higher numbers than before the pandemic [ 40 ]. Although services provided on company\\npremises still predominate (66.5%), hybrid and remote models already represent 33.5% of activities [ 2 ]. Barrero et al. [ 7 ] suggest this\\nis a trend, with the prediction that at least one working day per\\nweek will be conducted remotely in the coming years. Likewise,\\neducational institutions are also adapting to new circumstances.\\nThe growing acceptance of distance learning (DL) is evidenced\\nby the 166.4% increase in course enrollments in these modalities\\nbetween 2015 and 2021, according to the Higher Education Map in\\nBrazil [ 19 ]. On the other hand, enrollment in face-to-face courses\\n\\n\\n197\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Souza et al.\\n\\n\\ndecreased by 20.6%, indicating a significant change in the Brazilian\\neducational landscape with an increasing focus on DL.\\nHowever, these models accompany a recurring challenge that\\nnegatively impacts both school [ 12, 57 ] and work performance [ 6 ]:\\nmaintaining attentional focus. Nakayama et al. [ 37 ] and Wang et\\nal. [ 57 ] point out digital distractions - such as messages, notifications, and social networks, among others - along with the multitasking environment, as the main factors that harm our ability to\\nmaintain focus on essential tasks. This occurs because our brain\\n\\ncannot process many perceptual stimuli simultaneously, depending\\non the complex cognitive process of “attention” to select relevant\\ninformation and discard irrelevant information [34, 52].\\nWith the growing demand for remote and hybrid activities,\\nseeking effective strategies to mitigate distractions and promote\\ngreater concentration becomes increasingly important. One approach adopted is attentional analysis based on eye tracking data\\nbecause, according to Posner [ 42 ], there is a direct relationship between eye movement and changes in attention. Thus, this research\\naims to **synthesize and organize existing knowledge about**\\n**attentional analysis conducted based on data obtained via**\\n**webcam** - a low-cost eye movement capture device, with possibilities for large-scale study [ 9 ]. To do this, we carried out a literature\\nreview from 2013 to 2023, using a hybrid search strategy, through\\nwhich we selected 57 papers. These papers were analyzed and categorized, offering a comprehensive overview of application domains,\\ndata, and techniques, identifying research gaps, and pointing out\\npossible future directions. The main contributions of this work are:\\n\\n  - Presentation of the state of the art related to the topic, identifying features that enable attentional analysis;\\n\\n  - Development and availability of the tool “ *AttentionVis Browser* ”\\nthat presents this study’s results in a visual and interactive\\nformat, as demonstrated in Figure 1.\\n\\n  - Presentation of identified lessons learned, allowing researchers\\nto identify promising areas, avoid errors, and contribute to\\nthe development of this field;\\n\\n   - Presentation of a set of limitations of current solutions;\\n\\n  - Identification of research opportunities on attentional analysis.\\n\\nThe following sections present the methodological process, analyze the results, and discuss lessons learned, limitations, and research opportunities.\\n### **2 METHODOLOGY**\\n\\nFor the literature review, we adopted a hybrid search strategy based\\non Mourão et al. [ 36 ] guidelines to ensure greater efficiency in retrieving relevant studies, as corroborated by Wohlin et al. [ 60 ]. The\\nprocess involves preparing a search string based on the proposed\\nresearch questions, conducting a database search for studies in a\\nsingle digital library, applying inclusion and exclusion criteria, and\\nthen using the selected studies as a seed set for applying the Backward Snowballing (BS) and Forward Snowballing (FS) techniques.\\nThe BS technique involves reviewing all references of papers\\nselected in the seed set to identify additional studies relevant to\\nthe research. The FS seeks to identify new studies referencing the\\npublications that make up the seed set [ 59 ]. The previously defined\\nselection criteria must be strictly followed when analyzing these\\n\\n198\\n\\n\\nnew papers. In this hybrid search strategy, papers obtained via\\nBS are not submitted to FS, and vice versa, avoiding overlaps as\\nindicated by Mourão et al. [ 36 ] and Wohlin et al. [ 60 ]. The papers\\nare analyzed for data extraction and synthesis at the end of the\\nprocess. Figure 2 summarizes the steps that are detailed below.\\n\\n**Figure 2: Selection process carried out using the hybrid search**\\n**strategy.**\\n\\nAs a way of guiding this study, employing the PICO (Population,\\nIntervention, Comparison, Outcome) [ 27 ] criteria, we defined the\\nfollowing questions that this research aims to answer:\\n\\n  - **RQ1:** What are the most prominent areas of concentration\\nin attention and user behavior studies using webcam data,\\nand what practical applications are derived from?\\n\\n  - **RQ2:** What information can be obtained through a webcam\\nwhile a user performs their tasks, and which features are\\nrelevant for behavioral analysis?\\n\\n  - **RQ3:** What insights can be obtained from analyzing data to\\nidentify user attention and behavior patterns, and how are\\nthese presented?\\n\\n  - **RQ4:** How are data visualization techniques applied to convey the insights resulting from this analysis clearly?\\n\\nAlthough Mourão [ 36 ] suggests the use of a single digital library,\\nwe expanded our searches to include the IEEE Xplore Digital Library [1] and the ACM Digital Library [2], due to their significance in\\nthe computing area and the relevance of their publications. Furthermore, these libraries include publications from journals and\\n\\n1 https://ieeexplore.ieee.org/Xplore/home.jsp\\n2 https://dl.acm.org\\n\\n\\n-----\\n\\nInvestigating User’s Attentional Focus in Computational Environments WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\nconference proceedings that are relevant to our research, such as\\nETRA [3], CVPR [4], and ICMI-MLMI [5]\\n\\nAfter defining the search databases, we identified the most relevant terms to compose a search string to return papers related\\nto our research topic. The resulting string, composed of the terms\\n((“eye track*” OR “gaze\" AND (“attention” OR “attentiveness” OR\\n“engagement” OR “monitoring” OR “behavior”) AND (“webcam”)),\\nwas applied exclusively in the abstract field. When using the search\\nstring in the selected databases, 78 studies were identified. The\\ninclusion (IC) and exclusion (EC) criteria expressed in Table 1 were\\napplied to keep only relevant works.\\nThe initial stage of the filtering process eliminated studies that\\ndid not meet the basic criteria, such as IC1, IC2, EC1, EC2, and EC4.\\nWe only considered the last ten years of research to analyze the\\ncurrent context. The resulting papers were analyzed based on the\\ntitle to provide a basis for removing duplicate works (EC3). For a\\nbetter understanding of the focus of each study and the application of the EC5, EC6, EC7, EC8, EC9, and EC10 criteria, we used\\nthe Three-pass [ 25 ] method. As a result, we arrived at a seed set\\nconsisting of ten papers.\\n\\n**Table 1: Inclusion and exclusion criteria.**\\n\\n**Criteria** **Descri** **p** **tion**\\n\\n**Inclusion criteria**\\n\\nIC1 Published between January 2013 and October 2023.\\n\\nIC2 Should be p ublished in a conference, worksho p, or j ournal.\\n\\n**Exclusion criteria**\\n\\nEC1 Full-text is not available online.\\n\\nEC2 The study that is not written in English.\\nEC3 Duplicated studies returned by different search engines.\\nEC4 Books, book chapters, abstracts, and gray literature.\\nEC5 The studies that are reviews or mapping studies.\\nEC6 Do not use data from webcams as the primary data source.\\nEC7 Do not provide user attention, behavior, or engagement insights.\\nEC8 Studies focused on comparative analysis (algorithms, methods, or devices)\\nEC9 Focused on the emotions or sentiment analysis.\\n\\nEC10 The stud y does not answer at least one research question.\\n\\nFollowing the hybrid search strategy, the next stage of the process involved applying the BS technique to the papers that make up\\nthe seed set. From the 204 references identified, the selection processes were applied as in the previous stage, resulting in 12 accepted\\npapers. Afterward, we used the FS technique to deepen our analysis\\nfurther. As suggested by Mourão et al. [ 36 ], Google *Scholar* [6] was\\nused to search for studies that cite the papers contained in the seed\\nset. In this process, carried out in November 2023, 256 studies were\\nidentified. In the same way, as in the BS, new studies were only\\naccepted if they met the selection criteria. Thus, 35 more papers\\nwere identified.\\nApplying the hybrid search strategy in this literature review\\nresulted in the selection of 57 relevant papers. We analyzed them\\nthoroughly, observing aspects such as the application domain, relevant features, Machine Learning (ML) algorithms and methods,\\ndatasets, and visualization techniques. This data was collected and\\norganized in an electronic spreadsheet. We then summarize this\\n\\n3 Eye Tracking Research and Applications\\n4 Computer Vision and Pattern Recognition\\n5 Multimodal Interfaces and Machine Learning for Multimodal Interaction\\n6 https://scholar.google.com\\n\\n\\ninformation to provide an overview of the state of the art based on\\nthe directions presented in the research questions.\\n### **3 ANALYSIS OF THE RESULTS**\\n\\nThis section analyzes the results obtained from the previous step.\\nThis information serves as a basis for answering the research questions, identifying the contributions and limitations of studies, and\\nsuggesting research opportunities.\\n### **3.1 Research Topics Overview**\\n\\nThe analysis shows a significant increase in publications on the\\nresearch topic from 2021 onwards, as illustrated in Figure 3. The\\ngrowing interest in investigating webcam data to analyze user attention in digital environments can be attributed to the impacts of the\\nCOVID-19 pandemic. The need to adapt to a new reality, marked\\nby the widespread adoption of remote work and distance learning,\\nbrought challenges directly related to maintaining focus [ 37 ]. Additionally, the wide availability of webcams integrated with laptops\\nand mobile devices and their capability to serve as low-cost eye\\ntrackers likely contribute to this investigation.\\n\\n**Figure 3: Number of papers per year, categorized by domain**\\n**application.**\\n\\nThe Figure 3, in addition to illustrating the number of papers\\nper year of publication, also classifies them into four areas of study:\\n**Education**, **Work**, **Privacy**, and the intersection of **Work and**\\n**Education** . Among them, Education stands out with 95% of the publications, demonstrating the importance of this area for society and\\nthe academic interest in mitigating attentional challenges related\\nto the learning context. The intersection of Work and Education\\nis only explored in the study by Ozgen et al. [ 38 ], which analyzes\\ncheating behaviors in job interviews and online exams. Likewise,\\nit should be observed that the areas related to Work [ 39 ] and Privacy [ 49 ] appear in just one post each publication, suggesting a\\nminimal representation of these topics.\\nOnline activities represent 91% of studies, reflecting the adaptation to digital transformations, including remote classes, online\\nassessments, collaborative games, and digital documents. The remaining 9% are directed to face-to-face interactions, even though\\nin computing environments, such as computer labs [ 48 ]. Thus, four\\nstudy topics stand out:\\n\\n(1) **Cheating detection** (37 papers). Involve user analysis during online educational assessments or selection processes,\\npreventing fraud;\\n(2) **Attention monitoring** (11 papers). Focused on observing\\nand analyzing how users direct their attention during specific\\nactivities;\\n\\n\\n199\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Souza et al.\\n\\n\\n(3) **Engagement level** (8 papers). Involve aspects such as interest, motivation, emotions, and active participation during\\ntask performance;\\n(4) **User privacy** (1 paper). Based on observing where and how\\nusers direct their attention, including time spent in specific\\nareas, to infer their preferences.\\n### **3.2 Webcam Feature Extraction**\\n\\nData extracted from videos are essential for identifying patterns\\nrelated to human behavior. Therefore, capturing videos with adequate resolution and frame rate is essential, ensuring data quality\\nand, consequently, accuracy and effectiveness in subsequent analysis [ 29 ]. Through these data, it is possible to extract features such\\nas **Facial expressions** (analyzing facial landmarks, such as eyes,\\nmouth, nose, chin, or any relevant structure [ 41 ]), **Head position**,\\n**Eye movements**, **Body pose**, and **Objects** present in the environment. By analyzing them - individually or combined - it is possible\\nto understand many details about the user, from how they are positioned and where they are looking to what their facial expressions\\ncan reveal about their reactions or emotional state. Figure 4 presents\\nthe features explored by the selected studies.\\n\\n**Figure 4: Webcam features.**\\n\\nImage processing and computer vision techniques extract this\\ninformation from each frame. There are several tools for this purpose; however, the most cited were: OpenCV (in 32% of papers),\\nDlib (26%), and Yolo (15%). One of the purposes of these extracted\\nfeatures is to provide relevant information to ML algorithms, allowing them to learn and perform tasks such as classification, pattern\\nrecognition, and prediction.\\n### **3.3 Used Datasets**\\n\\nWe explored the datasets used to identify the main sources of data\\navailable to investigate aspects such as engagement, attention, emotional state, and related factors. Although many studies do not\\nprovide clear information, we identified that 38% of the studies\\nconducted chose to create their own datasets. This approach offers\\ngreater control over characteristics and metrics but requires a long\\nlabeling process to categorize the data correctly.\\nThe datasets made available on public platforms or shared by the\\nscientific community represent a valuable resource for advancing\\nresearch in the area. These datasets (shown in Table 2), composed\\nof images or videos, facilitate immediate study application. Furthermore, because they are already labeled, it significantly reduces the\\neffort required to manually categorize data, optimizing researchers’\\ntime and increasing efficiency in conducting experiments.\\n\\n\\n**Table 2: Open-source datasets and some use cases**\\n\\n**Dataset** **Pur** **p** **ose** **Use case**\\n\\nPAFE [29] Predicting attention and mind-wandering [29]\\nLFW [18] Facial recognition for cheat detection [51] [44]\\nAFLW2000-3D [64] Facial landmarks for exam integrity [20]\\nEngageWild [24] Engagement detection [24] [63]\\n\\nOEP [5] Webcam and wearcam data for cheat detection [5] [62]\\n\\nFER-2013 [15] Emotion recognition in proctoring systems [56]\\nImageNet [46] Object detection [53]\\nCOCO [31] Person and/or object detection [56] [38] [39]\\n\\nPandora [8] Head pose, and shoulder estimation [17]\\nCASIA-WebFace [61] Facial verification and identification [43]\\nBIWI [14] Head p ose estimation [17]\\n### **3.4 Machine Learning**\\n\\nIn the reviewed literature, we observed a variety of ML approaches\\nadopted for the analysis of attention, which reflects the diversity of\\ntechniques and strategies used. Thus, the following emerge: **Con-**\\n**volutional Neural Network (CNN)**, **Support Vector Machine**\\n**(SVM)**, **Deep Neural Network (DNN)**, **Random Forest (RF)**, **Re-**\\n**current Neural Network (RNN)**, and **Decision Tree (DT)** . The\\nfirst two algorithms mentioned are the most used for attentional\\nfeature analysis, with 21 and 12 studies, respectively. It is necessary\\nto emphasize that seven papers do not specify the algorithm used,\\nlimiting themselves to using the general term “Machine Learning”\\nas the applied approach. The most used algorithms are illustrated\\nin Figure 5, and those less conventional, mentioned in only two\\nwords or less, are not included in the graph.\\n\\n**Figure 5: Main ML algorithms used by the studies.**\\n\\nWe also identified that four studies do not mention the use of\\nML in their methods, which suggests a possible application of more\\ntraditional image processing techniques or statistical methods. On\\nthe other hand, 16 studies used multiple algorithms to compare\\ntheir performance, emphasizing that the effectiveness of ML algorithms varies with the approach, features, and context. This makes\\nit difficult to claim one algorithm as universally superior.\\n### **3.5 Combination with Multimodal Data**\\n\\nIn addition to the analyses carried out using webcam features, an\\ninteresting approach to complementing and enhancing the results\\nis the combination with multimodal data. This approach is particularly used in cheating detection outcomes and was considered\\nin 47% of studies. Figure 6 presents the data sources identified in\\nthese studies: **Microphone**, **Screenshot/screenshare**, **System**\\n**logs**, **Eye tracker**, **Keyboard**, **Mouse**, and **Form** . Among them,\\nthe most used device was the microphone attached to webcams\\n(30%), which allows voice recognition, external noises, and parallel discussions. Additionally, 26% of papers use screenshots or\\nscreenshare features. The first one can be used to identify the active\\n\\n\\n200\\n\\n\\n-----\\n\\nInvestigating User’s Attentional Focus in Computational Environments WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\nwindow [ 20 ] and screen sharing to support teachers and supervisors during the exams [ 35 ]. System logs are considered in 15% of\\nthe papers and can come from learning platforms [ 4 ], chats [ 13 ]\\nand operational systems [45].\\n\\n**Figure 6: Multimodal data sources.**\\n### **3.6 Outcome Data**\\n\\nThe outputs derived from the analysis of attentional data can be\\ncategorized in two ways. The first approach is more simplified,\\nusing binary outputs that classify the user’s states directly, such\\nas attentive or inattentive/distracted [ 26, 32, 48 ]; focused or not\\nfocused [ 29 ]; and, cheating or not [ 5 ]. The second way involves\\nadditional information, expressed in intensity levels, such as sleeping, drowsy or awake [ 54 ]; or “Not engaged at all”, “Nominally\\nengaged”, “Engaged in task”, and “Very engaged” [ 24, 58, 63 ]. This\\ndetailed approach offers more opportunities to analyze the individual’s behavior and/or emotional state, allowing for an in-depth\\nanalysis.\\nThe way these data are displayed can change depending on the\\npurpose of the analysis. Studies such as those by Shata et al. [ 50 ]\\nand Jadi [ 21 ] choose to display warning messages on the screen\\nto alert about potential cheating during exams. Ozgen et al. [ 39 ]\\nuse labels to do it in this same topic. These approaches are useful\\nfor quick interpretation without presenting the details or basis for\\nsuch information. On the other hand, Li et al. [ 30 ]’s study uses data\\nvisualization techniques to help synthesize the different available\\noutputs into more understandable formats.\\n### **3.7 Visualization Techniques**\\n\\nTo understand how authors present the complex information related\\nto attention analysis - from input and processing to data output\\n\\n- we explore how different studies employ data visualization and\\nexamine their application methods. Thirteen different visualization\\ntechniques were used to express some information. Line graphs,\\ntypically used to represent time series data, were the most used,\\nappearing in 23 studies. Next, histograms, used for distribution\\ndata, were found in 16 studies. The confusion matrix, applied in 10\\nstudies, indicates the assertiveness levels of ML models.\\nThe purposes for using visualizations in the selected studies\\nwere classified into six main categories: **Pattern Recognition**,\\n**Insights**, **Evaluation Metrics**, **Addicional Details**, **Comparison**,\\nand **Other purposes** . Each reflects a distinct set of goals, as seen\\nin Table 3.\\n\\n\\n**Table 3: Purpose of data visualization usage in selected stud-**\\n**ies.**\\n\\n**Cate** **g** **or** **y** **Pur** **p** **ose of Visualizations** **Total** **p** **a** **p** **ers**\\n\\n\\nPattern\\n\\nRecognition\\n\\n\\nHead pose 1\\nFacial expressions 4\\nMultimodal data 3\\nE y e movements or g aze directions 8\\n\\n\\nShow results to the user 8\\nInsights Attention, engagement or behavior level 7\\nCheatin g behaviors p robabilit y 4\\n\\nAccuracy 11\\n\\nEvaluation Precision 3\\n\\nmetrics Recall 1\\n\\nF-1 score 1\\n\\nAdditional Distribution of data collected 7\\n\\ndetails Results of interviews 5\\n\\nComparative analysis of ML classifiers 9\\nComparison Data-driven ex p eriment anal y sis 3\\n\\nCorrelation between features 2\\nOther purposes Outliers 2\\nS y stem architecture 1\\n\\nWhile data visualization is a powerful tool for effectively communicating insights derived from data analysis [ 22 ], Table 3 underscores that its use is mainly associated with aspects that demonstrate, compare, and analyze ML models. The objective of using\\nthese visual resources is primarily to offer support and clarity to\\nthe paper’s reader, thus synthesizing, in a graphic form, complex\\ninformation related to the analyses. The use of visual techniques\\nto demonstrate results to the end user of the proposed solution is\\nfound in only eight papers.\\n### **4 INTERACTIVE VISUAL SURVEY**\\n\\nBased on the *TextVis Browser* project, developed by Kostiantyn et\\nal. [ 28 ], we propose an interactive visual survey of attentional data\\nanalysis, called *AttentionVis Browser*, available at:\\n**https://davintlab.github.io/AttentionVis-Browser**\\nThis tool, developed in HTML and JavaScript, was designed to\\nquickly and intuitively summarize and present the results of this\\nstudy, including only those papers that present visualization techniques (43 papers). Figure 1 illustrates the user interface, consisting\\nof a main and interaction panels.\\nIn the main panel, thumbnails of the visualization techniques\\nrepresenting each paper are organized in a grid format. They are\\nordered by publication year (in descending order) and then by the\\nprimary author’s surname. Clicking on a specific thumbnail displays\\ndetails of the selected study, including the complete bibliographic\\nreference, a URL link to access the full paper, a BiBTeX file link,\\nand a list of categories assigned, as illustrated in Figure 7.\\n\\n**Figure 7: Details of a survey entry.**\\n\\n\\n201\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Souza et al.\\n\\n\\nThe interaction panel allows the user to filter the content displayed on the main panel, selecting specific works through textual\\nsearch or restricting the results by year of publication or category.\\nA summarized view of the papers can be consulted using the menu\\noption “Summary”. Additionally, we provide a form for authors\\nwho wish to contribute with additional entries. The information\\nwill then be verified and added.\\n### **5 DISCUSSION**\\n\\nThis section presents the research questions and their answers,\\nallowing insights into the topic. After that, lessons learned and\\nlimitations of the research are also presented.\\n### **5.1 Research Questions**\\n\\nNext, we discuss the main findings of our literature review, organized according to the research questions.\\n\\n***RQ1:*** *“What are the most prominent areas of concentration in attention*\\n*and user behavior studies using webcam data, and what practical*\\n*applications are derived from?”*\\n\\nWe found that 95% of the publications are concentrated in the Education area, highlighting its importance in research on attentional\\nfocus in digital environments. Although in smaller numbers, other\\nareas were also identified, such as Work, with two publications, and\\nPrivacy, with one publication. The data indicate that user attention\\ncan be explored as a central point or part of a broader context.\\nThe greatest focus is on cheating detection systems (63%), aiming\\nto ensure integrity in educational [ 5, 30 ] and work contexts [ 39 ]).\\nOther applications aim to improve the educational process, such\\nas classes [ 47 ] and teaching materials [ 47, 48 ]). They also address\\nissues such as mind wandering [ 29 ], tiredness [ 1 ], and drowsiness [54] during the execution of activities.\\n\\n***RQ2:*** *“What information can be obtained through a webcam while a*\\n*user performs their tasks, and which features are relevant for behav-*\\n*ioral analysis?”*\\n\\nThrough a webcam, it is possible to extract several crucial pieces\\nof information about the user’s behavior while performing tasks.\\nThis includes **facial expressions**, which encompass the movement\\nof the lips, eyebrows, and other aspects to understand the user’s\\nemotions and reactions; **eye movements**, which allow identifying areas of visual focus through coordinates; the assessment of\\n**body posture** and the observation of **head movement** (roll, pitch\\nand yaw); and the detection of **objects** in the environment. These\\nfeatures play a fundamental role in understanding the user’s behavioral patterns. They can be analyzed in isolation or combination,\\nallowing the development of solutions related to attentional focus,\\nengagement, and emotional states.\\n\\n***RQ3:*** *“What insights can be obtained from analyzing data to identify*\\n*user attention and behavior patterns, and how are these presented?”*\\n\\nThe analysis of data obtained through a webcam, using ML algorithms, makes it possible to identify patterns, trends, and behaviors\\nof users while interacting in digital environments. This analysis\\ncan reveal information about the occurrence of cheating [ 5 ], levels\\nof attention [ 26 ] and focus [ 29 ], indicators of distraction [ 26, 48 ]\\n\\n\\nor fatigue [ 54 ], degree of engagement [ 24, 58, 63 ] or user preferences [ 33 ]. Information is presented through alert [ 21 ], labels [ 35 ],\\nflags [21, 50], prompt [51], tables [58], and graphs [63].\\n\\n***RQ4:*** *“How are data visualization techniques applied to convey the*\\n*insights resulting from this analysis clearly?”*\\n\\nTable 3, in the “insights” category, highlights how visualizations\\nare used to present the results obtained. Visualization techniques\\nare applied to (I) present results to the end users of the application,\\n(II) illustrate levels of attention, engagement, and behavior, and\\n(III) indicate the probability of cheating. Although this category\\nrepresents 33% of the studies, we observed that these visualizations\\nlack details or explanations that would help the end user to better\\nunderstand the results.\\n### **5.2 Lessons Learned**\\n\\nThis section describes the knowledge acquired while conducting\\nthe research. To this end, we created a list of lessons learned, as\\nsummarized in Figure 8 and detailed below.\\n\\n**Figure 8: Summary list of Lessons Learned.**\\n\\n**Multifaceted Approach.** The literature approaches attention in\\ndifferent ways, depending on the objective of the study: it can be\\nused to identify inappropriate behaviors during assessment activities, analyze the level of engagement during studies, help educators\\nin decision-making, or even discover certain user preferences by\\nanalyzing the direction of their gaze while filling out a simple online\\nform. .\\n**Types of attention.** The term “attention” is broad without delving\\ninto its different types. Even so, the evidence presented by Tzeng\\net al. [ 55 ], for example, shows how eye patterns differ depending on the type of task being performed. This can contribute to\\ninvestigations involving the concept of “divided attention” (focusing on different tasks simultaneously) and “sustained attention”\\n(prolonged periods without distractions). Likewise, the various approaches to identifying cheating, as presented by Irfan et al. [ 20 ],\\ncontribute to the analysis of “alternating attention” by enabling the\\nidentification of external noises, parallel conversations, and head\\nmovements, evidencing the change in focus.\\n\\n\\n202\\n\\n\\n-----\\n\\nInvestigating User’s Attentional Focus in Computational Environments WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\n**Correlation between Eye Movements and Attention.** Despite\\nthe relationship between eye movements and attention described by\\nPosner [ 42 ], this feature is not essential for attentional predictions\\nand classifications. The approaches adopted by Alyuz et al. [ 3 ],\\nIrfan et al. [ 20 ], and Cote et al. [ 11 ], for example, consider only the\\nuser’s head movements to indicate their attentional focus, obtaining\\nsatisfactory results within their objectives.\\n**Approaches to Data Classification.** The selected works demonstrated the importance of ML for the analysis of data extracted\\nfrom a webcam since 89% of the presented solutions clearly express\\nits use. Khan et al.[ 26 ] demonstrated the effectiveness of machine\\nlearning models in automatically classifying attention using eyetracking metrics. Furthermore, Seiden et al. [ 49 ] emphasize the\\naccuracy of the algorithms in predicting the location of the visual\\nfocus on the screen, which is essential for understanding where the\\nuser is directing their attention.\\n**Predominant Algorithms for Analysis.** There is a predominance\\nof certain algorithms in attention analysis, such as CNNs, which\\nhave been widely used to identify patterns in data from webcams.\\nThis popularity is due to the effectiveness of CNNs in classifying\\naudio and video data [ 16 ], which makes them especially suitable for\\nanalyzing eye movements and facial expressions. Another approach\\nthat stands out is using SVMs, especially in contexts involving\\nsmaller-scale datasets.\\n\\n**Use of Visual Representations.** Approximately 75% of the studies\\nuse visualization techniques to elucidate information or results. One\\ntrend observed is using these resources to express data related to ML\\nprocesses, such as metrics, comparisons, the influence of features,\\nand model performance.\\n### **6 LIMITATIONS AND RESEARCH** **OPPORTUNITIES**\\n\\nThis section presents the limitations of this work regarding the\\nreviewed studies and the research conducted. These limitations,\\nin turn, represent opportunities that can be investigated in future\\nresearch and are also presented.\\nThe identified **limitations regarding the reviewed studies**\\nare described below.\\n\\n**Limited scope.** The studies are predominantly focused on Education, highlighting the importance of broadening the scope and\\nencompassing professional environments, which is equally important in the current context. In addition, the analyses are restricted\\nto a single type of task, excluding the possibility of simultaneously\\nmonitoring the development of other types of activities.\\n**Aspects related to attention.** When there is a change of focus to\\nan activity outside the investigated scope, this transition is seen\\nonly as a distraction or, in certain contexts, as cheating. This opens\\nthe opportunity to identify whether this change is, in fact, a distraction or whether the user chose to redirect his attention to another\\nactivity relevant to his work or study, characterizing ‘alternating\\nattention’ [23].\\n**Detailing of methods for analyzing and extracting patterns.**\\nThe replicability of the proposed solutions is compromised by the\\nlack of detailing of the versions of the tools used, especially for\\nthe extraction of features (only 19% of the studies provide this\\nessential information). This makes it difficult for other researchers\\n\\n\\nto identify, analyze, and reproduce the results. In addition, there\\nare challenges in identifying methods and algorithms due to the\\nlack of such information.\\n\\n**Analyses Report.** In the studies reviewed, we often found generic\\ndescriptions of attention monitoring, such as normal or abnormal\\nbehavior, attentive or distracted state, and the possibility of cheating.\\nSometimes, data are communicated only through labels or flags,\\nhindering users’ understanding.\\n**Uninformative visualizations.** The limitations of the visualiza\\ntions lie in the lack of details about the data presented, the direction\\nof focus at each moment, and the precise definition of what is considered a distraction. Only in the studies by Li et al. [ 30 ] and Ozgen\\net al. [ 38 ] do we find a more detailed approach (focused on detecting\\ncheating). The user must be aware of the periods of distraction and\\nthe reasons associated with these moments, as this can help them\\nidentify behavioral patterns and implement strategies to mitigate\\nthese distractions, promoting self-regulation.\\n\\nThe identified **limitations regarding the conducted research**\\nare listed below.\\n\\n**Specific devices for data capture.** This research builds on studies primarily using webcams to collect visual data. While these\\nare widely available and widely used, it is important to note that\\nalternative devices, such as electroencephalography (EEG) and commercial eye trackers (Tobii [7], and SR Research [8], for example), offer\\nmore accurate and detailed measurements.\\n\\n**Comprehensive approach.** The broad approach adopted in our\\nwork offers global understanding and contextualization advantages\\nbut may lack detailed depth on specific topics.\\n**Focus on solutions and applications.** This review is defined\\nby the exclusion criteria presented in Table 1, which delimit the\\nscope due to the large volume of related works. The main focus is\\non practical solutions, excluding comparative studies (between devices, methods, and techniques), wearable devices, and EEG, among\\nothers.\\nThroughout our investigation, we identified areas that are underexplored or not addressed by current studies. Thus, some **research**\\n**opportunities** are described below.\\nWe suggest the development of more comprehensive solutions,\\nexpanding attention analysis beyond the educational scope; the\\ndevelopment of evaluation frameworks that assist in measuring\\nthe effectiveness and usability of these tools; the application of\\nnarrative visualizations in the results obtained through the ML\\nclassifiers on attentional analysis, aiming at a more intuitive communication, facilitating understanding by the user and providing\\nsubsidies that contribute to self-regulation. Furthermore, we consider promising the classification of different “types of attention”\\nduring data analysis, such as transitions of focus between different\\nactivities, distinguishing distractions from deliberate choices, and\\ncharacterizing “alternating attention”, for example.\\n### **7 FINAL REMARKS**\\n\\nThe research presented here aimed to synthesize and organize the\\nexisting knowledge related to data analysis, especially data captures\\n\\n7 http://www.tobii.com\\n8 http://www.sr-research.com\\n\\n\\n203\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Souza et al.\\n\\n\\nfrom webcams. In this sense, it offers a comprehensive view of\\napplication domains, data, features, techniques, trends, gaps, and\\nopportunities. Finally, as our investigation’s result, we identified\\nfive **contributions** :\\n\\n**State of the art mapping.** We explore a decade of related studies\\nproviding an overview of the state of the art related to attentional\\nanalysis using webcam data.\\n***AttentionVis Browser*** **.** The interactive visual tool built upon the\\nreview results offers an overview of the field. It can be used by the\\ngeneral community (for educational purposes) and the scientific\\ncommunity (to aid in searching for related works and extend it).\\n**Presenting lessons learned.** The lessons learned highlight the\\nknowledge acquired with this study and aim to provide researchers\\nwith support for the continued development of the topic. Pondering\\non these lessons allows for adjusting and improving methods and\\nprocesses, avoiding repeating errors, and supporting new hypotheses for future investigation.\\n**Presenting limitations research.** Examining the limitations identified in current studies and our own research, we aim to encourage\\nfurther investigations in areas that require better development.\\n**Presenting research opportunities.** The identified opportunities\\ncan inspire the exploration of new directions, approaches, and\\nsolutions to the challenges that permeate the analysis of attention.\\n### **ACKNOWLEDGMENTS**\\n\\nThis paper was supported by the Ministry of Science, Technology,\\nand Innovations, with resources from Law No. 8.248, dated October\\n23, 1991, within the scope of PPI-SOFTEX, coordinated by Softex.\\nIsabel H. Manssour would like to thank the financial support of the\\nCNPq Scholarship - Brazil (303208/2023-6).\\n### **REFERENCES**\\n\\n[1] Andrea F. Abate, Lucia Cascone, Michele Nappi, Fabio Narducci, and Ignazio\\nPassero. 2021. Attention monitoring for synchronous distance learning. *Future*\\n*Generation Computer Systems* 125 (2021), 774–784. https://doi.org/10.1016/j.\\nfuture.2021.07.026\\n\\n[2] Cevat Giray Aksoy, Jose Maria Barrero, Nicholas Bloom, Steven J Davis, Mathias\\nDolls, and Pablo Zarate. 2023. *Working from home around the globe: 2023 Report* .\\nTechnical Report. EconPol Policy Brief.\\n\\n[3] Nese Alyuz, Eda Okur, Utku Genc, Sinem Aslan, Cagri Tanriover, and Asli Arslan\\nEsme. 2017. An unobtrusive and multimodal approach for behavioral engagement detection of students. In *Proceedings of the 1st International Workshop*\\n*on Multimodal Interaction for Education* . ACM, Glasgow, UK, 26–32. https:\\n//doi.org/10.1145/3139513.3139521\\n\\n[4] Sinem Aslan, Nese Alyuz, Cagri Tanriover, Sinem E. Mete, Eda Okur, Sidney K.\\nD’Mello, and Asli Arslan Esme. 2019. Investigating the Impact of a Real-time,\\nMultimodal Student Engagement Analytics Technology in Authentic Classrooms.\\nIn *Proceedings of the CHI Conference on Human Factors in Computing Systems* .\\nACM, Glasgow, Uk, 1–12. https://doi.org/10.1145/3290605.3300534\\n\\n[5] Yousef Atoum, Liping Chen, Alex X. Liu, Stephen D. H. Hsu, and Xiaoming Liu.\\n2017. Automated Online Exam Proctoring. *IEEE Transactions on Multimedia* 19,\\n7 (July 2017), 1609–1624. https://doi.org/10.1109/TMM.2017.2656064\\n\\n[6] Charles E. Bailey. 2007. Cognitive Accuracy and Intelligent Executive Function\\nin the Brain and in Business. *Annals of the New York Academy of Sciences* 1118, 1\\n(2007), 122–141. https://doi.org/10.1196/annals.1412.011\\n\\n[7] Jose Maria Barrero, Nicholas Bloom, and Steven J Davis. 2021. *Internet access*\\n*and its implications for productivity, inequality, and resilience* . Technical Report.\\nNational Bureau of Economic Research.\\n\\n[8] Guido Borghi, Marco Venturelli, Roberto Vezzani, and Rita Cucchiara. 2017.\\nPoseidon: Face-from-depth for driver pose estimation. In *Proceedings of the IEEE*\\n*Conference on Computer Vision and Pattern Recognition* . IEEE Computer Society,\\nLos Alamitos, US, 5494–5503. https://doi.org/10.48550/arXiv.1611.10195\\n\\n[9] Benjamin T. Carter and Steven G. Luke. 2020. Best practices in eye tracking\\nresearch. *International Journal of Psychophysiology* 155 (2020), 49–62. https:\\n//doi.org/10.1016/j.ijpsycho.2020.05.010\\n\\n\\n\\n[10] Comitê Gestor da Internet no Brasil. 2022. *Pesquisa sobre o uso das tecnologias*\\n*de informação e comunicação nas escolas brasileiras - TIC Educação 2021* (1 ed.).\\nNúcleo de Informação e Coordenação do Ponto BR, São Paulo, BR.\\n\\n[11] Melissa Cote, Frederic Jean, Alexandra Branzan Albu, and David Capson. 2016.\\nVideo summarization for remote invigilation of online exams. In *Proceedings of*\\n*the IEEE Winter Conference on Applications of Computer Vision* . IEEE, Lake Placid,\\nUS, 1–9. https://doi.org/10.1109/WACV.2016.7477704\\n\\n[12] Adele Diamond and Kathleen Lee. 2011. Interventions Shown to Aid Executive\\nFunction Development in Children 4 to 12 Years Old. *Science* 333, 6045 (2011),\\n959–964. https://doi.org/10.1126/science.1204529\\n\\n[13] Fahmid Morshed Fahid, Seung Lee, Bradford Mott, Jessica Vandenberg, Halim\\nAcosta, Thomas Brush, Krista Glazewski, Cindy Hmelo-Silver, and James Lester.\\n2023. Effects of Modalities in Detecting Behavioral Engagement in Collaborative\\nGame-Based Learning. In *Proceedings of the 13th International Learning Analytics*\\n*and Knowledge Conference* . ACM, Arlington, US, 208–218. https://doi.org/10.\\n1145/3576050.3576079\\n\\n[14] Gabriele Fanelli, Thibaut Weise, Juergen Gall, and Luc Van Gool. 2011. Real Time\\nHead Pose Estimation from Consumer Depth Cameras. *Pattern Recognition* 6835\\n(2011), 101–110. https://doi.org/10.1109/3DV.2014.54\\n\\n[15] Ian J Goodfellow, Dumitru Erhan, Pierre Luc Carrier, Aaron Courville, Mehdi\\nMirza, Ben Hamner, Will Cukierski, Yichuan Tang, David Thaler, Dong-Hyun Lee,\\nYingbo Zhou, Chetan Ramaiah, Fangxiang Feng, Rui Li, Xiaojie Wang, Dimitris\\nAthanasakis, John Shawe-Taylor, Maximilian Milakov, John Park, Radu Ionescu,\\nMarius Popescu, Cristian Grozea, James Bergstra, Jingjing Xie, Lukasz Romaszko,\\nBing Xu, Zhang Chuang, and Yoshua Bengio. 2015. Challenges in representation\\nlearning: A report on three machine learning contests. *Neural Networks* 64 (2015),\\n59–63. https://doi.org/10.1016/j.neunet.2014.09.005\\n\\n[16] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren\\nJansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan\\nSeybold, et al . 2017. CNN architectures for large-scale audio classification. In\\n*Proceedings of the 2017 IEEE International Conference on Acoustics, Speech and*\\n*Signal Processing* . IEEE, IEEE Press, New Orleans, US, 131–135. https://doi.org/\\n10.48550/arXiv.1609.09430\\n\\n[17] Basavaraj N Hiremath, Anushree Mitra, Aman Thapa, S Amoolya, and A Tameem.\\n2023. Proctoring using Artificial Intelligence. *SSRN Electronic Journal* (2023).\\nhttps://doi.org/10.2139/ssrn.4411332\\n\\n[18] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. 2007. *La-*\\n*beled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained*\\n*Environments* . Technical Report 07-49. University of Massachusetts, Amherst.\\n\\n[19] Instituto Semesp. 2023. *Mapa do Ensino Superior no Brasil* . Semesp, São Paulo,\\nBR. https://www.semesp.org.br/mapa/edicao-13/download/ 13 [a] ed.\\n\\n[20] Mohamed Irfan, Mohammed Aslam, Ziyan Maraikar, Upul Jayasinghe, and Mohamed Fawzan. 2021. Ensuring Academic Integrity of Online Examinations. In\\n*Proceedings of the IEEE 16th International Conference on Industrial and Information*\\n*Systems* . IEEE, Kandy, LK, 295–300. https://doi.org/10.1109/ICIIS53135.2021.\\n9660737\\n\\n[21] Amr Jadi. 2021. New Detection Cheating Method of Online-Exams during COVID19 Pandemic. *International Journal of Computer Science and Network Security* 21,\\n4 (April 2021), 123–130. https://doi.org/10.22937/IJCSNS.2021.21.4.17\\n\\n[22] Marcos Kalinowski, Tatiana Escovedo, Hugo Villamizar, and Hélio Lopes. 2023.\\n*Engenharia de Software para Ciência de Dados: Um guia de boas práticas com*\\n*ênfase na construção de sistemas de Machine Learning em Python* . Casa do Código,\\nSão Paulo, BR.\\n\\n[23] Virgínia Kastrup. 2007. O funcionamento da atenção no trabalho do cartógrafo.\\n*Psicologia e Sociedade* 19, 1 (Jan 2007), 15–22. https://doi.org/10.1590/S010271822007000100003\\n\\n[24] Amanjot Kaur, Aamir Mustafa, Love Mehta, and Abhinav Dhall. 2018. Prediction\\nand Localization of Student Engagement in the Wild. In *Proceedings of the Digital*\\n*Image Computing: Techniques and Applications* . IEEE, Canberra, AU, 1–8. https:\\n//doi.org/10.48550/arXiv.1804.00858\\n\\n[25] S. Keshav. 2007. How to Read a Paper. *Special Interest Group on Data Com-*\\n*munication da Association for Computing Machinery* 37, 3 (jul 2007), 83–84.\\nhttps://doi.org/10.1145/1273445.1273458\\n\\n[26] Ahsan Raza Khan, Sara Khosravi, Sajjad Hussain, Rami Ghannam, Ahmed Zoha,\\nand Muhammad Ali Imran. 2022. EXECUTE: Exploring Eye Tracking to Support\\nE-learning. In *Proceedings of the IEEE Global Engineering Education Conference* .\\nIEEE, Tunis, TN, 670–676. https://doi.org/10.1109/EDUCON52537.2022.9766506\\n\\n[27] Barbara Kitchenham and Stuart Charters. 2007. *Guidelines for performing system-*\\n*atic literature reviews in software engineering* . Technical Report EBSE 2007-001.\\nUniversity of Durham.\\n\\n[28] Kostiantyn Kucher and Andreas Kerren. 2015. Text visualization techniques:\\nTaxonomy, visual survey, and community insights. In *Proceedings of the IEEE*\\n*Pacific Visualization Symposium* . IEEE, Hangzhou, CN, 117–121. https://doi.org/\\n10.1109/PACIFICVIS.2015.7156366\\n\\n[29] Taeckyung Lee, Dain Kim, Sooyoung Park, Dongwhi Kim, and Sung-Ju Lee. 2022.\\nPredicting Mind-Wandering with Facial Videos in Online Lectures. In *Proceedings*\\n*of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops* .\\nIEEE, New Orleans, US, 2103–2112. https://doi.org/10.1109/CVPRW56347.2022.\\n\\n\\n204\\n\\n\\n-----\\n\\nInvestigating User’s Attentional Focus in Computational Environments WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\n00228\\n\\n[30] Haotian Li, Min Xu, Yong Wang, Huan Wei, and Huamin Qu. 2021. A visual\\nanalytics approach to facilitate the proctoring of online exams. In *Proceedings of*\\n*the CHI Conference on Human Factors in Computing Systems* . ACM, New York,\\nUS, 1–17. https://doi.org/10.1145/3411764.3445294\\n\\n[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva\\nRamanan, Piotr Dollár, and C. Lawrence Zitnick. 2014. Microsoft COCO: Common\\nObjects in Context. In *Proceedings of the 2014 Conference on Computer Vision*,\\nVol. 8693. Springer, Zurich, CH, 740–755. https://doi.org/10.48550/arXiv.1405.\\n0312\\n\\n[32] Jens Madsen, Sara U. Júlio, Pawel J. Gucik, Richard Steinberg, and Lucas C.\\nParra. 2021. Synchronized eye movements predict test scores in online video\\neducation. *Proceedings of the National Academy of Sciences* 118, 5 (2021). https:\\n//doi.org/10.1073/pnas.2016980118\\n\\n[33] Rachna Maithani. 2023. Securelens: Enhancing Trustworthiness in Remote Exams\\nwith Opencv Protective Eye. *Journal of Modernization in Engineering Technology*\\n*and Science* 05, 6 (2023), 1857–1861.\\n\\n[34] Margaret W Matlin. 2004. *Psicologia cognitiva, 5* *[a]* *ed* . LTC, Rio de Janeiro, BR.\\n\\n[35] Samuel Monteiro, Rutuja Bhate, Lav Sharma, and Phiroj Shaikh. 2022. Proct-Xam\\n– AI Based Proctoring. In *Proceedings of the 2nd Asian Conference on Innovation*\\n*in Technology* . IEEE, Ravet, IN, 1–6. https://doi.org/10.1109/ASIANCON55314.\\n2022.9908817\\n\\n[36] Erica Mourão, Marcos Kalinowski, Leonardo Murta, Emilia Mendes, and Claes\\nWohlin. 2017. Investigating the Use of a Hybrid Search Strategy for Systematic Reviews. In *Proceedings of the 2017 ACM/IEEE International Symposium on*\\n*Empirical Software Engineering and Measurement* . IEEE, Toronto, CA, 193–198.\\nhttps://doi.org/10.1109/ESEM.2017.30\\n\\n[37] Makoto Nakayama and Charlie C Chen. 2022. Digital Distractions and Remote\\nWork: A Balancing Act at Home. *Information Resources Management Journal* 35,\\n1 (2022), 1–17. https://doi.org/10.4018/IRMJ.308675\\n\\n[38] Azmi Can Ozgen, Mahiye Uluyagmur Ozturk, and Umut Bayraktar. 2021. Cheating Detection Pipeline for Online Interviews and Exams. *ArXiv* abs/2106.14483\\n(2021), 4. https://doi.org/10.48550/arXiv.2106.14483\\n\\n[39] Azmi Can Ozgen, Mahiye Uluyagmur Ozturk, Orkun Torun, Jianguo Yang, and\\nMehmet Zahit Alparslan. 2021. Cheating Detection Pipeline for Online Interviews.\\nIn *Proceedings of the 29th Signal Processing and Communications Applications Con-*\\n*ference* . IEEE, Istanbul, TR, 1–4. https://doi.org/10.1109/SIU53274.2021.9477950\\n\\n[40] Kim Parker. 2023. About a third of U.S. workers who can work from home now do\\nso all the time. https://www.pewresearch.org/short-reads/2023/03/30/about-athird-of-us-workers-who-can-work-from-home-do-so-all-the-time Accessed:\\n\\n2024-02-07.\\n\\n[41] Leandro Persona, Fernando Meloni, and Alessandra Alaniz Macedo. 2023. An\\naccurate real-time method to detect the smile facial expression. In *Proceedings of*\\n*the 29th Brazilian Symposium on Multimedia and the Web* (Ribeirão Preto, Brazil)\\n*(WebMedia ’23)* . Association for Computing Machinery, New York, USA, 46–55.\\nhttps://doi.org/10.1145/3617023.3617031\\n\\n[42] Michael I Posner. 1980. Orienting of attention. *Quarterly journal of experimental*\\n*psychology* 32, 1 (1980), 3–25. https://doi.org/10.1080/00335558008248231\\n\\n[43] Tejaswi Potluri, Venkatarama Phani Kumar, and K Venkata Krishna Kishore. 2023.\\nAn automated online proctoring system using attentive-net to assess student\\nmischievous behavior. *Multimedia Tools and Applications* 82, 20 (Aug. 2023),\\n30375–30404. https://doi.org/10.1007/s11042-023-14604-w\\n\\n[44] Tejaswi Potluri, Venkatarama Phani Kumar Sistla, and Drkv Krishna. 2023. Detecting autism of examinee in automated online proctoring using eye-tracking.\\n*Journal of Theoretical and Applied Information Technology* 101, 3 (2023).\\n\\n[45] Swathi Prathish, Athi Narayanan S., and Kamal Bijlani. 2016. An intelligent system for online exam monitoring. In *Proceedings of the International Conference on*\\n*Information Science* . IEEE, Kochi, IN, 138–143. https://doi.org/10.1109/INFOSCI.\\n2016.7845315\\n\\n[46] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,\\nZhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C.\\nBerg, and Li Fei-Fei. 2015. ImageNet Large Scale Visual Recognition Challenge.\\n*International Journal of Computer Vision* 115, 3 (2015), 211–252. https://doi.org/\\n10.1007/s11263-015-0816-y\\n\\n[47] Andrew Sanders, Bradley Boswell, Andrew Allen, Gursimran Singh Walia, and\\nMd Shakil Hossain. 2022. Development and Field-Testing of a Non-intrusive\\nClassroom Attention Tracking System (NiCATS) for Tracking Student Attention\\nin CS Classrooms. In *Proceedings of the IEEE Frontiers in Education Conference* .\\nIEEE, Uppsala, SE, 1–9. https://doi.org/10.1109/FIE56618.2022.9962447\\n\\n[48] Andrew Sanders, Bradley Boswell, Gursimran Singh Walia, and Andrew Allen.\\n2021. Non-Intrusive Classroom Attention Tracking System (NiCATS). In *Pro-*\\n*ceedings of the IEEE Frontiers in Education Conference* . IEEE, Lincoln, US, 1–9.\\nhttps://doi.org/10.1109/FIE49875.2021.9637411\\n\\n[49] Steven Seiden, Long Huang, and Chen Wang. 2023. Poster: Snooping Online\\nForm Choice Privacy in Video Calls. In *Proceedings of the IEEE Symposium on*\\n*Security and Privacy* . IEEE, San Francisco, US, 1–2.\\n\\n[50] Abdelrahman Shata, Zineddine N. Haitaamar, and Abdsamad Benkrid. 2023.\\nBaseline Estimation in Face Detection for AI Proctored Examinations through\\n\\n\\nConvoluted Neural Networks. In *Proceedings of the International Conference*\\n*on IT Innovation and Knowledge Discovery* . IEEE, Manama, BH, 1–5. https:\\n//doi.org/10.1109/ITIKD56332.2023.10100328\\n\\n[51] Merzoug Soltane and Mohamed Ridda Laouar. 2021. A Smart System to Detect\\nCheating in the Online Exam. In *Proceedings of the International Conference on*\\n*Information Systems and Advanced Technologies* . IEEE, Tebessa, DZ, 1–5. https:\\n//doi.org/10.1109/ICISAT54145.2021.9678418\\n\\n[52] Robert J Sternberg. 2008. *Psicologia cognitiva* (4 [a] ed.). artmed, Rio de Janeiro, BR.\\n\\n[53] P. Tejaswi, S. Venkatramaphanikumar, and K. Venkata Krishna Kishore. 2023.\\nProctor net: An AI framework for suspicious activity detection in online proctored\\nexaminations. *Measurement* 206 (Jan. 2023), 112266. https://doi.org/10.1016/j.\\nmeasurement.2022.112266\\n\\n[54] Shogo Terai, Shizuka Shirai, Mehrasa Alizadeh, Ryosuke Kawamura, Noriko\\nTakemura, Yuki Uranishi, Haruo Takemura, and Hajime Nagahara. 2020. Detecting Learner Drowsiness Based on Facial Expressions and Head Movements in\\nOnline Courses. In *Proceedings of the 25th International Conference on Intelligent*\\n*User Interfaces Companion* . ACM, Cagliari, IT, 124–125. https://doi.org/10.1145/\\n3379336.3381500\\n\\n[55] Jian-Wei Tzeng, Cheng-Yu Hsueh, Chia-An Lee, and Wei-Yun Shih. 2023. Identifying the Correlation Between Online Exam Answer Trajectory and Test Behavior\\nBased on Artificial Intelligence and Eye Movement Detection Technology. In *Pro-*\\n*ceedings of the International Conference on Consumer Electronics* . IEEE, PingTung,\\nTW, 503–504. https://doi.org/10.1109/ICCE-Taiwan58799.2023.10226745\\n\\n[56] Puru Verma, Neil Malhotra, Ram Suri, and Rajesh Kumar. 2024. Automated\\nsmart artificial intelligence-based proctoring system using deep learning. *Soft*\\n*Computing* 28, 4 (Feb. 2024), 3479–3489. https://doi.org/10.1007/s00500-02308696-7\\n\\n[57] Chenghao Wang et al . 2022. Comprehensively summarizing what distracts\\nstudents from online learning: A literature review. *Human Behavior and Emerging*\\n*Technologies* 2022, 1 (2022), 1483531. https://doi.org/10.1155/2022/1483531\\n\\n[58] Jacob Whitehill, Zewelanji Serpell, Yi-Ching Lin, Aysha Foster, and Javier R.\\nMovellan. 2014. The Faces of Engagement: Automatic Recognition of Student\\nEngagementfrom Facial Expressions. *IEEE Transactions on Affective Computing*\\n5, 1 (Jan. 2014), 86–98. https://doi.org/10.1109/TAFFC.2014.2316163\\n\\n[59] Claes Wohlin. 2014. Guidelines for Snowballing in Systematic Literature Studies\\nand a Replication in Software Engineering. In *Proceedings of the 18th Interna-*\\n*tional Conference on Evaluation and Assessment in Software Engineering* (London,\\nEngland, United Kingdom) *(EASE ’14)* . Association for Computing Machinery,\\nNew York, US, 10 pages. https://doi.org/10.1145/2601248.2601268\\n\\n[60] Claes Wohlin, Marcos Kalinowski, Katia Romero Felizardo, and Emilia Mendes.\\n2022. Successful combination of database search and snowballing for identification of primary studies in systematic literature studies. *Information and Software*\\n*Technology* 147 (2022), 106908. https://doi.org/10.1016/j.infsof.2022.106908\\n\\n[61] Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. 2014. Learning face representation from scratch. *arXiv preprint* 1411.7923 (2014), 9 pages. https:\\n//doi.org/10.48550/arXiv.1411.7923\\n\\n[62] Intan Nurma Yulita, Fauzan Akmal Hariz, Ino Suryana, and Anton Satria Prabuwono. 2023. Educational Innovation Faced with COVID-19: Deep Learning\\nfor Online Exam Cheating Detection. *Education Sciences* 13, 2 (Feb. 2023), 194.\\nhttps://doi.org/10.3390/educsci13020194\\n\\n[63] Cheng Zhang, Cheng Chang, Lei Chen, and Yang Liu. 2018. Online PrivacySafe Engagement Tracking System. In *Proceedings of the 20th ACM International*\\n*Conference on Multimodal Interaction* . ACM, Boulder, US, 553–554. https://doi.\\norg/10.1145/3242969.3266295\\n\\n[64] Xiangyu Zhu, Zhen Lei, Xiaoming Liu, Hailin Shi, and Stan Z Li. 2016. Face\\nalignment across large poses: A 3D solution. In *Proceedings of the IEEE Conference*\\n*on Computer Vision and Pattern Recognition* . IEEE, Las Vegas, US, 146–155. https:\\n//doi.org/10.1109/CVPR.2016.23\\n\\n\\n205\\n\\n\\n-----\\n\\n',\n",
       "  'artigo_tokenizado': ['#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Investigating',\n",
       "   'User',\n",
       "   '’s',\n",
       "   'Attentional',\n",
       "   'Focus',\n",
       "   'in',\n",
       "   'Computational',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Environments',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'A',\n",
       "   'Literature',\n",
       "   'Review',\n",
       "   'with',\n",
       "   'Emphasis',\n",
       "   'on',\n",
       "   'Webcam',\n",
       "   'Data',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Cassiano',\n",
       "   'da',\n",
       "   'Silva',\n",
       "   'Souza',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'cassiano.souza@edu.pucrs.br',\n",
       "   'Pontifical',\n",
       "   'Catholic',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Rio',\n",
       "   'Grande',\n",
       "   'do',\n",
       "   'Sul',\n",
       "   'and',\n",
       "   'Federal',\n",
       "   'Institute',\n",
       "   'Sul',\n",
       "   '-',\n",
       "   'rio',\n",
       "   '-',\n",
       "   'grandense',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Milene',\n",
       "   'Selbach',\n",
       "   'Silveira',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'milene.silveira@pucrs.br',\n",
       "   'Pontifical',\n",
       "   'Catholic',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Rio',\n",
       "   'Grande',\n",
       "   'do',\n",
       "   'Sul',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Isabel',\n",
       "   'Harb',\n",
       "   'Manssour',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'isabel.manssour@pucrs.br',\n",
       "   'Pontifical',\n",
       "   'Catholic',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Rio',\n",
       "   'Grande',\n",
       "   'do',\n",
       "   'Sul',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '\\n\\n\\n',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Figure',\n",
       "   '1',\n",
       "   ':*',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   'AttentionVis',\n",
       "   'Browser',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   ':',\n",
       "   'a',\n",
       "   'web',\n",
       "   '-',\n",
       "   'based',\n",
       "   'user',\n",
       "   'interface',\n",
       "   'of',\n",
       "   'our',\n",
       "   'visual',\n",
       "   'survey',\n",
       "   ',',\n",
       "   'which',\n",
       "   'is',\n",
       "   'composed',\n",
       "   'of',\n",
       "   '(',\n",
       "   'A',\n",
       "   ')',\n",
       "   'the',\n",
       "   'interaction',\n",
       "   'panel',\n",
       "   ',',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n',\n",
       "   '*',\n",
       "   '*',\n",
       "   'including',\n",
       "   'the',\n",
       "   'search',\n",
       "   'field',\n",
       "   'and',\n",
       "   'filters',\n",
       "   'by',\n",
       "   'category',\n",
       "   ',',\n",
       "   'and',\n",
       "   '(',\n",
       "   'B',\n",
       "   ')',\n",
       "   'the',\n",
       "   'main',\n",
       "   'panel',\n",
       "   '-',\n",
       "   'thumbnails',\n",
       "   'representing',\n",
       "   'each',\n",
       "   'paper',\n",
       "   '.',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'ABSTRACT',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Maintaining',\n",
       "   'the',\n",
       "   'user',\n",
       "   '’s',\n",
       "   'attentional',\n",
       "   'focus',\n",
       "   'has',\n",
       "   'become',\n",
       "   'a',\n",
       "   'recurring',\n",
       "   '\\n',\n",
       "   'concern',\n",
       "   'in',\n",
       "   'recent',\n",
       "   'years',\n",
       "   '.',\n",
       "   'This',\n",
       "   'is',\n",
       "   'due',\n",
       "   'to',\n",
       "   'the',\n",
       "   'consolidation',\n",
       "   'of',\n",
       "   'remote',\n",
       "   '\\n',\n",
       "   'and',\n",
       "   'hybrid',\n",
       "   'models',\n",
       "   'for',\n",
       "   'study',\n",
       "   'and',\n",
       "   'work',\n",
       "   ',',\n",
       "   'which',\n",
       "   'were',\n",
       "   'widely',\n",
       "   'experienced',\n",
       "   'during',\n",
       "   'the',\n",
       "   'social',\n",
       "   'distancing',\n",
       "   'caused',\n",
       "   'by',\n",
       "   'COVID-19',\n",
       "   '.',\n",
       "   'This',\n",
       "   'paper',\n",
       "   '\\n',\n",
       "   'presents',\n",
       "   'a',\n",
       "   'review',\n",
       "   'of',\n",
       "   'works',\n",
       "   'that',\n",
       "   'address',\n",
       "   'this',\n",
       "   'problem',\n",
       "   'by',\n",
       "   'analyzing',\n",
       "   'webcam',\n",
       "   'data',\n",
       "   ',',\n",
       "   'a',\n",
       "   'promising',\n",
       "   'device',\n",
       "   'for',\n",
       "   'behavioral',\n",
       "   'studies',\n",
       "   '.',\n",
       "   'The',\n",
       "   '\\n',\n",
       "   'literature',\n",
       "   'review',\n",
       "   'from',\n",
       "   '2013',\n",
       "   'to',\n",
       "   '2023',\n",
       "   'was',\n",
       "   'carried',\n",
       "   'out',\n",
       "   'using',\n",
       "   'a',\n",
       "   'hybrid',\n",
       "   '\\n',\n",
       "   'search',\n",
       "   'strategy',\n",
       "   ',',\n",
       "   'through',\n",
       "   'which',\n",
       "   'we',\n",
       "   'selected',\n",
       "   'and',\n",
       "   'analyzed',\n",
       "   '57',\n",
       "   'papers',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'The',\n",
       "   'summary',\n",
       "   'of',\n",
       "   'this',\n",
       "   'study',\n",
       "   'is',\n",
       "   'presented',\n",
       "   'in',\n",
       "   'an',\n",
       "   'interactive',\n",
       "   'visual',\n",
       "   '\\n',\n",
       "   'survey',\n",
       "   'format',\n",
       "   'called',\n",
       "   'the',\n",
       "   '*',\n",
       "   'AttentionVis',\n",
       "   'Browser',\n",
       "   '*',\n",
       "   'tool',\n",
       "   '.',\n",
       "   'As',\n",
       "   'additional',\n",
       "   '\\n',\n",
       "   'contributions',\n",
       "   ',',\n",
       "   'we',\n",
       "   'provide',\n",
       "   'a',\n",
       "   'list',\n",
       "   'of',\n",
       "   'lessons',\n",
       "   'learned',\n",
       "   ',',\n",
       "   'a',\n",
       "   'list',\n",
       "   'of',\n",
       "   'work',\n",
       "   '\\n',\n",
       "   'limitations',\n",
       "   ',',\n",
       "   'and',\n",
       "   'possibilities',\n",
       "   'for',\n",
       "   'future',\n",
       "   'research',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'KEYWORDS',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'attention',\n",
       "   'monitoring',\n",
       "   ',',\n",
       "   'webcam',\n",
       "   ',',\n",
       "   'data',\n",
       "   'analysis',\n",
       "   ',',\n",
       "   'literature',\n",
       "   'review',\n",
       "   '.',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   ':',\n",
       "   'Proceedings',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Web',\n",
       "   '(',\n",
       "   'WebMe',\n",
       "   '\\n',\n",
       "   'dia’2024',\n",
       "   ')',\n",
       "   '.',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '.',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   ':',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   ',',\n",
       "   '2024',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'SBC',\n",
       "   '–',\n",
       "   'Sociedade',\n",
       "   'Brasileira',\n",
       "   'de',\n",
       "   'Computação',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '-',\n",
       "   '2753',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   '1',\n",
       "   'INTRODUCTION',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'The',\n",
       "   'advancement',\n",
       "   'of',\n",
       "   'information',\n",
       "   'and',\n",
       "   'communication',\n",
       "   'technologies',\n",
       "   '\\n',\n",
       "   '(',\n",
       "   'ICT',\n",
       "   ')',\n",
       "   'has',\n",
       "   'significantly',\n",
       "   'impacted',\n",
       "   'our',\n",
       "   'daily',\n",
       "   'lives',\n",
       "   '.',\n",
       "   'According',\n",
       "   'to',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'Internet',\n",
       "   'Steering',\n",
       "   'Committee',\n",
       "   'in',\n",
       "   'Brazil',\n",
       "   '[',\n",
       "   '10',\n",
       "   ']',\n",
       "   ',',\n",
       "   'with',\n",
       "   'the',\n",
       "   'advent',\n",
       "   'of',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'COVID-19',\n",
       "   'pandemic',\n",
       "   'and',\n",
       "   'given',\n",
       "   'the',\n",
       "   'barriers',\n",
       "   'imposed',\n",
       "   'by',\n",
       "   'social',\n",
       "   'isolation',\n",
       "   'to',\n",
       "   'contain',\n",
       "   'the',\n",
       "   'spread',\n",
       "   'of',\n",
       "   'the',\n",
       "   'virus',\n",
       "   ',',\n",
       "   'the',\n",
       "   'demands',\n",
       "   'for',\n",
       "   'such',\n",
       "   '\\n',\n",
       "   'resources',\n",
       "   'proved',\n",
       "   'to',\n",
       "   'be',\n",
       "   'fundamental',\n",
       "   'for',\n",
       "   'the',\n",
       "   'continuity',\n",
       "   'of',\n",
       "   'work',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'education',\n",
       "   ',',\n",
       "   'and',\n",
       "   'social',\n",
       "   'interactions',\n",
       "   '.',\n",
       "   'In',\n",
       "   'this',\n",
       "   'context',\n",
       "   ',',\n",
       "   'remote',\n",
       "   'and',\n",
       "   'hybrid',\n",
       "   'models',\n",
       "   'stand',\n",
       "   'out',\n",
       "   ',',\n",
       "   'from',\n",
       "   'temporary',\n",
       "   'solutions',\n",
       "   'to',\n",
       "   'consolidated',\n",
       "   '\\n',\n",
       "   'practices',\n",
       "   'in',\n",
       "   'the',\n",
       "   'post',\n",
       "   '-',\n",
       "   'pandemic',\n",
       "   'scenario',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Recent',\n",
       "   'surveys',\n",
       "   'on',\n",
       "   'the',\n",
       "   'work',\n",
       "   'environment',\n",
       "   'show',\n",
       "   'a',\n",
       "   'growing',\n",
       "   'adoption',\n",
       "   'of',\n",
       "   'remote',\n",
       "   'work',\n",
       "   ',',\n",
       "   'with',\n",
       "   'significantly',\n",
       "   'higher',\n",
       "   'numbers',\n",
       "   'than',\n",
       "   'before',\n",
       "   'the',\n",
       "   'pandemic',\n",
       "   '[',\n",
       "   '40',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Although',\n",
       "   'services',\n",
       "   'provided',\n",
       "   'on',\n",
       "   'company',\n",
       "   '\\n',\n",
       "   'premises',\n",
       "   'still',\n",
       "   'predominate',\n",
       "   '(',\n",
       "   '66.5',\n",
       "   '%',\n",
       "   ')',\n",
       "   ',',\n",
       "   'hybrid',\n",
       "   'and',\n",
       "   'remote',\n",
       "   'models',\n",
       "   'already',\n",
       "   'represent',\n",
       "   '33.5',\n",
       "   '%',\n",
       "   'of',\n",
       "   'activities',\n",
       "   '[',\n",
       "   '2',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Barrero',\n",
       "   'et',\n",
       "   'al',\n",
       "   '.',\n",
       "   '[',\n",
       "   '7',\n",
       "   ']',\n",
       "   'suggest',\n",
       "   'this',\n",
       "   '\\n',\n",
       "   'is',\n",
       "   'a',\n",
       "   'trend',\n",
       "   ',',\n",
       "   'with',\n",
       "   'the',\n",
       "   'prediction',\n",
       "   'that',\n",
       "   'at',\n",
       "   'least',\n",
       "   'one',\n",
       "   'working',\n",
       "   'day',\n",
       "   'per',\n",
       "   '\\n',\n",
       "   'week',\n",
       "   'will',\n",
       "   'be',\n",
       "   'conducted',\n",
       "   'remotely',\n",
       "   'in',\n",
       "   'the',\n",
       "   'coming',\n",
       "   'years',\n",
       "   '.',\n",
       "   'Likewise',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'educational',\n",
       "   'institutions',\n",
       "   'are',\n",
       "   'also',\n",
       "   'adapting',\n",
       "   'to',\n",
       "   'new',\n",
       "   'circumstances',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'The',\n",
       "   'growing',\n",
       "   'acceptance',\n",
       "   'of',\n",
       "   'distance',\n",
       "   'learning',\n",
       "   '(',\n",
       "   'DL',\n",
       "   ')',\n",
       "   'is',\n",
       "   'evidenced',\n",
       "   '\\n',\n",
       "   'by',\n",
       "   'the',\n",
       "   '166.4',\n",
       "   '%',\n",
       "   'increase',\n",
       "   'in',\n",
       "   'course',\n",
       "   'enrollments',\n",
       "   'in',\n",
       "   'these',\n",
       "   'modalities',\n",
       "   '\\n',\n",
       "   'between',\n",
       "   '2015',\n",
       "   'and',\n",
       "   '2021',\n",
       "   ',',\n",
       "   'according',\n",
       "   'to',\n",
       "   'the',\n",
       "   'Higher',\n",
       "   'Education',\n",
       "   'Map',\n",
       "   'in',\n",
       "   '\\n',\n",
       "   'Brazil',\n",
       "   '[',\n",
       "   '19',\n",
       "   ']',\n",
       "   '.',\n",
       "   'On',\n",
       "   'the',\n",
       "   'other',\n",
       "   'hand',\n",
       "   ',',\n",
       "   'enrollment',\n",
       "   'in',\n",
       "   'face',\n",
       "   '-',\n",
       "   'to',\n",
       "   '-',\n",
       "   'face',\n",
       "   'courses',\n",
       "   '\\n\\n\\n',\n",
       "   '197',\n",
       "   '\\n\\n\\n',\n",
       "   '-----',\n",
       "   '\\n\\n',\n",
       "   'WebMedia’2024',\n",
       "   ',',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   'Souza',\n",
       "   'et',\n",
       "   'al',\n",
       "   '.',\n",
       "   '\\n\\n\\n',\n",
       "   'decreased',\n",
       "   'by',\n",
       "   '20.6',\n",
       "   '%',\n",
       "   ',',\n",
       "   'indicating',\n",
       "   'a',\n",
       "   'significant',\n",
       "   'change',\n",
       "   'in',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   '\\n',\n",
       "   'educational',\n",
       "   'landscape',\n",
       "   'with',\n",
       "   'an',\n",
       "   'increasing',\n",
       "   'focus',\n",
       "   'on',\n",
       "   'DL',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'However',\n",
       "   ',',\n",
       "   'these',\n",
       "   'models',\n",
       "   'accompany',\n",
       "   'a',\n",
       "   'recurring',\n",
       "   'challenge',\n",
       "   'that',\n",
       "   '\\n',\n",
       "   'negatively',\n",
       "   'impacts',\n",
       "   'both',\n",
       "   'school',\n",
       "   '[',\n",
       "   '12',\n",
       "   ',',\n",
       "   '57',\n",
       "   ']',\n",
       "   'and',\n",
       "   'work',\n",
       "   'performance',\n",
       "   '[',\n",
       "   '6',\n",
       "   ']',\n",
       "   ':',\n",
       "   '\\n',\n",
       "   'maintaining',\n",
       "   'attentional',\n",
       "   'focus',\n",
       "   '.',\n",
       "   'Nakayama',\n",
       "   'et',\n",
       "   'al',\n",
       "   '.',\n",
       "   '[',\n",
       "   '37',\n",
       "   ']',\n",
       "   'and',\n",
       "   'Wang',\n",
       "   'et',\n",
       "   '\\n',\n",
       "   'al',\n",
       "   '.',\n",
       "   '[',\n",
       "   '57',\n",
       "   ']',\n",
       "   'point',\n",
       "   'out',\n",
       "   'digital',\n",
       "   'distractions',\n",
       "   '-',\n",
       "   'such',\n",
       "   'as',\n",
       "   'messages',\n",
       "   ',',\n",
       "   'notifications',\n",
       "   ',',\n",
       "   'and',\n",
       "   'social',\n",
       "   'networks',\n",
       "   ',',\n",
       "   'among',\n",
       "   'others',\n",
       "   '-',\n",
       "   'along',\n",
       "   'with',\n",
       "   'the',\n",
       "   'multitasking',\n",
       "   'environment',\n",
       "   ',',\n",
       "   'as',\n",
       "   'the',\n",
       "   'main',\n",
       "   'factors',\n",
       "   'that',\n",
       "   'harm',\n",
       "   'our',\n",
       "   'ability',\n",
       "   'to',\n",
       "   '\\n',\n",
       "   'maintain',\n",
       "   'focus',\n",
       "   'on',\n",
       "   'essential',\n",
       "   'tasks',\n",
       "   '.',\n",
       "   'This',\n",
       "   'occurs',\n",
       "   'because',\n",
       "   'our',\n",
       "   'brain',\n",
       "   '\\n\\n',\n",
       "   'can',\n",
       "   'not',\n",
       "   'process',\n",
       "   'many',\n",
       "   'perceptual',\n",
       "   'stimuli',\n",
       "   'simultaneously',\n",
       "   ',',\n",
       "   'depending',\n",
       "   '\\n',\n",
       "   'on',\n",
       "   'the',\n",
       "   'complex',\n",
       "   'cognitive',\n",
       "   'process',\n",
       "   'of',\n",
       "   '“',\n",
       "   'attention',\n",
       "   '”',\n",
       "   'to',\n",
       "   'select',\n",
       "   'relevant',\n",
       "   '\\n',\n",
       "   'information',\n",
       "   'and',\n",
       "   'discard',\n",
       "   'irrelevant',\n",
       "   'information',\n",
       "   '[',\n",
       "   '34',\n",
       "   ',',\n",
       "   '52',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'With',\n",
       "   'the',\n",
       "   'growing',\n",
       "   'demand',\n",
       "   'for',\n",
       "   'remote',\n",
       "   'and',\n",
       "   'hybrid',\n",
       "   'activities',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'seeking',\n",
       "   'effective',\n",
       "   'strategies',\n",
       "   'to',\n",
       "   'mitigate',\n",
       "   'distractions',\n",
       "   'and',\n",
       "   'promote',\n",
       "   '\\n',\n",
       "   'greater',\n",
       "   'concentration',\n",
       "   'becomes',\n",
       "   'increasingly',\n",
       "   'important',\n",
       "   '.',\n",
       "   'One',\n",
       "   'approach',\n",
       "   'adopted',\n",
       "   'is',\n",
       "   'attentional',\n",
       "   'analysis',\n",
       "   'based',\n",
       "   'on',\n",
       "   'eye',\n",
       "   'tracking',\n",
       "   'data',\n",
       "   '\\n',\n",
       "   'because',\n",
       "   ',',\n",
       "   'according',\n",
       "   'to',\n",
       "   'Posner',\n",
       "   '[',\n",
       "   '42',\n",
       "   ']',\n",
       "   ',',\n",
       "   'there',\n",
       "   'is',\n",
       "   'a',\n",
       "   'direct',\n",
       "   'relationship',\n",
       "   'between',\n",
       "   'eye',\n",
       "   'movement',\n",
       "   'and',\n",
       "   'changes',\n",
       "   'in',\n",
       "   'attention',\n",
       "   '.',\n",
       "   'Thus',\n",
       "   ',',\n",
       "   'this',\n",
       "   'research',\n",
       "   '\\n',\n",
       "   'aims',\n",
       "   'to',\n",
       "   '*',\n",
       "   '*',\n",
       "   'synthesize',\n",
       "   'and',\n",
       "   'organize',\n",
       "   'existing',\n",
       "   'knowledge',\n",
       "   'about',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n',\n",
       "   '*',\n",
       "   '*',\n",
       "   'attentional',\n",
       "   'analysis',\n",
       "   'conducted',\n",
       "   'based',\n",
       "   'on',\n",
       "   'data',\n",
       "   'obtained',\n",
       "   'via',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n',\n",
       "   '*',\n",
       "   '*',\n",
       "   'webcam',\n",
       "   '*',\n",
       "   '*',\n",
       "   '-',\n",
       "   'a',\n",
       "   'low',\n",
       "   '-',\n",
       "   'cost',\n",
       "   'eye',\n",
       "   'movement',\n",
       "   'capture',\n",
       "   'device',\n",
       "   ',',\n",
       "   'with',\n",
       "   'possibilities',\n",
       "   'for',\n",
       "   'large',\n",
       "   '-',\n",
       "   'scale',\n",
       "   'study',\n",
       "   '[',\n",
       "   '9',\n",
       "   ']',\n",
       "   '.',\n",
       "   'To',\n",
       "   'do',\n",
       "   'this',\n",
       "   ',',\n",
       "   'we',\n",
       "   'carried',\n",
       "   'out',\n",
       "   'a',\n",
       "   'literature',\n",
       "   '\\n',\n",
       "   'review',\n",
       "   'from',\n",
       "   '2013',\n",
       "   'to',\n",
       "   '2023',\n",
       "   ',',\n",
       "   'using',\n",
       "   'a',\n",
       "   'hybrid',\n",
       "   'search',\n",
       "   'strategy',\n",
       "   ',',\n",
       "   'through',\n",
       "   '\\n',\n",
       "   'which',\n",
       "   'we',\n",
       "   'selected',\n",
       "   '57',\n",
       "   'papers',\n",
       "   '.',\n",
       "   'These',\n",
       "   'papers',\n",
       "   'were',\n",
       "   'analyzed',\n",
       "   'and',\n",
       "   'categorized',\n",
       "   ',',\n",
       "   'offering',\n",
       "   'a',\n",
       "   'comprehensive',\n",
       "   ...],\n",
       "  'pos_tagger': '',\n",
       "  'lema': ['investigate',\n",
       "   'user',\n",
       "   '’s',\n",
       "   'Attentional',\n",
       "   'Focus',\n",
       "   'in',\n",
       "   'Computational',\n",
       "   'environment',\n",
       "   'A',\n",
       "   'Literature',\n",
       "   'Review',\n",
       "   'with',\n",
       "   'Emphasis',\n",
       "   'on',\n",
       "   'Webcam',\n",
       "   'Data',\n",
       "   'Cassiano',\n",
       "   'da',\n",
       "   'Silva',\n",
       "   'Souza',\n",
       "   'cassiano.souza@edu.pucrs.br',\n",
       "   'Pontifical',\n",
       "   'Catholic',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Rio',\n",
       "   'Grande',\n",
       "   'do',\n",
       "   'Sul',\n",
       "   'and',\n",
       "   'Federal',\n",
       "   'Institute',\n",
       "   'Sul',\n",
       "   'rio',\n",
       "   'grandense',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   'Brazil',\n",
       "   'Milene',\n",
       "   'Selbach',\n",
       "   'Silveira',\n",
       "   'milene.silveira@pucrs.br',\n",
       "   'Pontifical',\n",
       "   'Catholic',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Rio',\n",
       "   'Grande',\n",
       "   'do',\n",
       "   'Sul',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   'Brazil',\n",
       "   'Isabel',\n",
       "   'Harb',\n",
       "   'Manssour',\n",
       "   'isabel.manssour@pucrs.br',\n",
       "   'Pontifical',\n",
       "   'Catholic',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Rio',\n",
       "   'Grande',\n",
       "   'do',\n",
       "   'Sul',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   'Brazil',\n",
       "   'figure',\n",
       "   '1',\n",
       "   'AttentionVis',\n",
       "   'Browser',\n",
       "   'a',\n",
       "   'web',\n",
       "   'base',\n",
       "   'user',\n",
       "   'interface',\n",
       "   'of',\n",
       "   'our',\n",
       "   'visual',\n",
       "   'survey',\n",
       "   'which',\n",
       "   'be',\n",
       "   'compose',\n",
       "   'of',\n",
       "   'a',\n",
       "   'the',\n",
       "   'interaction',\n",
       "   'panel',\n",
       "   'include',\n",
       "   'the',\n",
       "   'search',\n",
       "   'field',\n",
       "   'and',\n",
       "   'filter',\n",
       "   'by',\n",
       "   'category',\n",
       "   'and',\n",
       "   'B',\n",
       "   'the',\n",
       "   'main',\n",
       "   'panel',\n",
       "   'thumbnail',\n",
       "   'represent',\n",
       "   'each',\n",
       "   'paper',\n",
       "   'ABSTRACT',\n",
       "   'maintain',\n",
       "   'the',\n",
       "   'user',\n",
       "   '’s',\n",
       "   'attentional',\n",
       "   'focus',\n",
       "   'have',\n",
       "   'become',\n",
       "   'a',\n",
       "   'recur',\n",
       "   'concern',\n",
       "   'in',\n",
       "   'recent',\n",
       "   'year',\n",
       "   'this',\n",
       "   'be',\n",
       "   'due',\n",
       "   'to',\n",
       "   'the',\n",
       "   'consolidation',\n",
       "   'of',\n",
       "   'remote',\n",
       "   'and',\n",
       "   'hybrid',\n",
       "   'model',\n",
       "   'for',\n",
       "   'study',\n",
       "   'and',\n",
       "   'work',\n",
       "   'which',\n",
       "   'be',\n",
       "   'widely',\n",
       "   'experience',\n",
       "   'during',\n",
       "   'the',\n",
       "   'social',\n",
       "   'distancing',\n",
       "   'cause',\n",
       "   'by',\n",
       "   'covid-19',\n",
       "   'this',\n",
       "   'paper',\n",
       "   'present',\n",
       "   'a',\n",
       "   'review',\n",
       "   'of',\n",
       "   'work',\n",
       "   'that',\n",
       "   'address',\n",
       "   'this',\n",
       "   'problem',\n",
       "   'by',\n",
       "   'analyze',\n",
       "   'webcam',\n",
       "   'datum',\n",
       "   'a',\n",
       "   'promising',\n",
       "   'device',\n",
       "   'for',\n",
       "   'behavioral',\n",
       "   'study',\n",
       "   'the',\n",
       "   'literature',\n",
       "   'review',\n",
       "   'from',\n",
       "   '2013',\n",
       "   'to',\n",
       "   '2023',\n",
       "   'be',\n",
       "   'carry',\n",
       "   'out',\n",
       "   'use',\n",
       "   'a',\n",
       "   'hybrid',\n",
       "   'search',\n",
       "   'strategy',\n",
       "   'through',\n",
       "   'which',\n",
       "   'we',\n",
       "   'select',\n",
       "   'and',\n",
       "   'analyze',\n",
       "   '57',\n",
       "   'paper',\n",
       "   'the',\n",
       "   'summary',\n",
       "   'of',\n",
       "   'this',\n",
       "   'study',\n",
       "   'be',\n",
       "   'present',\n",
       "   'in',\n",
       "   'an',\n",
       "   'interactive',\n",
       "   'visual',\n",
       "   'survey',\n",
       "   'format',\n",
       "   'call',\n",
       "   'the',\n",
       "   'AttentionVis',\n",
       "   'Browser',\n",
       "   'tool',\n",
       "   'as',\n",
       "   'additional',\n",
       "   'contribution',\n",
       "   'we',\n",
       "   'provide',\n",
       "   'a',\n",
       "   'list',\n",
       "   'of',\n",
       "   'lesson',\n",
       "   'learn',\n",
       "   'a',\n",
       "   'list',\n",
       "   'of',\n",
       "   'work',\n",
       "   'limitation',\n",
       "   'and',\n",
       "   'possibility',\n",
       "   'for',\n",
       "   'future',\n",
       "   'research',\n",
       "   'keyword',\n",
       "   'attention',\n",
       "   'monitoring',\n",
       "   'webcam',\n",
       "   'datum',\n",
       "   'analysis',\n",
       "   'literature',\n",
       "   'review',\n",
       "   'in',\n",
       "   'proceeding',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'web',\n",
       "   'WebMe',\n",
       "   'dia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   '2024',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'sbc',\n",
       "   'Sociedade',\n",
       "   'Brasileira',\n",
       "   'de',\n",
       "   'Computação',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '2753',\n",
       "   '1',\n",
       "   'introduction',\n",
       "   'the',\n",
       "   'advancement',\n",
       "   'of',\n",
       "   'information',\n",
       "   'and',\n",
       "   'communication',\n",
       "   'technology',\n",
       "   'ICT',\n",
       "   'have',\n",
       "   'significantly',\n",
       "   'impact',\n",
       "   'our',\n",
       "   'daily',\n",
       "   'life',\n",
       "   'accord',\n",
       "   'to',\n",
       "   'the',\n",
       "   'Internet',\n",
       "   'Steering',\n",
       "   'Committee',\n",
       "   'in',\n",
       "   'Brazil',\n",
       "   '10',\n",
       "   'with',\n",
       "   'the',\n",
       "   'advent',\n",
       "   'of',\n",
       "   'the',\n",
       "   'covid-19',\n",
       "   'pandemic',\n",
       "   'and',\n",
       "   'give',\n",
       "   'the',\n",
       "   'barrier',\n",
       "   'impose',\n",
       "   'by',\n",
       "   'social',\n",
       "   'isolation',\n",
       "   'to',\n",
       "   'contain',\n",
       "   'the',\n",
       "   'spread',\n",
       "   'of',\n",
       "   'the',\n",
       "   'virus',\n",
       "   'the',\n",
       "   'demand',\n",
       "   'for',\n",
       "   'such',\n",
       "   'resource',\n",
       "   'prove',\n",
       "   'to',\n",
       "   'be',\n",
       "   'fundamental',\n",
       "   'for',\n",
       "   'the',\n",
       "   'continuity',\n",
       "   'of',\n",
       "   'work',\n",
       "   'education',\n",
       "   'and',\n",
       "   'social',\n",
       "   'interaction',\n",
       "   'in',\n",
       "   'this',\n",
       "   'context',\n",
       "   'remote',\n",
       "   'and',\n",
       "   'hybrid',\n",
       "   'model',\n",
       "   'stand',\n",
       "   'out',\n",
       "   'from',\n",
       "   'temporary',\n",
       "   'solution',\n",
       "   'to',\n",
       "   'consolidated',\n",
       "   'practice',\n",
       "   'in',\n",
       "   'the',\n",
       "   'post',\n",
       "   'pandemic',\n",
       "   'scenario',\n",
       "   'recent',\n",
       "   'survey',\n",
       "   'on',\n",
       "   'the',\n",
       "   'work',\n",
       "   'environment',\n",
       "   'show',\n",
       "   'a',\n",
       "   'grow',\n",
       "   'adoption',\n",
       "   'of',\n",
       "   'remote',\n",
       "   'work',\n",
       "   'with',\n",
       "   'significantly',\n",
       "   'high',\n",
       "   'number',\n",
       "   'than',\n",
       "   'before',\n",
       "   'the',\n",
       "   'pandemic',\n",
       "   '40',\n",
       "   'although',\n",
       "   'service',\n",
       "   'provide',\n",
       "   'on',\n",
       "   'company',\n",
       "   'premise',\n",
       "   'still',\n",
       "   'predominate',\n",
       "   '66.5',\n",
       "   'hybrid',\n",
       "   'and',\n",
       "   'remote',\n",
       "   'model',\n",
       "   'already',\n",
       "   'represent',\n",
       "   '33.5',\n",
       "   'of',\n",
       "   'activity',\n",
       "   '2',\n",
       "   'Barrero',\n",
       "   'et',\n",
       "   'al',\n",
       "   '7',\n",
       "   'suggest',\n",
       "   'this',\n",
       "   'be',\n",
       "   'a',\n",
       "   'trend',\n",
       "   'with',\n",
       "   'the',\n",
       "   'prediction',\n",
       "   'that',\n",
       "   'at',\n",
       "   'least',\n",
       "   'one',\n",
       "   'work',\n",
       "   'day',\n",
       "   'per',\n",
       "   'week',\n",
       "   'will',\n",
       "   'be',\n",
       "   'conduct',\n",
       "   'remotely',\n",
       "   'in',\n",
       "   'the',\n",
       "   'come',\n",
       "   'year',\n",
       "   'likewise',\n",
       "   'educational',\n",
       "   'institution',\n",
       "   'be',\n",
       "   'also',\n",
       "   'adapt',\n",
       "   'to',\n",
       "   'new',\n",
       "   'circumstance',\n",
       "   'the',\n",
       "   'grow',\n",
       "   'acceptance',\n",
       "   'of',\n",
       "   'distance',\n",
       "   'learning',\n",
       "   'DL',\n",
       "   'be',\n",
       "   'evidence',\n",
       "   'by',\n",
       "   'the',\n",
       "   '166.4',\n",
       "   'increase',\n",
       "   'in',\n",
       "   'course',\n",
       "   'enrollment',\n",
       "   'in',\n",
       "   'these',\n",
       "   'modality',\n",
       "   'between',\n",
       "   '2015',\n",
       "   'and',\n",
       "   '2021',\n",
       "   'accord',\n",
       "   'to',\n",
       "   'the',\n",
       "   'Higher',\n",
       "   'Education',\n",
       "   'Map',\n",
       "   'in',\n",
       "   'Brazil',\n",
       "   '19',\n",
       "   'on',\n",
       "   'the',\n",
       "   'other',\n",
       "   'hand',\n",
       "   'enrollment',\n",
       "   'in',\n",
       "   'face',\n",
       "   'to',\n",
       "   'face',\n",
       "   'course',\n",
       "   '197',\n",
       "   'WebMedia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Souza',\n",
       "   'et',\n",
       "   'al',\n",
       "   'decrease',\n",
       "   'by',\n",
       "   '20.6',\n",
       "   'indicate',\n",
       "   'a',\n",
       "   'significant',\n",
       "   'change',\n",
       "   'in',\n",
       "   'the',\n",
       "   'brazilian',\n",
       "   'educational',\n",
       "   'landscape',\n",
       "   'with',\n",
       "   'an',\n",
       "   'increase',\n",
       "   'focus',\n",
       "   'on',\n",
       "   'DL',\n",
       "   'however',\n",
       "   'these',\n",
       "   'model',\n",
       "   'accompany',\n",
       "   'a',\n",
       "   'recur',\n",
       "   'challenge',\n",
       "   'that',\n",
       "   'negatively',\n",
       "   'impact',\n",
       "   'both',\n",
       "   'school',\n",
       "   '12',\n",
       "   '57',\n",
       "   'and',\n",
       "   'work',\n",
       "   'performance',\n",
       "   '6',\n",
       "   'maintain',\n",
       "   'attentional',\n",
       "   'focus',\n",
       "   'Nakayama',\n",
       "   'et',\n",
       "   'al',\n",
       "   '37',\n",
       "   'and',\n",
       "   'Wang',\n",
       "   'et',\n",
       "   'al',\n",
       "   '57',\n",
       "   'point',\n",
       "   'out',\n",
       "   'digital',\n",
       "   'distraction',\n",
       "   'such',\n",
       "   'as',\n",
       "   'message',\n",
       "   'notification',\n",
       "   'and',\n",
       "   'social',\n",
       "   'network',\n",
       "   'among',\n",
       "   'other',\n",
       "   'along',\n",
       "   'with',\n",
       "   'the',\n",
       "   'multitaske',\n",
       "   'environment',\n",
       "   'as',\n",
       "   'the',\n",
       "   'main',\n",
       "   'factor',\n",
       "   'that',\n",
       "   'harm',\n",
       "   'our',\n",
       "   'ability',\n",
       "   'to',\n",
       "   'maintain',\n",
       "   'focus',\n",
       "   'on',\n",
       "   'essential',\n",
       "   'task',\n",
       "   'this',\n",
       "   'occur',\n",
       "   'because',\n",
       "   'our',\n",
       "   'brain',\n",
       "   'can',\n",
       "   'not',\n",
       "   'process',\n",
       "   'many',\n",
       "   'perceptual',\n",
       "   'stimulus',\n",
       "   'simultaneously',\n",
       "   'depend',\n",
       "   'on',\n",
       "   'the',\n",
       "   'complex',\n",
       "   'cognitive',\n",
       "   'process',\n",
       "   'of',\n",
       "   'attention',\n",
       "   'to',\n",
       "   'select',\n",
       "   'relevant',\n",
       "   'information',\n",
       "   'and',\n",
       "   'discard',\n",
       "   'irrelevant',\n",
       "   'information',\n",
       "   '34',\n",
       "   '52',\n",
       "   'with',\n",
       "   'the',\n",
       "   'grow',\n",
       "   'demand',\n",
       "   'for',\n",
       "   'remote',\n",
       "   'and',\n",
       "   'hybrid',\n",
       "   'activity',\n",
       "   'seek',\n",
       "   'effective',\n",
       "   'strategy',\n",
       "   'to',\n",
       "   'mitigate',\n",
       "   'distraction',\n",
       "   'and',\n",
       "   'promote',\n",
       "   'great',\n",
       "   'concentration',\n",
       "   'become',\n",
       "   'increasingly',\n",
       "   'important',\n",
       "   'one',\n",
       "   'approach',\n",
       "   'adopt',\n",
       "   'be',\n",
       "   'attentional',\n",
       "   'analysis',\n",
       "   'base',\n",
       "   'on',\n",
       "   'eye',\n",
       "   'tracking',\n",
       "   'datum',\n",
       "   'because',\n",
       "   'accord',\n",
       "   'to',\n",
       "   'Posner',\n",
       "   '42',\n",
       "   'there',\n",
       "   'be',\n",
       "   'a',\n",
       "   'direct',\n",
       "   'relationship',\n",
       "   'between',\n",
       "   'eye',\n",
       "   'movement',\n",
       "   'and',\n",
       "   'change',\n",
       "   'in',\n",
       "   'attention',\n",
       "   'thus',\n",
       "   'this',\n",
       "   'research',\n",
       "   'aim',\n",
       "   'to',\n",
       "   'synthesize',\n",
       "   'and',\n",
       "   'organize',\n",
       "   'exist',\n",
       "   'knowledge',\n",
       "   'about',\n",
       "   'attentional',\n",
       "   'analysis',\n",
       "   'conduct',\n",
       "   'base',\n",
       "   'on',\n",
       "   'datum',\n",
       "   'obtain',\n",
       "   'via',\n",
       "   'webcam',\n",
       "   'a',\n",
       "   'low',\n",
       "   'cost',\n",
       "   'eye',\n",
       "   'movement',\n",
       "   'capture',\n",
       "   'device',\n",
       "   'with',\n",
       "   'possibility',\n",
       "   'for',\n",
       "   'large',\n",
       "   'scale',\n",
       "   'study',\n",
       "   '9',\n",
       "   'to',\n",
       "   'do',\n",
       "   'this',\n",
       "   'we',\n",
       "   'carry',\n",
       "   'out',\n",
       "   'a',\n",
       "   'literature',\n",
       "   'review',\n",
       "   'from',\n",
       "   '2013',\n",
       "   'to',\n",
       "   '2023',\n",
       "   'use',\n",
       "   'a',\n",
       "   'hybrid',\n",
       "   'search',\n",
       "   'strategy',\n",
       "   'through',\n",
       "   'which',\n",
       "   'we',\n",
       "   'select',\n",
       "   '57',\n",
       "   'paper',\n",
       "   'these',\n",
       "   'paper',\n",
       "   'be',\n",
       "   'analyze',\n",
       "   'and',\n",
       "   'categorize',\n",
       "   'offer',\n",
       "   'a',\n",
       "   'comprehensive',\n",
       "   'overview',\n",
       "   'of',\n",
       "   'application',\n",
       "   'domain',\n",
       "   'datum',\n",
       "   'and',\n",
       "   'technique',\n",
       "   'identify',\n",
       "   'research',\n",
       "   'gap',\n",
       "   'and',\n",
       "   'point',\n",
       "   'out',\n",
       "   'possible',\n",
       "   'future',\n",
       "   'direction',\n",
       "   'the',\n",
       "   'main',\n",
       "   'contribution',\n",
       "   'of',\n",
       "   'this',\n",
       "   'work',\n",
       "   'be',\n",
       "   'presentation',\n",
       "   'of',\n",
       "   'the',\n",
       "   'state',\n",
       "   'of',\n",
       "   'the',\n",
       "   'art',\n",
       "   'relate',\n",
       "   'to',\n",
       "   'the',\n",
       "   'topic',\n",
       "   'identify',\n",
       "   'feature',\n",
       "   'that',\n",
       "   'enable',\n",
       "   'attentional',\n",
       "   'analysis',\n",
       "   'development',\n",
       "   'and',\n",
       "   'availability',\n",
       "   'of',\n",
       "   'the',\n",
       "   'tool',\n",
       "   'AttentionVis',\n",
       "   'Browser',\n",
       "   'that',\n",
       "   'present',\n",
       "   'this',\n",
       "   'study',\n",
       "   '’s',\n",
       "   'result',\n",
       "   'in',\n",
       "   'a',\n",
       "   'visual',\n",
       "   'and',\n",
       "   'interactive',\n",
       "   'format',\n",
       "   'as',\n",
       "   'demonstrate',\n",
       "   'in',\n",
       "   'Figure',\n",
       "   '1',\n",
       "   'Presentation',\n",
       "   'of',\n",
       "   'identify',\n",
       "   'lesson',\n",
       "   'learn',\n",
       "   'allow',\n",
       "   'researcher',\n",
       "   'to',\n",
       "   'identify',\n",
       "   'promising',\n",
       "   'area',\n",
       "   'avoid',\n",
       "   'error',\n",
       "   'and',\n",
       "   'contribute',\n",
       "   'to',\n",
       "   'the',\n",
       "   'development',\n",
       "   'of',\n",
       "   'this',\n",
       "   'field',\n",
       "   'presentation',\n",
       "   'of',\n",
       "   'a',\n",
       "   'set',\n",
       "   'of',\n",
       "   'limitation',\n",
       "   'of',\n",
       "   'current',\n",
       "   'solution',\n",
       "   'identification',\n",
       "   'of',\n",
       "   'research',\n",
       "   'opportunity',\n",
       "   'on',\n",
       "   'attentional',\n",
       "   'analysis',\n",
       "   'the',\n",
       "   'follow',\n",
       "   'section',\n",
       "   'present',\n",
       "   'the',\n",
       "   'methodological',\n",
       "   'process',\n",
       "   'analyze',\n",
       "   'the',\n",
       "   'result',\n",
       "   'and',\n",
       "   'discuss',\n",
       "   'lesson',\n",
       "   'learn',\n",
       "   'limitation',\n",
       "   'and',\n",
       "   'research',\n",
       "   'opportunity',\n",
       "   '2',\n",
       "   'METHODOLOGY',\n",
       "   'for',\n",
       "   'the',\n",
       "   'literature',\n",
       "   'review',\n",
       "   'we',\n",
       "   'adopt',\n",
       "   'a',\n",
       "   'hybrid',\n",
       "   'search',\n",
       "   'strategy',\n",
       "   'base',\n",
       "   'on',\n",
       "   'Mourão',\n",
       "   'et',\n",
       "   'al',\n",
       "   '36',\n",
       "   'guideline',\n",
       "   'to',\n",
       "   'ensure',\n",
       "   'great',\n",
       "   'efficiency',\n",
       "   'in',\n",
       "   'retrieve',\n",
       "   'relevant',\n",
       "   'study',\n",
       "   'as',\n",
       "   'corroborate',\n",
       "   'by',\n",
       "   'Wohlin',\n",
       "   'et',\n",
       "   'al',\n",
       "   '60',\n",
       "   'the',\n",
       "   'process',\n",
       "   'involve',\n",
       "   'prepare',\n",
       "   'a',\n",
       "   'search',\n",
       "   'string',\n",
       "   'base',\n",
       "   'on',\n",
       "   'the',\n",
       "   'propose',\n",
       "   'research',\n",
       "   'question',\n",
       "   'conduct',\n",
       "   'a',\n",
       "   'database',\n",
       "   'search',\n",
       "   'for',\n",
       "   'study',\n",
       "   'in',\n",
       "   'a',\n",
       "   'single',\n",
       "   'digital',\n",
       "   'library',\n",
       "   'apply',\n",
       "   'inclusion',\n",
       "   'and',\n",
       "   'exclusion',\n",
       "   'criterion',\n",
       "   'and',\n",
       "   'then',\n",
       "   'use',\n",
       "   'the',\n",
       "   'select',\n",
       "   'study',\n",
       "   'as',\n",
       "   'a',\n",
       "   'seed',\n",
       "   'set',\n",
       "   'for',\n",
       "   'apply',\n",
       "   'the',\n",
       "   'Backward',\n",
       "   'Snowballing',\n",
       "   'BS',\n",
       "   'and',\n",
       "   'Forward',\n",
       "   'Snowballing',\n",
       "   'FS',\n",
       "   'technique',\n",
       "   'the',\n",
       "   'bs',\n",
       "   'technique',\n",
       "   'involve',\n",
       "   'review',\n",
       "   'all',\n",
       "   'reference',\n",
       "   'of',\n",
       "   'paper',\n",
       "   'select',\n",
       "   'in',\n",
       "   'the',\n",
       "   'seed',\n",
       "   'set',\n",
       "   'to',\n",
       "   'identify',\n",
       "   'additional',\n",
       "   'study',\n",
       "   'relevant',\n",
       "   'to',\n",
       "   'the',\n",
       "   'research',\n",
       "   'the',\n",
       "   'FS',\n",
       "   'seek',\n",
       "   'to',\n",
       "   'identify',\n",
       "   'new',\n",
       "   'study',\n",
       "   'reference',\n",
       "   'the',\n",
       "   'publication',\n",
       "   'that',\n",
       "   'make',\n",
       "   'up',\n",
       "   'the',\n",
       "   'seed',\n",
       "   'set',\n",
       "   '59',\n",
       "   'the',\n",
       "   'previously',\n",
       "   'define',\n",
       "   'selection',\n",
       "   'criterion',\n",
       "   'must',\n",
       "   'be',\n",
       "   'strictly',\n",
       "   'follow',\n",
       "   'when',\n",
       "   'analyze',\n",
       "   'these',\n",
       "   '198',\n",
       "   'new',\n",
       "   'paper',\n",
       "   'in',\n",
       "   'this',\n",
       "   'hybrid',\n",
       "   'search',\n",
       "   'strategy',\n",
       "   'paper',\n",
       "   'obtain',\n",
       "   'via',\n",
       "   'BS',\n",
       "   'be',\n",
       "   'not',\n",
       "   'submit',\n",
       "   'to',\n",
       "   'FS',\n",
       "   'and',\n",
       "   'vice',\n",
       "   'versa',\n",
       "   'avoid',\n",
       "   'overlap',\n",
       "   'as',\n",
       "   'indicate',\n",
       "   'by',\n",
       "   'Mourão',\n",
       "   'et',\n",
       "   'al',\n",
       "   '36',\n",
       "   'and',\n",
       "   'Wohlin',\n",
       "   'et',\n",
       "   'al',\n",
       "   '60',\n",
       "   ...]},\n",
       " {'titulo': 'OntoDrug: Enhancing Brazilian Health System Interoperability with a National Medication Ontology',\n",
       "  'informacoes_url': '',\n",
       "  'idioma': 'english',\n",
       "  'storage_key': '../articles/original/english/985-24764-1-10-20240923.pdf',\n",
       "  'author': 'Nelson Miranda; Matheus Matos Machado; and Dilvan A. Moreira',\n",
       "  'data_publicacao': '11-09-2024',\n",
       "  'resumo': 'This paper presents OntoDrug, an ontology designed to enhance medicine management in Brazil by integrating regulatory frameworks and standardizing terminologies. OntoDrug improves patient safety and treatment efficacy by accurately identifying and classifying medications and supporting interoperability with health information systems. A proof-of-concept application integrated into the Hospital das Clínicas de Marília’s hospital EHR system demonstrated OntoDrug’s utility, achieving high precision and recall. An experimental study using large language models grounded on the ontology achieved, using GPT-4 turbo, 0.97 precision, 1.0 recall and an F1-score of 0.99. We also evaluated open-source models llama3-8b, llama3-70b, and gemma-7b-it. Their performance was close to GPT-4’s. The significant effectiveness is primarily due to the utilization of large language models (LLMs). While using these large language models enhanced performance, challenges related to cost, privacy, and service availability were identified. OntoDrug represents a significant advancement in Brazil’s medication information standardization and optimization. ###',\n",
       "  'keywords': 'Medication Ontologies, Drug Management, Semantic Interoperability, Health Informatics',\n",
       "  'referencias': ['[1] Burhan Ud Din Abbasi, Iram Fatima, Hamid Mukhtar, Sharifullah Khan, Abdulaziz\\nAlhumam, and Hafiz Farooq Ahmad. 2022. Autonomous schema markups based\\non intelligent computing for search engine optimization. *PeerJ Computer Science*\\n8 (2022), e1163.',\n",
       "   '[2] Agência Nacional de Vigilância Sanitária. 2004. *Política Vigente para a Regulamen-*\\n*tação de Medicamentos no Brasil* . Gabinete do Diretor-Presidente, Núcleo de Assessoramento em Comunicação Social e Institucional - Comin/Anvisa. https://bvsms.\\nsaude.gov.br/bvs/publicacoes/anvisa/manual_politica_medicamentos.pdf Impresso no Brasil. Permitida a reprodução parcial ou total desta obra, desde que\\ncitada a fonte..',\n",
       "   '[3] Agência Nacional de Vigilância Sanitária (Anvisa). 2022. *Vocabulário Controlado*\\n*de Formas Farmacêuticas, Vias de Administração e Embalagens* . https://www.gov.\\nbr/anvisa/pt-br/centraisdeconteudo/publicacoes/medicamentos/publicacoessobre-medicamentos/vocabulario-controlado.pdf/view Accessed: 2022-05-19.',\n",
       "   '[4] AI@Meta. 2024. Llama 3 Model Card. (2024). https://github.com/meta-llama/\\nllama3/blob/main/MODEL_CARD.md',\n",
       "   '[5] Dean Allemang and James Hendler. 2011. *Semantic web for the working ontologist:*\\n*effective modeling in RDFS and OWL* . Elsevier.',\n",
       "   '[6] Caio Viktor S Avila, Wellington Franco, Amanda DP Venceslau, Tulio Vidal\\nRolim, Vania MP Vidal, and Valéria M Pequeno. 2021. MediBot: an ontologybased chatbot to retrieve drug information and compare its prices. *Journal of*\\n*Information and Data Management* 12, 2 (2021).',\n",
       "   '[7] Letícia de Andrade Barbosa. 2013. Assistência farmacêutica no sistema único de\\nsaúde. (2013).',\n",
       "   '[8] Ricardo José Magalhães Barros, Marco Antônio de Araújo Fireman, and Mateus Rodrigues Westin. 2017. Relação nacional de medicamentos essenciais.\\n(2017).',\n",
       "   '[9] Erik Bülow. 2020. coder: An R package for code-based item classification and\\ncategorization. *J. Open Source Softw.* 5 (2020), 2916. https://api.semanticscholar.\\norg/CorpusID:233302997',\n",
       "   '[10] Wander de Almeida Limeira and Dilvan de Abreu Moreira. 2020. KGWE-A\\ncustomizable editor for OWL ontologies. In *Anais Estendidos do XXVI Simpósio*\\n*Brasileiro de Sistemas Multimídia e Web* . SBC, 91–95.',\n",
       "   '[11] Daniela Moulin Maciel de Vasconcelos, Gabriela Costa Chaves, Thiago Botelho\\nAzeredo, and Rondineli Mendes da Silva. 2017. National Medicines Policy in\\nretrospective : a review of ( almost ) 20 years of implementation. https://api.\\nsemanticscholar.org/CorpusID:22550643',\n",
       "   '[12] Agência Nacional de Vigilância Sanitária (Anvisa). 2020. *Novo Marco Regulatório*\\n*de Farmacovigilância* . https://www.gov.br/anvisa/pt-br/assuntos/noticias-anvisa/\\n2020/novo-marco-regulatorio-de-farmacovigilancia-confira Accessed: 2024-0604.',\n",
       "   '[13] Frederico Freitas and Stefan Schulz. 2009. Ontologias, Web semântica e saúde.\\n*Revista Eletrônica de Comunicação, Informação & Inovação em Saúde* 3, 1 (2009).',\n",
       "   '[14] Maurice Funk, Simon Hosemann, Jean Christoph Jung, and Carsten Lutz.\\n2023. Towards Ontology Construction with Language Models. *arXiv preprint*\\n*arXiv:2309.09898* (2023).',\n",
       "   '[15] Thomas R Gruber. 1995. Toward principles for the design of ontologies used\\nfor knowledge sharing? *International journal of human-computer studies* 43, 5-6\\n(1995), 907–928.',\n",
       "   '[16] Ramanathan V Guha, Dan Brickley, and Steve Macbeth. 2016. Schema. org:\\nevolution of structured data on the web. *Commun. ACM* 59, 2 (2016), 44–51.',\n",
       "   '[17] Curtis E. Haas, Mary Ann Kliethermes, Lori T. Armistead, Craig J. Beavers,\\nChristie A. Schumacher, Lisa Smith, John A. Armitstead, Roshni P. Emmons,\\nLucy I. Darakjian, Krystal L. Edwards, and Michael Barr. 2023. Comprehensive\\nmedication management: Review and recommendations for quality measures.\\n*Journal of the American College of Clinical Pharmacy* 6 (2023), 404 – 415. https:\\n//api.semanticscholar.org/CorpusID:256718733',\n",
       "   '[18] Josh Hanna, Eric Joseph, Mathias Brochhausen, and William R. Hogan. 2013.\\nBuilding a drug ontology based on RxNorm and other sources. *Journal of Biomedi-*\\n*cal Semantics* 4 (2013), 44 – 44. https://api.semanticscholar.org/CorpusID:2304775',\n",
       "   '[19] María Herrero-Zazo, Janna Hastings, Isabel Segura-Bedmar, Samuel Croset,\\nPaloma Martínez, and Christoph Steinbeck. 2013. An Ontology for Drug-drug\\nInteractions. In *Workshop on Semantic Web Applications and Tools for Life Sciences* .\\nhttps://api.semanticscholar.org/CorpusID:187394',\n",
       "   '[20] María Herrero-Zazo, Isabel Segura-Bedmar, Janna Hastings, and Paloma Martínez.\\n2015. DINTO: Using OWL Ontologies and SWRL Rules to Infer Drug-Drug\\nInteractions and Their Mechanisms. *Journal of chemical information and modeling*\\n55 8 (2015), 1698–707. https://api.semanticscholar.org/CorpusID:20599292',\n",
       "   '[21] Adriana M. Ivama-Brummell, Daniella PINGRET-KIPMAN, Priscila G. Louly, and\\nRosiene R. Andrade. 2022. Medicines regulation, pricing and reimbursement\\nin Brazil. *Revista Brasileira de Farmácia Hospitalar e Serviços de Saúde* (2022).\\nhttps://api.semanticscholar.org/CorpusID:247722354',\n",
       "   '[22] S Jagannatha, TV Suresh Kumar, and R RajaniKanth. [n. d.]. Comparative Study\\nof Back-End Vs Front-End System by Performance Analysis during Preliminary\\nDesign stages. ([n. d.]).',\n",
       "   '[23] Christopher Manning and Hinrich Schutze. 1999. *Foundations of statistical natural*\\n*language processing* . MIT press.',\n",
       "   '[24] Microsoft. 2023. .NET Framework 4.5. https://docs.microsoft.com/en-us/dotnet/\\nframework/. Accessed: January 2023.',\n",
       "   '[25] Nelson Miranda and Dilvan de A. Moreira. 2024. Improving Medication Identification Accuracy and Regulatory Compliance through NLP and Ontologies:\\nAn Analysis of Otorhinolaryngology Prescriptions. In *2024 IEEE International*\\n*Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises*\\n*(WETICE)* . IEEE.',\n",
       "   '[26] Ol’ga Alekseevna Mitina and Ivan Alexandrovich Yurchenkov. 2021. Data Classification in Medicine and Healthcare Service. *Artificial Intelligence in Intelligent*\\n*Systems* (2021). https://api.semanticscholar.org/CorpusID:237974497',\n",
       "   '[27] Mark A Musen. 2015. The protégé project: a look back and a look forward. *AI*\\n*matters* 1, 4 (2015), 4–12.',\n",
       "   '[28] Natalya Fridman Noy, Ray W Fergerson, and Mark A Musen. 2000. The knowledge\\nmodel of Protege-2000: Combining interoperability and flexibility. In *International*\\n*Conference on Knowledge Engineering and Knowledge Management* . Springer, 17–\\n32.',\n",
       "   '[29] Natalya F Noy, Deborah L McGuinness, et al . 2001. Ontology development 101:\\nA guide to creating your first ontology.',\n",
       "   '[30] OpenAI. 2024. GPT-4 Turbo: Advanced language model for natural language\\nunderstanding and generation. https://www.openai.com/models/gpt-4-turbo.\\nAccessed: 2024-03-22.',\n",
       "   '[31] Christian Pachl, Nils Frank, Jan Breitbart, and Stefan Bräse. 2020. Overview of\\nchemical ontologies. *arXiv preprint arXiv:2002.03842* (2020).',\n",
       "   '[32] Cecilia Reyes Peña, Mireya Tovar, Maricela Claudia Bravo, and Regina Motz.\\n2020. Drug Ontology for the Public Mexican Health System. In *SWH@ISWC* .\\nhttps://api.semanticscholar.org/CorpusID:229366837',\n",
       "   '[33] Deborah L. Pestka, Caitlin K. Frail, Lindsay A. Sorge, Kylee A. Funk, Kristin K.\\nJanke, Mary T. Roth McClurg, and Todd D. Sorensen. 2020. Development of\\nthe comprehensive medication management practice management assessment\\ntool: A resource to assess and prioritize areas for practice improvement. *Journal*\\n*of the American College of Clinical Pharmacy* 3 (2020), 448 – 454. https://api.\\nsemanticscholar.org/CorpusID:208485233',\n",
       "   '[34] Joselio Emar Araujo Queiroz, Robson Willian Melo Matos, Elivan Silva Souza,\\nPaula Xavier Santos, Laís Bié Pinto Bandeira, Daniel Catão Moreira Licio, Beatriz Faria Leao, and Rafael Santos Santana. 2023. Ontologia brasileira de medicamentos: rumo à padronização terminológica de medicamentos no Brasil. *JORNAL*\\n*DE ASSISTÊNCIA FARMACÊUTICA E FARMACOECONOMIA* 8, s. 2 (2023).',\n",
       "   '[35] Bartira Dantas Rocha, Larysse Silva, Thais Batista, Everton Cavalcante, and\\nPorfírio Gomes. 2020. An ontology-based information model for multi-domain\\nsemantic modeling and analysis of smart city data. In *Proceedings of the Brazilian*\\n*Symposium on Multimedia and the Web* . 73–80.',\n",
       "   '[36] Tomer Sagi, Matteo Lissandrini, T. Pedersen, and Katja Hose. 2022. A design\\nspace for RDF data representations. *The VLDB Journal* 31 (2022), 347 – 373.\\n\\n\\n247\\n\\n\\n-----\\n\\nOntoDrug: Enhancing Brazilian Health System Interoperability with a National Medication Ontology WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\nhttps://api.semanticscholar.org/CorpusID:246186959',\n",
       "   '[37] Mark E. Sharp. 2017. Toward a comprehensive drug ontology: extraction of\\ndrug-indication relations from diverse information sources. *Journal of Biomedical*\\n*Semantics* 8 (2017). https://api.semanticscholar.org/CorpusID:26441866',\n",
       "   '[38] S. Stelting and B. Maassen. 2020. Understanding Data Transfer Object (DTO)\\nin C#. https://code-maze.com/different-ways-handle-errors-net-core-webapi/.\\nAccessed: January 2023.',\n",
       "   '[39] Clarence Tauro, Nagesswary Ganesan, Saumya Mishra, and Anupama Bhagwat.\\n2012. Article: Object Serialization: A Study of Techniques of Implementing\\nBinary Serialization in C++, Java and .NET. *International Journal of Computer*\\n*Applications* 45 (05 2012), 25–29.',\n",
       "   '[40] Gemma Team, Thomas Mesnard, Cassidy Hardin, and Robert Dadashi et al.\\n2024. Gemma: Open Models Based on Gemini Research and Technology.\\n\\n\\narXiv:2403.08295',\n",
       "   '[41] Ruben Verborgh and Max De Wilde. 2013. *Using openrefine* . Packt Publishing\\nLtd.',\n",
       "   '[42] Zizhong Wei, Dongsheng Guo, Dengrong Huang, Qilai Zhang, Sijia Zhang, Kai\\nJiang, and Rui Li. 2023. Detecting and Mitigating the Ungrounded Hallucinations\\nin Text Generation by LLMs. In *Proceedings of the 2023 International Conference*\\n*on Artificial Intelligence, Systems and Network Security* . 77–81.',\n",
       "   '[43] Marieke Wermuth. 2021. Terminological cooperation in the biomedical field.\\n*Terminology* (2021). https://api.semanticscholar.org/CorpusID:237833789',\n",
       "   '[44] Yijia Zhang, Qingyu Chen, Zhihao Yang, Hongfei Lin, and Zhiyong Lu. 2019.\\nBioWordVec, improving biomedical word embeddings with subword information\\nand MeSH. *Scientific data* 6, 1 (2019), 52.\\n\\n\\n248\\n\\n\\n-----'],\n",
       "  'text': '# **OntoDrug: Enhancing Brazilian Health System Interoperability** **with a National Medication Ontology**\\n\\n## Nelson Miranda\\n#### nelson.miranda@usp.br Institute of Mathematical and Computer Sciences, USP São Carlos, São Paulo, Brazil\\n### **ABSTRACT**\\n\\n## Matheus Matos Machado\\n#### matheusmatos@usp.br Institute of Mathematical and Computer Sciences, USP São Carlos, São Paulo, Brazil\\n\\n## Dilvan A. Moreira\\n#### dilvan@usp.br Institute of Mathematical and Computer Sciences, USP São Carlos, São Paulo, Brazil\\n\\n\\nThis paper presents OntoDrug, an ontology designed to enhance\\nmedicine management in Brazil by integrating regulatory frameworks and standardizing terminologies. OntoDrug improves patient\\nsafety and treatment efficacy by accurately identifying and classifying medications and supporting interoperability with health\\ninformation systems. A proof-of-concept application integrated\\ninto the Hospital das Clínicas de Marília’s hospital EHR system\\ndemonstrated OntoDrug’s utility, achieving high precision and recall. An experimental study using large language models grounded\\non the ontology achieved, using GPT-4 turbo, 0.97 precision, 1.0\\nrecall and an F1-score of 0.99. We also evaluated open-source models llama3-8b, llama3-70b, and gemma-7b-it. Their performance\\nwas close to GPT-4’s. The significant effectiveness is primarily due\\nto the utilization of large language models (LLMs). While using\\nthese large language models enhanced performance, challenges\\nrelated to cost, privacy, and service availability were identified. OntoDrug represents a significant advancement in Brazil’s medication\\ninformation standardization and optimization.\\n### **KEYWORDS**\\n\\nMedication Ontologies, Drug Management, Semantic Interoperability, Health Informatics\\n### **1 INTRODUCTION**\\n\\nMedication management is a critical component of healthcare, encompassing the entire lifecycle of a patient’s medications, from\\nprocurement to discontinuation [ 33 ]. This cycle includes accurate\\ntranscription of prescriptions, tracking of medication lots and expiration dates, precise dispensation, and detailed administration.\\nAdditionally, continuous monitoring and adherence checks are\\nconducted to optimize therapeutic outcomes. Regular reviews are\\nperformed to adjust medication plans according to changing patient needs, leading to appropriate discontinuation of medications.\\nThis systematic approach is essential for enhancing patient safety,\\ntreatment efficacy, and optimizing healthcare resources [17].\\nDespite its importance, the medication management process is\\nfraught with challenges. One significant issue is the lack of standardization and controlled vocabularies, leading to ambiguities in\\nthe nomenclature used throughout the cycle. Such ambiguities can\\nresult in communication errors between prescribers, dispensers,\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n© 2024 SBC – Sociedade Brasileira de Computação.\\nISSN 2966-2753\\n\\n\\nand patients, ultimately compromising patient safety and treatment\\neffectiveness. Therefore, a controlled vocabulary is crucial in normalizing terms for pharmaceutical forms, administration routes,\\npackaging, and measurement units, ensuring uniform communication and reducing errors [ 13 ]. Such harmonization can reduce\\nerrors and ambiguity in medication prescription and administration,\\nfostering patient safety and effective treatment. [ 43 ] Furthermore,\\nthe system supports interoperability with other health information\\nsystems, making data management and exchange activities more\\neffective.\\nIn Brazil, managing medications is challenging due to the presence of different regulatory frameworks and lists such as the DCB\\n(Brazilian Common Denominations), CMED (Chamber of Medicines\\nMarket Regulation), and RENAME (National List of Essential Medicines). Prescribing drugs from RENAME is vital as it guarantees\\npatients access to treatments that are both effective and affordable,\\nsupporting rational drug use in the healthcare system[ 11 ].These\\nlists standardize drug names, regulate the pharmaceutical market,\\nand detail essential medications for public health, respectively [ 21 ].\\nHowever, integrating these regulatory frameworks into a cohesive system remains a challenge, often resulting in inconsistent\\nmedication identification and classification.\\nThe characteristics of medications and substances dictate their\\nclassification and prescription restrictions. Categories include generic drugs, similar drugs, biological drugs, and biosimilars, each with\\nspecific prescription levels indicated by colored stripes on their\\npackaging [ 2 ]. Additionally, regulatory lists categorize controlled\\nsubstances such as narcotics, psychotropic drugs, anorexigenic substances, immunosuppressants, antiretrovirals, anabolic steroids,\\nand precursors used in drug manufacturing. These classifications\\nensure responsible prescription and dispensation practices, adhering to health and legal standards to manage risks associated with\\ntheir use [12].\\nThe primary objectives of this research were to develop and\\nimplement the OntoDrug ontology to enhance medication management within the Brazilian healthcare system.\\nThe OntoDrug ontology aims to address these challenges by\\nproviding a framework that integrates Brazil’s regulatory lists into\\na standardized, computer-readable format. By leveraging controlled\\nvocabulary and coding systems like the CAS-Anatomical Therapeutic Chemical Classification System and RxNorm, OntoDrug\\nensures accurate identification and disambiguation of medications\\n\\n[ 26 ]. This ontology enhances the organization and accessibility of\\npharmacological knowledge and also supports interoperability with\\nother health information systems, improving data management and\\nexchange activities [9].\\n\\n\\n240\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Nelson Miranda, Matheus Matos Machado, and Dilvan A. Moreira\\n\\n\\nTo demonstrate the utility of OntoDrug, a proof-of-concept application was developed and integrated into the EHR of the Hospital\\ndas Clínicas de Marília hospital. Whenever a prescription is created\\nin the EHR system, this application reads it, identifies the medications, and warns physicians if some medication is not listed in the\\nRename, suggesting alternatives whenever possible. The ontology\\nis used to ground the LLMs doing the medicine recognition and to\\nsuggest replacements for the ones not in Rename. When recognizing medicines, its best results (when using the GPT-4 turbo LLM)\\nwere 1.0 recall, 0.974 precision, and 0.987 F1-score. The physicians\\nwere also happy with the replacement suggestions.\\nIn the following sections, we discuss the development of the\\nOntoDrug ontology, its implementation in a clinical setting, and\\nits potential applications in improving medication management\\npractices.\\n### **2 RELATED WORKS**\\n\\nIn our exploration of medication ontologies, we reviewed several\\nkey works that focus on the creation and utilization of drug ontologies. Peña (2020) [ 32 ] developed a Drug Ontology for the Mexican public health system to provide a well-structured medical\\nknowledge base accessible to various stakeholders. Based on the\\n\"Basic Table and Catalog of Medicines\" from the Secretary of Public Health, the ontology consists of 64 classes, 5 object properties,\\nand 18 data properties. Its evaluation, focusing on model competence and quality criteria, demonstrated its potential for enhancing\\nmedical knowledge management.\\nA unified Brazilian drug database is meant to advance patient\\nsafety and individualized care. Queiroz (2023) [ 34 ] emphasizes the\\naspect of terminology standardization and semantic alignment related to medication nomenclature, underlying how this system\\nwould be advantageous for healthcare providers in securing proper,\\nconsistent, and safe pharmaceutical care for patients.\\nAvila (2021) [ 6 ] developed MediBot, a chatbot that provides drug\\ninformation, including prices and potential substitutes. The bot\\nworks in two modes: Quick Response mode and Interactive mode.\\nIt uses a Linked Data Mashup to extract real-time information from\\nthe web. This system could be helpful for consumers and healthcare\\nprofessionals seeking accurate and up-to-date information related\\nto drugs.\\nDINTO [1] is a drug-drug interaction-focused ontology along with\\nmechanisms concerning the improvement of clinical safety [ 19 ].\\nIt efficiently classifies interactions and mechanisms, facilitating\\nlarge-scale prediction of potential interactions [20]. This helps detect severe adverse reactions and safety issues mentioned under\\nWarnings and Precautions on the labels of drugs. Moreover, DINTO\\nsupports the generation of new hypotheses on drug interaction and\\ncan be helpful during testing at the new drug’s preclinical stage.\\nSharp (2017) [ 37 ] and Hanna (2013) [ 18 ] contribute to the development of a comprehensive drug ontology. Sharp’s work focuses\\non the extraction of drug-indication relations from various sources,\\nwhile Hanna’s work builds the Drug Ontology (DrOn) [2] based on\\nRxNorm and other sources. Li 2019 and Herrero-Zazo 2013 further\\n\\n1 DINTO ontology details can be found at https://portal.bioontology.org/ontologies/\\nDINTO\\n2 DRON ontology details available at https://bioportal.bioontology.org/ontologies/\\nDRON\\n\\n\\nenhance the ontology by designing a drug-repurposing-oriented\\nAlzheimer’s Disease Ontology and an ontology for drug-drug interactions, respectively. These works collectively contribute to the\\ncreation of a comprehensive drug ontology that covers various\\naspects of drugs, including their properties, classifications, and\\nrelationships.\\nThe OntoDrug ontology aligns with Brazilian regulations like\\nRENAME and CMED, distinguishing it from the broader Mexican\\nDrug Ontology and the specialized MediBot. Unlike the Brazilian Medicines Ontology (OBM), which focuses on unifying drug\\nnomenclature for safety, OntoDrug includes detailed drug data and\\nregulatory information. It can enhance medication safety and be\\nused for educational purposes. Additionally, while DINTO focuses\\non drug-drug interactions and DRON on drug properties, OntoDrug’s specific adaptation to Brazilian standards makes it a tool for\\nhealthcare compliance and education.\\n### **3 METHOD DESCRIPTION**\\n\\nThe OntoDrug ontology integrates Brazil’s DCB, CMED, and RENAME listings into a standardized, computer-readable format. It\\nreuses the Schema.org [3] ontology to enhance interoperability across\\ncomputational systems, including medical suppliers, and to structure medical information effectively[ 15, 16, 23, 28, 29, 35 ]. The\\nSchema.org defines a widely used formal ontology for web page semantic markup, employing a collection of interconnected schemas\\n(classes) and properties (attributes) collaboratively developed by\\nmajor search engines like Google and Microsoft to standardize the\\nrepresentation of web content entities, including medicines. [1]\\nWe adopted Schema.org types such as *Thing*, *MedicalEntity*, *Drug*,\\nand *DrugClass*, facilitating a systematic categorization and enriching the ontology with entities like *DrugCost* and *MedicalCode* . We\\nreused the *DrugStrength* class for generic medicines but changed\\nits name (label) to *GenericMedicine* for clarity (but kept its original\\nID). Reusing schema terms not only aligns OntoDrug with global\\nstandards but also supports detailed descriptions of medicines. It\\npromotes a standardized representation compatible with semantic\\nweb content about medications on thousands of sites, enhancing\\nthe ontology’s functionality within web-based health information\\nsystems.\\nFor the development of the OntoDrug ontology, we have based\\nour work on the structured methodology of Noy and McGuinness\\n\\n[ 29 ] for ontology development. We used the Web Ontology Language (OWL) [5], which is a W3C standard for the representation\\nof ontologies, and the free, open-source Protégé ontology editor\\n\\n[27] [10].\\nOur ontology development targeted the medication domain, aiming to streamline drug identification by active ingredient or commercial name and to categorize drugs based on their listing in RENAME.\\nWe utilized OpenRefine 3.6.2, an open-source tool, for executing\\nETL (Extract, Transform, Load) processes on the data sourced from\\nCMED, RENAME, and DCB tables. Data were extracted and then imported into OpenRefine for necessary transformations. OpenRefine\\nwas used to manage unstructured data, offering robust functionalities for data cleaning, transformation, and reconciliation [41].\\n\\n3 https://schema.org\\n\\n\\n241\\n\\n\\n-----\\n\\nOntoDrug: Enhancing Brazilian Health System Interoperability with a National Medication Ontology WebMedia’2024, Juiz de Fora, Brazil\\n\\n**Table 1: Details of CMED, DCB, and RENAME Lists**\\n\\nList Records Comments\\n\\nCMED 26,062 Medications Marketed in Brazil\\nDCB 12,459 Active Ingredients\\nRename 921 Essential Medications\\n\\n\\nThe transformed RENAME, CMED, and DCB tables encapsulated essential drug information. The RENAME data, initially in\\nPDF format, was manually converted into CSV to facilitate the\\ndetailing of generic names, concentrations, pharmaceutical forms,\\ncomponents, and ATC codes. The CMED list encompasses detailed\\npharmaceutical data, including active ingredients, manufacturers,\\nregistration codes, presentations (concentration, dosage form, packaging, quantity), therapeutic classes, product types (reference, similar, or generic), pricing, and regulatory categories by colored stripes\\n(black, red, or none). Additionally, the DCB table enriched the ontology with detailed chemical data, including the substance name\\n(active ingredient), its Chemical Abstracts Service (CAS) Number,\\nand classifications such as API (Active Pharmaceutical Ingredient),\\nBiological, or Homeopathic, enhancing the ontology’s utility for diverse pharmaceutical applications [ 44 ]. For details on the three lists,\\nsee Table 1 for a summary of records and associated comments. Figure 1 displays the high-level structure of the ontology, highlighting\\nits primary classes. The central class, *GenericMedicine*, represents\\nthe specific dosage at which a drug is available, combining active\\ningredients ( *activeIngredient* property) with its concentration and\\nunit of measure ( *strengthUnit* and *strengthValue* properties), based\\non data from the CMED list. This list offers a comprehensive catalog of medications along with their attributes [ 7 ]. The *Drug* class\\ncorresponds to the active ingredients and is derived from the DCB\\nlisting.\\nMedications listed in RENAME are categorized under the *Gener-*\\n*icMedicine* class. They correspond to entries found in the CMED\\nlist and include additional details such as the pharmaceutical care\\ngroup( *RenameGroup* ) to which each medication belongs, whether\\nbasic, strategic, or specialized [8].\\nThe *CommercialMedicine* class describes commercial drug products from private laboratories. These products are linked to the\\n*GenericMedicine* class and, through it, to the *Drug* class, allowing\\nfor a connection based on shared attributes like active ingredient,\\ndosage form, and concentration. This linkage enables multiple products to be associated with a single *GenericMedicine* if they have\\nidentical properties. Such a structure guarantees that medications,\\nidentified by their commercial name or active ingredient, are consistently represented in the ontology, enhancing its practicality.\\nAll medications in the *GenericMedicine* class listed in RENAME\\n\\nare designated as belonging to the *Rename* class. To verify RENAME\\nmembership, we determine if a medication is a *GenericMedicine* of\\ntype *Rename* or a *CommercialMedicine* equivalent to such a *Gener-*\\n*icMedicine* . If there is no direct match, we check whether the medication belongs to a *CommercialMedicine* or *GenericMedicine* with an\\nactive component in RENAME. We then gather all *GenericMedicine*\\nentries with this active ingredient to see if alternative combinations\\n\\n\\n**Figure 1: OntoDrug: Overview of the Ontology Structure.**\\n\\nmatch the required strength. For example, if *Ibu-tab 400 mg* is prescribed and unavailable, the system might suggest two doses of the\\ngeneric medicine *Ibuprofen 200 mg* as an alternative.\\nThe controlled vocabulary detailing pharmaceutical forms, routes\\nof administration, and packaging, as published by Anvisa [ 3 ], was\\nsuccessfully and manually transformed into an RDF taxonomy for\\nintegration with the OntoDrug ontology. This transformation included:\\n\\n  - **Pharmaceutical Forms** : Solids, liquids, semi-solids, and\\ngases were identified as categories, each containing specific\\nentries under them, as in the case of tablets, capsules, creams,\\nand inhalants.\\n\\n  - **Routes of Administration** : This consisted of classifications under mechanisms like oral, intravenous, topic, and\\ninhalation.\\n\\n  - **Packaging** : It listed all kinds of packaging forms under this\\ncategory as ampoules, bottles, blister packs, and tubes, which\\nwere grouped under categories of primary, secondary, and\\nspecial packaging.\\n\\nIn RDF, each term from the controlled vocabulary was represented as an instance within its respective class, with properties to\\ndenote relationships and characteristics, enhancing interoperability\\nand standardization across healthcare systems.\\nIn developing the OntoDrug ontology, we included the Units\\nof Measurement Ontology (UO) to standardize measurements like\\nmg, g, mcg, mg/mL, and ml, ensuring clarity in medication properties across health systems. The UO facilitates data and knowledge\\nintegration, enabling interoperability and semantic information\\nprocessing across various biomedical resources and domains. [31]\\nThis integration supports precise drug administration and enhances\\n\\n\\n242\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Nelson Miranda, Matheus Matos Machado, and Dilvan A. Moreira\\n\\n\\nOntoDrug’s interoperability with health informatics systems [4] . The\\nontology data were exported in RDF format using the RDF extension in OpenRefine and stored in GraphDB 10.0.1 [5], a triplestore.\\nA triplestore, or RDF store, is a database specifically designed for\\nstoring and retrieving data via semantic queries, which facilitates\\nthe handling and querying of ontology data.[36]\\n### **4 ONTODRUG ONTOLOGY**\\n\\nThis section presents the ontology structure, such as the class hierarchy, primary axioms, and retrieval of individuals. OntoDrug\\ncontains about 33 classes, 18 object properties, 19 data properties,\\nand over 78,000 individuals. It includes around 430 axioms that\\nmake questioning and reasoning capabilities possible.\\nStructured relationships between key classes, such as *Gener-*\\n*icMedicine*, *Drug*, and *CommercialMedicine*, are shown in Figure 2.\\nIn this regard, having a diagram is important as a way to make sure\\nthat different elements relate in the proper way to one another.\\n\\n**Figure 2: Class Hierarchy of the OntoDrug Ontology**\\n\\nIn our work, a significant emphasis was placed on the integration\\nof instances from diverse data sources (DCB, CMED, and RENAME)\\ninto the ontology. For instance, we created an axiom to determine\\n\\n4 Units of Measurement Ontology details available at https://ontobee.org/ontology/UO\\n5 https://graphdb.ontotext.com/documentation/10.0/index.html\\n\\n\\nif a medicine is part of Rename. The formal definition specifies that\\nwithin the entire set of medicines, only those classified under the\\nGenericMedicine class are considered part of the Rename list. This\\nclassification is established through a relationship indicated by the\\nproperty *:categorizedAs* linking to an instance of the RenameGroup\\nclass. Additionally, a commercial medicine can be indirectly included in the Rename list because it has an equivalent generic.\\nThis inclusion occurs through the association of its generic name\\n( *:genericName* property) with an entity within the RenameGroup.\\nThe RenameGroup class is an enumerated type comprising 3 elements, rename-basic, renames-strategic, and rename-specialized,\\neach representing a different categorization within the Rename list.\\nThis axiom is represented in Equation 1. It is key to the correct\\nclassification of medicines as belonging to the Rename list.\\n\\nRename ≡(CommercialMedicine∧\\n\\n(∃:genericName (GenericMedicine∧\\n\\n(∃categorizedAs *.* RenameGroup))))\\n\\n∨(GenericMedicine∧\\n\\n(∃categorizedAs *.* RenameGroup)) (1)\\n\\nFigure 3 shows a part of the OntoDrug ontology related to the\\nactive ingredient *Ivermectin* . In the knowledge graph, products\\ncontaining different strengths of ivermectin are related, such as the\\ngeneric medicine *Ivermectin 6 mg* and the commercial *Revectina*\\n*6mg* . From this visualization, one can now realize that *Soolantra*\\n*10mg/g* is not part of the RENAME list as it is related to the generic\\nmedicine Ivermectin 10mg/g, which is not part of the Rename list.\\n\\n**Figure 3: Knowledge Graph Generated from OntoDrug**\\n### **5 USE CASE SCENARIOS**\\n\\nThe application of the OntoDrug ontology can extend beyond its\\nintegration with electronic health records (EHRs). This section\\noutlines some real-world scenarios where OntoDrug can enhance\\n\\n\\n243\\n\\n\\n-----\\n\\nOntoDrug: Enhancing Brazilian Health System Interoperability with a National Medication Ontology WebMedia’2024, Juiz de Fora, Brazil\\n\\npharmaceutical practices and healthcare services. The ontology can\\naid in the customization of drug therapies, streamline medicine\\nstock management, and serve as an educational resource. Each\\nscenario demonstrates the practical benefits and potential of OntoDrug to improve patient care, optimize operational efficiencies,\\nand support the continuous education of healthcare professionals.\\n### **5.1 Personalized Medication Therapy**\\n\\n\\nIntegrating OntoDrug into a clinical decision support system allows\\nmedication therapies to be personalized based on the individual\\nspecifics of each patient. It could document drug interactions with\\nallergies, other drugs, and pre-existing conditions. Such systems\\ncan use the detailed and structured information available in On\\ntoDrug to help identify problems and recommend safer or more\\nefficacious medications. This approach helps to optimize patient\\nsafety and therapeutic outcomes by ensuring that suggestions of optimal medication options are made available. Options more suitable\\nto the unique health profile of each patient.\\n### **5.2 Medication Inventory Management**\\n\\nThe OntoDrug ontology can also be leveraged to optimize inventory management in pharmacies and hospitals by offering possible\\noptions for medicines that are unavailable in the Rename list.\\n### **5.3 Pharmacy Education and Training**\\n\\nThe OntoDrug ontology can serve as an educational tool for students and healthcare professionals, offering a comprehensive knowledge base on drug characteristics, classifications, and proper usage.\\nThis resource can be integrated into e-learning modules that simulate prescription writing and drug administration scenarios. Such\\ntraining enhances learning through interaction with an ontologybased system, reinforcing the application of theoretical knowledge\\nin real-world settings, leading to better-informed clinical decisions\\nThe scenarios discussed illustrate the versatility and possible\\nuses of the OntoDrug ontology across various facets of healthcare\\nand education. The next section, *Proof of Concept*, shows a practical\\nuse of the OntoDrug ontology. It was integrated into the electronic\\nprescribing module of a major hospital’s EHR system.\\n### **6 PROOF OF CONCEPT**\\n\\nThis section reports the results of integrating the **OntoDrug** ontology into the prescription module of the Marília Medical School’s\\n(FAMEMA) Hospital **EHR system** . This real-world deployment\\nshowcases how the ontology can promote adherence to a prescription standard, enhancing the efficiency of medication management\\nin a clinical setting.\\nAs shown in Figure 4, the prescription module GUI incorporates\\na user-friendly interface for entering medical prescriptions, which\\nallows healthcare personnel to easily create free-text prescriptions.\\n\\nThis functionality is provided by the **Prescription Service**, implemented using .NET 4.5, which enhances the capabilities of the\\nhospital’s existing EHR system. It serves as an intermediary that\\norchestrates the data exchange between the existing EHR and a prescription database through the use of JSON objects (Listing 1). This\\n\\n\\n11 }\\n\\n**Listing 1: Translated JSON for Medicine Prescription**\\n\\nData interchange between the EHR and the Prescription Service\\ninvolves object serialization, which is managed by the **Prescription**\\n**Backend** . This backend handles business logic, data transformations, and interactions with the ontology [39][22].\\nIn the workflow of the Prescription Backend, an **Entity Extrac-**\\n**tion Service** utilized a traditional pre-trained model specifically tailored for Named Entity Recognition (NER) in medical prescriptions.\\nWe tested the quality of the NER extraction by asking physicians\\nhow many medications were successfully recognized in each prescription (Figure 5). We had feedback for 3,761 prescription reports.\\nNER achieved good precision with 61% or 2,295 prescriptions where\\nall medicines were successfully recognized and 33.5% or 1,261 prescriptions where at least one medicine was recognized [ 25 ]. This is\\nthe current version in production.\\nAfter recognizing medicines in the prescription, the system uses\\nthe ontology, employing the techniques explained earlier, to verify if\\nthese medicines are part of Rename and, if they are not, try to find a\\nmedicine or combination of medicines that can replace the original\\none. In this way, the system can not only warn physicians about\\nnon-compliant medicines but also suggest, if possible, substitutes.\\nThe system uses SPARQL queries to a triple store that houses\\nthe ontology to interact with it. This arrangement permits sophisticated handling of queries related to medication, encompassing\\ndrug classifications and potential alternatives based on active ingredients.\\n\\n244\\n\\n\\n**Figure 4: Proof of Concept: Medication Order Form**\\n\\nservice ensures robust data integrity and secure communication\\n\\n[24][38].\\n\\n1 {\\n\\n\\n\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Nelson Miranda, Matheus Matos Machado, and Dilvan A. Moreira\\n\\n\\nThe Prescription Backend constructs and dispatches SPARQL\\nqueries to the GraphDB triple store, which executes them and\\nreturns results in JSON format. Listing 2 shows a sample JSON\\nresponse from the backend. The **valid** section lists medications\\nsuccessfully verified.\\n\\n1 {\\n\\n19 }\\n\\n**Listing 2: JSON Representation for Medication Validation**\\n\\nFigure 5 displays the user interface, which presents a list of\\nmedications identified by the ontology along with information on\\nwhether they belong to the Rename list. Additionally, the interface\\nrequests physicians’ feedback to assess the information’s accuracy\\nand relevance.\\n\\n**Figure 5: Feedback Dialog: Display of Serialized Data in the**\\n**User Interface**\\n### **6.1 Using LLMs for medicine recognition**\\n\\nThe last section described the system currently in production at\\nthe FAMEMA school hospital. Subsequently, the recognition of\\nmedicines on the prescription based on traditional NER techniques\\nwas replaced by more advanced LLM-based implementations, such\\n\\n\\nas LLM GPT-4 Turbo, which enhanced natural language processing\\ncapabilities. The rest of the system remained the same.\\nFigure 6 shows the data transmission from the Prescription Service to the EHR system, offering a succinct visualization of the new\\nworkflow, which now incorporates the use of LLMs for enhanced\\nprocessing.\\n\\n**Figure 6: EHR to OntoDrug Data Flow Overview**\\n\\nWe could have asked the LLM to directly determine if the recognized medicines were or were not listed in the Rename. However,\\nthat could lead to hallucinations [ 42 ]: the LLM falsely stating that\\na medicine is part of Rename or, worst still, suggesting nonexistent\\nsubstitutes. We decided to use the ontology to “ground” the LLM\\n\\n[ 14 ]. We limited the LLM to medicine recognition and left the rest\\nto the ontology. In this way, all possible answers are limited by the\\nontology contents, reducing hallucinations. Notice that the LLM\\ncan still make a mistake and not recognize a medicine, but the system will not misclassify a correctly recognized medicine or suggest\\nnonexistent medicine substitutions.\\nFor this new implementation, we tested different LLMs, remembering that we still use the OntoDrug to find a medication Rename\\nmembership and to suggest alternative medications whenever possible. The GPT-4 Turbo model achieved perfect recall (1.0) and high\\nprecision (0.97), resulting in an overall F1-score of 0.99, showing\\nhighly effective performance metrics.\\n### **7 EXPERIMENTAL STUDY AND RESULTS**\\n\\nWe evaluated the new implementation across four different LLMs:\\nGPT-4 Turbo, LLaMA3-8b, LLaMA3-70b, and GEMMA-7b-IT to\\nassess their effectiveness [ 4, 30, 40 ]. One goal was to compare these\\nmodels to the original NER approach and determine their relative\\nperformance.\\nIn the original approach (NER + ontology), we used 3,761 prescriptions obtained with physician feedback provided in response\\nto the dialog box shown in Figure 5. The results for this approach\\n\\n[25] were:\\n\\n(1) **Complete Recognition** (61% or 2,295 Prescriptions): The\\nsystem accurately identified all medications as reported by\\nthe physicians in these instances.\\n(2) **Partial Recognition** (33.5% or 1,261 Prescriptions): The\\nsystem recognized only some of the medications reported\\nby the doctors.\\n\\n\\n245\\n\\n\\n-----\\n\\nOntoDrug: Enhancing Brazilian Health System Interoperability with a National Medication Ontology WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\n(3) **No Recognition** (5.5% or 205 Prescriptions): The system\\ndid not identify any medications in these cases, indicating a\\nneed for further investigation.\\n\\nThese results highlighted the possibility that better techniques\\ncould enhance the system’s accuracy.\\nIn our current study, we replaced the traditional NER-based\\nrecognition with LLMs to recognize medications and dosages directly from the prescription texts.\\nWe conducted tests with four models. A massive, more than 300\\nbillion parameters, proprietary top model, gpt-4 Turbo, and three\\nopen source smaller models, llama3-70b (70 billion parameters),\\nllama3-8b (8 billion), and gemma-7b-it (7 billion). We evaluated\\ntheir effectiveness and accuracy in comparison to the original NERbased approach.\\nFor each prescription, we used the list of the medicines actually\\nin the prescription, supplied by a physician, and the list of machinedetected medicines. All medicines in both lists were considered\\n\\ntrue positives (TP), medicines only on the physician’s list were false\\nnegatives (FN), and only in the machine-detected list were false\\npositives (FP). There are no true negatives (TN).\\nWe could not use all the original 3,761 prescriptions (from the\\nfirst study). For simplicity of the GUI, the physicians only informed\\nthe total number of medicines prescribed and how many were\\ncorrectly recognized in each prescription. We do not know which\\nmedicines were correctly identified. For that, physicians needed to\\ngo on each prescription and manually annotate which medicines\\nwere present.\\nAnother problem is price restrictions. Currently, high-end, very\\nexpensive hardware is needed to run LLMs locally. So, it is more\\ncost-effective to run them remotely using paid systems. That option\\nis cheaper but still quite expensive to run 3,761 examples. So, we\\nopted to use a random sample of 203 (5.4%) prescriptions from the\\noriginal set.\\nFor the experiments, gpt-4 Turbo models run using OpenAI’s\\ncloud [6] and llama3-70b, llama3-8b, and gemma-7b models used the\\nGroq Cloud [7] .\\nAfter each of the four models recognized the medicines in the prescriptions, we calculated the precision, recall, and F1-score shown\\nin Table 2. We did not include measurements, such as accuracy,\\nthat depend on true negative (TN) values, as these values are not\\navailable.\\n\\n**Model** **Precision** **Recall** **F1-score**\\n\\ngpt-4 turbo 0.974 1.000 0.987\\nllama3-70b-8192 0.990 0.985 0.988\\n\\ngemma-7b-it 0.995 0.971 0.983\\nllama3-8b-8192 0.985 0.973 0.979\\n\\n*ner-model** *0.976* *0.501* *0.662*\\n\\n*(*Included for comparison purposes.)*\\n\\n**Table 2: Metrics of different language models.**\\n\\nAs expected, gpt-4 turbo had the best numbers, even achieving\\n100% recall. But the three open source models were not far behind\\n\\n6 https://openai.com/\\n7 https://groq.com/\\n\\n\\nit. For instance, all four models achieved a precision above 98%.\\nIn general, the bigger the model, the better (Figure 7). However,\\nGemma-7b-it was slightly better than llama3-8b.\\nIt is important to highlight that the smaller LLMs had values\\nvery competitive in relation to GPT 4 performance. That indicates\\nthat medicine identification is not such a hard problem for LLMs,\\nopening the possibility that sooner such models can run locally,\\nreducing costs and improving privacy.\\n\\n**Figure 7: Performance metrics of different language models.**\\n\\nAll models outperformed the pre-trained NER model, which\\nachieved a Recall of 0.501, a Precision of 0.976, and an F1-score of\\n0.662 on the same sample.\\nDespite the significant improvement in accuracy with the LLM\\nuse, they have disadvantages. One primary concern is the cost;\\nrunning such advanced models can be expensive, especially when\\nprocessing large datasets, making it less feasible for continuous\\nor large-scale applications (a hospital may produce thousands of\\nprescriptions daily).\\nAdditionally, service availability can be an issue; reliance on\\nan external API means that any downtime or service disruption\\nfrom the provider or the internet could impact the accessibility\\nand functionality of the medication identification system. Privacy\\nis another issue because the sensitive data on patients is to be\\ntransferred and processed by the API provider.\\n### **8 CONCLUSION**\\n\\nIn this work, we presented the development and implementation\\nof the OntoDrug ontology, designed to enhance medication management within the Brazilian healthcare system. By integrating\\nregulatory frameworks and standardizing terminologies, OntoDrug\\nsupports accurate identification and classification of medications,\\nreducing ambiguities and improving patient safety. Our proof-ofconcept application demonstrated the practical utility of OntoDrug\\nin a clinical setting, showing significant improvements in the precision and efficiency of medication management.\\nThe experimental study revealed that the incorporation of advanced language models like GPT-4 turbo significantly outperforms\\ntraditional NER methods in terms of recall, precision, and F1-score.\\n\\n\\n246\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Nelson Miranda, Matheus Matos Machado, and Dilvan A. Moreira\\n\\n\\nHowever, these models provide substantial accuracy gains but come\\nwith cost, privacy, and service availability challenges.\\nOur findings highlight the potential of OntoDrug to be a valuable\\ntool for various stakeholders, including healthcare providers, pharmacists, and educators. OntoDrug can enhance clinical decisionmaking by seamlessly integrating medication information into electronic health records.\\n\\nFuture research should focus on expanding the prescription validation process to include warnings of possible harmful interactions\\nbetween prescribed drugs. Additionally, exploring the use of local open-source LLMs for entity recognition, as they can be more\\ncost-effective and privacy-preserving implementations of language\\nmodels.\\n### **9 ACKNOWLEDGMENTS**\\n\\nWe thank the Clinical Hospital of the Faculty of Medicine of Marília\\nand the Otorhinolaryngology department doctors for their support,\\ninfrastructure, and data management, which were crucial to our\\nresearch. This study was financed in part by the Coordenação de\\nAperfeiçoamento de Pessoal de Nível Superior (CAPES) Brazil – Finance Code 001 and by the São Paulo Research Foundation (FAPESP\\ngrants #2019/07665-4 and #2013/07375-0 CEPID).\\n### **REFERENCES**\\n\\n[1] Burhan Ud Din Abbasi, Iram Fatima, Hamid Mukhtar, Sharifullah Khan, Abdulaziz\\nAlhumam, and Hafiz Farooq Ahmad. 2022. Autonomous schema markups based\\non intelligent computing for search engine optimization. *PeerJ Computer Science*\\n8 (2022), e1163.\\n\\n[2] Agência Nacional de Vigilância Sanitária. 2004. *Política Vigente para a Regulamen-*\\n*tação de Medicamentos no Brasil* . Gabinete do Diretor-Presidente, Núcleo de Assessoramento em Comunicação Social e Institucional - Comin/Anvisa. https://bvsms.\\nsaude.gov.br/bvs/publicacoes/anvisa/manual_politica_medicamentos.pdf Impresso no Brasil. Permitida a reprodução parcial ou total desta obra, desde que\\ncitada a fonte..\\n\\n[3] Agência Nacional de Vigilância Sanitária (Anvisa). 2022. *Vocabulário Controlado*\\n*de Formas Farmacêuticas, Vias de Administração e Embalagens* . https://www.gov.\\nbr/anvisa/pt-br/centraisdeconteudo/publicacoes/medicamentos/publicacoessobre-medicamentos/vocabulario-controlado.pdf/view Accessed: 2022-05-19.\\n\\n[4] AI@Meta. 2024. Llama 3 Model Card. (2024). https://github.com/meta-llama/\\nllama3/blob/main/MODEL_CARD.md\\n\\n[5] Dean Allemang and James Hendler. 2011. *Semantic web for the working ontologist:*\\n*effective modeling in RDFS and OWL* . Elsevier.\\n\\n[6] Caio Viktor S Avila, Wellington Franco, Amanda DP Venceslau, Tulio Vidal\\nRolim, Vania MP Vidal, and Valéria M Pequeno. 2021. MediBot: an ontologybased chatbot to retrieve drug information and compare its prices. *Journal of*\\n*Information and Data Management* 12, 2 (2021).\\n\\n[7] Letícia de Andrade Barbosa. 2013. Assistência farmacêutica no sistema único de\\nsaúde. (2013).\\n\\n[8] Ricardo José Magalhães Barros, Marco Antônio de Araújo Fireman, and Mateus Rodrigues Westin. 2017. Relação nacional de medicamentos essenciais.\\n(2017).\\n\\n[9] Erik Bülow. 2020. coder: An R package for code-based item classification and\\ncategorization. *J. Open Source Softw.* 5 (2020), 2916. https://api.semanticscholar.\\norg/CorpusID:233302997\\n\\n[10] Wander de Almeida Limeira and Dilvan de Abreu Moreira. 2020. KGWE-A\\ncustomizable editor for OWL ontologies. In *Anais Estendidos do XXVI Simpósio*\\n*Brasileiro de Sistemas Multimídia e Web* . SBC, 91–95.\\n\\n[11] Daniela Moulin Maciel de Vasconcelos, Gabriela Costa Chaves, Thiago Botelho\\nAzeredo, and Rondineli Mendes da Silva. 2017. National Medicines Policy in\\nretrospective : a review of ( almost ) 20 years of implementation. https://api.\\nsemanticscholar.org/CorpusID:22550643\\n\\n[12] Agência Nacional de Vigilância Sanitária (Anvisa). 2020. *Novo Marco Regulatório*\\n*de Farmacovigilância* . https://www.gov.br/anvisa/pt-br/assuntos/noticias-anvisa/\\n2020/novo-marco-regulatorio-de-farmacovigilancia-confira Accessed: 2024-0604.\\n\\n[13] Frederico Freitas and Stefan Schulz. 2009. Ontologias, Web semântica e saúde.\\n*Revista Eletrônica de Comunicação, Informação & Inovação em Saúde* 3, 1 (2009).\\n\\n\\n\\n[14] Maurice Funk, Simon Hosemann, Jean Christoph Jung, and Carsten Lutz.\\n2023. Towards Ontology Construction with Language Models. *arXiv preprint*\\n*arXiv:2309.09898* (2023).\\n\\n[15] Thomas R Gruber. 1995. Toward principles for the design of ontologies used\\nfor knowledge sharing? *International journal of human-computer studies* 43, 5-6\\n(1995), 907–928.\\n\\n[16] Ramanathan V Guha, Dan Brickley, and Steve Macbeth. 2016. Schema. org:\\nevolution of structured data on the web. *Commun. ACM* 59, 2 (2016), 44–51.\\n\\n[17] Curtis E. Haas, Mary Ann Kliethermes, Lori T. Armistead, Craig J. Beavers,\\nChristie A. Schumacher, Lisa Smith, John A. Armitstead, Roshni P. Emmons,\\nLucy I. Darakjian, Krystal L. Edwards, and Michael Barr. 2023. Comprehensive\\nmedication management: Review and recommendations for quality measures.\\n*Journal of the American College of Clinical Pharmacy* 6 (2023), 404 – 415. https:\\n//api.semanticscholar.org/CorpusID:256718733\\n\\n[18] Josh Hanna, Eric Joseph, Mathias Brochhausen, and William R. Hogan. 2013.\\nBuilding a drug ontology based on RxNorm and other sources. *Journal of Biomedi-*\\n*cal Semantics* 4 (2013), 44 – 44. https://api.semanticscholar.org/CorpusID:2304775\\n\\n[19] María Herrero-Zazo, Janna Hastings, Isabel Segura-Bedmar, Samuel Croset,\\nPaloma Martínez, and Christoph Steinbeck. 2013. An Ontology for Drug-drug\\nInteractions. In *Workshop on Semantic Web Applications and Tools for Life Sciences* .\\nhttps://api.semanticscholar.org/CorpusID:187394\\n\\n[20] María Herrero-Zazo, Isabel Segura-Bedmar, Janna Hastings, and Paloma Martínez.\\n2015. DINTO: Using OWL Ontologies and SWRL Rules to Infer Drug-Drug\\nInteractions and Their Mechanisms. *Journal of chemical information and modeling*\\n55 8 (2015), 1698–707. https://api.semanticscholar.org/CorpusID:20599292\\n\\n[21] Adriana M. Ivama-Brummell, Daniella PINGRET-KIPMAN, Priscila G. Louly, and\\nRosiene R. Andrade. 2022. Medicines regulation, pricing and reimbursement\\nin Brazil. *Revista Brasileira de Farmácia Hospitalar e Serviços de Saúde* (2022).\\nhttps://api.semanticscholar.org/CorpusID:247722354\\n\\n[22] S Jagannatha, TV Suresh Kumar, and R RajaniKanth. [n. d.]. Comparative Study\\nof Back-End Vs Front-End System by Performance Analysis during Preliminary\\nDesign stages. ([n. d.]).\\n\\n[23] Christopher Manning and Hinrich Schutze. 1999. *Foundations of statistical natural*\\n*language processing* . MIT press.\\n\\n[24] Microsoft. 2023. .NET Framework 4.5. https://docs.microsoft.com/en-us/dotnet/\\nframework/. Accessed: January 2023.\\n\\n[25] Nelson Miranda and Dilvan de A. Moreira. 2024. Improving Medication Identification Accuracy and Regulatory Compliance through NLP and Ontologies:\\nAn Analysis of Otorhinolaryngology Prescriptions. In *2024 IEEE International*\\n*Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises*\\n*(WETICE)* . IEEE.\\n\\n[26] Ol’ga Alekseevna Mitina and Ivan Alexandrovich Yurchenkov. 2021. Data Classification in Medicine and Healthcare Service. *Artificial Intelligence in Intelligent*\\n*Systems* (2021). https://api.semanticscholar.org/CorpusID:237974497\\n\\n[27] Mark A Musen. 2015. The protégé project: a look back and a look forward. *AI*\\n*matters* 1, 4 (2015), 4–12.\\n\\n[28] Natalya Fridman Noy, Ray W Fergerson, and Mark A Musen. 2000. The knowledge\\nmodel of Protege-2000: Combining interoperability and flexibility. In *International*\\n*Conference on Knowledge Engineering and Knowledge Management* . Springer, 17–\\n32.\\n\\n[29] Natalya F Noy, Deborah L McGuinness, et al . 2001. Ontology development 101:\\nA guide to creating your first ontology.\\n\\n[30] OpenAI. 2024. GPT-4 Turbo: Advanced language model for natural language\\nunderstanding and generation. https://www.openai.com/models/gpt-4-turbo.\\nAccessed: 2024-03-22.\\n\\n[31] Christian Pachl, Nils Frank, Jan Breitbart, and Stefan Bräse. 2020. Overview of\\nchemical ontologies. *arXiv preprint arXiv:2002.03842* (2020).\\n\\n[32] Cecilia Reyes Peña, Mireya Tovar, Maricela Claudia Bravo, and Regina Motz.\\n2020. Drug Ontology for the Public Mexican Health System. In *SWH@ISWC* .\\nhttps://api.semanticscholar.org/CorpusID:229366837\\n\\n[33] Deborah L. Pestka, Caitlin K. Frail, Lindsay A. Sorge, Kylee A. Funk, Kristin K.\\nJanke, Mary T. Roth McClurg, and Todd D. Sorensen. 2020. Development of\\nthe comprehensive medication management practice management assessment\\ntool: A resource to assess and prioritize areas for practice improvement. *Journal*\\n*of the American College of Clinical Pharmacy* 3 (2020), 448 – 454. https://api.\\nsemanticscholar.org/CorpusID:208485233\\n\\n[34] Joselio Emar Araujo Queiroz, Robson Willian Melo Matos, Elivan Silva Souza,\\nPaula Xavier Santos, Laís Bié Pinto Bandeira, Daniel Catão Moreira Licio, Beatriz Faria Leao, and Rafael Santos Santana. 2023. Ontologia brasileira de medicamentos: rumo à padronização terminológica de medicamentos no Brasil. *JORNAL*\\n*DE ASSISTÊNCIA FARMACÊUTICA E FARMACOECONOMIA* 8, s. 2 (2023).\\n\\n[35] Bartira Dantas Rocha, Larysse Silva, Thais Batista, Everton Cavalcante, and\\nPorfírio Gomes. 2020. An ontology-based information model for multi-domain\\nsemantic modeling and analysis of smart city data. In *Proceedings of the Brazilian*\\n*Symposium on Multimedia and the Web* . 73–80.\\n\\n[36] Tomer Sagi, Matteo Lissandrini, T. Pedersen, and Katja Hose. 2022. A design\\nspace for RDF data representations. *The VLDB Journal* 31 (2022), 347 – 373.\\n\\n\\n247\\n\\n\\n-----\\n\\nOntoDrug: Enhancing Brazilian Health System Interoperability with a National Medication Ontology WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\nhttps://api.semanticscholar.org/CorpusID:246186959\\n\\n[37] Mark E. Sharp. 2017. Toward a comprehensive drug ontology: extraction of\\ndrug-indication relations from diverse information sources. *Journal of Biomedical*\\n*Semantics* 8 (2017). https://api.semanticscholar.org/CorpusID:26441866\\n\\n[38] S. Stelting and B. Maassen. 2020. Understanding Data Transfer Object (DTO)\\nin C#. https://code-maze.com/different-ways-handle-errors-net-core-webapi/.\\nAccessed: January 2023.\\n\\n[39] Clarence Tauro, Nagesswary Ganesan, Saumya Mishra, and Anupama Bhagwat.\\n2012. Article: Object Serialization: A Study of Techniques of Implementing\\nBinary Serialization in C++, Java and .NET. *International Journal of Computer*\\n*Applications* 45 (05 2012), 25–29.\\n\\n[40] Gemma Team, Thomas Mesnard, Cassidy Hardin, and Robert Dadashi et al.\\n2024. Gemma: Open Models Based on Gemini Research and Technology.\\n\\n\\narXiv:2403.08295\\n\\n[41] Ruben Verborgh and Max De Wilde. 2013. *Using openrefine* . Packt Publishing\\nLtd.\\n\\n[42] Zizhong Wei, Dongsheng Guo, Dengrong Huang, Qilai Zhang, Sijia Zhang, Kai\\nJiang, and Rui Li. 2023. Detecting and Mitigating the Ungrounded Hallucinations\\nin Text Generation by LLMs. In *Proceedings of the 2023 International Conference*\\n*on Artificial Intelligence, Systems and Network Security* . 77–81.\\n\\n[43] Marieke Wermuth. 2021. Terminological cooperation in the biomedical field.\\n*Terminology* (2021). https://api.semanticscholar.org/CorpusID:237833789\\n\\n[44] Yijia Zhang, Qingyu Chen, Zhihao Yang, Hongfei Lin, and Zhiyong Lu. 2019.\\nBioWordVec, improving biomedical word embeddings with subword information\\nand MeSH. *Scientific data* 6, 1 (2019), 52.\\n\\n\\n248\\n\\n\\n-----\\n\\n',\n",
       "  'artigo_tokenizado': ['#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'OntoDrug',\n",
       "   ':',\n",
       "   'Enhancing',\n",
       "   'Brazilian',\n",
       "   'Health',\n",
       "   'System',\n",
       "   'Interoperability',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   'with',\n",
       "   'a',\n",
       "   'National',\n",
       "   'Medication',\n",
       "   'Ontology',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Nelson',\n",
       "   'Miranda',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'nelson.miranda@usp.br',\n",
       "   'Institute',\n",
       "   'of',\n",
       "   'Mathematical',\n",
       "   'and',\n",
       "   'Computer',\n",
       "   'Sciences',\n",
       "   ',',\n",
       "   'USP',\n",
       "   'São',\n",
       "   'Carlos',\n",
       "   ',',\n",
       "   'São',\n",
       "   'Paulo',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'ABSTRACT',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Matheus',\n",
       "   'Matos',\n",
       "   'Machado',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'matheusmatos@usp.br',\n",
       "   'Institute',\n",
       "   'of',\n",
       "   'Mathematical',\n",
       "   'and',\n",
       "   'Computer',\n",
       "   'Sciences',\n",
       "   ',',\n",
       "   'USP',\n",
       "   'São',\n",
       "   'Carlos',\n",
       "   ',',\n",
       "   'São',\n",
       "   'Paulo',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Dilvan',\n",
       "   'A.',\n",
       "   'Moreira',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'dilvan@usp.br',\n",
       "   'Institute',\n",
       "   'of',\n",
       "   'Mathematical',\n",
       "   'and',\n",
       "   'Computer',\n",
       "   'Sciences',\n",
       "   ',',\n",
       "   'USP',\n",
       "   'São',\n",
       "   'Carlos',\n",
       "   ',',\n",
       "   'São',\n",
       "   'Paulo',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '\\n\\n\\n',\n",
       "   'This',\n",
       "   'paper',\n",
       "   'presents',\n",
       "   'OntoDrug',\n",
       "   ',',\n",
       "   'an',\n",
       "   'ontology',\n",
       "   'designed',\n",
       "   'to',\n",
       "   'enhance',\n",
       "   '\\n',\n",
       "   'medicine',\n",
       "   'management',\n",
       "   'in',\n",
       "   'Brazil',\n",
       "   'by',\n",
       "   'integrating',\n",
       "   'regulatory',\n",
       "   'frameworks',\n",
       "   'and',\n",
       "   'standardizing',\n",
       "   'terminologies',\n",
       "   '.',\n",
       "   'OntoDrug',\n",
       "   'improves',\n",
       "   'patient',\n",
       "   '\\n',\n",
       "   'safety',\n",
       "   'and',\n",
       "   'treatment',\n",
       "   'efficacy',\n",
       "   'by',\n",
       "   'accurately',\n",
       "   'identifying',\n",
       "   'and',\n",
       "   'classifying',\n",
       "   'medications',\n",
       "   'and',\n",
       "   'supporting',\n",
       "   'interoperability',\n",
       "   'with',\n",
       "   'health',\n",
       "   '\\n',\n",
       "   'information',\n",
       "   'systems',\n",
       "   '.',\n",
       "   'A',\n",
       "   'proof',\n",
       "   '-',\n",
       "   'of',\n",
       "   '-',\n",
       "   'concept',\n",
       "   'application',\n",
       "   'integrated',\n",
       "   '\\n',\n",
       "   'into',\n",
       "   'the',\n",
       "   'Hospital',\n",
       "   'das',\n",
       "   'Clínicas',\n",
       "   'de',\n",
       "   'Marília',\n",
       "   '’s',\n",
       "   'hospital',\n",
       "   'EHR',\n",
       "   'system',\n",
       "   '\\n',\n",
       "   'demonstrated',\n",
       "   'OntoDrug',\n",
       "   '’s',\n",
       "   'utility',\n",
       "   ',',\n",
       "   'achieving',\n",
       "   'high',\n",
       "   'precision',\n",
       "   'and',\n",
       "   'recall',\n",
       "   '.',\n",
       "   'An',\n",
       "   'experimental',\n",
       "   'study',\n",
       "   'using',\n",
       "   'large',\n",
       "   'language',\n",
       "   'models',\n",
       "   'grounded',\n",
       "   '\\n',\n",
       "   'on',\n",
       "   'the',\n",
       "   'ontology',\n",
       "   'achieved',\n",
       "   ',',\n",
       "   'using',\n",
       "   'GPT-4',\n",
       "   'turbo',\n",
       "   ',',\n",
       "   '0.97',\n",
       "   'precision',\n",
       "   ',',\n",
       "   '1.0',\n",
       "   '\\n',\n",
       "   'recall',\n",
       "   'and',\n",
       "   'an',\n",
       "   'F1',\n",
       "   '-',\n",
       "   'score',\n",
       "   'of',\n",
       "   '0.99',\n",
       "   '.',\n",
       "   'We',\n",
       "   'also',\n",
       "   'evaluated',\n",
       "   'open',\n",
       "   '-',\n",
       "   'source',\n",
       "   'models',\n",
       "   'llama3',\n",
       "   '-',\n",
       "   '8b',\n",
       "   ',',\n",
       "   'llama3',\n",
       "   '-',\n",
       "   '70b',\n",
       "   ',',\n",
       "   'and',\n",
       "   'gemma-7b',\n",
       "   '-',\n",
       "   'it',\n",
       "   '.',\n",
       "   'Their',\n",
       "   'performance',\n",
       "   '\\n',\n",
       "   'was',\n",
       "   'close',\n",
       "   'to',\n",
       "   'GPT-4',\n",
       "   '’s',\n",
       "   '.',\n",
       "   'The',\n",
       "   'significant',\n",
       "   'effectiveness',\n",
       "   'is',\n",
       "   'primarily',\n",
       "   'due',\n",
       "   '\\n',\n",
       "   'to',\n",
       "   'the',\n",
       "   'utilization',\n",
       "   'of',\n",
       "   'large',\n",
       "   'language',\n",
       "   'models',\n",
       "   '(',\n",
       "   'LLMs',\n",
       "   ')',\n",
       "   '.',\n",
       "   'While',\n",
       "   'using',\n",
       "   '\\n',\n",
       "   'these',\n",
       "   'large',\n",
       "   'language',\n",
       "   'models',\n",
       "   'enhanced',\n",
       "   'performance',\n",
       "   ',',\n",
       "   'challenges',\n",
       "   '\\n',\n",
       "   'related',\n",
       "   'to',\n",
       "   'cost',\n",
       "   ',',\n",
       "   'privacy',\n",
       "   ',',\n",
       "   'and',\n",
       "   'service',\n",
       "   'availability',\n",
       "   'were',\n",
       "   'identified',\n",
       "   '.',\n",
       "   'OntoDrug',\n",
       "   'represents',\n",
       "   'a',\n",
       "   'significant',\n",
       "   'advancement',\n",
       "   'in',\n",
       "   'Brazil',\n",
       "   '’s',\n",
       "   'medication',\n",
       "   '\\n',\n",
       "   'information',\n",
       "   'standardization',\n",
       "   'and',\n",
       "   'optimization',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'KEYWORDS',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Medication',\n",
       "   'Ontologies',\n",
       "   ',',\n",
       "   'Drug',\n",
       "   'Management',\n",
       "   ',',\n",
       "   'Semantic',\n",
       "   'Interoperability',\n",
       "   ',',\n",
       "   'Health',\n",
       "   'Informatics',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   '1',\n",
       "   'INTRODUCTION',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Medication',\n",
       "   'management',\n",
       "   'is',\n",
       "   'a',\n",
       "   'critical',\n",
       "   'component',\n",
       "   'of',\n",
       "   'healthcare',\n",
       "   ',',\n",
       "   'encompassing',\n",
       "   'the',\n",
       "   'entire',\n",
       "   'lifecycle',\n",
       "   'of',\n",
       "   'a',\n",
       "   'patient',\n",
       "   '’s',\n",
       "   'medications',\n",
       "   ',',\n",
       "   'from',\n",
       "   '\\n',\n",
       "   'procurement',\n",
       "   'to',\n",
       "   'discontinuation',\n",
       "   '[',\n",
       "   '33',\n",
       "   ']',\n",
       "   '.',\n",
       "   'This',\n",
       "   'cycle',\n",
       "   'includes',\n",
       "   'accurate',\n",
       "   '\\n',\n",
       "   'transcription',\n",
       "   'of',\n",
       "   'prescriptions',\n",
       "   ',',\n",
       "   'tracking',\n",
       "   'of',\n",
       "   'medication',\n",
       "   'lots',\n",
       "   'and',\n",
       "   'expiration',\n",
       "   'dates',\n",
       "   ',',\n",
       "   'precise',\n",
       "   'dispensation',\n",
       "   ',',\n",
       "   'and',\n",
       "   'detailed',\n",
       "   'administration',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Additionally',\n",
       "   ',',\n",
       "   'continuous',\n",
       "   'monitoring',\n",
       "   'and',\n",
       "   'adherence',\n",
       "   'checks',\n",
       "   'are',\n",
       "   '\\n',\n",
       "   'conducted',\n",
       "   'to',\n",
       "   'optimize',\n",
       "   'therapeutic',\n",
       "   'outcomes',\n",
       "   '.',\n",
       "   'Regular',\n",
       "   'reviews',\n",
       "   'are',\n",
       "   '\\n',\n",
       "   'performed',\n",
       "   'to',\n",
       "   'adjust',\n",
       "   'medication',\n",
       "   'plans',\n",
       "   'according',\n",
       "   'to',\n",
       "   'changing',\n",
       "   'patient',\n",
       "   'needs',\n",
       "   ',',\n",
       "   'leading',\n",
       "   'to',\n",
       "   'appropriate',\n",
       "   'discontinuation',\n",
       "   'of',\n",
       "   'medications',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'This',\n",
       "   'systematic',\n",
       "   'approach',\n",
       "   'is',\n",
       "   'essential',\n",
       "   'for',\n",
       "   'enhancing',\n",
       "   'patient',\n",
       "   'safety',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'treatment',\n",
       "   'efficacy',\n",
       "   ',',\n",
       "   'and',\n",
       "   'optimizing',\n",
       "   'healthcare',\n",
       "   'resources',\n",
       "   '[',\n",
       "   '17',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Despite',\n",
       "   'its',\n",
       "   'importance',\n",
       "   ',',\n",
       "   'the',\n",
       "   'medication',\n",
       "   'management',\n",
       "   'process',\n",
       "   'is',\n",
       "   '\\n',\n",
       "   'fraught',\n",
       "   'with',\n",
       "   'challenges',\n",
       "   '.',\n",
       "   'One',\n",
       "   'significant',\n",
       "   'issue',\n",
       "   'is',\n",
       "   'the',\n",
       "   'lack',\n",
       "   'of',\n",
       "   'standardization',\n",
       "   'and',\n",
       "   'controlled',\n",
       "   'vocabularies',\n",
       "   ',',\n",
       "   'leading',\n",
       "   'to',\n",
       "   'ambiguities',\n",
       "   'in',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'nomenclature',\n",
       "   'used',\n",
       "   'throughout',\n",
       "   'the',\n",
       "   'cycle',\n",
       "   '.',\n",
       "   'Such',\n",
       "   'ambiguities',\n",
       "   'can',\n",
       "   '\\n',\n",
       "   'result',\n",
       "   'in',\n",
       "   'communication',\n",
       "   'errors',\n",
       "   'between',\n",
       "   'prescribers',\n",
       "   ',',\n",
       "   'dispensers',\n",
       "   ',',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   ':',\n",
       "   'Proceedings',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Web',\n",
       "   '(',\n",
       "   'WebMe',\n",
       "   '\\n',\n",
       "   'dia’2024',\n",
       "   ')',\n",
       "   '.',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '.',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   ':',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   ',',\n",
       "   '2024',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'SBC',\n",
       "   '–',\n",
       "   'Sociedade',\n",
       "   'Brasileira',\n",
       "   'de',\n",
       "   'Computação',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '-',\n",
       "   '2753',\n",
       "   '\\n\\n\\n',\n",
       "   'and',\n",
       "   'patients',\n",
       "   ',',\n",
       "   'ultimately',\n",
       "   'compromising',\n",
       "   'patient',\n",
       "   'safety',\n",
       "   'and',\n",
       "   'treatment',\n",
       "   '\\n',\n",
       "   'effectiveness',\n",
       "   '.',\n",
       "   'Therefore',\n",
       "   ',',\n",
       "   'a',\n",
       "   'controlled',\n",
       "   'vocabulary',\n",
       "   'is',\n",
       "   'crucial',\n",
       "   'in',\n",
       "   'normalizing',\n",
       "   'terms',\n",
       "   'for',\n",
       "   'pharmaceutical',\n",
       "   'forms',\n",
       "   ',',\n",
       "   'administration',\n",
       "   'routes',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'packaging',\n",
       "   ',',\n",
       "   'and',\n",
       "   'measurement',\n",
       "   'units',\n",
       "   ',',\n",
       "   'ensuring',\n",
       "   'uniform',\n",
       "   'communication',\n",
       "   'and',\n",
       "   'reducing',\n",
       "   'errors',\n",
       "   '[',\n",
       "   '13',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Such',\n",
       "   'harmonization',\n",
       "   'can',\n",
       "   'reduce',\n",
       "   '\\n',\n",
       "   'errors',\n",
       "   'and',\n",
       "   'ambiguity',\n",
       "   'in',\n",
       "   'medication',\n",
       "   'prescription',\n",
       "   'and',\n",
       "   'administration',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'fostering',\n",
       "   'patient',\n",
       "   'safety',\n",
       "   'and',\n",
       "   'effective',\n",
       "   'treatment',\n",
       "   '.',\n",
       "   '[',\n",
       "   '43',\n",
       "   ']',\n",
       "   'Furthermore',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'system',\n",
       "   'supports',\n",
       "   'interoperability',\n",
       "   'with',\n",
       "   'other',\n",
       "   'health',\n",
       "   'information',\n",
       "   '\\n',\n",
       "   'systems',\n",
       "   ',',\n",
       "   'making',\n",
       "   'data',\n",
       "   'management',\n",
       "   'and',\n",
       "   'exchange',\n",
       "   'activities',\n",
       "   'more',\n",
       "   '\\n',\n",
       "   'effective',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'In',\n",
       "   'Brazil',\n",
       "   ',',\n",
       "   'managing',\n",
       "   'medications',\n",
       "   'is',\n",
       "   'challenging',\n",
       "   'due',\n",
       "   'to',\n",
       "   'the',\n",
       "   'presence',\n",
       "   'of',\n",
       "   'different',\n",
       "   'regulatory',\n",
       "   'frameworks',\n",
       "   'and',\n",
       "   'lists',\n",
       "   'such',\n",
       "   'as',\n",
       "   'the',\n",
       "   'DCB',\n",
       "   '\\n',\n",
       "   '(',\n",
       "   'Brazilian',\n",
       "   'Common',\n",
       "   'Denominations',\n",
       "   ')',\n",
       "   ',',\n",
       "   'CMED',\n",
       "   '(',\n",
       "   'Chamber',\n",
       "   'of',\n",
       "   'Medicines',\n",
       "   '\\n',\n",
       "   'Market',\n",
       "   'Regulation',\n",
       "   ')',\n",
       "   ',',\n",
       "   'and',\n",
       "   'RENAME',\n",
       "   '(',\n",
       "   'National',\n",
       "   'List',\n",
       "   'of',\n",
       "   'Essential',\n",
       "   'Medicines',\n",
       "   ')',\n",
       "   '.',\n",
       "   'Prescribing',\n",
       "   'drugs',\n",
       "   'from',\n",
       "   'RENAME',\n",
       "   'is',\n",
       "   'vital',\n",
       "   'as',\n",
       "   'it',\n",
       "   'guarantees',\n",
       "   '\\n',\n",
       "   'patients',\n",
       "   'access',\n",
       "   'to',\n",
       "   'treatments',\n",
       "   'that',\n",
       "   'are',\n",
       "   'both',\n",
       "   'effective',\n",
       "   'and',\n",
       "   'affordable',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'supporting',\n",
       "   'rational',\n",
       "   'drug',\n",
       "   'use',\n",
       "   'in',\n",
       "   'the',\n",
       "   'healthcare',\n",
       "   'system',\n",
       "   '[',\n",
       "   '11',\n",
       "   ']',\n",
       "   '.These',\n",
       "   '\\n',\n",
       "   'lists',\n",
       "   'standardize',\n",
       "   'drug',\n",
       "   'names',\n",
       "   ',',\n",
       "   'regulate',\n",
       "   'the',\n",
       "   'pharmaceutical',\n",
       "   'market',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'and',\n",
       "   'detail',\n",
       "   'essential',\n",
       "   'medications',\n",
       "   'for',\n",
       "   'public',\n",
       "   'health',\n",
       "   ',',\n",
       "   'respectively',\n",
       "   '[',\n",
       "   '21',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'However',\n",
       "   ',',\n",
       "   'integrating',\n",
       "   'these',\n",
       "   'regulatory',\n",
       "   'frameworks',\n",
       "   'into',\n",
       "   'a',\n",
       "   'cohesive',\n",
       "   'system',\n",
       "   'remains',\n",
       "   'a',\n",
       "   'challenge',\n",
       "   ',',\n",
       "   'often',\n",
       "   'resulting',\n",
       "   'in',\n",
       "   'inconsistent',\n",
       "   '\\n',\n",
       "   'medication',\n",
       "   'identification',\n",
       "   'and',\n",
       "   'classification',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'The',\n",
       "   'characteristics',\n",
       "   'of',\n",
       "   'medications',\n",
       "   'and',\n",
       "   'substances',\n",
       "   'dictate',\n",
       "   'their',\n",
       "   '\\n',\n",
       "   'classification',\n",
       "   'and',\n",
       "   'prescription',\n",
       "   'restrictions',\n",
       "   '.',\n",
       "   'Categories',\n",
       "   'include',\n",
       "   'generic',\n",
       "   'drugs',\n",
       "   ',',\n",
       "   'similar',\n",
       "   'drugs',\n",
       "   ',',\n",
       "   'biological',\n",
       "   'drugs',\n",
       "   ',',\n",
       "   'and',\n",
       "   'biosimilars',\n",
       "   ',',\n",
       "   'each',\n",
       "   'with',\n",
       "   '\\n',\n",
       "   'specific',\n",
       "   'prescription',\n",
       "   'levels',\n",
       "   'indicated',\n",
       "   'by',\n",
       "   'colored',\n",
       "   'stripes',\n",
       "   'on',\n",
       "   'their',\n",
       "   '\\n',\n",
       "   'packaging',\n",
       "   '[',\n",
       "   '2',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Additionally',\n",
       "   ',',\n",
       "   'regulatory',\n",
       "   'lists',\n",
       "   'categorize',\n",
       "   'controlled',\n",
       "   '\\n',\n",
       "   'substances',\n",
       "   'such',\n",
       "   'as',\n",
       "   'narcotics',\n",
       "   ',',\n",
       "   'psychotropic',\n",
       "   'drugs',\n",
       "   ',',\n",
       "   'anorexigenic',\n",
       "   'substances',\n",
       "   ',',\n",
       "   'immunosuppressants',\n",
       "   ',',\n",
       "   'antiretrovirals',\n",
       "   ',',\n",
       "   'anabolic',\n",
       "   'steroids',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'and',\n",
       "   'precursors',\n",
       "   'used',\n",
       "   'in',\n",
       "   'drug',\n",
       "   'manufacturing',\n",
       "   '.',\n",
       "   'These',\n",
       "   'classifications',\n",
       "   '\\n',\n",
       "   'ensure',\n",
       "   'responsible',\n",
       "   'prescription',\n",
       "   'and',\n",
       "   'dispensation',\n",
       "   'practices',\n",
       "   ',',\n",
       "   'adhering',\n",
       "   'to',\n",
       "   'health',\n",
       "   'and',\n",
       "   'legal',\n",
       "   'standards',\n",
       "   'to',\n",
       "   'manage',\n",
       "   'risks',\n",
       "   'associated',\n",
       "   'with',\n",
       "   '\\n',\n",
       "   'their',\n",
       "   'use',\n",
       "   '[',\n",
       "   '12',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'The',\n",
       "   'primary',\n",
       "   'objectives',\n",
       "   'of',\n",
       "   'this',\n",
       "   'research',\n",
       "   'were',\n",
       "   'to',\n",
       "   'develop',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'implement',\n",
       "   'the',\n",
       "   'OntoDrug',\n",
       "   'ontology',\n",
       "   'to',\n",
       "   'enhance',\n",
       "   'medication',\n",
       "   'management',\n",
       "   'within',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'healthcare',\n",
       "   'system',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'The',\n",
       "   'OntoDrug',\n",
       "   'ontology',\n",
       "   'aims',\n",
       "   'to',\n",
       "   'address',\n",
       "   'these',\n",
       "   'challenges',\n",
       "   'by',\n",
       "   '\\n',\n",
       "   'providing',\n",
       "   'a',\n",
       "   'framework',\n",
       "   'that',\n",
       "   'integrates',\n",
       "   'Brazil',\n",
       "   '’s',\n",
       "   'regulatory',\n",
       "   'lists',\n",
       "   'into',\n",
       "   '\\n',\n",
       "   'a',\n",
       "   'standardized',\n",
       "   ',',\n",
       "   'computer',\n",
       "   '-',\n",
       "   'readable',\n",
       "   'format',\n",
       "   '.',\n",
       "   'By',\n",
       "   'leveraging',\n",
       "   'controlled',\n",
       "   '\\n',\n",
       "   'vocabulary',\n",
       "   'and',\n",
       "   'coding',\n",
       "   'systems',\n",
       "   'like',\n",
       "   'the',\n",
       "   'CAS',\n",
       "   '-',\n",
       "   'Anatomical',\n",
       "   'Therapeutic',\n",
       "   'Chemical',\n",
       "   'Classification',\n",
       "   'System',\n",
       "   'and',\n",
       "   'RxNorm',\n",
       "   ',',\n",
       "   'OntoDrug',\n",
       "   '\\n',\n",
       "   'ensures',\n",
       "   'accurate',\n",
       "   'identification',\n",
       "   'and',\n",
       "   'disambiguation',\n",
       "   'of',\n",
       "   'medications',\n",
       "   '\\n\\n',\n",
       "   '[',\n",
       "   '26',\n",
       "   ']',\n",
       "   '.',\n",
       "   'This',\n",
       "   'ontology',\n",
       "   'enhances',\n",
       "   'the',\n",
       "   'organization',\n",
       "   'and',\n",
       "   'accessibility',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'pharmacological',\n",
       "   'knowledge',\n",
       "   'and',\n",
       "   'also',\n",
       "   'supports',\n",
       "   'interoperability',\n",
       "   'with',\n",
       "   '\\n',\n",
       "   'other',\n",
       "   'health',\n",
       "   'information',\n",
       "   ...],\n",
       "  'pos_tagger': '',\n",
       "  'lema': ['OntoDrug',\n",
       "   'enhance',\n",
       "   'Brazilian',\n",
       "   'Health',\n",
       "   'System',\n",
       "   'Interoperability',\n",
       "   'with',\n",
       "   'a',\n",
       "   'National',\n",
       "   'Medication',\n",
       "   'Ontology',\n",
       "   'Nelson',\n",
       "   'Miranda',\n",
       "   'nelson.miranda@usp.br',\n",
       "   'Institute',\n",
       "   'of',\n",
       "   'Mathematical',\n",
       "   'and',\n",
       "   'Computer',\n",
       "   'Sciences',\n",
       "   'USP',\n",
       "   'São',\n",
       "   'Carlos',\n",
       "   'São',\n",
       "   'Paulo',\n",
       "   'Brazil',\n",
       "   'ABSTRACT',\n",
       "   'Matheus',\n",
       "   'Matos',\n",
       "   'Machado',\n",
       "   'matheusmatos@usp.br',\n",
       "   'Institute',\n",
       "   'of',\n",
       "   'Mathematical',\n",
       "   'and',\n",
       "   'Computer',\n",
       "   'Sciences',\n",
       "   'USP',\n",
       "   'São',\n",
       "   'Carlos',\n",
       "   'São',\n",
       "   'Paulo',\n",
       "   'Brazil',\n",
       "   'Dilvan',\n",
       "   'A.',\n",
       "   'Moreira',\n",
       "   'dilvan@usp.br',\n",
       "   'Institute',\n",
       "   'of',\n",
       "   'Mathematical',\n",
       "   'and',\n",
       "   'Computer',\n",
       "   'Sciences',\n",
       "   'USP',\n",
       "   'São',\n",
       "   'Carlos',\n",
       "   'São',\n",
       "   'Paulo',\n",
       "   'Brazil',\n",
       "   'this',\n",
       "   'paper',\n",
       "   'present',\n",
       "   'OntoDrug',\n",
       "   'an',\n",
       "   'ontology',\n",
       "   'design',\n",
       "   'to',\n",
       "   'enhance',\n",
       "   'medicine',\n",
       "   'management',\n",
       "   'in',\n",
       "   'Brazil',\n",
       "   'by',\n",
       "   'integrate',\n",
       "   'regulatory',\n",
       "   'framework',\n",
       "   'and',\n",
       "   'standardize',\n",
       "   'terminology',\n",
       "   'OntoDrug',\n",
       "   'improve',\n",
       "   'patient',\n",
       "   'safety',\n",
       "   'and',\n",
       "   'treatment',\n",
       "   'efficacy',\n",
       "   'by',\n",
       "   'accurately',\n",
       "   'identify',\n",
       "   'and',\n",
       "   'classify',\n",
       "   'medication',\n",
       "   'and',\n",
       "   'support',\n",
       "   'interoperability',\n",
       "   'with',\n",
       "   'health',\n",
       "   'information',\n",
       "   'system',\n",
       "   'a',\n",
       "   'proof',\n",
       "   'of',\n",
       "   'concept',\n",
       "   'application',\n",
       "   'integrate',\n",
       "   'into',\n",
       "   'the',\n",
       "   'Hospital',\n",
       "   'das',\n",
       "   'Clínicas',\n",
       "   'de',\n",
       "   'Marília',\n",
       "   '’s',\n",
       "   'hospital',\n",
       "   'EHR',\n",
       "   'system',\n",
       "   'demonstrate',\n",
       "   'OntoDrug',\n",
       "   '’s',\n",
       "   'utility',\n",
       "   'achieve',\n",
       "   'high',\n",
       "   'precision',\n",
       "   'and',\n",
       "   'recall',\n",
       "   'an',\n",
       "   'experimental',\n",
       "   'study',\n",
       "   'use',\n",
       "   'large',\n",
       "   'language',\n",
       "   'model',\n",
       "   'ground',\n",
       "   'on',\n",
       "   'the',\n",
       "   'ontology',\n",
       "   'achieve',\n",
       "   'use',\n",
       "   'GPT-4',\n",
       "   'turbo',\n",
       "   '0.97',\n",
       "   'precision',\n",
       "   '1.0',\n",
       "   'recall',\n",
       "   'and',\n",
       "   'an',\n",
       "   'f1',\n",
       "   'score',\n",
       "   'of',\n",
       "   '0.99',\n",
       "   'we',\n",
       "   'also',\n",
       "   'evaluate',\n",
       "   'open',\n",
       "   'source',\n",
       "   'model',\n",
       "   'llama3',\n",
       "   '8b',\n",
       "   'llama3',\n",
       "   '70b',\n",
       "   'and',\n",
       "   'gemma-7b',\n",
       "   'it',\n",
       "   'their',\n",
       "   'performance',\n",
       "   'be',\n",
       "   'close',\n",
       "   'to',\n",
       "   'GPT-4',\n",
       "   '’s',\n",
       "   'the',\n",
       "   'significant',\n",
       "   'effectiveness',\n",
       "   'be',\n",
       "   'primarily',\n",
       "   'due',\n",
       "   'to',\n",
       "   'the',\n",
       "   'utilization',\n",
       "   'of',\n",
       "   'large',\n",
       "   'language',\n",
       "   'model',\n",
       "   'LLMs',\n",
       "   'while',\n",
       "   'use',\n",
       "   'these',\n",
       "   'large',\n",
       "   'language',\n",
       "   'model',\n",
       "   'enhance',\n",
       "   'performance',\n",
       "   'challenge',\n",
       "   'relate',\n",
       "   'to',\n",
       "   'cost',\n",
       "   'privacy',\n",
       "   'and',\n",
       "   'service',\n",
       "   'availability',\n",
       "   'be',\n",
       "   'identify',\n",
       "   'OntoDrug',\n",
       "   'represent',\n",
       "   'a',\n",
       "   'significant',\n",
       "   'advancement',\n",
       "   'in',\n",
       "   'Brazil',\n",
       "   '’s',\n",
       "   'medication',\n",
       "   'information',\n",
       "   'standardization',\n",
       "   'and',\n",
       "   'optimization',\n",
       "   'keyword',\n",
       "   'Medication',\n",
       "   'Ontologies',\n",
       "   'Drug',\n",
       "   'Management',\n",
       "   'Semantic',\n",
       "   'Interoperability',\n",
       "   'Health',\n",
       "   'Informatics',\n",
       "   '1',\n",
       "   'introduction',\n",
       "   'Medication',\n",
       "   'management',\n",
       "   'be',\n",
       "   'a',\n",
       "   'critical',\n",
       "   'component',\n",
       "   'of',\n",
       "   'healthcare',\n",
       "   'encompass',\n",
       "   'the',\n",
       "   'entire',\n",
       "   'lifecycle',\n",
       "   'of',\n",
       "   'a',\n",
       "   'patient',\n",
       "   '’s',\n",
       "   'medication',\n",
       "   'from',\n",
       "   'procurement',\n",
       "   'to',\n",
       "   'discontinuation',\n",
       "   '33',\n",
       "   'this',\n",
       "   'cycle',\n",
       "   'include',\n",
       "   'accurate',\n",
       "   'transcription',\n",
       "   'of',\n",
       "   'prescription',\n",
       "   'tracking',\n",
       "   'of',\n",
       "   'medication',\n",
       "   'lot',\n",
       "   'and',\n",
       "   'expiration',\n",
       "   'date',\n",
       "   'precise',\n",
       "   'dispensation',\n",
       "   'and',\n",
       "   'detailed',\n",
       "   'administration',\n",
       "   'additionally',\n",
       "   'continuous',\n",
       "   'monitoring',\n",
       "   'and',\n",
       "   'adherence',\n",
       "   'check',\n",
       "   'be',\n",
       "   'conduct',\n",
       "   'to',\n",
       "   'optimize',\n",
       "   'therapeutic',\n",
       "   'outcome',\n",
       "   'regular',\n",
       "   'review',\n",
       "   'be',\n",
       "   'perform',\n",
       "   'to',\n",
       "   'adjust',\n",
       "   'medication',\n",
       "   'plan',\n",
       "   'accord',\n",
       "   'to',\n",
       "   'change',\n",
       "   'patient',\n",
       "   'need',\n",
       "   'lead',\n",
       "   'to',\n",
       "   'appropriate',\n",
       "   'discontinuation',\n",
       "   'of',\n",
       "   'medication',\n",
       "   'this',\n",
       "   'systematic',\n",
       "   'approach',\n",
       "   'be',\n",
       "   'essential',\n",
       "   'for',\n",
       "   'enhance',\n",
       "   'patient',\n",
       "   'safety',\n",
       "   'treatment',\n",
       "   'efficacy',\n",
       "   'and',\n",
       "   'optimize',\n",
       "   'healthcare',\n",
       "   'resource',\n",
       "   '17',\n",
       "   'despite',\n",
       "   'its',\n",
       "   'importance',\n",
       "   'the',\n",
       "   'medication',\n",
       "   'management',\n",
       "   'process',\n",
       "   'be',\n",
       "   'fraught',\n",
       "   'with',\n",
       "   'challenge',\n",
       "   'one',\n",
       "   'significant',\n",
       "   'issue',\n",
       "   'be',\n",
       "   'the',\n",
       "   'lack',\n",
       "   'of',\n",
       "   'standardization',\n",
       "   'and',\n",
       "   'control',\n",
       "   'vocabulary',\n",
       "   'lead',\n",
       "   'to',\n",
       "   'ambiguity',\n",
       "   'in',\n",
       "   'the',\n",
       "   'nomenclature',\n",
       "   'use',\n",
       "   'throughout',\n",
       "   'the',\n",
       "   'cycle',\n",
       "   'such',\n",
       "   'ambiguity',\n",
       "   'can',\n",
       "   'result',\n",
       "   'in',\n",
       "   'communication',\n",
       "   'error',\n",
       "   'between',\n",
       "   'prescriber',\n",
       "   'dispenser',\n",
       "   'in',\n",
       "   'proceeding',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'web',\n",
       "   'WebMe',\n",
       "   'dia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   '2024',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'sbc',\n",
       "   'Sociedade',\n",
       "   'Brasileira',\n",
       "   'de',\n",
       "   'Computação',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '2753',\n",
       "   'and',\n",
       "   'patient',\n",
       "   'ultimately',\n",
       "   'compromise',\n",
       "   'patient',\n",
       "   'safety',\n",
       "   'and',\n",
       "   'treatment',\n",
       "   'effectiveness',\n",
       "   'therefore',\n",
       "   'a',\n",
       "   'control',\n",
       "   'vocabulary',\n",
       "   'be',\n",
       "   'crucial',\n",
       "   'in',\n",
       "   'normalize',\n",
       "   'term',\n",
       "   'for',\n",
       "   'pharmaceutical',\n",
       "   'form',\n",
       "   'administration',\n",
       "   'route',\n",
       "   'packaging',\n",
       "   'and',\n",
       "   'measurement',\n",
       "   'unit',\n",
       "   'ensure',\n",
       "   'uniform',\n",
       "   'communication',\n",
       "   'and',\n",
       "   'reduce',\n",
       "   'error',\n",
       "   '13',\n",
       "   'such',\n",
       "   'harmonization',\n",
       "   'can',\n",
       "   'reduce',\n",
       "   'error',\n",
       "   'and',\n",
       "   'ambiguity',\n",
       "   'in',\n",
       "   'medication',\n",
       "   'prescription',\n",
       "   'and',\n",
       "   'administration',\n",
       "   'foster',\n",
       "   'patient',\n",
       "   'safety',\n",
       "   'and',\n",
       "   'effective',\n",
       "   'treatment',\n",
       "   '43',\n",
       "   'furthermore',\n",
       "   'the',\n",
       "   'system',\n",
       "   'support',\n",
       "   'interoperability',\n",
       "   'with',\n",
       "   'other',\n",
       "   'health',\n",
       "   'information',\n",
       "   'system',\n",
       "   'make',\n",
       "   'data',\n",
       "   'management',\n",
       "   'and',\n",
       "   'exchange',\n",
       "   'activity',\n",
       "   'more',\n",
       "   'effective',\n",
       "   'in',\n",
       "   'Brazil',\n",
       "   'managing',\n",
       "   'medication',\n",
       "   'be',\n",
       "   'challenge',\n",
       "   'due',\n",
       "   'to',\n",
       "   'the',\n",
       "   'presence',\n",
       "   'of',\n",
       "   'different',\n",
       "   'regulatory',\n",
       "   'framework',\n",
       "   'and',\n",
       "   'list',\n",
       "   'such',\n",
       "   'as',\n",
       "   'the',\n",
       "   'DCB',\n",
       "   'brazilian',\n",
       "   'Common',\n",
       "   'Denominations',\n",
       "   'CMED',\n",
       "   'Chamber',\n",
       "   'of',\n",
       "   'Medicines',\n",
       "   'Market',\n",
       "   'Regulation',\n",
       "   'and',\n",
       "   'RENAME',\n",
       "   'National',\n",
       "   'List',\n",
       "   'of',\n",
       "   'Essential',\n",
       "   'Medicines',\n",
       "   'prescribe',\n",
       "   'drug',\n",
       "   'from',\n",
       "   'RENAME',\n",
       "   'be',\n",
       "   'vital',\n",
       "   'as',\n",
       "   'it',\n",
       "   'guarantee',\n",
       "   'patient',\n",
       "   'access',\n",
       "   'to',\n",
       "   'treatment',\n",
       "   'that',\n",
       "   'be',\n",
       "   'both',\n",
       "   'effective',\n",
       "   'and',\n",
       "   'affordable',\n",
       "   'support',\n",
       "   'rational',\n",
       "   'drug',\n",
       "   'use',\n",
       "   'in',\n",
       "   'the',\n",
       "   'healthcare',\n",
       "   'system',\n",
       "   '11',\n",
       "   '.these',\n",
       "   'list',\n",
       "   'standardize',\n",
       "   'drug',\n",
       "   'name',\n",
       "   'regulate',\n",
       "   'the',\n",
       "   'pharmaceutical',\n",
       "   'market',\n",
       "   'and',\n",
       "   'detail',\n",
       "   'essential',\n",
       "   'medication',\n",
       "   'for',\n",
       "   'public',\n",
       "   'health',\n",
       "   'respectively',\n",
       "   '21',\n",
       "   'however',\n",
       "   'integrate',\n",
       "   'these',\n",
       "   'regulatory',\n",
       "   'framework',\n",
       "   'into',\n",
       "   'a',\n",
       "   'cohesive',\n",
       "   'system',\n",
       "   'remain',\n",
       "   'a',\n",
       "   'challenge',\n",
       "   'often',\n",
       "   'result',\n",
       "   'in',\n",
       "   'inconsistent',\n",
       "   'medication',\n",
       "   'identification',\n",
       "   'and',\n",
       "   'classification',\n",
       "   'the',\n",
       "   'characteristic',\n",
       "   'of',\n",
       "   'medication',\n",
       "   'and',\n",
       "   'substance',\n",
       "   'dictate',\n",
       "   'their',\n",
       "   'classification',\n",
       "   'and',\n",
       "   'prescription',\n",
       "   'restriction',\n",
       "   'category',\n",
       "   'include',\n",
       "   'generic',\n",
       "   'drug',\n",
       "   'similar',\n",
       "   'drug',\n",
       "   'biological',\n",
       "   'drug',\n",
       "   'and',\n",
       "   'biosimilar',\n",
       "   'each',\n",
       "   'with',\n",
       "   'specific',\n",
       "   'prescription',\n",
       "   'level',\n",
       "   'indicate',\n",
       "   'by',\n",
       "   'colored',\n",
       "   'stripe',\n",
       "   'on',\n",
       "   'their',\n",
       "   'packaging',\n",
       "   '2',\n",
       "   'additionally',\n",
       "   'regulatory',\n",
       "   'list',\n",
       "   'categorize',\n",
       "   'control',\n",
       "   'substance',\n",
       "   'such',\n",
       "   'as',\n",
       "   'narcotic',\n",
       "   'psychotropic',\n",
       "   'drug',\n",
       "   'anorexigenic',\n",
       "   'substance',\n",
       "   'immunosuppressant',\n",
       "   'antiretroviral',\n",
       "   'anabolic',\n",
       "   'steroid',\n",
       "   'and',\n",
       "   'precursor',\n",
       "   'use',\n",
       "   'in',\n",
       "   'drug',\n",
       "   'manufacturing',\n",
       "   'these',\n",
       "   'classification',\n",
       "   'ensure',\n",
       "   'responsible',\n",
       "   'prescription',\n",
       "   'and',\n",
       "   'dispensation',\n",
       "   'practice',\n",
       "   'adhere',\n",
       "   'to',\n",
       "   'health',\n",
       "   'and',\n",
       "   'legal',\n",
       "   'standard',\n",
       "   'to',\n",
       "   'manage',\n",
       "   'risk',\n",
       "   'associate',\n",
       "   'with',\n",
       "   'their',\n",
       "   'use',\n",
       "   '12',\n",
       "   'the',\n",
       "   'primary',\n",
       "   'objective',\n",
       "   'of',\n",
       "   'this',\n",
       "   'research',\n",
       "   'be',\n",
       "   'to',\n",
       "   'develop',\n",
       "   'and',\n",
       "   'implement',\n",
       "   'the',\n",
       "   'OntoDrug',\n",
       "   'ontology',\n",
       "   'to',\n",
       "   'enhance',\n",
       "   'medication',\n",
       "   'management',\n",
       "   'within',\n",
       "   'the',\n",
       "   'brazilian',\n",
       "   'healthcare',\n",
       "   'system',\n",
       "   'the',\n",
       "   'OntoDrug',\n",
       "   'ontology',\n",
       "   'aim',\n",
       "   'to',\n",
       "   'address',\n",
       "   'these',\n",
       "   'challenge',\n",
       "   'by',\n",
       "   'provide',\n",
       "   'a',\n",
       "   'framework',\n",
       "   'that',\n",
       "   'integrate',\n",
       "   'Brazil',\n",
       "   '’s',\n",
       "   'regulatory',\n",
       "   'list',\n",
       "   'into',\n",
       "   'a',\n",
       "   'standardized',\n",
       "   'computer',\n",
       "   'readable',\n",
       "   'format',\n",
       "   'by',\n",
       "   'leverage',\n",
       "   'control',\n",
       "   'vocabulary',\n",
       "   'and',\n",
       "   'code',\n",
       "   'system',\n",
       "   'like',\n",
       "   'the',\n",
       "   'CAS',\n",
       "   'anatomical',\n",
       "   'Therapeutic',\n",
       "   'Chemical',\n",
       "   'Classification',\n",
       "   'System',\n",
       "   'and',\n",
       "   'RxNorm',\n",
       "   'OntoDrug',\n",
       "   'ensure',\n",
       "   'accurate',\n",
       "   'identification',\n",
       "   'and',\n",
       "   'disambiguation',\n",
       "   'of',\n",
       "   'medication',\n",
       "   '26',\n",
       "   'this',\n",
       "   'ontology',\n",
       "   'enhance',\n",
       "   'the',\n",
       "   'organization',\n",
       "   'and',\n",
       "   'accessibility',\n",
       "   'of',\n",
       "   'pharmacological',\n",
       "   'knowledge',\n",
       "   'and',\n",
       "   'also',\n",
       "   'support',\n",
       "   'interoperability',\n",
       "   'with',\n",
       "   'other',\n",
       "   'health',\n",
       "   'information',\n",
       "   'system',\n",
       "   'improve',\n",
       "   'data',\n",
       "   'management',\n",
       "   'and',\n",
       "   'exchange',\n",
       "   'activity',\n",
       "   '9',\n",
       "   '240',\n",
       "   'WebMedia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Nelson',\n",
       "   'Miranda',\n",
       "   'Matheus',\n",
       "   'Matos',\n",
       "   'Machado',\n",
       "   'and',\n",
       "   'Dilvan',\n",
       "   'A.',\n",
       "   'Moreira',\n",
       "   'to',\n",
       "   'demonstrate',\n",
       "   'the',\n",
       "   'utility',\n",
       "   'of',\n",
       "   'OntoDrug',\n",
       "   'a',\n",
       "   'proof',\n",
       "   'of',\n",
       "   'concept',\n",
       "   'application',\n",
       "   'be',\n",
       "   'develop',\n",
       "   'and',\n",
       "   'integrate',\n",
       "   'into',\n",
       "   'the',\n",
       "   'EHR',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Hospital',\n",
       "   'das',\n",
       "   'Clínicas',\n",
       "   'de',\n",
       "   'Marília',\n",
       "   'hospital',\n",
       "   'whenever',\n",
       "   'a',\n",
       "   'prescription',\n",
       "   'be',\n",
       "   'create',\n",
       "   'in',\n",
       "   'the',\n",
       "   'EHR',\n",
       "   'system',\n",
       "   'this',\n",
       "   'application',\n",
       "   'read',\n",
       "   'it',\n",
       "   'identify',\n",
       "   'the',\n",
       "   'medication',\n",
       "   'and',\n",
       "   'warn',\n",
       "   'physician',\n",
       "   'if',\n",
       "   'some',\n",
       "   'medication',\n",
       "   'be',\n",
       "   'not',\n",
       "   'list',\n",
       "   'in',\n",
       "   'the',\n",
       "   'Rename',\n",
       "   'suggest',\n",
       "   'alternative',\n",
       "   'whenever',\n",
       "   'possible',\n",
       "   'the',\n",
       "   'ontology',\n",
       "   'be',\n",
       "   'use',\n",
       "   'to',\n",
       "   'ground',\n",
       "   'the',\n",
       "   'LLMs',\n",
       "   'do',\n",
       "   'the',\n",
       "   'medicine',\n",
       "   'recognition',\n",
       "   'and',\n",
       "   'to',\n",
       "   'suggest',\n",
       "   'replacement',\n",
       "   'for',\n",
       "   'the',\n",
       "   'one',\n",
       "   'not',\n",
       "   'in',\n",
       "   'Rename',\n",
       "   'when',\n",
       "   'recognize',\n",
       "   'medicine',\n",
       "   'its',\n",
       "   'good',\n",
       "   'result',\n",
       "   'when',\n",
       "   'use',\n",
       "   'the',\n",
       "   'GPT-4',\n",
       "   'turbo',\n",
       "   'LLM',\n",
       "   'be',\n",
       "   '1.0',\n",
       "   'recall',\n",
       "   '0.974',\n",
       "   'precision',\n",
       "   'and',\n",
       "   '0.987',\n",
       "   'f1',\n",
       "   'score',\n",
       "   'the',\n",
       "   'physician',\n",
       "   'be',\n",
       "   'also',\n",
       "   'happy',\n",
       "   'with',\n",
       "   'the',\n",
       "   'replacement',\n",
       "   'suggestion',\n",
       "   'in',\n",
       "   'the',\n",
       "   'follow',\n",
       "   'section',\n",
       "   'we',\n",
       "   'discuss',\n",
       "   'the',\n",
       "   'development',\n",
       "   'of',\n",
       "   'the',\n",
       "   'OntoDrug',\n",
       "   'ontology',\n",
       "   'its',\n",
       "   'implementation',\n",
       "   'in',\n",
       "   'a',\n",
       "   'clinical',\n",
       "   'setting',\n",
       "   'and',\n",
       "   'its',\n",
       "   'potential',\n",
       "   'application',\n",
       "   'in',\n",
       "   'improve',\n",
       "   'medication',\n",
       "   'management',\n",
       "   'practice',\n",
       "   '2',\n",
       "   'relate',\n",
       "   'works',\n",
       "   'in',\n",
       "   'our',\n",
       "   'exploration',\n",
       "   'of',\n",
       "   'medication',\n",
       "   'ontology',\n",
       "   'we',\n",
       "   'review',\n",
       "   'several',\n",
       "   'key',\n",
       "   'work',\n",
       "   'that',\n",
       "   'focus',\n",
       "   'on',\n",
       "   'the',\n",
       "   'creation',\n",
       "   'and',\n",
       "   'utilization',\n",
       "   'of',\n",
       "   'drug',\n",
       "   'ontology',\n",
       "   'peña',\n",
       "   '2020',\n",
       "   '32',\n",
       "   'develop',\n",
       "   'a',\n",
       "   'Drug',\n",
       "   'Ontology',\n",
       "   'for',\n",
       "   'the',\n",
       "   'mexican',\n",
       "   'public',\n",
       "   'health',\n",
       "   'system',\n",
       "   'to',\n",
       "   'provide',\n",
       "   'a',\n",
       "   'well',\n",
       "   'structure',\n",
       "   'medical',\n",
       "   'knowledge',\n",
       "   'base',\n",
       "   'accessible',\n",
       "   'to',\n",
       "   'various',\n",
       "   'stakeholder',\n",
       "   'base',\n",
       "   'on',\n",
       "   'the',\n",
       "   'Basic',\n",
       "   'Table',\n",
       "   'and',\n",
       "   'Catalog',\n",
       "   'of',\n",
       "   'Medicines',\n",
       "   'from',\n",
       "   'the',\n",
       "   'Secretary',\n",
       "   'of',\n",
       "   'Public',\n",
       "   'Health',\n",
       "   'the',\n",
       "   'ontology',\n",
       "   'consist',\n",
       "   'of',\n",
       "   '64',\n",
       "   'class',\n",
       "   '5',\n",
       "   'object',\n",
       "   'property',\n",
       "   'and',\n",
       "   '18',\n",
       "   'datum',\n",
       "   'property',\n",
       "   'its',\n",
       "   'evaluation',\n",
       "   'focus',\n",
       "   'on',\n",
       "   'model',\n",
       "   'competence',\n",
       "   'and',\n",
       "   'quality',\n",
       "   'criterion',\n",
       "   'demonstrate',\n",
       "   'its',\n",
       "   'potential',\n",
       "   'for',\n",
       "   'enhance',\n",
       "   'medical',\n",
       "   'knowledge',\n",
       "   'management',\n",
       "   'a',\n",
       "   'unified',\n",
       "   'brazilian',\n",
       "   'drug',\n",
       "   'database',\n",
       "   'be',\n",
       "   'mean',\n",
       "   'to',\n",
       "   'advance',\n",
       "   'patient',\n",
       "   'safety',\n",
       "   'and',\n",
       "   'individualized',\n",
       "   ...]},\n",
       " {'titulo': 'Interoperability Testing Guide for the Internet of Things',\n",
       "  'informacoes_url': '',\n",
       "  'idioma': 'english',\n",
       "  'storage_key': '../articles/original/english/985-24758-1-10-20240923.pdf',\n",
       "  'author': 'Karina da Silva Castelo Branco; Valéria Lelli Leitão Dantas; and Liana Mara Carvalho',\n",
       "  'data_publicacao': '11-09-2024',\n",
       "  'resumo': 'The Internet of Things (IoT) has expanded the Internet by integrating smart objects, which when interconnected, can collect and share information to provide services. However, the intense data traffic and the diversity of interaction methods of smart objects, which vary based on the protocols and standards, bring several challenges for IoY Interoperability Testing. Such testing evaluates the capability of systems and devices to cooperate effectively. Regarding the challenges in IoT interoperability testing, we highlight the complexity of IoT architecture, the devices heterogeneity, and the guarantee of effective connectivity among the smart objects. In this context, this paper presents a interoperability testing guide for IoT. The guide was developed based on a literature review using systematic mapping and an analysis of real IoT environments. The guide’s evaluation consisted of two steps: (1) a structural assessment using the Technology Acceptance Model (TAM), and (2) a controlled experiment applying the guide to test a real IoT application. ###',\n",
       "  'keywords': 'Interoperability, Internet of Things, Interoperability Testing',\n",
       "  'referencias': ['[1] Home Assistant. 2024. Awaken your home. https://www.home-assistant.io/',\n",
       "   '[2] Luigi Atzori, Antonio Iera, and Giacomo Morabito. 2010. The internet of things:\\nA survey. *Computer networks* 54, 15 (2010), 2787–2805.',\n",
       "   '[3] Luigi Atzori, Antonio Iera, and Giacomo Morabito. 2010. The internet of things:\\nA survey. *Computer networks* 54, 15 (2010), 2787–2805.',\n",
       "   '[4] Miroslav Bures, Bestoun S. Ahmed, Vaclav Rechtberger, Matej Klima, Michal\\nTrnka, Miroslav Jaros, Xavier Bellekens, Dani Almog, and Pavel Herout. 2021.\\nPatrIoT: IoT Automated Interoperability and Integration Testing Framework. In\\n*2021 14th IEEE Conference on Software Testing, Verification and Validation (ICST)* .\\n454–459. https://doi.org/10.1109/ICST49551.2021.00059',\n",
       "   '[5] Eduardo Alves Lima Caldas. 2023. Checklist para avaliação da interoperabilidade\\nem dispositivos iot com foco em casas inteligentes. (2023).',\n",
       "   '[6] Liana M Carvalho, Valéria Lelli, and Rossana MC Andrade. 2022. Performance\\nTesting Guide for IoT Applications.. In *ICEIS (1)* . 667–678.',\n",
       "   '[7] CoAPthon. 2024. https://github.com/Tanganelli/CoAPthon',\n",
       "   '[8] Mariela Cortés, Raphael Saraiva, Marcia Souza, Patricia Mello, and Pamella Soares.\\n2019. Adoption of software testing in internet of things: A systematic literature mapping. In *Proceedings of the IV Brazilian Symposium on Systematic and*\\n*Automated Software Testing* . 3–11.',\n",
       "   '[9] Fred D. Davis. 1989. Perceived Usefulness, Perceived Ease of Use, and User\\nAcceptance of Information Technology. *MIS Quarterly* 13, 3 (1989), 319–340.',\n",
       "   '[10] Alexandra Desmoulin and César Viho. 2009. Formalizing interoperability for test\\ncase generation purpose. *International journal on software tools for technology*\\n*transfer* 11, 3 (2009), 261–267.',\n",
       "   '[11] freeRTOS. 2024. Simplifying Authenticated Cloud Connectivity for Any Device.\\nhttps://www.freertos.org/index.html',\n",
       "   '[12] Sara Nieves Matheu García, José Luis Hernández-Ramos, and Antonio F. Skarmeta.\\n2018. Test-based risk assessment and security certification proposal for the\\nInternet of Things. *2018 IEEE 4th World Forum on Internet of Things (WF-IoT)*\\n(2018), 641–646.',\n",
       "   '[13] Daniel Giusto, Antonio Iera, Giacomo Morabito, and Luigi Atzori. 2010. *The*\\n*internet of things: 20th Tyrrhenian workshop on digital communications* . Springer\\nScience & Business Media.',\n",
       "   '[14] Jayavardhana Gubbi, Rajkumar Buyya, Slaven Marusic, and Marimuthu\\nPalaniswami. 2013. Internet of Things (IoT): A vision, architectural elements, and\\nfuture directions. *Future generation computer systems* 29, 7 (2013), 1645–1660.',\n",
       "   '[15] Jon Atle Gulla, Stein L Tomassen, and Darijus Strasunskas. 2006. Semantic\\nInteroperability in the Norwegian Petroleum Industry.. In *ISTA* . 81–93.',\n",
       "   '[16] Prageeth Gunathilaka, Daisuke Mashima, and Binbin Chen. 2016. Softgrid: A\\nsoftware-based smart grid testbed for evaluating substation cybersecurity solutions. In *Proceedings of the 2nd ACM Workshop on Cyber-Physical Systems Security*\\n*and Privacy* . 113–124.',\n",
       "   '[17] Eclipse IoT. 2024. Open Source for IoT. Eclipse IoT technologies power the world’s\\nleading commercial IoT solutions. https://iot.eclipse.org/',\n",
       "   '[18] Open IoT. 2024. Open IoT Org. https://github.com/OpenIotOrg/openiot',\n",
       "   '[19] IoTIFY. 2024. IoTIFY Network Simulator. https://docs.iotify.io/',\n",
       "   '[20] ISO 15926. 2011. ISO 15926 - Industrial automation systems and integration Integration of life-cycle data for process plants including oil and gas production\\nfacilities. Geneva: ISO. https://www.iso.org/standard/50694.html Accessed on\\n28 Mar. 2023.',\n",
       "   '[21] ISO 25012. 2008. ISO/IEC 25012:2008 - Software Engineering - Software Product\\nQuality Requirements and Evaluation (SQuaRE) - Data Quality Model. International Organization for Standardization. https://www.iso.org/standard/35736.\\nhtml',\n",
       "   '[22] ISO/IEC 25010. 2011. ISO/IEC 25010:2011, Systems and software engineering —\\nSystems and software Quality Requirements and Evaluation (SQuaRE) — System\\nand software quality models.',\n",
       "   '[23] ISO/IEC 30141. 2018. ISO/IEC 30141:2018 - Systems and software engineering –\\nContent of systems and software life cycle process information products (Documentation). International Standard. https://www.iso.org/standard/65132.html',\n",
       "   '[24] Eunsook Eunah Kim and Sebastien Ziegler. 2017. Towards an open framework of\\nonline interoperability and performance tests for the Internet of Things. In *2017*\\n*Global Internet of Things Summit (GIoTS)* . 1–6. https://doi.org/10.1109/GIOTS.\\n2017.8016248',\n",
       "   '[25] Barbara Kitchenham, O Pearl Brereton, David Budgen, Mark Turner, John Bailey, and Stephen Linkman. 2009. Systematic literature reviews in software\\nengineering–a systematic literature review. *Information and software technology*\\n51, 1 (2009), 7–15.',\n",
       "   '[26] Maciej Kuzniar, Peter Peresini, Marco Canini, Daniele Venzano, and Dejan Kostic.\\n2012. A soft way for openflow switch interoperability testing. In *Proceedings of the*\\n*8th international conference on Emerging networking experiments and technologies* .\\n265–276.',\n",
       "   '[27] Friedemann Mattern and Christian Floerkemeier. 2010. From the Internet of\\nComputers to the Internet of Things. In *From active data management to event-*\\n*based systems and more* . Springer, 242–259.',\n",
       "   '[28] Javier Miranda, Niko Mäkitalo, Jose Garcia-Alonso, Javier Berrocal, Tommi Mikkonen, Carlos Canal, and Juan M Murillo. 2015. From the Internet of Things to the\\nInternet of People. *IEEE Internet Computing* 19, 2 (2015), 40–47.',\n",
       "   '[29] Rebeca Campos Motta, Káthia Marçal De Oliveira, and Guilherme Horta Travassos. 2017. Rethinking interoperability in contemporary software systems. In *2017*\\n*IEEE/ACM Joint 5th International Workshop on Software Engineering for Systems-*\\n*of-Systems and 11th Workshop on Distributed Software Development, Software*\\n*Ecosystems and Systems-of-Systems (JSOS)* . IEEE, 9–15.',\n",
       "   '[30] Rebeca C Motta, Káthia M de Oliveira, and Guilherme H Travassos. 2019. A\\nconceptual perspective on interoperability in context-aware software systems.\\n*Information and Software Technology* 114 (2019), 231–257.',\n",
       "   '[31] Srikanth Mujjiga and Srihari Sukumaran. 2007. Modelling and test generation\\nusing SAL for interoperability testing in Consumer Electronics. In *Proceedings of*\\n*the second workshop on Automated formal methods* . 32–40.',\n",
       "   '[32] Mohammad Abdur Razzaque, Marija Milojevic-Jevric, Andrei Palade, and Siobhán\\nClarke. 2015. Middleware for internet of things: a survey. *IEEE Internet of things*\\n*journal* 3, 1 (2015), 70–95.',\n",
       "   '[33] S Revell. 2013. Internet of things (IoT) and machine to machine communications\\n(M2M) challenges and opportunities. *Final Paper, London, UK Google Scholar*\\n(2013).',\n",
       "   '[34] Bruno P Santos, Lucas A Silva, CSFS Celes, João B Borges, Bruna S Peres Neto,\\nMarcos Augusto M Vieira, Luiz Filipe M Vieira, Olga N Goussevskaia, and Antonio\\nLoureiro. 2016. Internet das coisas: da teoria à prática. *Minicursos SBRC-Simpósio*\\n*Brasileiro de Redes de Computadores e Sistemas Distribuıdos* 31 (2016), 16.',\n",
       "   '[35] Luis Fernando Sayão and Carlos Henrique Marcondes. 2008. O desafio da interoperabilidade e as novas perspectivas para as bibliotecas digitais. *Transinformação*\\n20 (2008), 133–148.',\n",
       "   '[36] Sabrina Sicari, Alessandra Rizzardi, Luigi Alfredo Grieco, and Alberto CoenPorisini. 2015. Security, privacy and trust in Internet of Things: The road ahead.\\n*Computer networks* 76 (2015), 146–164.',\n",
       "   '[37] Valéria Martins da Silva. 2019. ScenarIoT: support for scenario specification of\\ninternet of things-based software systems. (2019).',\n",
       "   '[38] Delfina de Sá Soares. 2010. Interoperabilidade entre sistemas de informação na\\nAdministração Pública. (2010).',\n",
       "   '[39] Harald Sundmaeker, Patrick Guillemin, Peter Friess, Sylvie Woelfflé, et al . 2010.\\nVision and challenges for realising the Internet of Things. *Cluster of European*\\n*research projects on the internet of things, European Commision* 3, 3 (2010), 34–36.',\n",
       "   '[40] Tasmota. 2024. Open source firmware for ESP devices. https://tasmota.github.\\nio/docs/',\n",
       "   '[41] Souvik Pal Valentina Emilia Balas. 2020. *Healthcare Paradigms in the Internet of*\\n*Things Ecosystem* . Academic Press; 1st edition.',\n",
       "   '[42] Leila Cristina Weiss et al . 2019. Interoperabilidade semântica: uma análise sob a\\nperspectiva da abordagem ontológica de Willard van Orman Quine. (2019).',\n",
       "   '[43] Wireshark. 2024. The world’s most popular network protocol analyzer. https:\\n//www.wireshark.org/',\n",
       "   '[44] Claes Wohlin, Per Runeson, Martin Höst, Magnus C Ohlsson, Björn Regnell, and\\nAnders Wesslén. 2012. *Experimentation in Software Engineering* . Springer Science\\n& Business Media.',\n",
       "   '[45] Fatiha Zaidi, Emmanuel Bayse, and Ana Cavalli. 2009. Network protocol interoperability testing based on contextual signatures and passive testing. In *Proceedings*\\n*of the 2009 ACM symposium on Applied Computing* . 2–7.\\n\\n\\n196\\n\\n\\n-----'],\n",
       "  'text': '# **Interoperability Testing Guide for the Internet of Things**\\n\\n## Karina da Silva Castelo Branco\\n#### Group of Computer Networks, Software Engineering and Systems (GREat) Federal University of Ceará (UFC) Fortaleza, Ceará, Brazil karinascb@alu.ufc.br\\n### **ABSTRACT**\\n\\n## Valéria Lelli Leitão Dantas\\n#### Computer Science Department Group of Computer Networks, Software Engineering and Systems (GREat) Federal University of Ceará (UFC) Fortaleza, Ceará, Brazil valerialelli@ufc.br\\n\\n## Liana Mara Carvalho\\n#### Group of Computer Networks, Software Engineering and Systems (GREat) Federal University of Ceará (UFC) Fortaleza, Ceará, Brazil lianacdemenezes@gmail.com\\n\\n\\nThe Internet of Things (IoT) has expanded the Internet by integrating smart objects, which when interconnected, can collect and\\nshare information to provide services. However, the intense data\\ntraffic and the diversity of interaction methods of smart objects,\\nwhich vary based on the protocols and standards, bring several\\nchallenges for IoY Interoperability Testing. Such testing evaluates\\nthe capability of systems and devices to cooperate effectively. Regarding the challenges in IoT interoperability testing, we highlight\\nthe complexity of IoT architecture, the devices heterogeneity, and\\nthe guarantee of effective connectivity among the smart objects. In\\nthis context, this paper presents a interoperability testing guide for\\nIoT. The guide was developed based on a literature review using\\nsystematic mapping and an analysis of real IoT environments. The\\nguide’s evaluation consisted of two steps: (1) a structural assessment\\nusing the Technology Acceptance Model (TAM), and (2) a controlled\\nexperiment applying the guide to test a real IoT application.\\n### **KEYWORDS**\\n\\nInteroperability, Internet of Things, Interoperability Testing\\n### **1 INTRODUCTION**\\n\\nTechnology has significantly transformed human interactions with\\neveryday objects, expanding their communication. Internet access\\nhas also evolved, becoming more accessible and faster [ 32 ], positively contributing to these objects’ connectivity. This broad connectivity has driven the “Internet of Things (IoT)” to expand the Internet through the integration of smart objects. When interconnected,\\nthese objects have the capability to collect and share information,\\nenabling them to provide several services.\\nThe interconnection facilitated by IoT has outlined new perspectives regarding the smart objects that interact to automate various\\ndaily tasks. For example, refrigerators, air conditioners, smart locks\\nin the context of a smart home, or even autonomous vehicles operating independently, guided by a variety of smart sensors [13, 27].\\nNonetheless, the IoT scenario bring challenges for testing the\\nquality characteristics of IoT applications [ 5, 6, 8, 12, 24 ]. Security,\\nInteroperability, and Performance characteristics are identified as\\nthe most relevant, receiving considerable testing efforts [6].\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n© 2024 SBC – Brazilian Computing Society.\\nISSN 2966-2753\\n\\n\\nInteroperability in IoT is a crucial facet to be addressed in the\\ndevelopment of IoT systems [ 30 ]. It refers to the capability of two\\nor more systems to communicate effectively while ensuring data\\nintegrity [ 38 ]. Therefore, IoT interoperability testing verifies the\\nability of systems to interact consistently and cohesively. Such\\ntesting also involves evaluating their efficiency in communicating\\nand sharing information, ensuring resources can be accessed and\\nused appropriately accross different systems and organizations [ 23 ].\\nIn this context, the intense data traffic and the diverse interaction\\nmethods of smart objects, which vary depending on protocols (\\nMQTT, HTTP, CoAP, Bluetooth, and Zigbee), pose considerable\\nchallenges in interoperability testing. It is worth mentioning that\\nwhile Bluetooth and Zigbee are sometimes referred to as standards,\\nthey, like MQTT, HTTP, and CoAP, are also protocols that operate\\nat different tier of the communication stack [15].\\nFor example, considering an IoT smart home scenario with diverse devices ( voice assistants, security cameras, thermostats, smart\\nbulbs, and locks), promoting their communication and integration\\nacross different technologies for flawless operation poses challenges\\nregarding the architectural complexity, the device heterogeneity,\\nthe effective connectivity, and the management of bandwidth and\\ndevice resource limitations for real-time data processing.\\nTherefore, the goal of this paper is to present the *Interoperability*\\n*Testing Guide for IoT applications* . The following research questions\\n(RQ) are investigated in this work:\\n**RQ1.** How to evaluate the interoperability characteristic in IoT\\napplications?\\n**RQ2.** What approaches are used to evaluate interoperability in\\nIoT applications?\\n**RQ3.** What are the main challenges related to *Interoperability*\\ntesting in IoT applications?\\nThe development of the guide follows the methodology proposed\\nby [ 6 ], which suggests a general structure based on 11 key topics.\\nInitially, we performed a literature review to develop the guide’\\ncontent‘according that topics. This review also aimed to investigate the interoperability testing in different application domains.\\nNext, we focused on identifying interoperability subcharacteristics,\\nstandards and approaches used in IoT interoperability testing. The\\nfinal version of the guide is structured in 12 topics, including an\\nadditional topic called “Challenges of Interoperability Testing”.\\nTo evaluate the guide, two evaluations were conducted: (i) guide\\nevaluation using the Technology Acceptance Model [ 9 ] (TAM); and\\n(ii) a controlled experiment [ 44 ] to assess the use of the guide for\\ntesting the interoperability of an IoT application.\\n\\n\\n188\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Castelo Branco et al.\\n\\nThe remaining structure of this paper is organized as follows:\\nSection 2 presents a theoretical basis necessary to understand the\\nresearch. Section 3 describes the methodology. Section 4 presents\\nthe structure and the content of the guide. Section 5 presents the\\nguide evaluation and Section 6 addresses the research questions.\\nSection 8 discusses related work, and finally, Section 9 presents\\nconclusions and future work.\\n### **2 INTEROPERABILITY TESTING IN IOT**\\n\\n\\nThe Internet of Things (IoT) has transformed connectivity by linking devices globally, turning physical objects into smart, interconnected entities with advanced functions [ 28, 33, 34 ]. It facilitates\\nuniversal interaction by keeping people and smart objects connected through various networks [39].\\nIoT can be defined from three perspectives: devices, which are\\nthe sensing elements; the Internet, which serves as the network\\nframework; and semantics, which involves communication protocols [ 41 ]. It includes essential components such as data-collecting\\nsensors, connectivity technologies like Wi-Fi and 5G, and systems\\nfor data processing and storage in the cloud, all working together\\nto ensure efficient and secure information flow [2].\\nIoT encompasses various quality characteristics, with at least 27\\nidentified, including interoperability, security, performance, availability, and maintainability [ 6 ]. Interoperability allows devices from\\ndifferent manufacturers to communicate effectively, though it can\\nsometimes negatively impact security, especially concerning data\\nencryption.\\nTo evaluate these characteristics, they are often divided into\\nsubcharacteristics. For example, interoperability is broken down\\ninto four subcharacteristics: Communication Protocol, System Integration, Data Semantics, and Network Protocol, as suggested by\\n\\n[ 6, 23 ]. This division helps in assessing different aspects that affect\\ninteroperability.\\nIoT interoperability testing checks if devices and systems from\\nvarious manufacturers can work together seamlessly, exchanging\\ndata correctly and following specified behaviors. This testing is\\nchallenging due to the diverse range of devices, manufacturers,\\nand protocols, as well as the dynamic environments in which IoT\\napplications operate [10, 29, 35, 42].\\n### **3 METHODOLOGY**\\n\\nTo develop the *Interoperability* Testing Guide, we followed the\\nmethodology proposed by Carvalho et al. [ 6 ]. The authors recommend a structure organized into 11 topics.\\nOnce we defined the structure, we develop the guide’s content\\nbased on the instantiation methodology for an IoT characteristic\\n\\n[ 6 ], in our case, Interoperability. This methodology consist of six\\nactivities as shown in Figure 1.\\nThe first activity of **“Literature Review”** was conducted following the guidelines of systematic mapping [ 25 ]. Thus, a search\\n*string* focusing on the Interoperability characteristic was defined,\\nas presented in Table 1. This string was formulated by combining\\nkeywords such as: “Internet of Things”, “interoperability testing”,\\n“method”, “approach”, “challenge”, “tool”, among others.\\n\\n\\n**Figure 1: Methodology for instantiation. Adapted from [6].**\\n\\n**Table 1: String de busca**\\n\\n**Table 2: Inclusion and Exclusion Criteria**\\n\\n**ID** **Description**\\n\\nIC1 Studies related to intero p erabilit y testin g in IoT a pp s\\nIC2 Studies that p resent IoT testin g g uide or similar artifacts\\nEC1 Studies that do not address intero p erabilit y testin g in IoT\\nEC2 Studies for which the full text is not accessible\\n\\nEC3 Studies that are shortened versions of others\\n\\nIn the second activity, **“Analysis of the obtained studies”**,\\nwe used the online tool Parsifal [1] to analyze the data and organize\\nthe protocol elements such as research questions, search terms and\\nselection criteria. The analysis of the studies occurred in two phases.\\nIn the first round, the titles and abstracts of the 681 identified studies\\nwere read, resulting in the selection of 102 preliminary studies. Of\\nthese, 50 were from ACM, 24 from Scopus and 28 from IEEE.\\nThe data extraction focused on the following aspects: definitions of interoperability; correlations of interoperability with other\\ncharacteristics; challenges related to IoT interoperability; configuration requirements for IoT test environments; subdivision of\\ninteroperability into subcharacteristics; reported metrics and their\\ncalculation methods for evaluating interoperability; properties used\\nto assess these subcharacteristics; base test cases; cost-benefit analyses; and tools employed in the studies to assess interoperability.\\nThe third activity, **“Construction of the guide”**, consisted of\\ndeveloping the content of the guide regarding the 12 topics. The content was provided using the data obtained in the previous activity.\\nFor example, the topic named “Challenges of IoT Interoperability\\nTesting” presents several challenges identified in the literature review related to IoT architecture complex, Communication standards,\\nDevice heterogeneity and communication. Additionally, other topics were enriched with examples of test cases, explanations of metric\\nformulas, and suggested tools for automating the measurement.\\n\\n1 https://parsif.al/login/\\n\\n\\n189\\n\\n\\n-----\\n\\nInteroperability Testing Guide for the Internet of Things WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\nIn the fourth activity, **“Observation of IoT applications’ be-**\\n**havior”**, the use of the guide was analyzed in an IoT application.\\nThis app aimed to provide information to public transport users\\n( bus schedules and route details) through a mobile device that\\ncollects information using GPS. Using the guide to test this application allowed us to identify challenges in interoperability testing as\\nreported in the guide, as well as create new test cases.\\nIn the fifth activity, **“Evaluation of the Interoperability Gui-**\\n**de”**, the guide was evaluated in two steps to ensure its quality. Initially, an expert who had developed a similar guide for another IoT\\ncharacteristic assessed our guide, leading to several improvements.\\nWe also conducted two additional evaluations with undergraduate\\nand graduate students into a V&V course at a university: one using the TAM model and another through a controlled experiment\\ntesting a real IoT application.\\nIn the last activity, **“Refinement of the guide”**, we improved\\nthe guide based on the analysis and the evaluations conducted in\\nactivities 4 and 5 to enhance its utility and effectiveness.\\nThe materials used for the conception and evaluation of the\\nguide are available in the repository of this study [2] .\\n### **4 INTEROPERABILITY TESTING GUIDE**\\n\\nThe guide proposed in this paper was developed to cover a wide\\nvariety of testing scenarios related to *Interoperability* in IoT. The\\nstructure of the guide is based on the strucuture of 11 topics proposed by [ 6 ]. Thus, our guide addresses the following topics: “Characteristic definition”, “Correlation of Characteristcs”, “Challenges\\nof IoT Testing Interoperability”, “Test Environment configuration”,\\n“Impact of Subcharacteristics”, “Cost-Benefit”, “Tool Suggestions”\\nand “Example of Guide Use”. The guide also includes the recommended topics such as “Introduction”, “Instructions for Using the\\nGuide” and “References”.\\n\\nAs recommended by the authors, a characteristic should be divided into one or more susubcharacteristics. In the case of Interoperability, we divided it into four subcharacteristics according to ISO\\n30141:2018 [ 23 ]: “Data Semantics”, “Communication Protocol”, “System Integration”, “Network Protocol”. For each subcharacteristic,\\nthe guide includes topics such as “Definition”, “Contextualization”,\\n“Abstract Test Cases”, and “Measurements”.\\nFurthermore, we provided an extra topic named “Challenges of\\nIoT Interoperability Testing”. Therefore, the guide is structured into\\n12 distinct topics organized in sections. The full version of the guide\\nis available in the repository of this study [3] .\\nThe following subsections introduce the Interoperability Testing\\nGuide, with each one corresponding to a section of the guide.\\n### **4.1** **Interoperability Definition**\\n\\nThis section of the guide defines the characteristic of “Interoperability”. The goal is to standardize the knowledge about what will\\nbe tested and facilitate understanding for software engineers and\\nprofessionals from various fields. The section includes six definitions sourced from the results of the literature review and ISO/IEC\\n30141:2018 [23]. Below, we presented two of these definitions:\\n\\n2 https://drive.google.com/drive/folders/1y4wVQTvfIxoO0t0NG9tAF0_lwa5mwGFk?\\nusp=drive_link\\n3 https://drive.google.com/drive/folders/1DozFXdxNVxTbI5fs3pI81T0uQ2JEclJU?\\nusp=sharing\\n\\n\\n(1) Interoperability is *“the ability of a system to exchange data*\\n*and information with other systems without loss or corruption*\\n*of information.”* [20]\\n(2) Interoperability is *“the ability of different systems and organi-*\\n*zations to work together (exchange of information and actions)*\\n*effectively and efficiently.”* [23]\\n### **4.2 Correlation of Characteristics**\\n\\nThe systematic mapping presented by Carvalho et al. [ 6 ] identified a set of 27 quality characteristics related to IoT. From this set,\\n14 characteristics were selected based on their correlation with\\n\\nInteroperability, as illustrated in Figure 2. The Correlation of Characteristics section is essential, as it clarifies how Interoperability\\nis influenced by other IoT characteristics, guiding definitions, optimizations, and improvements in testing strategies.\\n\\n**Figure 2: Characteristics correlations with interoperability**\\n\\nThe correlations of IoT characteristics are organized into three\\ngroups: (i) positive (green rectangles), which indicate a favorable\\ninfluence on interoperability; (ii) negative (red rectangles), which\\nindicate the opposite; and (iii) variable (yellow rectangles), which\\ndepend on the context of a applications under test. As illustrated\\nin Figure 2, we identified four positive correlations, five negative\\ncorrelations, and five variable correlations regarding “Interoperability”. “Availability”, for instance, may enhance ’Interoperability’ with\\nsufficient servers but can have a negative effect otherwise. Similarly, \"Instability\" can have a negative impact on Interoperability by\\ncausing frequent communication failures, hindering the exchange\\nof information between IoT devices. Another example is ‘High ‘Performance”, which in certain situations can ensure smoother and\\nmore efficient communication between devices, but may also result\\nin system overload and reduced performance.\\nThe guide provides all definitions of the IoT characteristics correlated with “Interoperability”, which were extracted from ISO\\nstandards [ 21, 22 ]. Below, we present the definitions of the four\\ncharacteristics with positive correlations:\\n\\n(1) **Availability:** refers to the system’s ability to be operational\\nand accessible when needed, minimizing interruptions or\\nfailures.\\n\\n(2) **Instability:** relates to the system’s propensity for failures\\nor unexpected crashes, resulting in inconsistent operation.\\n\\n\\n190\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Castelo Branco et al.\\n\\n\\n(3) **Performance:** concerns the system’s ability to effectively\\nrespond to requests and operate within established limits,\\nensuring acceptable response times.\\n(4) **Portability:** refers to how easily a system can be transferred\\nor adapted to different environments or platforms without\\nsignificant loss of functionality.\\n### **4.3 Challenges of IoT Interoperability Testing**\\n\\nThe challenges regarding the “Interoperability Testing” in IoT are\\nillustrated in Figure 3. In our research, we have identified 20 challenges, which are categorized into three groups: (i) *challenges most*\\n*critical mentioned by literature* (red rows); (ii) *challenges most cited*\\nin the literature (green rows), and (iii) *challenges observed in pratical*\\nIoT applications (yellow rows).\\n\\n**Figure 3: Main challenges in interoperability testing**\\n\\nBelow, we present an example of a challenge per group:\\n\\n(1) **Communication Standards** . Common standards facilitate\\nintegration and communication between different devices\\nand systems, promoting interoperability. In IoT, heterogeneous devices operate using several protocols (MQTT, HTTP,\\nand CoAP) and standards (Bluetooth and Zigbee). This diversity impacts the complexity of testing activity, for instance,\\nmost testing tools cannot interact properly with IoT applications, leading to challenges in test automation. One relevant\\nissue posed by diverse communication standards is “How to\\nensure that IoT systems work correctly across all platforms\\nand technologies?” [3, 4].\\n(2) **Security** . Interoperability must ensure that communication\\nbetween devices is secure and reliable, adhering to security\\nstandards to protect the information exchanged. The challenge posed by security in IoT Devices is: “How to guarantee\\nsecurity in communication between IoT devices?” [36].\\n(3) **Proprietary Protocols** . These types of protocols pose challenges to interoperability with devices from other manufacturers, creating technical barriers. In this context, testing\\n\\n\\nactivities must address limited compatibility with standard\\nprotocols; restricted technical information; and higher costs\\nand complexity associated with customizing testing procedures. The main challenge is: “How to overcome the technical\\nbarriers imposed by proprietary protocols?” [14, 15].\\n### **4.4** **Test Environment Configuration**\\n\\nThe environment setup for IoT interoperability testing encompasses\\ndiverse devices, protocols, manufacturers, and network conditions.\\nThis section outlines the essential elements:\\n\\n**IoT Devices:** sensors and control devices compatible with various communication protocols.\\n**Network Infrastructure:** configuration of wireless networks\\n(Wi-Fi, Bluetooth), switches, routers, and firewalls to ensure secure\\nand reliable communication.\\n\\n**Actuators:** devices that perform actions based on sensor data\\nor external commands.\\n\\n**Decision and Command Application:** platform that coordinates devices from different manufacturers and protocols to ensure\\nsystem interoperability.\\n### **4.5 Interoperability Subcharacteristics**\\n\\nAccording to ISO/IEC 30141:2018[ 23 ], we divided Interoperability\\ninto four characteristics: (1) “Data Semantics”, which refers to the\\nability to interpret data, enabling systems and devices to share and\\nuse data efficiently; (2) ““Communication Protocols”, which concerns how devices must communicate with each other, ensuring the\\nefficient exchange of information and interoperability; (3) “System\\nIntegration”, is the process of standardizing the way different systems connect and communicate with each other; and (4) “Network\\nProtocol”, which allows communication and coordination between\\ndevices, services and applications on a network, defining rules for\\ndata exchange and synchronization.\\nSections 5 to 8 of the guide address these subcharacteristics with\\nthe following topics:\\n\\n  - *Definition* presents the explanation of each subcharacteristic.\\n\\n  - *Contextualization* describes the properties to evaluate each\\nsubcharacteristic, extracted from the literature.\\n\\n  - *Abstract Test Cases* provides structured and implementationindependent steps to test a subcharacteristic. We define 25\\ntest cases covering four subcharacteristics: six for “Data\\nSemantics”, seven for “Communication Protocols”, five for\\n“System Integration”, and seven for “Network Protocols”. An\\nexample of a test case is illustrated in Table 3.\\n\\n  - *Measurements* describes the methods and metrics to quantify\\nspecific aspects of the system under evaluation. In addition,\\n19 metrics were provided to assist the evaluation of *Interop-*\\n*erability* . Table 4 shows an example of a metric.\\n### **4.6 Impact of Subcharacteristics**\\n\\nThe four subcharacteristics of Interoperability may impact each\\nother. In the scope of validation, it is crucial to evaluate the impact\\namong interoperability subcharacteristics to prevent incompatible\\nsystems,, those that cannot communicate with each other.\\n\\n\\n191\\n\\n\\n-----\\n\\nInteroperability Testing Guide for the Internet of Things WebMedia’2024, Juiz de Fora, Brazil\\n\\n**Table 3: Example of an abstract test case**\\n\\n**Test Case 01 - TC01**\\n\\n**Title** Data Readin g\\n**Test** **Environ-** A network of heterogeneous IoT devices\\n\\n**ment**\\n\\n**Precondition** Devices connected to the same Wi-Fi\\n\\n1 - Start the mobile application\\n**Step-by-Step** 2 - Select reading of data\\n3 - Verif y the data dis p la y ed\\n**Postcondition** The data displayed on the mobile device\\nscreen should correspond to the same data\\nre q uested b y the actuator\\n\\n**Table 4: Example of a metric**\\n\\n\\n**Device ca** **p** **acit** **y** **- M01**\\n**Purpose** Evaluate the ability of different IoT devices to\\ninteract with each other effectivel y .\\n**Method** Perform interoperability tests with different IoT\\ndevice firmwares.\\n**Measure** **Success Rate =** (Number of Successful Interactions / Total Number of Interactions ) x 100\\n**Explanation** Calculates the success rate as percentage (%),\\nwhere the number of successful interactions is\\n\\ndivided by the total number of interactions and\\nmulti p lied b y 100 to obtain the % re p resentation.\\n**Reference** [ 23 ][ 2 ]\\n\\nThe influence of an subcharacteristic is contextual and depends\\non the application under test. Our guide provides an overview of the\\ncorrelations among the four subcharacteristics of Interoperability.\\n**Figure 4** shows the correlations between “Data Semantics” and\\nthe other three subcharacteristics. The colors represent the properties of each characteristic: (i) yellow for “Data Semantics“; (ii) green\\nfor System Integration”; (iii) blue for Communication Protocol”;\\nand (iv) red for Network Protocol”. We identified 25 properties to\\nthese characteristics: eight for “Data Semantics”, seven for “Communication Protocol”, five for “System Integration”, and five for\\n“Network Protocol”. The figure illustrates that when we evaluate\\na specific property of “Data Semantics”, we must consider related\\nproperties of the other subcharacteristics. For example, assessing\\n“Common Interpretation” property involves considering “Protocol\\nCompatibility”, “Data “Compatibility”, “Consistency” and “Compatibility”. Similarly, evaluating “Semantic Compatibility” implies\\nconsidering “Reliability” and “Scalability”.\\n### **4.7** **Cost-Benefit**\\n\\nThe cost-benefit (CB) calculation is based on the formula proposed\\nby [ 6 ]. According the authors, the “cost-benefit” evaluates the testing effort based on correlations of IoT characteristics. The CB calculation considers the impact of an IoT characteristic, associated\\nthe tests’ cost using specific parameters. Thus, the CB formula can\\nbe applied to evaluate the Interoperability characteristic, in which\\nwe identified 14 correlated characteristics (see subsection 4.2). The\\ncalculation of the CB formula is described below.\\n\\n\\n*𝐶𝑀𝐷* = *𝑎𝑣𝑒𝑟𝑎𝑔𝑒𝑐𝑜𝑠𝑡𝑜𝑓𝑠𝑡𝑎𝑛𝑑𝑎𝑟𝑑𝑖𝑧𝑒𝑑𝑚𝑒𝑎𝑠𝑢𝑟𝑒𝑚𝑒𝑛𝑡𝑠.* (5)\\n\\nThus, the Effort (ESF) is defined as follows:\\n\\n*𝐸𝑆𝐹* = ( *𝐶𝐶𝑇* + *𝐶𝑀𝐷* )/2 (6)\\n\\nThe result is analyzed from a Cartesian plane that varies from 0\\nto 1 and depends on the quadrant. In the CB of interoperability, the\\n\\n\\n**Figure 4: Impact of ’Data Semantics’ on subcharacteristics**\\n\\n*𝐶𝐼* = *𝑂𝑅𝐶* / *𝑅𝐶* (1)\\n\\n  - ORC: number of characteristics correlated to interoperability\\nprioritized in the application\\n\\n  - RC: total number of characteristics related to interoperability\\nImpact (CI) and effort metrics are used to calculate the costbenefit ratio, aiding in test prioritization. The calculation consists of\\nthe estimated cost of each test case based on the average execution\\ntime and the tester’s hourly rate, using the following formula:\\n\\n*𝐶𝑇𝑖* = *𝑇𝐶𝑖* ∗ *𝑉𝐻𝐶𝑖* (2)\\n\\n  - CTi: Estimated cost to execute the test case\\n\\n  - TCi: Average time of a tester to execute a test case\\n\\n  - VHCi: Value of the time of a tester who will execute the test\\n\\ncase\\n\\nWhen all CTs are completed, the maximum value found is obtained as follows:\\n\\n*𝑀𝐶𝑇* = *𝑚𝑎𝑥* ( *𝐶𝑇* ) (3)\\n\\n  - MCT: highest cost for performing the test case\\n\\n  - CT: all cost estimates\\n\\nNormalizes the average costs of the test cases as follows:\\n\\n\\n*𝐶𝐶𝑇* = (\\n\\n\\n*𝑛*\\n\\n*𝑖* =1\\n\\n∑︁\\n\\n\\n*𝐶𝑇* *𝑖* / *𝑀𝐶𝑇* )/ *𝑛* (4)\\n\\n*𝑖* =1\\n\\n∑︁\\n\\n\\n\\n - CCT: average cost of standardized test cases\\n\\n - CTi/MCT: estimated value of the cost of test case i normalized to the highest cost\\n\\n - n: number of test cases\\n\\nRepeat the process for the measurements to obtain the CMD:\\n\\n\\n192\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Castelo Branco et al.\\n\\n\\nx-axis represents Impact (CI) and the y-axis represents Effort (ESF).\\nTests are prioritized as follows: Group I (High Effort, Low Impact)\\nhas low priority; Group II (Low Effort, Low Impact) has medium\\npriority; Group III (High Effort, High Impact) has high priority; and\\nGroup IV (Low Effort, High Impact) has very high priority.\\n### **4.8 Tool Suggestions**\\n\\nIn the guide, we catalog a list of eight tools to test interoperability in\\nIoT applications. These tools include Eclipse IoT [ 17 ], OpenIoT[ 18 ],\\nWireshark [ 43 ], IoTIFY [ 19 ], CoAPthon [ 7 ], FreeRTOS [ 11 ], Tasmota\\n\\n[ 40 ], and Home Assistant [ 1 ]. Each tool is detailed with aspects\\nsuch as description, testing methodology, testing environment, test\\nexecution, type of license, and availability. Notably, IoTIFY is the\\nonly proprietary tool, while the others are open source.\\n### **4.9 Example of Guide Use**\\n\\nAn example use case for the Interoperability Testing Guide in an\\nIoT application is provided. The use of the guide aims to improve\\nstudent travel planning and reduce waiting times at bus stops on a\\nuniversity campus. To ensure effective interoperability, several test\\nscenarios address different aspects of this application. The steps to\\nconduct interoperability testing on this application include:\\n\\n(1) **Definitions of Interoperability:** Understand the fundamental definitions of interoperability outlined in the guide.\\n(2) **Characteristic Selection:** Identify the key characteristics\\nrelevant to the IoT application under test.\\n(3) **Environment Setup:** Prepare the test environment according to specified requirements.\\n(4) **Subcharacteristics and Properties:** Explore relevant subcharacteristics and select their properties for evaluation.\\n(5) **Impact of Subcharacteristics:** Assess how the chosen subcharacteristics might be affected by various decisions.\\n(6) **Metric Selection:** Choose appropriate metrics for evaluating the selected subcharacteristics.\\n(7) **Cost-Benefit Calculation:** Perform a cost-benefit analysis\\nto justify the necessary investments in testing.\\n(8) **Tool Utilization:** Consider the recommended tools for effective metrics collection.\\n\\n(9) **Test Execution:** Finalize and execute the abstract test cases\\nwith the provided data as planned.\\n\\nThe key interoperability characteristics prioritized for that IoT\\napplication under test (AUT) include availability, performance, security, portability, and systems integration. They are crucial for\\neffective operation in transport scheduling and real-time updates.\\nThe test environment configuration meet AUT requirements,\\ninvolving smart devices, actuators, and an external application for\\nreal-time location commands.\\n\\nThe selected metrics evaluate properties such as response time,\\nadaptability to different transport systems, security in data exchange, ease of integration with third-party systems, and platform\\nportability.\\nThe Cost-Benefit (CB) calculation uses hypothetical values to\\njustify investment in interoperability tests.\\nThe above example illustrates the application of the Interoperability guide in specific scenarios, emphasizing key characteristics,\\nmetrics, and cost-benefit analysis.\\n\\n### **5 GUIDE EVALUATION**\\n\\nTo evaluate the proposed guide, we conducted two evaluations:\\n\\n(1) A analysis of the guide’s structure using the Technology\\nAcceptance Model (TAM); and\\n(2) A controlled experiment with 18 students, 16 undergraduate\\nand 2 graduate, in a Software Verification and Validation\\n(V&V) course at a university.\\n\\nRegarding experience levels, 10 students have worked in both\\nindustry and academia, while 8 students are dedicated exclusively\\nto academia. In terms of interoperability testing knowledge, four\\nstudents had prior experience, whereas 14 students had experience\\nin testing non-functional requirements.\\nThe evaluation was conducted in the last two face-to-face classes\\n\\nof the V&V course, following the completion of the Validation module (unit, functional, and non-functional testing). To standardize\\nthe students’ knowledge of interoperability testing, the first class\\ncovered theoretical concepts and a practical application using real\\nIoT devices like Amazon Alexa. In the second class, both evaluations (1 and 2) were conducted in a real IoT application designed\\nto assist students plan their trips and reduce their waiting times at\\nbus stops on a university campus. The app features include route\\nand schedule visualization; real-time tracking; arrival estimation;\\nand stop location.\\nNext, we presented the two evaluations.\\n### **5.1 Evaluation using the TAM model**\\n\\nThe TAM model was used to evaluate the structure, acceptance,\\nand adaptation of users to the Interoperability Testing Guide. First,\\nwe presented the IoT application under test and the Guide in PDF\\nformat. Six of the 18 students in the second class evaluation did\\nnot attend the first leveling class and were invited only for the\\nTAM evaluation. They were organized into pairs to evaluate the\\nguide using the IoT application. After completing the evaluation,\\nthe students filled out the TAM questionnaire, consisting of 14\\nLikert Scale questions, covering 5 categories: *Perceived Usefulness*\\n(PU), which measures users’ perceptions of the guide’s utility for\\neffective testing; *Perceived Ease of Use* (PEOU), which assesses the\\nguide’s ease of understanding and learning; *Intention to Use in*\\n*the Future* (IU), which checks users’ intentions to future adoption\\nand recommendations of the guide; *Impact on Test Efficiency* (ITE),\\nwhich examines the guide’s contributions to test efficiency; and\\n*Overall Satisfaction* (OS).\\n### **5.2 Results of the TAM model**\\n\\nThe TAM model results are detailed in Figure 5 [4] . Using a scale\\nwhere “strongly disagree” is 1, “‘neutral” is 3 and “strongly agree” is\\n5, the global mode was 5. Thus, the \"Strongly Agree\" response was\\nmost frequent for most questions, indicating high acceptance and\\nsatisfaction with the Interoperability Testing Guide, demonstrating\\nits effectiveness and utility.\\nIn the *Perceived Usefulness*, all students fully agreed that the\\nguide is useful for conducting tests (Q1) and effective in planning\\nand specifying tests (Q2) in IoT. However, six students fully or\\npartially agreed on the guide’s effectiveness during test execution\\n\\n4 Chart generated with Likertplot tool . Available on: https://www.likertplot.com/\\n\\n\\n193\\n\\n\\n-----\\n\\nInteroperability Testing Guide for the Internet of Things WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\n**Figure 5: Results per question of TAM model**\\n\\n(Q3). Regarding *Perceived Ease of Use* of the guide, six students fully\\nor partially agreed that the structure and instructions are easy to\\nunderstand (Q4). Three students fully and two partially agreed that\\nthe learning curve was smooth (Q5), while one student partially\\ndisagreed. Four students fully and two partially agreed that the\\norganization of the topics and their sequence were clear (Q6). All\\nsix students agreed that guide facilitates the testing planning and\\nspecification (Q7), whereas five students fully and one partially\\nagreed that the guide facilitates the testing exectuion in IoT (Q8). In\\nthe *Intention to Use in the Future (IU)*, four students expressed their\\nintention to use the guide in future IoT test projects (Q9) and five\\nwould recommend it to their colleagues (Q10). In the terms of *Im-*\\n*pact on Test Efficiency*, four students fully and two partially agreed\\nthat the use of the guide contributed to the overall effectiveness of\\ninteroperatibitly tests in IoT (Q11). Regarding *Overall Satisfaction*,\\nfour students fully and two partially agreed that the guide’s instructions and approach to test planning were clear and understandable\\n(Q12), while for tests execution, two fully students and four partially\\nagreed. Additionally, five students fully and one partially agreed on\\ntheir satisfactions with using the Interoperability Test Guide (Q14).\\n### **5.3 Controlled Experiment**\\n\\nWe conducted the experiment with 12 students who attended the\\nfirst theoretical-practical class. They received materials, including\\na manual, presentation, videos, experiment design, failure report\\ntemplate, testing plan examples, and an explanation of the IoT app\\nunder test. In the second class, students were organized into pairs\\nand divided into two groups: G1 (using the guide) and G2 (without\\nthe guide). They individually answered a pre-test questionnaire to\\nassess their understanding of interoperability testing. The hypotheses of the experiment are as follows:\\n\\n  - **Null Hypotheses. H** **0,0**   - The structured guide-based approach to conducting interoperability testing activities requires the same effort as traditional interoperability testing.\\n**H** **0,1**  - The structured guide-based approach to conducting\\ninteroperability testing activities detects the same number\\nof IoT failures as traditional interoperability testing.\\n\\n\\n\\n  - **Alternative Hypotheses. H** **1,1**   - The structured guide-based\\napproach to conducting interoperability testing activities reduces testing effort more than traditional interoperability\\ntesting. H 1,1 : *Effort with the guide* *<* *Effort without the guide* .\\n**H** **1,2**  - The structured guide-based approach to conducting\\ninteroperability testing activities produces more effective\\ntest cases than traditional interoperability testing. H 1,2 : *Ef-*\\n*fectiveness of test cases with the guide* *>* *Effectiveness of test*\\n*cases without the guide* . **H** **1,3**    - The structured guide-based\\napproach to conducting interoperability testing activities\\nfinds more IoT failures than traditional interoperability testing. H 1,3 : *Number of IoT failures with the guide* *>* *Number of*\\n*IoT failures without the guide* .\\n\\n  - **Dependent Variables:** Test cases\\n\\n  - **Independent Variables:** Specific failures for IoT interoperability, effort in planning and executing tests.\\n### **5.4 Results of the Controlled Experiment**\\n\\nTable 5 gives an overview of the experiment results regarding the\\ngroups that used the guide (CG) and those that did not (SG). The\\nfigure shows the ID, planning time, number of test cases and reported IoT failures for each group. Planning time refers to the effort\\nspent on setting up the test environment, devising test scenarios,\\nchoosing metrics, and defining the test plan scope.\\n\\n**Table 5: Experiment’s results per group**\\n\\n**ID** **Time (min)** **Test Cases (#)** **IoT Failures (#)**\\n\\n**Group 1 - CG**\\n\\nCG1 50 8 3\\n\\nCG2 45 10 2\\n\\nCG3 40 6 0\\n\\n**Group 2 - SG**\\n\\nSG1 90 4 0\\n\\nSG2 60 3 0\\n\\nSG3 50 6 2\\n\\nBased on the analysis of the experiment data, the hypotheses\\nwere evaluated using the Student’s T-test [ 44 ]. The objective of the\\nhypothesis analysis is to verify if there is a significant difference\\n(p-value < 0.05) in the effort to plan the tests, the effectiveness of\\ntest cases, and the number of IoT failures between the participants\\nwho used the guide and those who did not.\\nRegarding **hypothesis H** **1,1**, planning time was collected to compare the efforts between the two groups (GC and SG). The comparison showed a statistically significant difference in planning effort\\n(p-value = 0.035), leading to the rejection of the null hypothesis H 0,1 .\\nThus, the alternative hypothesis H 1,1 is accepted, indicating that\\nthe guide-based approach significantly reduces the testing effort.\\nFor **hypothesis H** **1,2**, the effectiveness of the test cases generated\\nby each group was analyzed. The analysis revealed a statistically\\nsignificant difference (p-value = 0.023) between GC and SG, leading to the rejection of the null hypothesis H 0,2 . The alternative\\nhypothesis H 1,2 is accepted, indicating that the guide-based approach generates more effective test cases. Regarding **hypothesis**\\n\\n\\n194\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Castelo Branco et al.\\n\\n\\n**H** **1,3**, which addresses the identification of IoT faults, the number\\nof faults reported by each group was analyzed. The comparison\\nbetween CG and SG revealed statistically significant differences\\n(p-value = 0.03), leading to the rejection of the null hypothesis H 0,3 .\\nThus, the alternative hypothesis H 1,3 is accepted, indicating that\\nthe guide-based approach detects more IoT faults.\\nBased on the results of the statistical analysis, the null hypotheses\\nH 0,0, H 0,1, H 0,2 were rejected in favor of the alternative hypotheses\\nH 1,1, H 1,2, H 1,3 respectively. This result indicated that the structured\\nguide-based approach is more efficient in terms of effort, test case\\neffectiveness, and fault detection in IoT compared to traditional\\ninteroperability testing.\\n### **6 DISCUSSION**\\n\\nIn this section, the research questions are discussed.\\n**RQ1. How to evaluate the** ***Interoperability*** **characteristic**\\n**in IoT applications?**\\nTo evaluate *Interoperability* in IoT applications is crucial to carefully plan the tests. Thus, the test plan should provide the characaracteristics correlated to IoT, their impact in Interoperability, abstract\\ntest cases, and properties and metrics specific for IoT. Moreover,\\nthe plan should guide the proper configuration of the test environment to replicate real-world conditions. By covering these aspects\\naccurately, it is possible to conduct an effective evaluation of interoperability in IoT applications.\\n**RQ2. What are the testing approaches used to evaluate**\\n***Interoperability*** **in IoT applications?** Approaches to evaluate interoperability in IoT applications were found, including a checklist\\nmodel proposed by [ 5 ], framework-based evaluation as per [ 26 ],\\nand an automated test generation framework by [ 31 ]. However,\\nnone of these approaches are specifically tailored for IoT interoperability according to ISO [ 23 ] standards. This gap motivated this\\nwork to focus on interoperability in specific IoT contexts, guided\\nby a dedicated framework developed for this purpose.\\n**RQ3. What are the main challenges related to testing** ***In-***\\n***teroperability*** **in IoT applications?**\\nIn our literature review, we identified 20 challenges of interoperability testing in IoT. Based on these findings, we included a new\\nsection titled “Challenges of Interoperability Testing” in our guide\\nto explain the types of challenges found in interoperability testing.\\nThis section aims to assist users in identifying potential issues and\\nplan alternative solutions. Additionally, this section strengthens the\\nguide by becoming more tailored and comprehensive, specifically\\naddressing the specific challenges present in IoT environments.\\n### **7 THREATS TO VALIDITY**\\n\\nIn our research, we identified threats to validity related to the\\ncreation and evaluation of the guide, such as potential issues with\\nits generalization, participants’ varying levels of prior knowledge,\\nand the limited number of participants (six for TAM and 12 in the\\ncontrolled experiment). To mitigate these threats, we developed the\\nguide based on a comprehensive literature review using systematic\\nmapping guidelines. The two evaluations were conducted after\\nstudents completed a V&V course. We employed two evaluation\\nmethods: TAM model focused on students who did not attend the\\nfirst leveling class on Interoperability testing, and a controlled\\n\\n\\nexperiment involving diverse student profiles ( undergraduate and\\ngraduates students, and professionals) organized in two groups.\\n### **8 RELATED WORK**\\n\\nGiven the challenges outlined in Section 4, a literature review was\\nconducted to explore related studies addressing gaps in IoT interoperability testing.\\nZaid et al. [ 45 ] present a methodology based on contextual signatures for testing IoT interoperability. This approach focuses on\\nprotocol layers and interoperability properties through event correlation and signature verification. While their study includes environment configuration and test execution, it primarily emphasizes\\ntest execution for interoperability. In contrast, our guide offers\\ncomprehensive steps for test planning, specification, and execution.\\nOther studies, such as those by Caldas [ 5 ] and Silva et al. [ 37 ],\\nfocus on checklists for evaluating IoT interoperability. Caldas proposes a checklist for smart home devices, identifying interoperability levels and common devices. Silva et al. introduce ScenarIoT,\\na checklist for evaluating interactions in various environments,\\ncovering IoT components, requirements, and device interactions.\\nOur guide differs by providing a detailed framework of 12 topics,\\nincluding abstract test cases, properties, and metrics, and addresses\\ncorrelations between interoperability and other IoT characteristics.\\nGunathilaka et al. [ 16 ] propose a smart grid testing system for\\nevaluating the interoperability of security solutions in IoT. Their\\nmodel focuses on message translation and communication at gateways but lacks structured steps and does not correlate interoperability with other IoT characteristics.\\nCarvalho et al. [ 6 ] present a structured approach for testing IoT\\ncharacteristics, specifically for performance. We adapted this guidebased approach to develop our guide for testing interoperability,\\nincluding an additional topic on the challenges faced in this area.\\n### **9 CONCLUSION AND FUTURE WORK**\\n\\nIn this article, we present the IoT interoperability testing guide.\\nThis guide was developed based on literature reviews and ISO/IEC\\n30141:2018 [ 23 ]. In our research, we conducted two literature reviews: the first aimed to broadly understand and identify the challenges in IoT interoperability testing, while the second focused on\\ndeveloping the proposed guide.\\nThe guide was created following the methodology proposed by\\n\\n[ 6 ], which recommends a structure based on 11 topics. Our guide\\ncovers 12 topics, including one specifically for “IoT Interoperability\\nTesting Challenges”. These topics are organized into sections that\\ndefine Interoperability and address its four sub-characteristics: *Data*\\n*Semantics* ; *Communication Protocols* ; *System Integration* ; and *Net-*\\n*work Protocols* . For each feature, the guide provides related abstract\\ntest cases, property measurements.\\nWe evaluated the guide through two evaluations (TAM and experiment) conducted during a VV course. The results showed the\\nusefulness of the guide in assisting users with interoperability testing, identifying IoT failures in this context.\\nAs future work, we plan to conduct evaluations with industry\\nexperts to refine the practical use of the guide. We also intend to\\ndevelop a wiki to facilitate the use of the guide by automatically\\nproviding a test plan.\\n\\n\\n195\\n\\n\\n-----\\n\\nInteroperability Testing Guide for the Internet of Things WebMedia’2024, Juiz de Fora, Brazil\\n\\n### **REFERENCES**\\n\\n[1] Home Assistant. 2024. Awaken your home. https://www.home-assistant.io/\\n\\n[2] Luigi Atzori, Antonio Iera, and Giacomo Morabito. 2010. The internet of things:\\nA survey. *Computer networks* 54, 15 (2010), 2787–2805.\\n\\n[3] Luigi Atzori, Antonio Iera, and Giacomo Morabito. 2010. The internet of things:\\nA survey. *Computer networks* 54, 15 (2010), 2787–2805.\\n\\n[4] Miroslav Bures, Bestoun S. Ahmed, Vaclav Rechtberger, Matej Klima, Michal\\nTrnka, Miroslav Jaros, Xavier Bellekens, Dani Almog, and Pavel Herout. 2021.\\nPatrIoT: IoT Automated Interoperability and Integration Testing Framework. In\\n*2021 14th IEEE Conference on Software Testing, Verification and Validation (ICST)* .\\n454–459. https://doi.org/10.1109/ICST49551.2021.00059\\n\\n[5] Eduardo Alves Lima Caldas. 2023. Checklist para avaliação da interoperabilidade\\nem dispositivos iot com foco em casas inteligentes. (2023).\\n\\n[6] Liana M Carvalho, Valéria Lelli, and Rossana MC Andrade. 2022. Performance\\nTesting Guide for IoT Applications.. In *ICEIS (1)* . 667–678.\\n\\n[7] CoAPthon. 2024. https://github.com/Tanganelli/CoAPthon\\n\\n[8] Mariela Cortés, Raphael Saraiva, Marcia Souza, Patricia Mello, and Pamella Soares.\\n2019. Adoption of software testing in internet of things: A systematic literature mapping. In *Proceedings of the IV Brazilian Symposium on Systematic and*\\n*Automated Software Testing* . 3–11.\\n\\n[9] Fred D. Davis. 1989. Perceived Usefulness, Perceived Ease of Use, and User\\nAcceptance of Information Technology. *MIS Quarterly* 13, 3 (1989), 319–340.\\n\\n[10] Alexandra Desmoulin and César Viho. 2009. Formalizing interoperability for test\\ncase generation purpose. *International journal on software tools for technology*\\n*transfer* 11, 3 (2009), 261–267.\\n\\n[11] freeRTOS. 2024. Simplifying Authenticated Cloud Connectivity for Any Device.\\nhttps://www.freertos.org/index.html\\n\\n[12] Sara Nieves Matheu García, José Luis Hernández-Ramos, and Antonio F. Skarmeta.\\n2018. Test-based risk assessment and security certification proposal for the\\nInternet of Things. *2018 IEEE 4th World Forum on Internet of Things (WF-IoT)*\\n(2018), 641–646.\\n\\n[13] Daniel Giusto, Antonio Iera, Giacomo Morabito, and Luigi Atzori. 2010. *The*\\n*internet of things: 20th Tyrrhenian workshop on digital communications* . Springer\\nScience & Business Media.\\n\\n[14] Jayavardhana Gubbi, Rajkumar Buyya, Slaven Marusic, and Marimuthu\\nPalaniswami. 2013. Internet of Things (IoT): A vision, architectural elements, and\\nfuture directions. *Future generation computer systems* 29, 7 (2013), 1645–1660.\\n\\n[15] Jon Atle Gulla, Stein L Tomassen, and Darijus Strasunskas. 2006. Semantic\\nInteroperability in the Norwegian Petroleum Industry.. In *ISTA* . 81–93.\\n\\n[16] Prageeth Gunathilaka, Daisuke Mashima, and Binbin Chen. 2016. Softgrid: A\\nsoftware-based smart grid testbed for evaluating substation cybersecurity solutions. In *Proceedings of the 2nd ACM Workshop on Cyber-Physical Systems Security*\\n*and Privacy* . 113–124.\\n\\n[17] Eclipse IoT. 2024. Open Source for IoT. Eclipse IoT technologies power the world’s\\nleading commercial IoT solutions. https://iot.eclipse.org/\\n\\n[18] Open IoT. 2024. Open IoT Org. https://github.com/OpenIotOrg/openiot\\n\\n[19] IoTIFY. 2024. IoTIFY Network Simulator. https://docs.iotify.io/\\n\\n[20] ISO 15926. 2011. ISO 15926 - Industrial automation systems and integration Integration of life-cycle data for process plants including oil and gas production\\nfacilities. Geneva: ISO. https://www.iso.org/standard/50694.html Accessed on\\n28 Mar. 2023.\\n\\n[21] ISO 25012. 2008. ISO/IEC 25012:2008 - Software Engineering - Software Product\\nQuality Requirements and Evaluation (SQuaRE) - Data Quality Model. International Organization for Standardization. https://www.iso.org/standard/35736.\\nhtml\\n\\n[22] ISO/IEC 25010. 2011. ISO/IEC 25010:2011, Systems and software engineering —\\nSystems and software Quality Requirements and Evaluation (SQuaRE) — System\\nand software quality models.\\n\\n[23] ISO/IEC 30141. 2018. ISO/IEC 30141:2018 - Systems and software engineering –\\nContent of systems and software life cycle process information products (Documentation). International Standard. https://www.iso.org/standard/65132.html\\n\\n[24] Eunsook Eunah Kim and Sebastien Ziegler. 2017. Towards an open framework of\\nonline interoperability and performance tests for the Internet of Things. In *2017*\\n*Global Internet of Things Summit (GIoTS)* . 1–6. https://doi.org/10.1109/GIOTS.\\n2017.8016248\\n\\n[25] Barbara Kitchenham, O Pearl Brereton, David Budgen, Mark Turner, John Bailey, and Stephen Linkman. 2009. Systematic literature reviews in software\\nengineering–a systematic literature review. *Information and software technology*\\n51, 1 (2009), 7–15.\\n\\n[26] Maciej Kuzniar, Peter Peresini, Marco Canini, Daniele Venzano, and Dejan Kostic.\\n2012. A soft way for openflow switch interoperability testing. In *Proceedings of the*\\n*8th international conference on Emerging networking experiments and technologies* .\\n265–276.\\n\\n[27] Friedemann Mattern and Christian Floerkemeier. 2010. From the Internet of\\nComputers to the Internet of Things. In *From active data management to event-*\\n*based systems and more* . Springer, 242–259.\\n\\n\\n\\n[28] Javier Miranda, Niko Mäkitalo, Jose Garcia-Alonso, Javier Berrocal, Tommi Mikkonen, Carlos Canal, and Juan M Murillo. 2015. From the Internet of Things to the\\nInternet of People. *IEEE Internet Computing* 19, 2 (2015), 40–47.\\n\\n[29] Rebeca Campos Motta, Káthia Marçal De Oliveira, and Guilherme Horta Travassos. 2017. Rethinking interoperability in contemporary software systems. In *2017*\\n*IEEE/ACM Joint 5th International Workshop on Software Engineering for Systems-*\\n*of-Systems and 11th Workshop on Distributed Software Development, Software*\\n*Ecosystems and Systems-of-Systems (JSOS)* . IEEE, 9–15.\\n\\n[30] Rebeca C Motta, Káthia M de Oliveira, and Guilherme H Travassos. 2019. A\\nconceptual perspective on interoperability in context-aware software systems.\\n*Information and Software Technology* 114 (2019), 231–257.\\n\\n[31] Srikanth Mujjiga and Srihari Sukumaran. 2007. Modelling and test generation\\nusing SAL for interoperability testing in Consumer Electronics. In *Proceedings of*\\n*the second workshop on Automated formal methods* . 32–40.\\n\\n[32] Mohammad Abdur Razzaque, Marija Milojevic-Jevric, Andrei Palade, and Siobhán\\nClarke. 2015. Middleware for internet of things: a survey. *IEEE Internet of things*\\n*journal* 3, 1 (2015), 70–95.\\n\\n[33] S Revell. 2013. Internet of things (IoT) and machine to machine communications\\n(M2M) challenges and opportunities. *Final Paper, London, UK Google Scholar*\\n(2013).\\n\\n[34] Bruno P Santos, Lucas A Silva, CSFS Celes, João B Borges, Bruna S Peres Neto,\\nMarcos Augusto M Vieira, Luiz Filipe M Vieira, Olga N Goussevskaia, and Antonio\\nLoureiro. 2016. Internet das coisas: da teoria à prática. *Minicursos SBRC-Simpósio*\\n*Brasileiro de Redes de Computadores e Sistemas Distribuıdos* 31 (2016), 16.\\n\\n[35] Luis Fernando Sayão and Carlos Henrique Marcondes. 2008. O desafio da interoperabilidade e as novas perspectivas para as bibliotecas digitais. *Transinformação*\\n20 (2008), 133–148.\\n\\n[36] Sabrina Sicari, Alessandra Rizzardi, Luigi Alfredo Grieco, and Alberto CoenPorisini. 2015. Security, privacy and trust in Internet of Things: The road ahead.\\n*Computer networks* 76 (2015), 146–164.\\n\\n[37] Valéria Martins da Silva. 2019. ScenarIoT: support for scenario specification of\\ninternet of things-based software systems. (2019).\\n\\n[38] Delfina de Sá Soares. 2010. Interoperabilidade entre sistemas de informação na\\nAdministração Pública. (2010).\\n\\n[39] Harald Sundmaeker, Patrick Guillemin, Peter Friess, Sylvie Woelfflé, et al . 2010.\\nVision and challenges for realising the Internet of Things. *Cluster of European*\\n*research projects on the internet of things, European Commision* 3, 3 (2010), 34–36.\\n\\n[40] Tasmota. 2024. Open source firmware for ESP devices. https://tasmota.github.\\nio/docs/\\n\\n[41] Souvik Pal Valentina Emilia Balas. 2020. *Healthcare Paradigms in the Internet of*\\n*Things Ecosystem* . Academic Press; 1st edition.\\n\\n[42] Leila Cristina Weiss et al . 2019. Interoperabilidade semântica: uma análise sob a\\nperspectiva da abordagem ontológica de Willard van Orman Quine. (2019).\\n\\n[43] Wireshark. 2024. The world’s most popular network protocol analyzer. https:\\n//www.wireshark.org/\\n\\n[44] Claes Wohlin, Per Runeson, Martin Höst, Magnus C Ohlsson, Björn Regnell, and\\nAnders Wesslén. 2012. *Experimentation in Software Engineering* . Springer Science\\n& Business Media.\\n\\n[45] Fatiha Zaidi, Emmanuel Bayse, and Ana Cavalli. 2009. Network protocol interoperability testing based on contextual signatures and passive testing. In *Proceedings*\\n*of the 2009 ACM symposium on Applied Computing* . 2–7.\\n\\n\\n196\\n\\n\\n-----\\n\\n',\n",
       "  'artigo_tokenizado': ['#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Interoperability',\n",
       "   'Testing',\n",
       "   'Guide',\n",
       "   'for',\n",
       "   'the',\n",
       "   'Internet',\n",
       "   'of',\n",
       "   'Things',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Karina',\n",
       "   'da',\n",
       "   'Silva',\n",
       "   'Castelo',\n",
       "   'Branco',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Group',\n",
       "   'of',\n",
       "   'Computer',\n",
       "   'Networks',\n",
       "   ',',\n",
       "   'Software',\n",
       "   'Engineering',\n",
       "   'and',\n",
       "   'Systems',\n",
       "   '(',\n",
       "   'GREat',\n",
       "   ')',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Ceará',\n",
       "   '(',\n",
       "   'UFC',\n",
       "   ')',\n",
       "   'Fortaleza',\n",
       "   ',',\n",
       "   'Ceará',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   'karinascb@alu.ufc.br',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'ABSTRACT',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Valéria',\n",
       "   'Lelli',\n",
       "   'Leitão',\n",
       "   'Dantas',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Computer',\n",
       "   'Science',\n",
       "   'Department',\n",
       "   'Group',\n",
       "   'of',\n",
       "   'Computer',\n",
       "   'Networks',\n",
       "   ',',\n",
       "   'Software',\n",
       "   'Engineering',\n",
       "   'and',\n",
       "   'Systems',\n",
       "   '(',\n",
       "   'GREat',\n",
       "   ')',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Ceará',\n",
       "   '(',\n",
       "   'UFC',\n",
       "   ')',\n",
       "   'Fortaleza',\n",
       "   ',',\n",
       "   'Ceará',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   'valerialelli@ufc.br',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Liana',\n",
       "   'Mara',\n",
       "   'Carvalho',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Group',\n",
       "   'of',\n",
       "   'Computer',\n",
       "   'Networks',\n",
       "   ',',\n",
       "   'Software',\n",
       "   'Engineering',\n",
       "   'and',\n",
       "   'Systems',\n",
       "   '(',\n",
       "   'GREat',\n",
       "   ')',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Ceará',\n",
       "   '(',\n",
       "   'UFC',\n",
       "   ')',\n",
       "   'Fortaleza',\n",
       "   ',',\n",
       "   'Ceará',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   'lianacdemenezes@gmail.com',\n",
       "   '\\n\\n\\n',\n",
       "   'The',\n",
       "   'Internet',\n",
       "   'of',\n",
       "   'Things',\n",
       "   '(',\n",
       "   'IoT',\n",
       "   ')',\n",
       "   'has',\n",
       "   'expanded',\n",
       "   'the',\n",
       "   'Internet',\n",
       "   'by',\n",
       "   'integrating',\n",
       "   'smart',\n",
       "   'objects',\n",
       "   ',',\n",
       "   'which',\n",
       "   'when',\n",
       "   'interconnected',\n",
       "   ',',\n",
       "   'can',\n",
       "   'collect',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'share',\n",
       "   'information',\n",
       "   'to',\n",
       "   'provide',\n",
       "   'services',\n",
       "   '.',\n",
       "   'However',\n",
       "   ',',\n",
       "   'the',\n",
       "   'intense',\n",
       "   'data',\n",
       "   '\\n',\n",
       "   'traffic',\n",
       "   'and',\n",
       "   'the',\n",
       "   'diversity',\n",
       "   'of',\n",
       "   'interaction',\n",
       "   'methods',\n",
       "   'of',\n",
       "   'smart',\n",
       "   'objects',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'which',\n",
       "   'vary',\n",
       "   'based',\n",
       "   'on',\n",
       "   'the',\n",
       "   'protocols',\n",
       "   'and',\n",
       "   'standards',\n",
       "   ',',\n",
       "   'bring',\n",
       "   'several',\n",
       "   '\\n',\n",
       "   'challenges',\n",
       "   'for',\n",
       "   'IoY',\n",
       "   'Interoperability',\n",
       "   'Testing',\n",
       "   '.',\n",
       "   'Such',\n",
       "   'testing',\n",
       "   'evaluates',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'capability',\n",
       "   'of',\n",
       "   'systems',\n",
       "   'and',\n",
       "   'devices',\n",
       "   'to',\n",
       "   'cooperate',\n",
       "   'effectively',\n",
       "   '.',\n",
       "   'Regarding',\n",
       "   'the',\n",
       "   'challenges',\n",
       "   'in',\n",
       "   'IoT',\n",
       "   'interoperability',\n",
       "   'testing',\n",
       "   ',',\n",
       "   'we',\n",
       "   'highlight',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'complexity',\n",
       "   'of',\n",
       "   'IoT',\n",
       "   'architecture',\n",
       "   ',',\n",
       "   'the',\n",
       "   'devices',\n",
       "   'heterogeneity',\n",
       "   ',',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'guarantee',\n",
       "   'of',\n",
       "   'effective',\n",
       "   'connectivity',\n",
       "   'among',\n",
       "   'the',\n",
       "   'smart',\n",
       "   'objects',\n",
       "   '.',\n",
       "   'In',\n",
       "   '\\n',\n",
       "   'this',\n",
       "   'context',\n",
       "   ',',\n",
       "   'this',\n",
       "   'paper',\n",
       "   'presents',\n",
       "   'a',\n",
       "   'interoperability',\n",
       "   'testing',\n",
       "   'guide',\n",
       "   'for',\n",
       "   '\\n',\n",
       "   'IoT.',\n",
       "   'The',\n",
       "   'guide',\n",
       "   'was',\n",
       "   'developed',\n",
       "   'based',\n",
       "   'on',\n",
       "   'a',\n",
       "   'literature',\n",
       "   'review',\n",
       "   'using',\n",
       "   '\\n',\n",
       "   'systematic',\n",
       "   'mapping',\n",
       "   'and',\n",
       "   'an',\n",
       "   'analysis',\n",
       "   'of',\n",
       "   'real',\n",
       "   'IoT',\n",
       "   'environments',\n",
       "   '.',\n",
       "   'The',\n",
       "   '\\n',\n",
       "   'guide',\n",
       "   '’s',\n",
       "   'evaluation',\n",
       "   'consisted',\n",
       "   'of',\n",
       "   'two',\n",
       "   'steps',\n",
       "   ':',\n",
       "   '(',\n",
       "   '1',\n",
       "   ')',\n",
       "   'a',\n",
       "   'structural',\n",
       "   'assessment',\n",
       "   '\\n',\n",
       "   'using',\n",
       "   'the',\n",
       "   'Technology',\n",
       "   'Acceptance',\n",
       "   'Model',\n",
       "   '(',\n",
       "   'TAM',\n",
       "   ')',\n",
       "   ',',\n",
       "   'and',\n",
       "   '(',\n",
       "   '2',\n",
       "   ')',\n",
       "   'a',\n",
       "   'controlled',\n",
       "   '\\n',\n",
       "   'experiment',\n",
       "   'applying',\n",
       "   'the',\n",
       "   'guide',\n",
       "   'to',\n",
       "   'test',\n",
       "   'a',\n",
       "   'real',\n",
       "   'IoT',\n",
       "   'application',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'KEYWORDS',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Interoperability',\n",
       "   ',',\n",
       "   'Internet',\n",
       "   'of',\n",
       "   'Things',\n",
       "   ',',\n",
       "   'Interoperability',\n",
       "   'Testing',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   '1',\n",
       "   'INTRODUCTION',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Technology',\n",
       "   'has',\n",
       "   'significantly',\n",
       "   'transformed',\n",
       "   'human',\n",
       "   'interactions',\n",
       "   'with',\n",
       "   '\\n',\n",
       "   'everyday',\n",
       "   'objects',\n",
       "   ',',\n",
       "   'expanding',\n",
       "   'their',\n",
       "   'communication',\n",
       "   '.',\n",
       "   'Internet',\n",
       "   'access',\n",
       "   '\\n',\n",
       "   'has',\n",
       "   'also',\n",
       "   'evolved',\n",
       "   ',',\n",
       "   'becoming',\n",
       "   'more',\n",
       "   'accessible',\n",
       "   'and',\n",
       "   'faster',\n",
       "   '[',\n",
       "   '32',\n",
       "   ']',\n",
       "   ',',\n",
       "   'positively',\n",
       "   'contributing',\n",
       "   'to',\n",
       "   'these',\n",
       "   'objects',\n",
       "   '’',\n",
       "   'connectivity',\n",
       "   '.',\n",
       "   'This',\n",
       "   'broad',\n",
       "   'connectivity',\n",
       "   'has',\n",
       "   'driven',\n",
       "   'the',\n",
       "   '“',\n",
       "   'Internet',\n",
       "   'of',\n",
       "   'Things',\n",
       "   '(',\n",
       "   'IoT',\n",
       "   ')',\n",
       "   '”',\n",
       "   'to',\n",
       "   'expand',\n",
       "   'the',\n",
       "   'Internet',\n",
       "   'through',\n",
       "   'the',\n",
       "   'integration',\n",
       "   'of',\n",
       "   'smart',\n",
       "   'objects',\n",
       "   '.',\n",
       "   'When',\n",
       "   'interconnected',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'these',\n",
       "   'objects',\n",
       "   'have',\n",
       "   'the',\n",
       "   'capability',\n",
       "   'to',\n",
       "   'collect',\n",
       "   'and',\n",
       "   'share',\n",
       "   'information',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'enabling',\n",
       "   'them',\n",
       "   'to',\n",
       "   'provide',\n",
       "   'several',\n",
       "   'services',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'The',\n",
       "   'interconnection',\n",
       "   'facilitated',\n",
       "   'by',\n",
       "   'IoT',\n",
       "   'has',\n",
       "   'outlined',\n",
       "   'new',\n",
       "   'perspectives',\n",
       "   'regarding',\n",
       "   'the',\n",
       "   'smart',\n",
       "   'objects',\n",
       "   'that',\n",
       "   'interact',\n",
       "   'to',\n",
       "   'automate',\n",
       "   'various',\n",
       "   '\\n',\n",
       "   'daily',\n",
       "   'tasks',\n",
       "   '.',\n",
       "   'For',\n",
       "   'example',\n",
       "   ',',\n",
       "   'refrigerators',\n",
       "   ',',\n",
       "   'air',\n",
       "   'conditioners',\n",
       "   ',',\n",
       "   'smart',\n",
       "   'locks',\n",
       "   '\\n',\n",
       "   'in',\n",
       "   'the',\n",
       "   'context',\n",
       "   'of',\n",
       "   'a',\n",
       "   'smart',\n",
       "   'home',\n",
       "   ',',\n",
       "   'or',\n",
       "   'even',\n",
       "   'autonomous',\n",
       "   'vehicles',\n",
       "   'operating',\n",
       "   'independently',\n",
       "   ',',\n",
       "   'guided',\n",
       "   'by',\n",
       "   'a',\n",
       "   'variety',\n",
       "   'of',\n",
       "   'smart',\n",
       "   'sensors',\n",
       "   '[',\n",
       "   '13',\n",
       "   ',',\n",
       "   '27',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Nonetheless',\n",
       "   ',',\n",
       "   'the',\n",
       "   'IoT',\n",
       "   'scenario',\n",
       "   'bring',\n",
       "   'challenges',\n",
       "   'for',\n",
       "   'testing',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'quality',\n",
       "   'characteristics',\n",
       "   'of',\n",
       "   'IoT',\n",
       "   'applications',\n",
       "   '[',\n",
       "   '5',\n",
       "   ',',\n",
       "   '6',\n",
       "   ',',\n",
       "   '8',\n",
       "   ',',\n",
       "   '12',\n",
       "   ',',\n",
       "   '24',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Security',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'Interoperability',\n",
       "   ',',\n",
       "   'and',\n",
       "   'Performance',\n",
       "   'characteristics',\n",
       "   'are',\n",
       "   'identified',\n",
       "   'as',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'most',\n",
       "   'relevant',\n",
       "   ',',\n",
       "   'receiving',\n",
       "   'considerable',\n",
       "   'testing',\n",
       "   'efforts',\n",
       "   '[',\n",
       "   '6',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   ':',\n",
       "   'Proceedings',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Web',\n",
       "   '(',\n",
       "   'WebMe',\n",
       "   '\\n',\n",
       "   'dia’2024',\n",
       "   ')',\n",
       "   '.',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '.',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   ':',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   ',',\n",
       "   '2024',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'SBC',\n",
       "   '–',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '-',\n",
       "   '2753',\n",
       "   '\\n\\n\\n',\n",
       "   'Interoperability',\n",
       "   'in',\n",
       "   'IoT',\n",
       "   'is',\n",
       "   'a',\n",
       "   'crucial',\n",
       "   'facet',\n",
       "   'to',\n",
       "   'be',\n",
       "   'addressed',\n",
       "   'in',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'development',\n",
       "   'of',\n",
       "   'IoT',\n",
       "   'systems',\n",
       "   '[',\n",
       "   '30',\n",
       "   ']',\n",
       "   '.',\n",
       "   'It',\n",
       "   'refers',\n",
       "   'to',\n",
       "   'the',\n",
       "   'capability',\n",
       "   'of',\n",
       "   'two',\n",
       "   '\\n',\n",
       "   'or',\n",
       "   'more',\n",
       "   'systems',\n",
       "   'to',\n",
       "   'communicate',\n",
       "   'effectively',\n",
       "   'while',\n",
       "   'ensuring',\n",
       "   'data',\n",
       "   '\\n',\n",
       "   'integrity',\n",
       "   '[',\n",
       "   '38',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Therefore',\n",
       "   ',',\n",
       "   'IoT',\n",
       "   'interoperability',\n",
       "   'testing',\n",
       "   'verifies',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'ability',\n",
       "   'of',\n",
       "   'systems',\n",
       "   'to',\n",
       "   'interact',\n",
       "   'consistently',\n",
       "   'and',\n",
       "   'cohesively',\n",
       "   '.',\n",
       "   'Such',\n",
       "   '\\n',\n",
       "   'testing',\n",
       "   'also',\n",
       "   'involves',\n",
       "   'evaluating',\n",
       "   'their',\n",
       "   'efficiency',\n",
       "   'in',\n",
       "   'communicating',\n",
       "   '\\n',\n",
       "   'and',\n",
       "   'sharing',\n",
       "   'information',\n",
       "   ',',\n",
       "   'ensuring',\n",
       "   'resources',\n",
       "   'can',\n",
       "   'be',\n",
       "   'accessed',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'used',\n",
       "   'appropriately',\n",
       "   'accross',\n",
       "   'different',\n",
       "   'systems',\n",
       "   'and',\n",
       "   'organizations',\n",
       "   '[',\n",
       "   '23',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'In',\n",
       "   'this',\n",
       "   'context',\n",
       "   ',',\n",
       "   'the',\n",
       "   'intense',\n",
       "   'data',\n",
       "   'traffic',\n",
       "   'and',\n",
       "   'the',\n",
       "   'diverse',\n",
       "   'interaction',\n",
       "   '\\n',\n",
       "   'methods',\n",
       "   'of',\n",
       "   'smart',\n",
       "   'objects',\n",
       "   ',',\n",
       "   'which',\n",
       "   'vary',\n",
       "   'depending',\n",
       "   'on',\n",
       "   'protocols',\n",
       "   '(',\n",
       "   '\\n',\n",
       "   'MQTT',\n",
       "   ',',\n",
       "   'HTTP',\n",
       "   ',',\n",
       "   'CoAP',\n",
       "   ',',\n",
       "   'Bluetooth',\n",
       "   ',',\n",
       "   'and',\n",
       "   'Zigbee',\n",
       "   ')',\n",
       "   ',',\n",
       "   'pose',\n",
       "   'considerable',\n",
       "   '\\n',\n",
       "   'challenges',\n",
       "   'in',\n",
       "   'interoperability',\n",
       "   'testing',\n",
       "   '.',\n",
       "   'It',\n",
       "   'is',\n",
       "   'worth',\n",
       "   'mentioning',\n",
       "   'that',\n",
       "   '\\n',\n",
       "   'while',\n",
       "   'Bluetooth',\n",
       "   'and',\n",
       "   'Zigbee',\n",
       "   'are',\n",
       "   'sometimes',\n",
       "   'referred',\n",
       "   'to',\n",
       "   'as',\n",
       "   'standards',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'they',\n",
       "   ',',\n",
       "   'like',\n",
       "   'MQTT',\n",
       "   ',',\n",
       "   'HTTP',\n",
       "   ',',\n",
       "   'and',\n",
       "   'CoAP',\n",
       "   ',',\n",
       "   'are',\n",
       "   'also',\n",
       "   'protocols',\n",
       "   'that',\n",
       "   'operate',\n",
       "   '\\n',\n",
       "   'at',\n",
       "   'different',\n",
       "   'tier',\n",
       "   'of',\n",
       "   'the',\n",
       "   'communication',\n",
       "   'stack',\n",
       "   '[',\n",
       "   '15',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'For',\n",
       "   'example',\n",
       "   ',',\n",
       "   'considering',\n",
       "   'an',\n",
       "   'IoT',\n",
       "   'smart',\n",
       "   'home',\n",
       "   'scenario',\n",
       "   'with',\n",
       "   'diverse',\n",
       "   'devices',\n",
       "   '(',\n",
       "   'voice',\n",
       "   'assistants',\n",
       "   ',',\n",
       "   'security',\n",
       "   'cameras',\n",
       "   ',',\n",
       "   'thermostats',\n",
       "   ',',\n",
       "   'smart',\n",
       "   '\\n',\n",
       "   'bulbs',\n",
       "   ',',\n",
       "   'and',\n",
       "   'locks',\n",
       "   ')',\n",
       "   ',',\n",
       "   'promoting',\n",
       "   'their',\n",
       "   'communication',\n",
       "   'and',\n",
       "   'integration',\n",
       "   '\\n',\n",
       "   'across',\n",
       "   'different',\n",
       "   'technologies',\n",
       "   'for',\n",
       "   'flawless',\n",
       "   'operation',\n",
       "   'poses',\n",
       "   'challenges',\n",
       "   '\\n',\n",
       "   'regarding',\n",
       "   'the',\n",
       "   'architectural',\n",
       "   'complexity',\n",
       "   ',',\n",
       "   'the',\n",
       "   'device',\n",
       "   'heterogeneity',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'effective',\n",
       "   'connectivity',\n",
       "   ',',\n",
       "   'and',\n",
       "   'the',\n",
       "   'management',\n",
       "   'of',\n",
       "   'bandwidth',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'device',\n",
       "   'resource',\n",
       "   'limitations',\n",
       "   'for',\n",
       "   'real',\n",
       "   '-',\n",
       "   'time',\n",
       "   'data',\n",
       "   'processing',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Therefore',\n",
       "   ',',\n",
       "   'the',\n",
       "   'goal',\n",
       "   'of',\n",
       "   'this',\n",
       "   'paper',\n",
       "   'is',\n",
       "   'to',\n",
       "   'present',\n",
       "   'the',\n",
       "   '*',\n",
       "   'Interoperability',\n",
       "   '*',\n",
       "   '\\n',\n",
       "   '*',\n",
       "   'Testing',\n",
       "   'Guide',\n",
       "   'for',\n",
       "   'IoT',\n",
       "   'applications',\n",
       "   '*',\n",
       "   '.',\n",
       "   'The',\n",
       "   'following',\n",
       "   'research',\n",
       "   'questions',\n",
       "   '\\n',\n",
       "   '(',\n",
       "   'RQ',\n",
       "   ')',\n",
       "   'are',\n",
       "   'investigated',\n",
       "   'in',\n",
       "   'this',\n",
       "   'work',\n",
       "   ':',\n",
       "   '\\n',\n",
       "   '*',\n",
       "   '*',\n",
       "   'RQ1',\n",
       "   '.',\n",
       "   '*',\n",
       "   '*',\n",
       "   'How',\n",
       "   'to',\n",
       "   'evaluate',\n",
       "   'the',\n",
       "   'interoperability',\n",
       "   'characteristic',\n",
       "   'in',\n",
       "   'IoT',\n",
       "   '\\n',\n",
       "   'applications',\n",
       "   '?',\n",
       "   '\\n',\n",
       "   '*',\n",
       "   '*',\n",
       "   'RQ2',\n",
       "   '.',\n",
       "   '*',\n",
       "   '*',\n",
       "   'What',\n",
       "   'approaches',\n",
       "   'are',\n",
       "   'used',\n",
       "   'to',\n",
       "   'evaluate',\n",
       "   'interoperability',\n",
       "   'in',\n",
       "   '\\n',\n",
       "   'IoT',\n",
       "   'applications',\n",
       "   '?',\n",
       "   '\\n',\n",
       "   '*',\n",
       "   '*',\n",
       "   'RQ3',\n",
       "   '.',\n",
       "   '*',\n",
       "   '*',\n",
       "   'What',\n",
       "   'are',\n",
       "   'the',\n",
       "   'main',\n",
       "   'challenges',\n",
       "   'related',\n",
       "   'to',\n",
       "   '*',\n",
       "   'Interoperability',\n",
       "   '*',\n",
       "   '\\n',\n",
       "   'testing',\n",
       "   'in',\n",
       "   'IoT',\n",
       "   'applications',\n",
       "   '?',\n",
       "   '\\n',\n",
       "   'The',\n",
       "   'development',\n",
       "   'of',\n",
       "   'the',\n",
       "   'guide',\n",
       "   'follows',\n",
       "   'the',\n",
       "   'methodology',\n",
       "   'proposed',\n",
       "   '\\n',\n",
       "   'by',\n",
       "   '[',\n",
       "   '6',\n",
       "   ']',\n",
       "   ',',\n",
       "   'which',\n",
       "   'suggests',\n",
       "   'a',\n",
       "   'general',\n",
       "   'structure',\n",
       "   'based',\n",
       "   'on',\n",
       "   '11',\n",
       "   'key',\n",
       "   'topics',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Initially',\n",
       "   ',',\n",
       "   'we',\n",
       "   ...],\n",
       "  'pos_tagger': '',\n",
       "  'lema': ['Interoperability',\n",
       "   'Testing',\n",
       "   'Guide',\n",
       "   'for',\n",
       "   'the',\n",
       "   'internet',\n",
       "   'of',\n",
       "   'thing',\n",
       "   'Karina',\n",
       "   'da',\n",
       "   'Silva',\n",
       "   'Castelo',\n",
       "   'Branco',\n",
       "   'Group',\n",
       "   'of',\n",
       "   'Computer',\n",
       "   'Networks',\n",
       "   'Software',\n",
       "   'Engineering',\n",
       "   'and',\n",
       "   'Systems',\n",
       "   'GREat',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Ceará',\n",
       "   'UFC',\n",
       "   'Fortaleza',\n",
       "   'Ceará',\n",
       "   'Brazil',\n",
       "   'karinascb@alu.ufc.br',\n",
       "   'ABSTRACT',\n",
       "   'Valéria',\n",
       "   'Lelli',\n",
       "   'Leitão',\n",
       "   'Dantas',\n",
       "   'Computer',\n",
       "   'Science',\n",
       "   'Department',\n",
       "   'Group',\n",
       "   'of',\n",
       "   'Computer',\n",
       "   'Networks',\n",
       "   'Software',\n",
       "   'Engineering',\n",
       "   'and',\n",
       "   'Systems',\n",
       "   'GREat',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Ceará',\n",
       "   'UFC',\n",
       "   'Fortaleza',\n",
       "   'Ceará',\n",
       "   'Brazil',\n",
       "   'valerialelli@ufc.br',\n",
       "   'Liana',\n",
       "   'Mara',\n",
       "   'Carvalho',\n",
       "   'Group',\n",
       "   'of',\n",
       "   'Computer',\n",
       "   'Networks',\n",
       "   'Software',\n",
       "   'Engineering',\n",
       "   'and',\n",
       "   'Systems',\n",
       "   'GREat',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Ceará',\n",
       "   'UFC',\n",
       "   'Fortaleza',\n",
       "   'Ceará',\n",
       "   'Brazil',\n",
       "   'lianacdemenezes@gmail.com',\n",
       "   'the',\n",
       "   'internet',\n",
       "   'of',\n",
       "   'thing',\n",
       "   'IoT',\n",
       "   'have',\n",
       "   'expand',\n",
       "   'the',\n",
       "   'internet',\n",
       "   'by',\n",
       "   'integrate',\n",
       "   'smart',\n",
       "   'object',\n",
       "   'which',\n",
       "   'when',\n",
       "   'interconnect',\n",
       "   'can',\n",
       "   'collect',\n",
       "   'and',\n",
       "   'share',\n",
       "   'information',\n",
       "   'to',\n",
       "   'provide',\n",
       "   'service',\n",
       "   'however',\n",
       "   'the',\n",
       "   'intense',\n",
       "   'datum',\n",
       "   'traffic',\n",
       "   'and',\n",
       "   'the',\n",
       "   'diversity',\n",
       "   'of',\n",
       "   'interaction',\n",
       "   'method',\n",
       "   'of',\n",
       "   'smart',\n",
       "   'object',\n",
       "   'which',\n",
       "   'vary',\n",
       "   'base',\n",
       "   'on',\n",
       "   'the',\n",
       "   'protocol',\n",
       "   'and',\n",
       "   'standard',\n",
       "   'bring',\n",
       "   'several',\n",
       "   'challenge',\n",
       "   'for',\n",
       "   'ioy',\n",
       "   'Interoperability',\n",
       "   'Testing',\n",
       "   'such',\n",
       "   'testing',\n",
       "   'evaluate',\n",
       "   'the',\n",
       "   'capability',\n",
       "   'of',\n",
       "   'system',\n",
       "   'and',\n",
       "   'device',\n",
       "   'to',\n",
       "   'cooperate',\n",
       "   'effectively',\n",
       "   'regard',\n",
       "   'the',\n",
       "   'challenge',\n",
       "   'in',\n",
       "   'iot',\n",
       "   'interoperability',\n",
       "   'testing',\n",
       "   'we',\n",
       "   'highlight',\n",
       "   'the',\n",
       "   'complexity',\n",
       "   'of',\n",
       "   'iot',\n",
       "   'architecture',\n",
       "   'the',\n",
       "   'device',\n",
       "   'heterogeneity',\n",
       "   'and',\n",
       "   'the',\n",
       "   'guarantee',\n",
       "   'of',\n",
       "   'effective',\n",
       "   'connectivity',\n",
       "   'among',\n",
       "   'the',\n",
       "   'smart',\n",
       "   'object',\n",
       "   'in',\n",
       "   'this',\n",
       "   'context',\n",
       "   'this',\n",
       "   'paper',\n",
       "   'present',\n",
       "   'a',\n",
       "   'interoperability',\n",
       "   'testing',\n",
       "   'guide',\n",
       "   'for',\n",
       "   'iot.',\n",
       "   'the',\n",
       "   'guide',\n",
       "   'be',\n",
       "   'develop',\n",
       "   'base',\n",
       "   'on',\n",
       "   'a',\n",
       "   'literature',\n",
       "   'review',\n",
       "   'use',\n",
       "   'systematic',\n",
       "   'mapping',\n",
       "   'and',\n",
       "   'an',\n",
       "   'analysis',\n",
       "   'of',\n",
       "   'real',\n",
       "   'iot',\n",
       "   'environment',\n",
       "   'the',\n",
       "   'guide',\n",
       "   '’s',\n",
       "   'evaluation',\n",
       "   'consist',\n",
       "   'of',\n",
       "   'two',\n",
       "   'step',\n",
       "   '1',\n",
       "   'a',\n",
       "   'structural',\n",
       "   'assessment',\n",
       "   'use',\n",
       "   'the',\n",
       "   'Technology',\n",
       "   'Acceptance',\n",
       "   'Model',\n",
       "   'TAM',\n",
       "   'and',\n",
       "   '2',\n",
       "   'a',\n",
       "   'control',\n",
       "   'experiment',\n",
       "   'apply',\n",
       "   'the',\n",
       "   'guide',\n",
       "   'to',\n",
       "   'test',\n",
       "   'a',\n",
       "   'real',\n",
       "   'iot',\n",
       "   'application',\n",
       "   'keyword',\n",
       "   'Interoperability',\n",
       "   'internet',\n",
       "   'of',\n",
       "   'thing',\n",
       "   'Interoperability',\n",
       "   'Testing',\n",
       "   '1',\n",
       "   'introduction',\n",
       "   'Technology',\n",
       "   'have',\n",
       "   'significantly',\n",
       "   'transform',\n",
       "   'human',\n",
       "   'interaction',\n",
       "   'with',\n",
       "   'everyday',\n",
       "   'object',\n",
       "   'expand',\n",
       "   'their',\n",
       "   'communication',\n",
       "   'internet',\n",
       "   'access',\n",
       "   'have',\n",
       "   'also',\n",
       "   'evolve',\n",
       "   'become',\n",
       "   'more',\n",
       "   'accessible',\n",
       "   'and',\n",
       "   'fast',\n",
       "   '32',\n",
       "   'positively',\n",
       "   'contribute',\n",
       "   'to',\n",
       "   'these',\n",
       "   'object',\n",
       "   'connectivity',\n",
       "   'this',\n",
       "   'broad',\n",
       "   'connectivity',\n",
       "   'have',\n",
       "   'drive',\n",
       "   'the',\n",
       "   'internet',\n",
       "   'of',\n",
       "   'thing',\n",
       "   'IoT',\n",
       "   'to',\n",
       "   'expand',\n",
       "   'the',\n",
       "   'internet',\n",
       "   'through',\n",
       "   'the',\n",
       "   'integration',\n",
       "   'of',\n",
       "   'smart',\n",
       "   'object',\n",
       "   'when',\n",
       "   'interconnect',\n",
       "   'these',\n",
       "   'object',\n",
       "   'have',\n",
       "   'the',\n",
       "   'capability',\n",
       "   'to',\n",
       "   'collect',\n",
       "   'and',\n",
       "   'share',\n",
       "   'information',\n",
       "   'enable',\n",
       "   'they',\n",
       "   'to',\n",
       "   'provide',\n",
       "   'several',\n",
       "   'service',\n",
       "   'the',\n",
       "   'interconnection',\n",
       "   'facilitate',\n",
       "   'by',\n",
       "   'IoT',\n",
       "   'have',\n",
       "   'outline',\n",
       "   'new',\n",
       "   'perspective',\n",
       "   'regard',\n",
       "   'the',\n",
       "   'smart',\n",
       "   'object',\n",
       "   'that',\n",
       "   'interact',\n",
       "   'to',\n",
       "   'automate',\n",
       "   'various',\n",
       "   'daily',\n",
       "   'task',\n",
       "   'for',\n",
       "   'example',\n",
       "   'refrigerator',\n",
       "   'air',\n",
       "   'conditioner',\n",
       "   'smart',\n",
       "   'lock',\n",
       "   'in',\n",
       "   'the',\n",
       "   'context',\n",
       "   'of',\n",
       "   'a',\n",
       "   'smart',\n",
       "   'home',\n",
       "   'or',\n",
       "   'even',\n",
       "   'autonomous',\n",
       "   'vehicle',\n",
       "   'operate',\n",
       "   'independently',\n",
       "   'guide',\n",
       "   'by',\n",
       "   'a',\n",
       "   'variety',\n",
       "   'of',\n",
       "   'smart',\n",
       "   'sensor',\n",
       "   '13',\n",
       "   '27',\n",
       "   'nonetheless',\n",
       "   'the',\n",
       "   'iot',\n",
       "   'scenario',\n",
       "   'bring',\n",
       "   'challenge',\n",
       "   'for',\n",
       "   'test',\n",
       "   'the',\n",
       "   'quality',\n",
       "   'characteristic',\n",
       "   'of',\n",
       "   'iot',\n",
       "   'application',\n",
       "   '5',\n",
       "   '6',\n",
       "   '8',\n",
       "   '12',\n",
       "   '24',\n",
       "   'security',\n",
       "   'Interoperability',\n",
       "   'and',\n",
       "   'performance',\n",
       "   'characteristic',\n",
       "   'be',\n",
       "   'identify',\n",
       "   'as',\n",
       "   'the',\n",
       "   'most',\n",
       "   'relevant',\n",
       "   'receive',\n",
       "   'considerable',\n",
       "   'testing',\n",
       "   'effort',\n",
       "   '6',\n",
       "   'in',\n",
       "   'proceeding',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'web',\n",
       "   'WebMe',\n",
       "   'dia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   '2024',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'sbc',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '2753',\n",
       "   'interoperability',\n",
       "   'in',\n",
       "   'iot',\n",
       "   'be',\n",
       "   'a',\n",
       "   'crucial',\n",
       "   'facet',\n",
       "   'to',\n",
       "   'be',\n",
       "   'address',\n",
       "   'in',\n",
       "   'the',\n",
       "   'development',\n",
       "   'of',\n",
       "   'iot',\n",
       "   'system',\n",
       "   '30',\n",
       "   'it',\n",
       "   'refer',\n",
       "   'to',\n",
       "   'the',\n",
       "   'capability',\n",
       "   'of',\n",
       "   'two',\n",
       "   'or',\n",
       "   'more',\n",
       "   'system',\n",
       "   'to',\n",
       "   'communicate',\n",
       "   'effectively',\n",
       "   'while',\n",
       "   'ensure',\n",
       "   'datum',\n",
       "   'integrity',\n",
       "   '38',\n",
       "   'therefore',\n",
       "   'IoT',\n",
       "   'interoperability',\n",
       "   'testing',\n",
       "   'verify',\n",
       "   'the',\n",
       "   'ability',\n",
       "   'of',\n",
       "   'system',\n",
       "   'to',\n",
       "   'interact',\n",
       "   'consistently',\n",
       "   'and',\n",
       "   'cohesively',\n",
       "   'such',\n",
       "   'testing',\n",
       "   'also',\n",
       "   'involve',\n",
       "   'evaluate',\n",
       "   'their',\n",
       "   'efficiency',\n",
       "   'in',\n",
       "   'communicating',\n",
       "   'and',\n",
       "   'share',\n",
       "   'information',\n",
       "   'ensure',\n",
       "   'resource',\n",
       "   'can',\n",
       "   'be',\n",
       "   'access',\n",
       "   'and',\n",
       "   'use',\n",
       "   'appropriately',\n",
       "   'accross',\n",
       "   'different',\n",
       "   'system',\n",
       "   'and',\n",
       "   'organization',\n",
       "   '23',\n",
       "   'in',\n",
       "   'this',\n",
       "   'context',\n",
       "   'the',\n",
       "   'intense',\n",
       "   'datum',\n",
       "   'traffic',\n",
       "   'and',\n",
       "   'the',\n",
       "   'diverse',\n",
       "   'interaction',\n",
       "   'method',\n",
       "   'of',\n",
       "   'smart',\n",
       "   'object',\n",
       "   'which',\n",
       "   'vary',\n",
       "   'depend',\n",
       "   'on',\n",
       "   'protocol',\n",
       "   'MQTT',\n",
       "   'HTTP',\n",
       "   'CoAP',\n",
       "   'Bluetooth',\n",
       "   'and',\n",
       "   'Zigbee',\n",
       "   'pose',\n",
       "   'considerable',\n",
       "   'challenge',\n",
       "   'in',\n",
       "   'interoperability',\n",
       "   'testing',\n",
       "   'it',\n",
       "   'be',\n",
       "   'worth',\n",
       "   'mention',\n",
       "   'that',\n",
       "   'while',\n",
       "   'Bluetooth',\n",
       "   'and',\n",
       "   'Zigbee',\n",
       "   'be',\n",
       "   'sometimes',\n",
       "   'refer',\n",
       "   'to',\n",
       "   'as',\n",
       "   'standard',\n",
       "   'they',\n",
       "   'like',\n",
       "   'MQTT',\n",
       "   'HTTP',\n",
       "   'and',\n",
       "   'CoAP',\n",
       "   'be',\n",
       "   'also',\n",
       "   'protocol',\n",
       "   'that',\n",
       "   'operate',\n",
       "   'at',\n",
       "   'different',\n",
       "   'tier',\n",
       "   'of',\n",
       "   'the',\n",
       "   'communication',\n",
       "   'stack',\n",
       "   '15',\n",
       "   'for',\n",
       "   'example',\n",
       "   'consider',\n",
       "   'an',\n",
       "   'iot',\n",
       "   'smart',\n",
       "   'home',\n",
       "   'scenario',\n",
       "   'with',\n",
       "   'diverse',\n",
       "   'device',\n",
       "   'voice',\n",
       "   'assistant',\n",
       "   'security',\n",
       "   'camera',\n",
       "   'thermostat',\n",
       "   'smart',\n",
       "   'bulb',\n",
       "   'and',\n",
       "   'lock',\n",
       "   'promote',\n",
       "   'their',\n",
       "   'communication',\n",
       "   'and',\n",
       "   'integration',\n",
       "   'across',\n",
       "   'different',\n",
       "   'technology',\n",
       "   'for',\n",
       "   'flawless',\n",
       "   'operation',\n",
       "   'pose',\n",
       "   'challenge',\n",
       "   'regard',\n",
       "   'the',\n",
       "   'architectural',\n",
       "   'complexity',\n",
       "   'the',\n",
       "   'device',\n",
       "   'heterogeneity',\n",
       "   'the',\n",
       "   'effective',\n",
       "   'connectivity',\n",
       "   'and',\n",
       "   'the',\n",
       "   'management',\n",
       "   'of',\n",
       "   'bandwidth',\n",
       "   'and',\n",
       "   'device',\n",
       "   'resource',\n",
       "   'limitation',\n",
       "   'for',\n",
       "   'real',\n",
       "   'time',\n",
       "   'datum',\n",
       "   'processing',\n",
       "   'therefore',\n",
       "   'the',\n",
       "   'goal',\n",
       "   'of',\n",
       "   'this',\n",
       "   'paper',\n",
       "   'be',\n",
       "   'to',\n",
       "   'present',\n",
       "   'the',\n",
       "   'interoperability',\n",
       "   'Testing',\n",
       "   'Guide',\n",
       "   'for',\n",
       "   'iot',\n",
       "   'application',\n",
       "   'the',\n",
       "   'following',\n",
       "   'research',\n",
       "   'question',\n",
       "   'RQ',\n",
       "   'be',\n",
       "   'investigate',\n",
       "   'in',\n",
       "   'this',\n",
       "   'work',\n",
       "   'rq1',\n",
       "   'how',\n",
       "   'to',\n",
       "   'evaluate',\n",
       "   'the',\n",
       "   'interoperability',\n",
       "   'characteristic',\n",
       "   'in',\n",
       "   'iot',\n",
       "   'application',\n",
       "   'rq2',\n",
       "   'what',\n",
       "   'approach',\n",
       "   'be',\n",
       "   'use',\n",
       "   'to',\n",
       "   'evaluate',\n",
       "   'interoperability',\n",
       "   'in',\n",
       "   'iot',\n",
       "   'application',\n",
       "   'RQ3',\n",
       "   'what',\n",
       "   'be',\n",
       "   'the',\n",
       "   'main',\n",
       "   'challenge',\n",
       "   'relate',\n",
       "   'to',\n",
       "   'interoperability',\n",
       "   'testing',\n",
       "   'in',\n",
       "   'iot',\n",
       "   'application',\n",
       "   'the',\n",
       "   'development',\n",
       "   'of',\n",
       "   'the',\n",
       "   'guide',\n",
       "   'follow',\n",
       "   'the',\n",
       "   'methodology',\n",
       "   'propose',\n",
       "   'by',\n",
       "   '6',\n",
       "   'which',\n",
       "   'suggest',\n",
       "   'a',\n",
       "   'general',\n",
       "   'structure',\n",
       "   'base',\n",
       "   'on',\n",
       "   '11',\n",
       "   'key',\n",
       "   'topic',\n",
       "   'initially',\n",
       "   'we',\n",
       "   'perform',\n",
       "   'a',\n",
       "   'literature',\n",
       "   'review',\n",
       "   'to',\n",
       "   'develop',\n",
       "   'the',\n",
       "   'guide',\n",
       "   'content‘accorde',\n",
       "   'that',\n",
       "   'topic',\n",
       "   'this',\n",
       "   'review',\n",
       "   'also',\n",
       "   'aim',\n",
       "   'to',\n",
       "   'investigate',\n",
       "   'the',\n",
       "   'interoperability',\n",
       "   'testing',\n",
       "   'in',\n",
       "   'different',\n",
       "   'application',\n",
       "   'domain',\n",
       "   'next',\n",
       "   'we',\n",
       "   'focus',\n",
       "   'on',\n",
       "   'identify',\n",
       "   'interoperability',\n",
       "   'subcharacteristic',\n",
       "   'standard',\n",
       "   'and',\n",
       "   'approach',\n",
       "   'use',\n",
       "   'in',\n",
       "   'iot',\n",
       "   'interoperability',\n",
       "   'testing',\n",
       "   'the',\n",
       "   'final',\n",
       "   'version',\n",
       "   'of',\n",
       "   'the',\n",
       "   'guide',\n",
       "   'be',\n",
       "   'structure',\n",
       "   'in',\n",
       "   '12',\n",
       "   'topic',\n",
       "   'include',\n",
       "   'an',\n",
       "   'additional',\n",
       "   'topic',\n",
       "   'call',\n",
       "   'Challenges',\n",
       "   'of',\n",
       "   'Interoperability',\n",
       "   'Testing',\n",
       "   'to',\n",
       "   'evaluate',\n",
       "   'the',\n",
       "   'guide',\n",
       "   'two',\n",
       "   'evaluation',\n",
       "   'be',\n",
       "   'conduct',\n",
       "   'i',\n",
       "   'guide',\n",
       "   'evaluation',\n",
       "   'use',\n",
       "   'the',\n",
       "   'Technology',\n",
       "   'Acceptance',\n",
       "   'Model',\n",
       "   '9',\n",
       "   'TAM',\n",
       "   'and',\n",
       "   'ii',\n",
       "   'a',\n",
       "   'control',\n",
       "   'experiment',\n",
       "   '44',\n",
       "   'to',\n",
       "   'assess',\n",
       "   'the',\n",
       "   'use',\n",
       "   'of',\n",
       "   'the',\n",
       "   'guide',\n",
       "   'for',\n",
       "   'test',\n",
       "   'the',\n",
       "   'interoperability',\n",
       "   'of',\n",
       "   'an',\n",
       "   'iot',\n",
       "   'application',\n",
       "   '188',\n",
       "   'WebMedia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Castelo',\n",
       "   'Branco',\n",
       "   'et',\n",
       "   'al',\n",
       "   'the',\n",
       "   'remain',\n",
       "   'structure',\n",
       "   'of',\n",
       "   'this',\n",
       "   'paper',\n",
       "   'be',\n",
       "   'organize',\n",
       "   'as',\n",
       "   'follow',\n",
       "   'section',\n",
       "   '2',\n",
       "   'present',\n",
       "   'a',\n",
       "   'theoretical',\n",
       "   'basis',\n",
       "   'necessary',\n",
       "   'to',\n",
       "   'understand',\n",
       "   'the',\n",
       "   'research',\n",
       "   'section',\n",
       "   '3',\n",
       "   'describe',\n",
       "   'the',\n",
       "   'methodology',\n",
       "   'section',\n",
       "   '4',\n",
       "   'present',\n",
       "   'the',\n",
       "   'structure',\n",
       "   'and',\n",
       "   'the',\n",
       "   'content',\n",
       "   'of',\n",
       "   'the',\n",
       "   'guide',\n",
       "   'section',\n",
       "   '5',\n",
       "   'present',\n",
       "   'the',\n",
       "   'guide',\n",
       "   'evaluation',\n",
       "   'and',\n",
       "   'section',\n",
       "   '6',\n",
       "   'address',\n",
       "   'the',\n",
       "   'research',\n",
       "   'question',\n",
       "   'section',\n",
       "   '8',\n",
       "   'discuss',\n",
       "   'related',\n",
       "   'work',\n",
       "   'and',\n",
       "   'finally',\n",
       "   'section',\n",
       "   '9',\n",
       "   'present',\n",
       "   'conclusion',\n",
       "   'and',\n",
       "   'future',\n",
       "   'work',\n",
       "   '2',\n",
       "   'interoperability',\n",
       "   'testing',\n",
       "   'in',\n",
       "   'IOT',\n",
       "   'the',\n",
       "   'internet',\n",
       "   'of',\n",
       "   'thing',\n",
       "   'IoT',\n",
       "   'have',\n",
       "   'transform',\n",
       "   'connectivity',\n",
       "   'by',\n",
       "   'link',\n",
       "   'device',\n",
       "   'globally',\n",
       "   'turn',\n",
       "   'physical',\n",
       "   'object',\n",
       "   'into',\n",
       "   'smart',\n",
       "   'interconnect',\n",
       "   'entity',\n",
       "   'with',\n",
       "   'advanced',\n",
       "   'function',\n",
       "   '28',\n",
       "   '33',\n",
       "   '34',\n",
       "   'it',\n",
       "   'facilitate',\n",
       "   'universal',\n",
       "   'interaction',\n",
       "   'by',\n",
       "   'keep',\n",
       "   'people',\n",
       "   'and',\n",
       "   'smart',\n",
       "   'object',\n",
       "   'connect',\n",
       "   'through',\n",
       "   'various',\n",
       "   'network',\n",
       "   '39',\n",
       "   'IoT',\n",
       "   'can',\n",
       "   'be',\n",
       "   'define',\n",
       "   'from',\n",
       "   'three',\n",
       "   'perspective',\n",
       "   'device',\n",
       "   'which',\n",
       "   'be',\n",
       "   'the',\n",
       "   'sense',\n",
       "   'element',\n",
       "   'the',\n",
       "   'internet',\n",
       "   'which',\n",
       "   'serve',\n",
       "   'as',\n",
       "   'the',\n",
       "   'network',\n",
       "   'framework',\n",
       "   'and',\n",
       "   'semantic',\n",
       "   'which',\n",
       "   'involve',\n",
       "   'communication',\n",
       "   'protocol',\n",
       "   '41',\n",
       "   'it',\n",
       "   'include',\n",
       "   'essential',\n",
       "   'component',\n",
       "   'such',\n",
       "   'as',\n",
       "   'data',\n",
       "   'collect',\n",
       "   'sensor',\n",
       "   'connectivity',\n",
       "   'technology',\n",
       "   'like',\n",
       "   'Wi',\n",
       "   'Fi',\n",
       "   'and',\n",
       "   '5',\n",
       "   'g',\n",
       "   'and',\n",
       "   'system',\n",
       "   'for',\n",
       "   'datum',\n",
       "   'processing',\n",
       "   'and',\n",
       "   'storage',\n",
       "   'in',\n",
       "   'the',\n",
       "   'cloud',\n",
       "   'all',\n",
       "   'work',\n",
       "   'together',\n",
       "   'to',\n",
       "   'ensure',\n",
       "   'efficient',\n",
       "   'and',\n",
       "   'secure',\n",
       "   'information',\n",
       "   'flow',\n",
       "   '2',\n",
       "   'IoT',\n",
       "   'encompass',\n",
       "   'various',\n",
       "   'quality',\n",
       "   'characteristic',\n",
       "   'with',\n",
       "   'at',\n",
       "   'least',\n",
       "   '27',\n",
       "   'identify',\n",
       "   'include',\n",
       "   ...]},\n",
       " {'titulo': 'Recognition of Emotions through Facial Geometry with Normalized Landmarks',\n",
       "  'informacoes_url': '',\n",
       "  'idioma': 'english',\n",
       "  'storage_key': '../articles/original/english/985-24766-1-10-20240923.pdf',\n",
       "  'author': 'Alessandra Alaniz Macedo; Leandro Persona; and Fernando Meloni',\n",
       "  'data_publicacao': '11-09-2024',\n",
       "  'resumo': 'Emotion recognition holds pivotal significance in human social interactions, as it entails the discernment of facial patterns intricately linked to diverse emotional states. The scientific, artistic, medical, and marketing domains have all demonstrated substantial interest in comprehending emotions, resulting in the emergence and refinement of techniques and computational methodologies to facilitate automated emotion recognition. In this study, we introduce a novel method named REGL (Recognizing Emotions through Facial Expression and Landmark normalization) aimed at recognizing facial expressions and human emotions depicted in images. REGL comprises a sequential set of steps designed to minimize sample variability, thereby facilitating a finer calibration of the informative aspects that delineate facial patterns. REGL carries out the normalization of facial fiducial points, called landmarks. Through the use of landmark positions, the reliability of the emotion recognition process is significantly improved. REGL also exploits classifiers explicitly tailored for the accurate identification of facial emotions. As related works, the outcomes of our experimentation yielded an average accuracy over 90% by employing Machine Learning algorithms. Differently, we have experimented REGL with varied architectures and datasets including racial factors. We surpass related works considering the following contributions: the REGL method represents an enhanced approach in terms of hit rate and response time, and REGL generates resilient outcomes by demonstrating reduced reliance on both the training set and classifier architecture. Moreover, REGL demonstrated excellent performance in terms of response time enabling low-cost and real-time processing, particularly suitable for devices with limited processing capabilities, such as cellphones. We intend to foster the advancement of robust assistive technologies, facilitate enhancements in computational synthesis techniques, and computational resources. ###',\n",
       "  'keywords': 'Multimedia Processing, Affective Computing, Machine LearningMultimodal Interaction, Facial Patterns, Image Understanding.',\n",
       "  'referencias': ['[1] Mauricio Alvarez, David Luengo, and Neil Lawrence. 2013. Linear Latent Force\\nModels Using Gaussian Processes. *IEEE Transactions on Pattern Analysis and*\\n*Machine Intelligence* 35, 11 (Nov 2013), 2693–2705. https://doi.org/10.1109/TPAMI.\\n2013.86',\n",
       "   '[2] Mitra B., Sharma K., Acharya S., Mishra P., and Guglani A. 2022. Real-time\\nSmile Detection using Integrated ML Model. In *2022 6th International Conference*\\n*on Intelligent Computing and Control Systems (ICICCS)* . IEEE, Madurai, India,\\n1374–1381. https://doi.org/10.1109/ICICCS53718.2022.9788399',\n",
       "   '[3] Hugo Bohy, Kevin El Haddad, and Thierry Dutoit. 2022. A New Perspective on\\nSmiling and Laughter Detection: Intensity Levels Matter. In *2022 10th International*\\n*Conference on Affective Computing and Intelligent Interaction (ACII)* . 1–8. https:\\n//doi.org/10.1109/ACII55700.2022.9953896',\n",
       "   '[4] Guilherme Campos, Arthur Zimek, Joerg Sander, Ricardo Campello, Barbora\\nMicenková, Erich Schubert, Ira Assent, and Michael Houle. 2016. On the\\nevaluation of unsupervised outlier detection: measures, datasets, and an empirical study. *Data Mining and Knowledge Discovery* 30 (07 2016). https:\\n//doi.org/10.1007/s10618-015-0444-8',\n",
       "   '[5] V. Chaugule, D. Abhishek, A. Vijayakumar, P. B. Ramteke, and S. G. Koolagudi.\\n2016. Product review based on optimized facial expression detection. In *2016*\\n*Ninth International Conference on Contemporary Computing (IC3)* . IEEE, Noida,\\nIndia, 1–6. https://doi.org/10.1109/IC3.2016.7880213',\n",
       "   '[6] Yufang Cheng and Shuhui Ling. 2008. 3D Animated Facial Expression and Autism\\nin Taiwan. In *IEEE International Conference on Advanced Learning Technologies*\\n*(ICALT 2008)* . IEEE Computer Society, Los Alamitos, CA, USA, 17–19. https:\\n//doi.org/10.1109/ICALT.2008.220',\n",
       "   '[7] Francois Chollet. 2017. *Deep Learning with Python* (1st ed.). Manning Publications\\nCo., Greenwich, CT, USA.',\n",
       "   '[8] Jeffrey Cohn and Takeo Kanade. 2010. The Extended Cohn-Kanade Dataset\\n(CK+): A complete dataset for action unit and emotion-specified expression. *2010*\\n*IEEE Computer Society Conference on Computer Vision and Pattern Recognition -*\\n*Workshops, CVPRW 2010*, 94 – 101. https://doi.org/10.1109/CVPRW.2010.5543262',\n",
       "   '[9] Dongshun Cui, Guang-Bin Huang, and Tianchi Liu. 2018. ELM based smile\\ndetection using Distance Vector. *Pattern Recognition* 79 (2018), 356–369. https:\\n//doi.org/10.1016/j.patcog.2018.02.019',\n",
       "   '[10] D Dalal and B. Triggs. 2005. Histograms of oriented gradients for human detection. In *2005 IEEE Computer Society Conference on Computer Vision and Pat-*\\n*tern Recognition (CVPR’05)*, Vol. 1. IEEE, San Diego, CA, USA, 886–893 vol. 1.\\nhttps://doi.org/10.1109/CVPR.2005.177',\n",
       "   '[11] Charles Darwin. 2013. *The Expression of the Emotions in Man and Animals* . Cambridge University Press, England. https://doi.org/10.1017/CBO9781139833813',\n",
       "   '[12] Alex Davies and Zoubin Ghahramani. 2014. The Random Forest Kernel and other\\nkernels for big data from random partitions. arXiv:1402.4293 [stat.ML]',\n",
       "   '[13] Paul Ekman and Wallace V. Friesen. 1971. Constants across cultures in the face\\nand emotion. *Journal of Personality and Social Psychology* 17, 2 (1971), 124–129.\\nhttps://doi.org/10.1037/h0030377',\n",
       "   '[14] Hugo. Filho, Oge Marques; Vieira Neto. 1999. *Processamento Digital de Imagens* .\\nBrasport, Brasil. 30–31 pages.',\n",
       "   '[15] Gabriel Garrido and Prateek Joshi. 2018. *OpenCV 3.X with Python By Example:*\\n*Make the most of OpenCV and Python to build applications for object recognition*\\n*and augmented reality* (2nd ed.). Packt Publishing, US.',\n",
       "   '[16] A. T. Ghorbani, G; Targhi and M. Dehshibi. 2015. HOG and LBP: Towards a\\nrobust face recognition system. In *2015 Tenth International Conference on Digital*\\n*Information Management (ICDIM)* . IEEE, Jeju, South Korea, 138–141. https:\\n//doi.org/10.1109/ICDIM.2015.7381860',\n",
       "   '[17] Ellen Goeleven, Rudi De Raedt, Lemke Leyman, and Bruno Verschuere. 2008.\\nThe Karolinska Directed Emotional Faces: A validation study. *Cognition and*\\n*Emotion* 22, 6 (2008), 1094–1118. https://doi.org/10.1080/02699930701626582\\narXiv:https://doi.org/10.1080/02699930701626582',\n",
       "   '[18] Rafael C Gonzales and Richard E. Woods. 2008. *Digital Image Processing* (3rd ed.).\\nPearson, New Jersey, US.',\n",
       "   '[19] Isabelle Guyon and André Elisseeff. 2003. An Introduction to Variable and Feature\\nSelection. *J. Mach. Learn. Res.* 3 (March 2003), 1157–1182.',\n",
       "   '[20] T. Hassner, E. Harel, S.and Paz, and R. Enbar. 2015. Effective face frontalization\\nin unconstrained images. In *2015 IEEE Conference on Computer Vision and Pattern*\\n*Recognition (CVPR)* . IEEE, Boston, MA, US, 4295–4304. https://doi.org/10.1109/\\nCVPR.2015.7299058',\n",
       "   '[21] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. 2009. *The elements of*\\n*statistical learning: data mining, inference and prediction* (2 ed.). Springer, USA.\\nhttp://www-stat.stanford.edu/~tibs/ElemStatLearn/',\n",
       "   '[22] Jiabei He, Xiaoyu Wen, and Juxiang Zhou. 2023. Advances and Application of\\nFacial Expression and Learning Emotion Recognition in Classroom. In *Proceed-*\\n*ings of the 2023 6th International Conference on Image and Graphics Processing*\\n(Chongqing, China) *(ICIGP ’23)* . Association for Computing Machinery, New\\nYork, NY, USA, 23–30. https://doi.org/10.1145/3582649.3582670',\n",
       "   '[23] Ursula Hess. 2001. The Communication of Emotion. In *Emotions, Qualia and*\\n*Consciousness* . Singapore, 397–409. https://doi.org/10.1142/9789812810687_0031',\n",
       "   '[24] Nurulhuda Ismail and Mas Idayu Md. Sabri. 2009. Review of Existing Algorithms\\nfor Face Detection and Recognition. In *Proceedings of the 8th WSEAS International*\\n*Conference on Computational Intelligence, Man-Machine Systems and Cybernetics*\\n(Puerto De La Cruz, Tenerife, Canary Islands, Spain) *(CIMMACS’09)* . World Scientific and Engineering Academy and Society (WSEAS), Stevens Point, Wisconsin,\\nUSA, 30–39.',\n",
       "   '[25] A. Kumar, K.M. Baalamurugan, and B. Balamurugan. 2022. Real-Time Facial\\nComponents Detection Using Haar Classifiers. In *2022 International Conference*\\n*on Applied Artificial Intelligence and Computing (ICAAIC)* . IEEE, Salem, India,\\n01–08. https://doi.org/10.1109/ICAAIC53929.2022.9793034',\n",
       "   '[26] Uttama Lahiri, Esube Bekele, Elizabeth Dohrmann, Zachary Warren, and Nilanjan\\nSarkar. 2011. Design of a Virtual Reality Based Adaptive Response Technology for\\nChildren with Autism Spectrum Disorder. In *Affective Computing and Intelligent*\\n*Interaction*, Sidney D’Mello, Arthur Graesser, Björn Schuller, and Jean-Claude\\nMartin (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 165–174.',\n",
       "   '[27] Oliver Langner, Ron Dotsch, Gijsbert Bijlstra, Daniel H. J. Wigboldus, Skyler T.\\nHawk, and Ad van Knippenberg. 2010. Presentation and validation of the Radboud\\nFaces Database. *Cognition and Emotion* 24, 8 (2010), 1377–1388. https://doi.org/\\n10.1080/02699930903485076 arXiv:https://doi.org/10.1080/02699930903485076',\n",
       "   '[28] Oliver Langner, Ron Dotsch, Gijsbert Bijlstra, Daniel H. J. Wigboldus, Skyler T.\\nHawk, and Ad van Knippenberg. 2010. Presentation and validation of the Radboud\\nFaces Database. *Cognition and Emotion* 24, 8 (2010), 1377–1388. https://doi.org/\\n10.1080/02699930903485076 arXiv:https://doi.org/10.1080/02699930903485076',\n",
       "   '[29] K. Li, F. Xu, J. Wang, Q. Dai, and Y. Liu. 2012. A data-driven approach for facial\\nexpression synthesis in video. In *2012 IEEE Conference on Computer Vision and*\\n*Pattern Recognition* . IEEE, Providence, RI, US, 57–64. https://doi.org/10.1109/\\nCVPR.2012.6247658',\n",
       "   '[30] Shan Li and Weihong Deng. 2018. Deep Facial Expression Recognition: A Survey.\\n*Computing Research Repository (CoRR)* abs/1804.08348 (2018). https://doi.org/10.\\n1109/TAFFC.2020.2981446',\n",
       "   '[31] Michael Lyons, Miyuki Kamachi, and Jiro Gyoba. 2017. Japanese Female Facial\\nExpression (JAFFE) Database. (7 2017). https://doi.org/10.6084/m9.figshare.\\n5245003.v2',\n",
       "   '[32] Dhwani Mehta, Mohammad Faridul Haque Siddiqui, and Ahmad Y. Javaid. 2018.\\nFacial Emotion Recognition: A Survey and Real-World User Experiences in Mixed\\nReality. *Sensors* 18, 2 (2018). https://doi.org/10.3390/s18020416',\n",
       "   '[33] Karnati Mohan, Ayan Seal, Ondrej Krejcar, and Anis Yazidi. 2021. Facial Expression Recognition Using Local Gravitational Force Descriptor-Based Deep\\nConvolution Neural Networks. *IEEE Transactions on Instrumentation and Mea-*\\n*surement* 70 (2021), 1–12. https://doi.org/10.1109/TIM.2020.3031835',\n",
       "   '[34] A Monzo, D; Albiol and M. J. Mossi. 2010. A Comparative Study of Facial\\nLandmark Localization Methods for Face Recognition Using HOG descriptors. In\\n*2010 20th International Conference on Pattern Recognition* . IEEE, Istanbul, Turkey,\\n1330–1333. https://doi.org/10.1109/ICPR.2010.1145',\n",
       "   '[35] Leandro Persona, Fernando Meloni, and Alessandra Macedo. 2023. An accurate\\nreal-time method to detect the smile facial expression. In *Anais do XXIX Simpósio*\\n*Brasileiro de Sistemas Multimídia e Web* (Ribeirão Preto/SP). SBC, Porto Alegre,\\nRS, Brasil, 46–55. https://sol.sbc.org.br/index.php/webmedia/article/view/25865',\n",
       "   '[36] Rosalind W. Picard. 2016. Automating the Recognition of Stress and Emotion:\\nFrom Lab to Real-World Impact. *IEEE MultiMedia* 23, 3 (July 2016), 3–7. https:\\n//doi.org/10.1109/MMUL.2016.38',\n",
       "   '[37] I.Michael Revina and W.R. Sam Emmanuel. 2018. A Survey on Human Face\\nExpression Recognition Techniques. *Journal of King Saud University - Computer*\\n*and Information Sciences* (2018). https://doi.org/10.1016/j.jksuci.2018.09.002',\n",
       "   '[38] Jia S, Wang S, Hu C., Webster PJ, and Li X. 2021. Detection of Genuine and Posed\\nFacial Expressions of Emotion: Databases and Methods. *Front. Psychol. - Sec.*\\n*Perception Science* 11 (15 January 2021), 12p. https://doi.org/10.3389/fpsyg.2020.\\n580287',\n",
       "   '[39] F. Z. SALMAN, A. MADANI, and M. KISSI. 2016. Facial Expression Recognition\\nUsing Decision Trees. In *2016 13th International Conference on Computer Graphics,*\\n*Imaging and Visualization (CGiV)* . IEEE, Beni Mellal, Morocco, 125–130. https:\\n//doi.org/10.1109/CGiV.2016.33',\n",
       "   '[40] Paul F. Smith, Siva Ganesh, and Ping Liu. 2013. A comparison of random forest\\nregression and multiple linear regression for prediction in neuroscience. *Journal*\\n\\n\\n265\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Macedo et al.\\n\\n\\n*of Neuroscience Methods* 220, 1 (2013), 85–91. https://doi.org/10.1016/j.jneumeth.\\n2013.08.024',\n",
       "   '[41] Rafael Luiz Testa, Cléber Gimenez Corrêa, Ariane Machado-Lima, and Fátima\\nL. S. Nunes. 2019. Synthesis of Facial Expressions in Photographs: Characteristics,\\nApproaches, and Challenges. *ACM Comput. Surv.* 51, 6, Article 124 (jan 2019),\\n35 pages. https://doi.org/10.1145/3292652',\n",
       "   '[42] Nim Tottenham, James Tanaka, Andrew Leon, Thomas Mccarry, Marcella Nurse,\\nTodd Hare, David Marcus, Alissa Westerlund, Bj Casey, and Charles Nelson. 2009.\\nThe NimStim set of Facial Expressions: Judgments from Untrained Research\\nParticipants. *Psychiatry research* 168 (07 2009), 242–9. https://doi.org/10.1016/j.\\npsychres.2008.05.006',\n",
       "   '[43] Sana Ullah and Wenhong Tian. 2021. A Systematic Literature Review of\\nRecognition of Compound Facial Expression of Emotions. In *Proceedings of the*\\n*2020 4th International Conference on Video and Image Processing* (Xi’an, China)\\n*(ICVIP ’20)* . Association for Computing Machinery, New York, NY, USA, 116–121.\\nhttps://doi.org/10.1145/3447450.3447469',\n",
       "   '[44] Jean Vaillancourt. 2010. Statistical Methods for Data Mining and Knowledge\\nDiscovery. In *Proceedings of the 8th International Conference on Formal Concept*\\n*Analysis* (Agadir, Morocco) *(ICFCA’10)* . Springer-Verlag, Berlin, Heidelberg, 51–\\n60.',\n",
       "   '[45] Paul Viola and Michael J. Jones. 2001. Robust Real-Time Face Detection. *Interna-*\\n*tional Journal of Computer Vision* 57, 2 (2001), 137–154. https://doi.org/10.1023/B:\\n\\n\\nVISI.0000013087.49260.fb',\n",
       "   '[46] V. Vonikakis and S. Winkler. 2020. Identity-Invariant Facial Landmark Frontalization For Facial Expression Analysis. In *International Conference on Image*\\n*Processing (ICIP)* . 2020 IEEE ICIP, Abu Dhabi, United Arab Emirates, 2281–2285.\\nhttps://doi.org/10.1109/ICIP40778.2020.9190989',\n",
       "   '[47] P. Winterle. 2014. *Vetores e Geometria Analítica* . MAKRON. https://books.google.\\ncom.br/books?id=AKhivgAACAAJ',\n",
       "   '[48] Yue Wu and Qiang Ji. 2018. Facial Landmark Detection: A Literature Survey.\\n*International Journal of Computer Vision* 2 (2018), 115–142. https://doi.org/10.\\n1007/s11263-018-1097-z',\n",
       "   '[49] L. Xie, W.; Shen and J. Jiang. 2017. A Novel Transient Wrinkle Detection Algorithm and Its Application for Expression Synthesis. *IEEE Transactions on*\\n*Multimedia* 19, 2 (Feb 2017), 279–292. https://doi.org/10.1109/TMM.2016.2614429',\n",
       "   '[50] W. XIE, L. SHEB, M. YANG, and Q. HOU. 2015. Lighting difference based wrinkle\\nmapping for expression synthesis. In *2015 8th International Congress on Image*\\n*and Signal Processing (CISP)* . IEEE, Shenyang, China, 636–641. https://doi.org/\\n10.1109/CISP.2015.7407956',\n",
       "   '[51] Xiaoming Zhao and Shiqing Zhang. 2016. A Review on Facial Expression Recognition: Feature Extraction and Classification. *IETE Technical Re-*\\n*view* 33, 5 (2016), 505–517. https://doi.org/10.1080/02564602.2015.1117403\\narXiv:https://doi.org/10.1080/02564602.2015.1117403\\n\\n\\n266\\n\\n\\n-----'],\n",
       "  'text': '# **Recognition of Emotions through Facial Geometry with** **Normalized Landmarks**\\n\\n## Alessandra Alaniz Macedo\\n#### ale.alaniz@usp.br University of São Paulo (USP) FFCLRP, DCM, PPG-CA Ribeirão Preto, São Paulo, Brazil\\n### **ABSTRACT**\\n\\nEmotion recognition holds pivotal significance in human social interactions, as it entails the discernment of facial patterns intricately\\nlinked to diverse emotional states. The scientific, artistic, medical,\\nand marketing domains have all demonstrated substantial interest in comprehending emotions, resulting in the emergence and\\nrefinement of techniques and computational methodologies to facilitate automated emotion recognition. In this study, we introduce\\na novel method named REGL (Recognizing Emotions through Facial Expression and Landmark normalization) aimed at recognizing\\nfacial expressions and human emotions depicted in images. REGL\\ncomprises a sequential set of steps designed to minimize sample\\nvariability, thereby facilitating a finer calibration of the informative\\naspects that delineate facial patterns. REGL carries out the normalization of facial fiducial points, called landmarks. Through the use\\nof landmark positions, the reliability of the emotion recognition\\nprocess is significantly improved. REGL also exploits classifiers explicitly tailored for the accurate identification of facial emotions. As\\nrelated works, the outcomes of our experimentation yielded an average accuracy over 90% by employing Machine Learning algorithms.\\nDifferently, we have experimented REGL with varied architectures\\nand datasets including racial factors. We surpass related works\\nconsidering the following contributions: the REGL method represents an enhanced approach in terms of hit rate and response\\ntime, and REGL generates resilient outcomes by demonstrating\\nreduced reliance on both the training set and classifier architecture. Moreover, REGL demonstrated excellent performance in terms\\nof response time enabling low-cost and real-time processing, particularly suitable for devices with limited processing capabilities,\\nsuch as cellphones. We intend to foster the advancement of robust\\nassistive technologies, facilitate enhancements in computational\\nsynthesis techniques, and computational resources.\\n### **KEYWORDS**\\n\\nMultimedia Processing, Affective Computing, Machine LearningMultimodal Interaction, Facial Patterns, Image Understanding.\\n### **1 INTRODUCTION**\\n\\nThe recognition of emotions is an intrinsic part of human relationships. Even in early childhood, children learn to map and interpret\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n© 2024 SBC – Brazilian Computing Society.\\nISSN 2966-2753\\n\\n## Leandro Persona Fernando Meloni\\n#### leandro.persona@alumni.usp.br melonifernando@yahoo.com.br University of São Paulo, FFCLRP, DCM, PPG-CA Ribeirão Preto, São Paulo, Brazil\\n\\nthe emotions of others, utilizing the result as an indicator of their\\nsurrounding context [ 11, 23, 27 ]. The mapping is possible because\\nhumans typically translate their emotions into detectable physical\\nmovements, particularly facial expressions, which are vital for social interactions. Facial expressions are ubiquitous behaviors and\\nexhibit weak dependence on cultural factors as demonstrated by\\n\\n[ 13 ]. Thus, same researchers identified the following seven different\\nuniversal emotions across cultures: fear, anger, sadness, happiness,\\nsurprise, disgust, along with the neutral emotion [ 13 ]. Due to their\\ndistinct and stable patterns, facial expressions can be recognized\\neven in unfamiliar individuals [ 1 ]. As a result, recognition can\\nbe organized with significant potential for automation, enabling\\nmachines to interpret a select range of human emotions.\\nIn recent years, facial expression-based methods for emotion\\nrecognition have witnessed rapid advancements due to the growing scientific, medical, and commercial interest in the field [ 22, 26,\\n43, 50 ]. The most common approaches for recognizing individuals and facial expressions focus on automated pattern detection\\nin digital images. Notably, social media platforms, mobile devices,\\nand digital cameras, now possess the capability to discern whether\\na person is smiling or not [ 5 ]. Assistive technologies are developed to aid individuals with behavioral syndromes (e.g., autism and\\nmood disorders) [ 6, 26, 36 ]. By enabling individuals with disabilities to react differently upon perceiving expressed emotions, these\\ntechnologies contribute to enhancing their social interactions. In\\nsummary, automatic facial expression recognition offers new avenues for interaction between humans and machines, facilitated by\\nthe detection of both voluntary and involuntary facial movements.\\nIn general, the interpretation of information from digital images\\ninvolves automation through Machine Learning (ML) [ 7 ]. However,\\nimage manipulation requires initial pre-processing steps [ 44 ] to\\nenable the detection of specific shapes such as a human face. Next\\npre-trained models scan segments of the image and use probabilities\\nto confirm patterns [ 48 ]. Some models are capable of detecting\\nhuman faces in images with high accuracy rates, above 90% [ 45,\\n48 ]. Once the face is identified, the next step in evaluating facial\\nexpressions is to map the Regions of Interest (ROIs), which are\\nspecific facial structures such as eyes, nose, mouth, chin, etc. The\\nrelative positions of these structures are then marked as landmarks,\\nwhich are assigned bi or tridimensional coordinates, adding an\\nadditional layer of information.\\nThe use of landmarks provides a straightforward and objective way to recognize facial expressions by comparing patterns for\\nboth facial identification (facial recognition) and changes in facial\\n\\n\\n257\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Macedo et al.\\n\\n\\npatterns (expression and emotion recognition). Although implementations may vary, both approaches consider the coordinates of\\nlandmarks as problem variables (reference values) for calculating\\ndistances (e.g., Euclidean, cosine, etc.) [ 15, 29, 32 ]. In the case of\\nemotions, the muscle movements responsible for each facial expression cause geometric changes in the relative positions of ROIs [ 49 ].\\nThese changes alter the coordinates of landmarks and the values of\\ndistances between them. Given that a neutral face exhibits different\\nrelative distances from a fearful or smiling face, the set of variations\\ncan be utilized to identify expressions [ 32 ]. The generalizability of\\nemotional responses among humans tends to simplify the problem,\\nenabling the identification of which distances are most affected\\nby each type of facial expression. At the end of the analysis of\\nthese variations, ML classifiers can be trained for the recognition\\nof human emotions [48].\\nDespite the current methods yielding satisfactory results for\\nspecific contexts, such as photos acquired in controlled environments, there are still significant challenges related to the subject.\\nThe main problems involve the variability of patterns found in\\nfacial shapes, poses and in images obtained under real-world conditions (uncontrolled variables such as lighting, brightness, distance\\nfrom the capturing device, etc.) [ 32, 41 ]. Differences in facial shapes\\ntend to introduce noise into the problem, making training dependent on factors such as race, age, sex, etc. Aspects like focal length,\\nluminosity, framing, people’s poses (rotated face images and expression of sentiment), and hardware configurations significantly affect\\nthe concentration and location of pixels, adding noise to the data.\\nThese sources of variability negatively impact the performance of\\nclassifiers, making the ML process more challenging [ 7, 32 ]. Our\\nhypothesis is that the reduction of variability in the data can contribute to improving computational performance. However, the\\ndomain of information in images is represented by pixels, and thus\\ntraditional techniques for data alignment and normalization need\\nto be adapted for this context, requiring computationally creative\\nand conceptually elaborate strategies.\\nAccording to Oge and Gonzales, one of the major challenges\\nfaced by algorithms in Digital Image Processing (DIP) in uncontrolled environments is the difficulty of achieving good results\\nunder varying conditions of luminosity and contrast [ 14, 18 ]. These\\nconditions introduce noise and variability into the data, thereby posing significant challenges for algorithm performance. Additionally,\\nthe relative positioning of objects within the scene tends to generate\\nvariability, further reducing the efficiency of the algorithms.\\nIn this paper, we present a novel method for emotion recognition in digital images, called REGL (Recognizing Emotions through\\nFacial Expression and Landmark normalization), which operates\\nunder various conditions of variability, including scale, rotation,\\nracial factors, and image acquisition. The method incorporates\\nwell-known image manipulation techniques such as histogram\\nequalization and facial alignment to harmonize the information\\nand enable appropriate normalization. A set of normalization steps\\nis performed to reduce data variability, which finally produced an\\noptimized context to identify facial expressions. Subsequently, classifiers suitable for the methodology of the study are employed to\\nassess potential performance gains. The processing and normalization steps aimed at reducing data variability and simplifying the\\nproblem. As mentioned, we hypothesized that reducing variability\\n\\n\\nwould lead to classifiers with improved performance, regardless\\nof the ML method employed. Thus, the development of methods\\nthat harmonize image information has the potential to yield better\\nresults.\\n\\nThe remainder of this paper is structured as follows: a brief\\nreview of image deblurring and attention mechanism is provided\\nin Section 2. The proposed REGL method is discussed in Section 3.\\nThe results of the experiments are presented in Section 4. Section 5\\nbrings related work. Finally, Section 6 provides final remarks and\\nfuture work.\\n### **2 BACKGROUND**\\n\\nIn the early 1970s, Paul Ekman conducted a groundbreaking scientific experiment that revolutionized the field of human emotion\\nrecognition [ 13 ]. At that time, it was believed that individuals used\\ntheir facial muscles according to a set of social conventions and\\nexpressions shaped by societal interactions, much like languages,\\nwith each region of the world having its own variations.\\nEkman captured numerous images of men and women displaying\\nvarious facial expressions. He then traveled to Brazil, Argentina,\\nand Japan to conduct his experiments. To his surprise, individuals\\nfrom different countries obtained the same results in classifying\\nthe images. The experiment was further extended to the forests\\nof Papua New Guinea in Oceania, reaching the most remote and\\nisolated villages. Even among the inhabitants of these regions, the\\nresults did not differ, leading to the conclusion that human emotions\\nexpressed through facial expressions are universal and independent\\nof ethnic and social factors.\\nAnother significant contribution of Ekman’s work was the creation of the Facial Action Coding System (FACS), a system for\\nclassifying human emotions. FACS provides a standardized framework for systematically categorizing the physical expression of\\nemotions, allowing for the labeling of any anatomically possible\\nfacial expression. Figure 1 presents the six primary emotions, along\\nwith the neutral expression, subdivided into action units, which are\\nthe building blocks of FACS and their main differences.\\n\\n**Figure 1: Illustration of FACS (Facial Action Coding Sys-**\\n**tem) [27]**\\n\\n.\\n\\n\\n258\\n\\n\\n-----\\n\\nRecognition of Emotions through Facial Geometry with Normalized Landmarks WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\nThis section offers the necessary context for our method by\\nproviding general information about the topic of our REGL method.\\nTo effectively present this context, we have divided the section\\ninto *Imaging Process, Face Detection, Landmarks Setting* subsection,\\nand *Classification* subsection. Each subsection focuses on a specific\\naspect, providing a comprehensive overview of the underlying\\nconcepts and techniques related to our proposal.\\n### **2.1 Imaging Process, Face Detection, and** **Landmarks Setting**\\n\\nThe real-time process of interpreting emotions from images includes managing images, e.g. pixel normalization, detecting faces,\\nextracting features, and classifying the patterns [ 7, 18 ]. The initial\\npre-processing of images works out to minimize noise, like pixels\\nacquired outside the standard [ 44 ], reducing variance from data.\\nThis step allows training more precise classifiers and achieves good\\nperformance even if limited datasets are employed for training.\\nThe next step is analyzing human faces in images, which starts\\nby detecting the region of the image that contains the aimed object\\n\\n[ 25 ]. In the early 2000s, Viola and Jones developed a method capable\\nof detecting human faces with sufficient speed and accuracy for\\nuse in popular cameras [ 45 ]. Currently, more efficient and reliable\\nsolutions exist, such as the use of Gradient Histograms oriented\\n(HOG), specifically for this purpose [ 16, 34 ]. HOG is a digital image\\nprocessing technique widely used in computer vision and multimedia processing. It characterizes objects by their shape and texture,\\nby evaluating the distribution of the density gradient or the edge\\ndirections [ 10 ]. It has been used as a reference in the facial detection\\nand recognition of various types of objects [ 18 ]. Models trained\\nto detect human faces in images can scan segments through the\\nimage, employing probabilities to determine bounds and confirm\\nthe whole pattern [ 48 ]. Some available models can detect human\\nfaces in images with high accuracy, even in low-quality images\\nsuch as [2, 41, 45, 48].\\nOnce face identification is confirmed, the next step is to identify\\nthe Regions of Interest (ROIs), like eyes, mouth, nose, chin, or any\\nrelevant structure related to facial expressions. The facial landmarks\\nare subsets of the shape prediction problem, which involves locating key points and the overall shape of an object. For instance, the\\nDLib library, which is available in C, C++, and Python, is commonly\\nused to extract landmarks and provides a feature matrix of the *x*\\nand *y* coordinates with 68 facial points, including eyes, nose, mouth,\\nand face bounds. Landmarks may assume bi or three-dimensional\\ncoordinates according to the implementation, and their relative\\npositions in the face carry information from relative distances of\\nface structures, which can be useful for face recognition or emotion\\nanalysis. Regarding facial expressions, muscular movements lead\\nto changes in the relative positions of face structures and, therefore,\\nthe relative distances between landmarks shall reflect the face features [ 44 ]. For this reason, relative distances between landmarks are\\nuseful for objectively analyzing facial expressions and emotions, or\\neven for face recognition. In both cases, the landmark coordinates\\nserve as reference values for calculating distances using various\\nmetrics such as Euclidean, Manhattan, or Minkowski distances\\n\\n[ 15, 30, 32 ], despite each implementation may greatly differ from\\nthe others.\\n\\n\\nThe use of fixed common distances, such as the distance between\\nthe eyes, to normalize facial landmarks in images with different\\nscales has been proposed as a way to make the landmarks comparable across all faces [ 24 ]. However, this normalization approach is\\nnot highly effective in reducing variability related to image acquisition conditions, particularly rotation and racial variations. Certain\\nindividuals may exhibit prominent facial characteristics that can\\nnegatively influence the performance of Machine Learning algorithms. For instance, the width of the mouth can be a critical factor\\nin identifying the emotion of happiness, which is often manifested\\nthrough a smile. In such scenarios, a classifier trained on the character Joker, the villain from the Batman superhero series, would\\nface difficulties in converging and distinguishing a person’s expression of happiness when using the Euclidean distance metric for\\nlandmark normalization.\\n\\nFigure 2 illustrates the automatic localization of 68 facial landmarks, which aim to identify various structural components of\\nthe human face, including the face contour, eyebrows, eyes, nose,\\nmouth, and others. Regarding the number of detected landmarks,\\nthe range between 60 and 80 coordinates predominates, appearing\\nin nearly half of the studies in the literature.\\n\\n**Figure 2: Locations of all 68 facial landmarks (adapted [** **35** **]).**\\n\\nWith the extraction of facial landmarks, the Image Processing\\nstage is completed. The processed data transitions from the pixel\\nspace of the image to the two-dimensional ( *𝑥* and *𝑦* ) coordinates\\nrepresenting the positions of the facial landmarks, reducing the\\ndimensionality of the problem being studied. Mathematically, rotation is a linear transformation that involves spatial coordinates, in\\nthis case, in two dimensions, preserving the magnitude of vector\\nlengths and orientation in physical space [ 47 ]. Therefore, using\\ncoordinates tends to introduce less variability than using interlandmark distances, while also avoiding the additional processing\\ntime required to compute distance metrics.\\nFrontalization techniques enable the artificial synthesis of frontal\\nviews for various types of objects, including human faces, from\\nrotated original sources. Their use tends to substantially improve\\nthe performance of classification and recognition systems that rely\\n\\n\\n259\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Macedo et al.\\n\\n\\non images by normalizing the variations that may occur during\\nthe acquisition process [ 20 ]. Additionally, the training and testing\\nstages employed by Machine Learning algorithms can be performed\\nwith standardized samples in a consistent position. Image frontalization techniques can be classified into two distinct forms, as stated\\nby [ 46 ]: appearance-based frontalization, where the original image\\nis rendered to the frontal view, and coordinate-based frontalization,\\nwhich uses a model trained with images of the object under study.\\n### **2.2** **Classification**\\n\\nMachine learning classifiers are algorithms that learn patterns and\\nrelationships within data to make predictions or classify new instances. These classifiers play a crucial role in emotion recognition,\\nsuch as detection of smiles, fraud detection, and recommendation\\nsystems. There are several types of machine learning classifiers,\\neach with its own structures, rules, processes, and reasoning. The\\nfollowing five classifiers were exploited by our proposal.\\nThe *Perceptron* is a connectionist algorithm that can effectively\\nidentify complex nonlinear relationships between input and output\\ndata [ 51 ]. It has only one input and one output layer and does not\\nhave any hidden layers. The K-nearest neighbor (KNN) algorithm\\nis an instance-based learning classification technique that classifies a sample based on the labels of its k-nearest neighbors in a\\nfeature space [ 4 ]. KNN algorithm is usually implemented using the\\nEuclidean distance as the distance metric between the samples.\\nThe *Decision Tree* (D3) algorithm creates rules for learning and\\nmimics human logical reasoning for classification, by splitting the\\nproblem’s attributes based on the highest performance gain at each\\ndivision[ 21 ]. Decision trees are made up of a collection of nodes\\nthat store information at their ends. The Random Forest (RFC)\\nalgorithm is an ensemble learning method that constructs multiple\\ndecision trees, each of which is trained on a random subset of\\nthe input features and data samples [ 40 ]. The final prediction is\\nthen made by aggregating the predictions of all individual decision\\ntrees. This method can reduce overfitting and improve the model’s\\ngeneralization ability[12].\\nThe *Multilayer Perceptron (MLP)* is a neural network structure\\nthat consists of multiple layers, including an input layer, one or\\nmore hidden layers, and an output layer[ 21 ]. MLP can handle nonlinearly separable problems due to the nonlinearity introduced by\\nthe activation functions in each neuron. SVM stands for *Support*\\n*Vector Machine*, which is a powerful and widely used supervised\\nmachine learning algorithm. It is primarily used for classification\\ntasks but can also be adapted for regression and outlier detection.\\n### **3 REGL**\\n\\nThe REGL (Recognizing Emotions through Facial Expression and\\nLandmark normalization) method aims at recognizing facial expressions and human emotions depicted in digital images. The method\\nextracts relative positional data from facial structures and calculates\\na more accurate measure of facial muscle movements, such as those\\nproduced by facial expressions. The steps of the REGL method, as\\nshown in Figure 3, are as follows:\\n\\n(1) **Histogram Equalization** for reduction of radiometric variability\\n(brightness and contrast).\\n\\n\\n(2) **Gray Scale Conversion** for reduction of the three color channels\\nto a single channel.\\n(3) **Facial Detection** for reduction of the search area by delineating\\nthe Region of Interest.\\n(4) **Landmark Extraction** for changing in data dimensionality, where\\npixels are replaced by the two-dimensional coordinates of landmarks.\\nAfter the changes, the domain comprises only 136 variables (68\\ncoordinates for the x-axis and another 68 for the y-axis).\\n(5) **Min-max Normalization of Coordinates** for reduction of the\\nscale factor, which accounts for the proximity difference between\\nthe actor and the capturing device. Innovation in the emotion recognition process.\\n(6) **Frontalization** for reduction of geometric variability caused by\\nvarious rotations in both the x-axis and y-axis. The coordinates are\\nadjusted to simulate a frontal position.\\n(7) **Normalization by Actor’s Face (Delta standard)** eliminates anatomical variations.\\n\\n(8) **Coordinate Vector** for concatenation of the coordinates into a\\nvector to facilitate the induction of Machine Learning algorithms.\\nThe final dimension of the vector is 1 row for each actor with 136\\ncolumns, i.e., 68 columns for the x-coordinates and 68 columns for\\nthe y-coordinates.\\n\\nThe first two steps involve pre-processing tasks to enable facial\\ndetection. After, the REGL method encompasses three main stages\\n– (1) extraction and normalization of landmark coordinates, (2)\\nfrontalization, and normalization, and (3) extraction of relative\\nposition measures of the facial expression in the image with respect\\nto the neutral face of the actor. These steps precede the construction\\nof Machine Learning-based algorithms, i.e., they pertain to data\\npre-processing aimed at achieving better standardization of input\\ninformation. Next, we present details considering each main stage.\\n### **3.1 Manipulation of Landmarks**\\n\\nAfter delineating the Region of Interest in each image, the landmark\\nextraction occurs changing pixels by the two-dimensional coordinates of landmarks, as mentioned. The advantage is the reduction\\nin data dimensionality. Furthermore, to minimize the effects of scale\\nvariability of the cordinates, the coordinates are normalized using\\nthe minimum and maximum (min-max) values for each axis, as\\ndescribed by Equation 1 for the horizontal coordinates on the *𝑥*\\naxis and Equation 2 for the vertical coordinates on the *𝑦* axis. This\\nnormalization process has a constant computational cost of O(1),\\nand it is defined as follows:\\n\\n� *𝑥* *𝑖* − *𝑚𝑖𝑛* ( *𝑥* )\\n*𝑥* *𝑖* = (1)\\n*𝑚𝑎𝑥* ( *𝑥* ) − *𝑚𝑖𝑛* ( *𝑥* )\\n\\n*𝑦* � *𝑖* = *𝑦* *𝑖* − *𝑚𝑖𝑛* ( *𝑦* ) (2)\\n*𝑚𝑎𝑥* ( *𝑦* ) − *𝑚𝑖𝑛* ( *𝑦* )\\n\\nThus, all *𝑥* and *𝑦* coordinates are scaled to values between zero\\nand one, minimizing the scale effect. It is important to note that\\nthis transformation does not alter the proportions between the\\ncoordinates. Therefore, the rescaled face remains similar to the\\noriginal, enabling its use for facial recognition purposes. Hence, the\\nmethodology presented here requires prior knowledge of the actor\\ndepicted in the image.\\n\\n\\n260\\n\\n\\n-----\\n\\nRecognition of Emotions through Facial Geometry with Normalized Landmarks WebMedia’2024, Juiz de Fora, Brazil\\n\\n**Figure 3: REGL: Recognizing Emotions through Facial Expression and Landmark normalization. Steps: 1 - Original image (raw**\\n**data); 2 - Image processing to gray-scale image; 3 - Pre-trained model recognizes the face in the image; 4 - Landmarks are set; 5 -**\\n**Another pre-trained model transforms the 2D landmarks into 3D landmarks; 6 - The 3D landmarks are rotated to the frontal**\\n**position; 7 - A new round of face recognition and landmarks setting is performed; 8 - Coordinates of the evaluated expression**\\n**are compared with coordinates of the neutral face of the same person, and the delta divergences are used to create a data vector**\\n**and train classifiers.**\\n\\n### **3.2 Coordinates Frontalization and** **Normalization**\\n\\nThe process of frontalization aims to reduce variations in face rotation, both horizontally and vertically, with respect to the image plane, by centering the pose in a frontal manner. Before the\\nfrontalization, it is necessary to recognize the variations in facial\\nexpressions.\\nThe REGL method first needs to recognize variations in facial\\nexpressions because anatomical differences among different actors\\nin images can be a significant source of noise, making it challenging\\nto detect morpho-geometric patterns of the face when aiming for\\nemotion recognition. In light of this, we propose evaluating the\\nvariation in patterns when a facial expression is performed. This approach requires a neutral facial pattern instead of evaluating static\\nmorpho-geometric patterns, as is typically done in facial expression\\ndetection [ 9, 33, 37 ]. Specifically, we propose that a frontal image\\n*𝐴* of the resting face (neutral facial expression), serving as a reference, undergoes facial detection, landmark extraction, coordinate\\nnormalization, and frontalization, resulting in the generation of 136\\nreference coordinates, which form the vector [−→] *𝐴* .\\nA second image *𝐵*, the subject of evaluation, undergoes the same\\nprocess, generating another set of 136 coordinates and forming the\\nvector [−→] *𝐵* . This leads to the creation of the final information vector,\\n\\n\\n−→\\nΔ *𝐴𝐵*, consisting of 136 variables, where each coordinate *𝑖* is obtained as Δ *𝐴𝐵* *𝑖* = *𝐴𝑖* − *𝐵𝑖* . Therefore, [−→] Δ *𝐴𝐵* contains the information\\nabout the relative variation of the frontalized coordinates of *𝐵* with\\n\\nrespect to the coordinates of the resting face *𝐴* .\\nSince the expression of interest in *𝐵* is always compared to the\\nexpression of the same actor in a neutral position, which has been\\npreviously labeled and known in *𝐴*, problems of anatomical variability are minimized. As an advantage, it is possible not only to\\nclassify a static geometric pattern, such as the geometric pattern\\ncharacterizing a smile, but also to **quantitatively measure the**\\n**deformation produced by this movement** (See Figure 3 - steps\\n6, 7 and 8). We began the recognition of emotions by detecting\\nsmiles, as detailed in [ 35 ]. Next, we present our overall algorithm\\nthat implements the REGL method.\\n### **3.3 An Overall Algorithm**\\n\\nIn summary, Algorithm 1 outlines the technical steps of facial emotion recognition in the REGL method. The input consists of all the\\nimages from the facial expression databases as Cohn-Kanade, RafD,\\nNimStim, KDEF, and Jaffe (see further), and the output is a reusable\\nMachine Learning Model (MLM) for emotion recognition.\\nThe algorithm iterates over all input images *𝐺* (line 2) and for\\neach image (line 3), initiates image processing and facial detection\\n(line 4). If successful (line 5), the processing to reduce data variability\\n\\n\\n261\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Macedo et al.\\n\\n\\nruns sequentially landmark extraction (line 6), normalization of xcoordinates using min-max normalization (line 7), normalization of\\ny-coordinates using min-max normalization (line 8), frontalization\\nof coordinates (line 9), and normalization by the actor’s face (delta\\npattern) (line 10). Finally, the coordinates are inserted into the\\ninformation vector (line 11). After processing all the images, the\\nfinal vector undergoes the classification process and the creation\\nof the MLM (line 14).\\n\\n**Al** **g** **orithm 1** - REGL\\n\\n**Input:** Datasets (G)\\n**Output..:** ML Models (MLM)\\n\\n1: **BEGIN**\\n\\n2: **WHILE G** has images\\n\\n3: Load Image G(I)\\n\\n4: face ← face detection G(I)\\n\\n5: **IF** face ≠ ∅ **SO**\\n\\n6: coordinates ← Extraction of 68 *𝑙𝑎𝑛𝑑𝑚𝑎𝑟𝑘𝑠*\\n\\n7: coordinates ← Norm. coordinates x\\n\\n8: coordinates ← Norm. coordinates y\\n\\n9: coordinates ← Coordinates Frontalization\\n\\n10: coordinates ← Normalization (Δ)\\n\\n11: coordinatesArray ← coordinates\\n\\n12: **END-IF**\\n\\n13: **END-WHILE**\\n\\n14: MLM ← Classifier(coordinatesArray)\\n\\n15: **RETURN** MLM\\n\\n16: **END**\\n### **4 EXPERIMENTS AND RESULTS**\\n\\nThis section presents the experiments conducted considering the\\ndatasets used, the obtained results, and the discussions regarding\\nthe processing of the REGL method.\\n### **4.1 Datasets**\\n\\nA facial expression database is a collection of digital images or\\nvideo clips featuring different actors. Its content is essential for\\ntraining, testing, and validating Machine Learning (ML) algorithms,\\nas well as for the development of facial recognition and facial expression recognition systems, which encompass emotions. These\\ndatabases of images are often guided by the theoretical basis of\\nhuman emotions [ 1 ], which assumes the existence of six different\\ntypes of facial expressions: happiness, fear, disgust, anger, surprise,\\nand sadness, in addition to the neutral (or indifferent) expression.\\nTo experiment REGL, we selected different databases aiming to\\naugment the variability of noises in the manipulated images. The\\nfree facial expression databases used in our study were:\\n\\n  - **Extended Cohn-Kanade Dataset (CK+):** attempts to standardize facial expressions according to the reference of FACS\\nproposed by [ 13 ]. It contains frontal images with all seven\\nuniversal emotions. CK+ was published in 2000’s by [8].\\n\\n  - **The Japanese Female Facial Expression (JAFFE) Data-**\\n**base:** comprises 210 images displaying the seven universal\\nfacial expressions, presented by ten Japanese women [ 31 ] in\\n\\n\\na frontal position. The images were obtained by the Department of Psychology at Kyushu University, Japan.\\n\\n  - **Radboud Faces Database (RafD):** consists of images of\\n67 actors (including Caucasian men and women, European\\nCaucasian children, and Moroccan men) [ 28 ]. Following the\\nFACS methodology, all actors were trained to express the\\nfollowing emotions: anger, disgust, fear, happiness, sadness,\\nsurprise, and neutral.\\n\\n  - **The Karolinska Directed Emotional Faces (KDEF):** comprises 4,900 images with all universal emotions of seventy\\nactors [ 17 ]. Each expression is captured from five different\\nangles. The image set was developed by the Department of\\nClinical Neuroscience, Psychology Section, at the Karolinska\\nInstitute in Sweden.\\n\\n  - **The Nimstim set of Facial Expressions:** is well-known in\\nmedical literature [ 42 ]. Although it is rarely used for emotion\\nrecognition, it has received over 2,000 citations in scientific\\npapers. Nimstim contains 672 frontal images of 43 professional actors, including 18 women and 25 men, aged between\\n21 and 30 years. All universal emotions are represented.\\n\\nThe reason for using different datasets was twofold: (i) to manipulate various ethnicities, ensuring that the classifier could generalize\\nwithout being influenced by ethnic traits, and (ii) to consider sources\\nof data variability, including variations in geometry, lighting, and\\n\\nrotation.\\n### **4.2 Training and Evaluation**\\n\\nThe proposed REGL method was evaluated using the mentioned\\ndatasets in its sequence of steps culminates in the creation of a feature vector, comprising a set of 136 coordinates, after performing\\nall normalization steps, which serves as the input for the SVM, MLP,\\nRandom Forests, Extra Trees, and Decision Tree Machine Learning\\nalgorithms [1] . For Decision Tree, the maximum node depth was\\nset to 4. Depths below this configuration failed to generalize the\\ncategories effectively, while increasing the depth did not improve\\nthe results. Both Random Forests and Extra Trees used the same\\ndepth configuration as Decision Trees, with the number of trees set\\nto 1000. Fewer trees did not consistently converge on the categories,\\nwhile more trees exponentially increased execution time, making\\nprocessing impractical. The MLP algorithm was configured with\\nthree main settings: 3000 iterations, a learning rate of 0.001, and\\nthe Adam optimizer to activate the neural network. This setup produced results proportional to processing time and model accuracy.\\nThe SVM algorithm used a radial basis function kernel to separate\\nthe categories and a C parameter of 5 to penalize incorrect classifications. These algorithms assessed the quality of the extracted\\nfeatures and the classification of emotions [2] .\\n\\n1 The optimal parameters for the machine learning algorithms were exhaustively tested\\non the HPC (high-performance computing) server Aguia4 at the University of São\\nPaulo. The cluster consists of 128 physical servers, each with 20 cores and 512 GB of\\nRAM. The processor used is an Intel(R) Xeon CPU E7-2870 at 2.40 GHz, with a 256 TB\\nfilesystem for temporary files.\\n2 All algorithms were implemented and processed on a laptop with an Intel(R) Core i5\\neighth-generation processor, model 8265U at 3.9 GHz, with 8 GB of RAM, a 128 GB\\nSSD, and an integrated Intel UHD graphics card. The software environment included\\nLinux Ubuntu 18.04 and Python 3.6.9.\\n\\n\\n262\\n\\n\\n-----\\n\\nRecognition of Emotions through Facial Geometry with Normalized Landmarks WebMedia’2024, Juiz de Fora, Brazil\\n\\n**Figure 4: Accuracy Evolution - REGL Method**\\n\\n**Figure 5: Processing Time - REGL Method**\\n\\n\\nTo present the results, graphics depicting the percentages of\\naccuracy and error were employed. ROC (Receiver Operating Characteristic) curves were created based on the true positive (TP) and\\nfalse positive (FP) rates to represent the discriminative capability\\nof the classifiers, i.e., their ability to correctly classify specific emotions. In the ROC graphics, points closer to (0,1), indicating higher\\nTP and lower FP, represent a more consistent and generalizable\\nclassification.\\nFigure 4 showcases the results of emotion recognition through\\nthe sequential execution of the REGL method steps. It is worth\\nhighlighting that the succession of coordinate normalization techniques played a crucial role in the improvement and accuracy of the\\nresults, irrespective of the algorithm employed for classification.\\nThe best performance of the REGL method was achieved using\\nthe SVM algorithm, yielding an accuracy of 87.1% and a processing\\ntime of only 4.33 seconds. It is important to note that the experiments utilized 10-fold cross-validation during training. It suggested\\nthat emotions are also universally recognizable by machines, and\\ntheir recognition can be effectively performed artificially.\\nAnother important aspect to be analyzed regarding the REGL\\nmethod is the processing time of the algorithms in relation to the\\nchaining of normalization steps. According to Figure 5, the data\\nis consistent and demonstrates that the steps of the REGL method\\ndecrease the processing time of all the experimented algorithms.\\nSVM and D3 achieved the best constant performance. Therefore,\\nthe reduction of sample variability in the coordinates, provided\\nby min-max normalization, frontalization, and normalization by\\nactor’s face, contributes to achieve a good performance of ML algorithms responsible for human emotion recognition and optimizes\\nthe processing time required in each step.\\nFigure 6 presents a comparative analysis of the performance of\\nvarious algorithms used in the emotion recognition process, after\\nimplementing all the variability reduction steps, through a ROC\\ngraphic. The dashed diagonal line represents the performance of\\na random classifier and serves as a reference for evaluating the\\nothers. Points above the diagonal in the ROC space represent better\\nclassification than points below the diagonal. A perfect classifier is\\none that produces a point close to ( 0 *,* 1 ) and an area close to 1. This\\nmeans that the classifier has a zero false positive rate and a 100%\\n\\n\\n**Figure 6: ROC Curve - Emotion Recognition with the REGL**\\n**Method**\\n\\ntrue positive rate. Analyzing Figure 6, the ROC curves of the SVM\\nand MLP algorithms yielded more accurate results than the others.\\nIn conclusion, the average performance of SVM was the best for all\\nmeasures evaluated.\\n### **5 RELATED WORK**\\n\\nMethods of emotion recognition encompass a variety of approaches\\naimed at identifying and categorizing human emotions based on\\nobservable cues. Differently of REGL, these methods do not usually explore these set of techniques of manipulation of landmarks,\\nfrontalization and coordinates normalization sequentially. Here, we\\npresent related work of our proposal. In the Background section,\\nwe have already described methods used for face detection and\\nlandmark extraction.\\n\\n\\n263\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Macedo et al.\\n\\n\\nIn Testa et al., the authors conducted a literature review of studies\\nthat discover the synthesis of facial expressions through landmark\\nextraction [ 41 ]. The predominant facial structures detected in these\\nstudies include the eyes, eyebrows, mouth, face contour, and nose.\\nThe authors observed that machine learning approaches are the\\nmost widely used and few studies used metrics to evaluate the\\nresults.\\nÁlvarez, Luengo, and Lawrence presented a method for emotion\\nrecognition using the Euclidean distance between landmarks in\\nthe eye and mouth regions [ 1 ]. The technique of relative error\\ndistribution was employed as a validation method for accuracy. The\\nfinal average accuracy and standard deviation were respectively\\n94.53% ± 2.47% using just the LabeledFacesInTheWild (LFW) dataset.\\nCui developed a method for smile recognition in images containing\\npeople, utilizing the Euclidean distance as an attribute to train an\\nalgorithm called Extreme Learning Machine [ 9 ]. An accuracy of\\n93.40% was achieved on the LFW dataset too.\\n\\nSalman, Madani, and Kissi proposed a facial expression recognition method for training a decision tree called Classification and\\nRegression Tree, which utilizes the measurements of the distances\\nbetween the width and height of the mouth as the feature vector [39].\\nHassner et al. explored a simpler approach using an unmodified\\n3D reference surface, which approximates the shape of all input\\nfaces [ 20 ]. This facilitated the development of a direct, efficient,\\nand easily implementable method for frontalization. Importantly, it\\ngenerated aesthetically pleasing frontal views and proved surprisingly effective for face recognition and gender estimation. In the\\nLabeledFacesInTheWild (LFW) dataset, 97.5% of the 1432 images\\nwere successfully frontalized.\\nJia et al. published a review article presenting spontaneous and\\nposed facial expression databases and various computer visionbased detection methods, including those specific to smile detection\\n\\n[ 38 ]. They highlighted the importance of generalization ability in\\nemotion detection and the detection of specific emotions based\\non unique facial features. Considering multimodal input, Bohy et\\nal. developed a deep learning-based multimodal smile and laugh\\nclassification system, considering the use of audio and vision-based\\nmodels as well as a fusion approach [3]\\nGuyon and Elisseeff demonstrated, through tests with linear and\\nnonlinear classifiers, that feature selection reduces the variability of\\nthe studied problem [ 19 ]. This is because certain classifiers struggle\\nwith duplicate data, and as a result, accuracy tends to increase.\\nThe choice of method for emotion recognition depends on several\\nfactors, including the available data, application context, and desired\\naccuracy. Using different datasets, REGL demonstrated equivalent\\naccuracy to other similar methods measured that considered specific datasets. When compared to related works, REGL addresses\\na different set of techniques and shows excellent response time,\\nwhich is a fundamental requirement for real-time emotion recognition in mobile applications. This is due to normalized landmarks are\\ngeneralized two-dimensional coordinates, enabling low-cost and\\nreal-time processing, particularly suitable for devices with limited\\nprocessing capabilities, such as cellphones.\\n\\n### **6 FINAL REMARKS**\\n\\nEmotions provide our first means of nonverbal communication\\ndeveloped throughout our lives. Through emotions, humans are\\nable to interact with others and the environment in which they\\nare immersed. This interaction is possible because humans almost\\nalways translate their emotions into detectable physical movements,\\nsuch as facial expressions, which are essential for social interactions.\\nDespite being a trivial mechanism easily recognized by all human\\nbeings, emotion recognition is a challenging task for machines and\\ncomputers. Thus, the aim of this work was to develop an artificial\\nemotion recognition method called REGL, to extract and analyze\\nthe morphometric characteristics of the facial region, similar to the\\nmethod used by human beings. REGL combined various techniques\\nof digital image processing and statistical methods to evaluate and\\nreduce variability in the images used. Among them, noteworthy\\nare the normalization of coordinates using min-max, capable of\\noptimizing the effects of scale factor, frontalization, responsible for\\nreducing the effects caused by face rotation, and delta normalization, which uses the actor’s neutral face to identify other emotions,\\nthereby minimizing the effects of anatomical and racial variations.\\nThe results of our experiments indicated the efficiency of the\\nREGL method in the classification and artificial recognition of human emotions, resulting in an accuracy rate above 90% for all processed facial expression from diverse databases. Regarding only\\nsmile detection, a final accuracy rate close to 95% was achieved,\\nsurpassing previous works in the literature. REGL surpassed time\\nprocessing of methods that evaluated this variable. Here, we did\\nnot exploit quantitative aspects of REGL because our focus was\\nclassification. However, REGL can measure the intensity and speed\\nof movements. We have used it in SofiaFala [3] .\\nConvolutional Neural Networks has been explored for emotion\\nrecognition despite their drawbacks: high computational demands,\\nthe need for large labeled datasets, sensitivity to data variability,\\noverfitting due to unbalanced data, and difficult to interpret. These\\nlimitations suggest that alternative methods as REGL may be useful.\\nThe main contribution extracted from this work is the creation\\n\\nof the min-max normalization technique for facial landmark coordinates, reducing the scale factor effect in images. The main limitation\\ninvolving the REGL method is associated with the fact that REGL\\nis tailored for contexts whose the identity of the actor is assured.\\nTherefore, experiments considering uncontrolled environments will\\ndemand coupling a routine of face recognition. Moreover, this work\\nwill be continued in terms of: (i) analyzes of the performance of\\nthe REGL with a greater number of facial landmark coordinates; (ii)\\nanalyzes of the REGL with three-dimensional coordinates of facial\\nlandmarks; (iii) evaluation of the performance of the REGL method\\nusing other databases, preferably with black and Asian actors because the used databases did not incorporate images with them; (iv)\\nassessment of the performance of the REGL method in video-based\\napplications, (v) investigation of facial recognition with coordinates\\nnormalized by min-max and frontalization; (vi) use of the quantitative responses of the REGL method, especially considering fear\\nand surprise.\\n\\n3 https://dcm.ffclrp.usp.br/sofiafala/\\n\\n\\n264\\n\\n\\n-----\\n\\nRecognition of Emotions through Facial Geometry with Normalized Landmarks WebMedia’2024, Juiz de Fora, Brazil\\n\\n### **ACKNOWLEDGMENTS**\\n\\nThis research was developed with the help of HPC resources provided by the Information Technology Superintendence of the University of São Paulo.\\nThe presented method was discussed in the SofiaFala project\\nsupported by CNPq (Brazil), through the process 442533/2016-0,\\nwhich supports the development of an application for mobile devices to assist children with Down Syndrome, to support training\\nspeech therapy for speech development.\\n### **REFERENCES**\\n\\n[1] Mauricio Alvarez, David Luengo, and Neil Lawrence. 2013. Linear Latent Force\\nModels Using Gaussian Processes. *IEEE Transactions on Pattern Analysis and*\\n*Machine Intelligence* 35, 11 (Nov 2013), 2693–2705. https://doi.org/10.1109/TPAMI.\\n2013.86\\n\\n[2] Mitra B., Sharma K., Acharya S., Mishra P., and Guglani A. 2022. Real-time\\nSmile Detection using Integrated ML Model. In *2022 6th International Conference*\\n*on Intelligent Computing and Control Systems (ICICCS)* . IEEE, Madurai, India,\\n1374–1381. https://doi.org/10.1109/ICICCS53718.2022.9788399\\n\\n[3] Hugo Bohy, Kevin El Haddad, and Thierry Dutoit. 2022. A New Perspective on\\nSmiling and Laughter Detection: Intensity Levels Matter. In *2022 10th International*\\n*Conference on Affective Computing and Intelligent Interaction (ACII)* . 1–8. https:\\n//doi.org/10.1109/ACII55700.2022.9953896\\n\\n[4] Guilherme Campos, Arthur Zimek, Joerg Sander, Ricardo Campello, Barbora\\nMicenková, Erich Schubert, Ira Assent, and Michael Houle. 2016. On the\\nevaluation of unsupervised outlier detection: measures, datasets, and an empirical study. *Data Mining and Knowledge Discovery* 30 (07 2016). https:\\n//doi.org/10.1007/s10618-015-0444-8\\n\\n[5] V. Chaugule, D. Abhishek, A. Vijayakumar, P. B. Ramteke, and S. G. Koolagudi.\\n2016. Product review based on optimized facial expression detection. In *2016*\\n*Ninth International Conference on Contemporary Computing (IC3)* . IEEE, Noida,\\nIndia, 1–6. https://doi.org/10.1109/IC3.2016.7880213\\n\\n[6] Yufang Cheng and Shuhui Ling. 2008. 3D Animated Facial Expression and Autism\\nin Taiwan. In *IEEE International Conference on Advanced Learning Technologies*\\n*(ICALT 2008)* . IEEE Computer Society, Los Alamitos, CA, USA, 17–19. https:\\n//doi.org/10.1109/ICALT.2008.220\\n\\n[7] Francois Chollet. 2017. *Deep Learning with Python* (1st ed.). Manning Publications\\nCo., Greenwich, CT, USA.\\n\\n[8] Jeffrey Cohn and Takeo Kanade. 2010. The Extended Cohn-Kanade Dataset\\n(CK+): A complete dataset for action unit and emotion-specified expression. *2010*\\n*IEEE Computer Society Conference on Computer Vision and Pattern Recognition -*\\n*Workshops, CVPRW 2010*, 94 – 101. https://doi.org/10.1109/CVPRW.2010.5543262\\n\\n[9] Dongshun Cui, Guang-Bin Huang, and Tianchi Liu. 2018. ELM based smile\\ndetection using Distance Vector. *Pattern Recognition* 79 (2018), 356–369. https:\\n//doi.org/10.1016/j.patcog.2018.02.019\\n\\n[10] D Dalal and B. Triggs. 2005. Histograms of oriented gradients for human detection. In *2005 IEEE Computer Society Conference on Computer Vision and Pat-*\\n*tern Recognition (CVPR’05)*, Vol. 1. IEEE, San Diego, CA, USA, 886–893 vol. 1.\\nhttps://doi.org/10.1109/CVPR.2005.177\\n\\n[11] Charles Darwin. 2013. *The Expression of the Emotions in Man and Animals* . Cambridge University Press, England. https://doi.org/10.1017/CBO9781139833813\\n\\n[12] Alex Davies and Zoubin Ghahramani. 2014. The Random Forest Kernel and other\\nkernels for big data from random partitions. arXiv:1402.4293 [stat.ML]\\n\\n[13] Paul Ekman and Wallace V. Friesen. 1971. Constants across cultures in the face\\nand emotion. *Journal of Personality and Social Psychology* 17, 2 (1971), 124–129.\\nhttps://doi.org/10.1037/h0030377\\n\\n[14] Hugo. Filho, Oge Marques; Vieira Neto. 1999. *Processamento Digital de Imagens* .\\nBrasport, Brasil. 30–31 pages.\\n\\n[15] Gabriel Garrido and Prateek Joshi. 2018. *OpenCV 3.X with Python By Example:*\\n*Make the most of OpenCV and Python to build applications for object recognition*\\n*and augmented reality* (2nd ed.). Packt Publishing, US.\\n\\n[16] A. T. Ghorbani, G; Targhi and M. Dehshibi. 2015. HOG and LBP: Towards a\\nrobust face recognition system. In *2015 Tenth International Conference on Digital*\\n*Information Management (ICDIM)* . IEEE, Jeju, South Korea, 138–141. https:\\n//doi.org/10.1109/ICDIM.2015.7381860\\n\\n[17] Ellen Goeleven, Rudi De Raedt, Lemke Leyman, and Bruno Verschuere. 2008.\\nThe Karolinska Directed Emotional Faces: A validation study. *Cognition and*\\n*Emotion* 22, 6 (2008), 1094–1118. https://doi.org/10.1080/02699930701626582\\narXiv:https://doi.org/10.1080/02699930701626582\\n\\n[18] Rafael C Gonzales and Richard E. Woods. 2008. *Digital Image Processing* (3rd ed.).\\nPearson, New Jersey, US.\\n\\n[19] Isabelle Guyon and André Elisseeff. 2003. An Introduction to Variable and Feature\\nSelection. *J. Mach. Learn. Res.* 3 (March 2003), 1157–1182.\\n\\n\\n\\n[20] T. Hassner, E. Harel, S.and Paz, and R. Enbar. 2015. Effective face frontalization\\nin unconstrained images. In *2015 IEEE Conference on Computer Vision and Pattern*\\n*Recognition (CVPR)* . IEEE, Boston, MA, US, 4295–4304. https://doi.org/10.1109/\\nCVPR.2015.7299058\\n\\n[21] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. 2009. *The elements of*\\n*statistical learning: data mining, inference and prediction* (2 ed.). Springer, USA.\\nhttp://www-stat.stanford.edu/~tibs/ElemStatLearn/\\n\\n[22] Jiabei He, Xiaoyu Wen, and Juxiang Zhou. 2023. Advances and Application of\\nFacial Expression and Learning Emotion Recognition in Classroom. In *Proceed-*\\n*ings of the 2023 6th International Conference on Image and Graphics Processing*\\n(Chongqing, China) *(ICIGP ’23)* . Association for Computing Machinery, New\\nYork, NY, USA, 23–30. https://doi.org/10.1145/3582649.3582670\\n\\n[23] Ursula Hess. 2001. The Communication of Emotion. In *Emotions, Qualia and*\\n*Consciousness* . Singapore, 397–409. https://doi.org/10.1142/9789812810687_0031\\n\\n[24] Nurulhuda Ismail and Mas Idayu Md. Sabri. 2009. Review of Existing Algorithms\\nfor Face Detection and Recognition. In *Proceedings of the 8th WSEAS International*\\n*Conference on Computational Intelligence, Man-Machine Systems and Cybernetics*\\n(Puerto De La Cruz, Tenerife, Canary Islands, Spain) *(CIMMACS’09)* . World Scientific and Engineering Academy and Society (WSEAS), Stevens Point, Wisconsin,\\nUSA, 30–39.\\n\\n[25] A. Kumar, K.M. Baalamurugan, and B. Balamurugan. 2022. Real-Time Facial\\nComponents Detection Using Haar Classifiers. In *2022 International Conference*\\n*on Applied Artificial Intelligence and Computing (ICAAIC)* . IEEE, Salem, India,\\n01–08. https://doi.org/10.1109/ICAAIC53929.2022.9793034\\n\\n[26] Uttama Lahiri, Esube Bekele, Elizabeth Dohrmann, Zachary Warren, and Nilanjan\\nSarkar. 2011. Design of a Virtual Reality Based Adaptive Response Technology for\\nChildren with Autism Spectrum Disorder. In *Affective Computing and Intelligent*\\n*Interaction*, Sidney D’Mello, Arthur Graesser, Björn Schuller, and Jean-Claude\\nMartin (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 165–174.\\n\\n[27] Oliver Langner, Ron Dotsch, Gijsbert Bijlstra, Daniel H. J. Wigboldus, Skyler T.\\nHawk, and Ad van Knippenberg. 2010. Presentation and validation of the Radboud\\nFaces Database. *Cognition and Emotion* 24, 8 (2010), 1377–1388. https://doi.org/\\n10.1080/02699930903485076 arXiv:https://doi.org/10.1080/02699930903485076\\n\\n[28] Oliver Langner, Ron Dotsch, Gijsbert Bijlstra, Daniel H. J. Wigboldus, Skyler T.\\nHawk, and Ad van Knippenberg. 2010. Presentation and validation of the Radboud\\nFaces Database. *Cognition and Emotion* 24, 8 (2010), 1377–1388. https://doi.org/\\n10.1080/02699930903485076 arXiv:https://doi.org/10.1080/02699930903485076\\n\\n[29] K. Li, F. Xu, J. Wang, Q. Dai, and Y. Liu. 2012. A data-driven approach for facial\\nexpression synthesis in video. In *2012 IEEE Conference on Computer Vision and*\\n*Pattern Recognition* . IEEE, Providence, RI, US, 57–64. https://doi.org/10.1109/\\nCVPR.2012.6247658\\n\\n[30] Shan Li and Weihong Deng. 2018. Deep Facial Expression Recognition: A Survey.\\n*Computing Research Repository (CoRR)* abs/1804.08348 (2018). https://doi.org/10.\\n1109/TAFFC.2020.2981446\\n\\n[31] Michael Lyons, Miyuki Kamachi, and Jiro Gyoba. 2017. Japanese Female Facial\\nExpression (JAFFE) Database. (7 2017). https://doi.org/10.6084/m9.figshare.\\n5245003.v2\\n\\n[32] Dhwani Mehta, Mohammad Faridul Haque Siddiqui, and Ahmad Y. Javaid. 2018.\\nFacial Emotion Recognition: A Survey and Real-World User Experiences in Mixed\\nReality. *Sensors* 18, 2 (2018). https://doi.org/10.3390/s18020416\\n\\n[33] Karnati Mohan, Ayan Seal, Ondrej Krejcar, and Anis Yazidi. 2021. Facial Expression Recognition Using Local Gravitational Force Descriptor-Based Deep\\nConvolution Neural Networks. *IEEE Transactions on Instrumentation and Mea-*\\n*surement* 70 (2021), 1–12. https://doi.org/10.1109/TIM.2020.3031835\\n\\n[34] A Monzo, D; Albiol and M. J. Mossi. 2010. A Comparative Study of Facial\\nLandmark Localization Methods for Face Recognition Using HOG descriptors. In\\n*2010 20th International Conference on Pattern Recognition* . IEEE, Istanbul, Turkey,\\n1330–1333. https://doi.org/10.1109/ICPR.2010.1145\\n\\n[35] Leandro Persona, Fernando Meloni, and Alessandra Macedo. 2023. An accurate\\nreal-time method to detect the smile facial expression. In *Anais do XXIX Simpósio*\\n*Brasileiro de Sistemas Multimídia e Web* (Ribeirão Preto/SP). SBC, Porto Alegre,\\nRS, Brasil, 46–55. https://sol.sbc.org.br/index.php/webmedia/article/view/25865\\n\\n[36] Rosalind W. Picard. 2016. Automating the Recognition of Stress and Emotion:\\nFrom Lab to Real-World Impact. *IEEE MultiMedia* 23, 3 (July 2016), 3–7. https:\\n//doi.org/10.1109/MMUL.2016.38\\n\\n[37] I.Michael Revina and W.R. Sam Emmanuel. 2018. A Survey on Human Face\\nExpression Recognition Techniques. *Journal of King Saud University - Computer*\\n*and Information Sciences* (2018). https://doi.org/10.1016/j.jksuci.2018.09.002\\n\\n[38] Jia S, Wang S, Hu C., Webster PJ, and Li X. 2021. Detection of Genuine and Posed\\nFacial Expressions of Emotion: Databases and Methods. *Front. Psychol. - Sec.*\\n*Perception Science* 11 (15 January 2021), 12p. https://doi.org/10.3389/fpsyg.2020.\\n580287\\n\\n[39] F. Z. SALMAN, A. MADANI, and M. KISSI. 2016. Facial Expression Recognition\\nUsing Decision Trees. In *2016 13th International Conference on Computer Graphics,*\\n*Imaging and Visualization (CGiV)* . IEEE, Beni Mellal, Morocco, 125–130. https:\\n//doi.org/10.1109/CGiV.2016.33\\n\\n[40] Paul F. Smith, Siva Ganesh, and Ping Liu. 2013. A comparison of random forest\\nregression and multiple linear regression for prediction in neuroscience. *Journal*\\n\\n\\n265\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Macedo et al.\\n\\n\\n*of Neuroscience Methods* 220, 1 (2013), 85–91. https://doi.org/10.1016/j.jneumeth.\\n2013.08.024\\n\\n[41] Rafael Luiz Testa, Cléber Gimenez Corrêa, Ariane Machado-Lima, and Fátima\\nL. S. Nunes. 2019. Synthesis of Facial Expressions in Photographs: Characteristics,\\nApproaches, and Challenges. *ACM Comput. Surv.* 51, 6, Article 124 (jan 2019),\\n35 pages. https://doi.org/10.1145/3292652\\n\\n[42] Nim Tottenham, James Tanaka, Andrew Leon, Thomas Mccarry, Marcella Nurse,\\nTodd Hare, David Marcus, Alissa Westerlund, Bj Casey, and Charles Nelson. 2009.\\nThe NimStim set of Facial Expressions: Judgments from Untrained Research\\nParticipants. *Psychiatry research* 168 (07 2009), 242–9. https://doi.org/10.1016/j.\\npsychres.2008.05.006\\n\\n[43] Sana Ullah and Wenhong Tian. 2021. A Systematic Literature Review of\\nRecognition of Compound Facial Expression of Emotions. In *Proceedings of the*\\n*2020 4th International Conference on Video and Image Processing* (Xi’an, China)\\n*(ICVIP ’20)* . Association for Computing Machinery, New York, NY, USA, 116–121.\\nhttps://doi.org/10.1145/3447450.3447469\\n\\n[44] Jean Vaillancourt. 2010. Statistical Methods for Data Mining and Knowledge\\nDiscovery. In *Proceedings of the 8th International Conference on Formal Concept*\\n*Analysis* (Agadir, Morocco) *(ICFCA’10)* . Springer-Verlag, Berlin, Heidelberg, 51–\\n60.\\n\\n[45] Paul Viola and Michael J. Jones. 2001. Robust Real-Time Face Detection. *Interna-*\\n*tional Journal of Computer Vision* 57, 2 (2001), 137–154. https://doi.org/10.1023/B:\\n\\n\\nVISI.0000013087.49260.fb\\n\\n[46] V. Vonikakis and S. Winkler. 2020. Identity-Invariant Facial Landmark Frontalization For Facial Expression Analysis. In *International Conference on Image*\\n*Processing (ICIP)* . 2020 IEEE ICIP, Abu Dhabi, United Arab Emirates, 2281–2285.\\nhttps://doi.org/10.1109/ICIP40778.2020.9190989\\n\\n[47] P. Winterle. 2014. *Vetores e Geometria Analítica* . MAKRON. https://books.google.\\ncom.br/books?id=AKhivgAACAAJ\\n\\n[48] Yue Wu and Qiang Ji. 2018. Facial Landmark Detection: A Literature Survey.\\n*International Journal of Computer Vision* 2 (2018), 115–142. https://doi.org/10.\\n1007/s11263-018-1097-z\\n\\n[49] L. Xie, W.; Shen and J. Jiang. 2017. A Novel Transient Wrinkle Detection Algorithm and Its Application for Expression Synthesis. *IEEE Transactions on*\\n*Multimedia* 19, 2 (Feb 2017), 279–292. https://doi.org/10.1109/TMM.2016.2614429\\n\\n[50] W. XIE, L. SHEB, M. YANG, and Q. HOU. 2015. Lighting difference based wrinkle\\nmapping for expression synthesis. In *2015 8th International Congress on Image*\\n*and Signal Processing (CISP)* . IEEE, Shenyang, China, 636–641. https://doi.org/\\n10.1109/CISP.2015.7407956\\n\\n[51] Xiaoming Zhao and Shiqing Zhang. 2016. A Review on Facial Expression Recognition: Feature Extraction and Classification. *IETE Technical Re-*\\n*view* 33, 5 (2016), 505–517. https://doi.org/10.1080/02564602.2015.1117403\\narXiv:https://doi.org/10.1080/02564602.2015.1117403\\n\\n\\n266\\n\\n\\n-----\\n\\n',\n",
       "  'artigo_tokenizado': ['#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Recognition',\n",
       "   'of',\n",
       "   'Emotions',\n",
       "   'through',\n",
       "   'Facial',\n",
       "   'Geometry',\n",
       "   'with',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Normalized',\n",
       "   'Landmarks',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Alessandra',\n",
       "   'Alaniz',\n",
       "   'Macedo',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'ale.alaniz@usp.br',\n",
       "   'University',\n",
       "   'of',\n",
       "   'São',\n",
       "   'Paulo',\n",
       "   '(',\n",
       "   'USP',\n",
       "   ')',\n",
       "   'FFCLRP',\n",
       "   ',',\n",
       "   'DCM',\n",
       "   ',',\n",
       "   'PPG',\n",
       "   '-',\n",
       "   'CA',\n",
       "   'Ribeirão',\n",
       "   'Preto',\n",
       "   ',',\n",
       "   'São',\n",
       "   'Paulo',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'ABSTRACT',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Emotion',\n",
       "   'recognition',\n",
       "   'holds',\n",
       "   'pivotal',\n",
       "   'significance',\n",
       "   'in',\n",
       "   'human',\n",
       "   'social',\n",
       "   'interactions',\n",
       "   ',',\n",
       "   'as',\n",
       "   'it',\n",
       "   'entails',\n",
       "   'the',\n",
       "   'discernment',\n",
       "   'of',\n",
       "   'facial',\n",
       "   'patterns',\n",
       "   'intricately',\n",
       "   '\\n',\n",
       "   'linked',\n",
       "   'to',\n",
       "   'diverse',\n",
       "   'emotional',\n",
       "   'states',\n",
       "   '.',\n",
       "   'The',\n",
       "   'scientific',\n",
       "   ',',\n",
       "   'artistic',\n",
       "   ',',\n",
       "   'medical',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'and',\n",
       "   'marketing',\n",
       "   'domains',\n",
       "   'have',\n",
       "   'all',\n",
       "   'demonstrated',\n",
       "   'substantial',\n",
       "   'interest',\n",
       "   'in',\n",
       "   'comprehending',\n",
       "   'emotions',\n",
       "   ',',\n",
       "   'resulting',\n",
       "   'in',\n",
       "   'the',\n",
       "   'emergence',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'refinement',\n",
       "   'of',\n",
       "   'techniques',\n",
       "   'and',\n",
       "   'computational',\n",
       "   'methodologies',\n",
       "   'to',\n",
       "   'facilitate',\n",
       "   'automated',\n",
       "   'emotion',\n",
       "   'recognition',\n",
       "   '.',\n",
       "   'In',\n",
       "   'this',\n",
       "   'study',\n",
       "   ',',\n",
       "   'we',\n",
       "   'introduce',\n",
       "   '\\n',\n",
       "   'a',\n",
       "   'novel',\n",
       "   'method',\n",
       "   'named',\n",
       "   'REGL',\n",
       "   '(',\n",
       "   'Recognizing',\n",
       "   'Emotions',\n",
       "   'through',\n",
       "   'Facial',\n",
       "   'Expression',\n",
       "   'and',\n",
       "   'Landmark',\n",
       "   'normalization',\n",
       "   ')',\n",
       "   'aimed',\n",
       "   'at',\n",
       "   'recognizing',\n",
       "   '\\n',\n",
       "   'facial',\n",
       "   'expressions',\n",
       "   'and',\n",
       "   'human',\n",
       "   'emotions',\n",
       "   'depicted',\n",
       "   'in',\n",
       "   'images',\n",
       "   '.',\n",
       "   'REGL',\n",
       "   '\\n',\n",
       "   'comprises',\n",
       "   'a',\n",
       "   'sequential',\n",
       "   'set',\n",
       "   'of',\n",
       "   'steps',\n",
       "   'designed',\n",
       "   'to',\n",
       "   'minimize',\n",
       "   'sample',\n",
       "   '\\n',\n",
       "   'variability',\n",
       "   ',',\n",
       "   'thereby',\n",
       "   'facilitating',\n",
       "   'a',\n",
       "   'finer',\n",
       "   'calibration',\n",
       "   'of',\n",
       "   'the',\n",
       "   'informative',\n",
       "   '\\n',\n",
       "   'aspects',\n",
       "   'that',\n",
       "   'delineate',\n",
       "   'facial',\n",
       "   'patterns',\n",
       "   '.',\n",
       "   'REGL',\n",
       "   'carries',\n",
       "   'out',\n",
       "   'the',\n",
       "   'normalization',\n",
       "   'of',\n",
       "   'facial',\n",
       "   'fiducial',\n",
       "   'points',\n",
       "   ',',\n",
       "   'called',\n",
       "   'landmarks',\n",
       "   '.',\n",
       "   'Through',\n",
       "   'the',\n",
       "   'use',\n",
       "   '\\n',\n",
       "   'of',\n",
       "   'landmark',\n",
       "   'positions',\n",
       "   ',',\n",
       "   'the',\n",
       "   'reliability',\n",
       "   'of',\n",
       "   'the',\n",
       "   'emotion',\n",
       "   'recognition',\n",
       "   '\\n',\n",
       "   'process',\n",
       "   'is',\n",
       "   'significantly',\n",
       "   'improved',\n",
       "   '.',\n",
       "   'REGL',\n",
       "   'also',\n",
       "   'exploits',\n",
       "   'classifiers',\n",
       "   'explicitly',\n",
       "   'tailored',\n",
       "   'for',\n",
       "   'the',\n",
       "   'accurate',\n",
       "   'identification',\n",
       "   'of',\n",
       "   'facial',\n",
       "   'emotions',\n",
       "   '.',\n",
       "   'As',\n",
       "   '\\n',\n",
       "   'related',\n",
       "   'works',\n",
       "   ',',\n",
       "   'the',\n",
       "   'outcomes',\n",
       "   'of',\n",
       "   'our',\n",
       "   'experimentation',\n",
       "   'yielded',\n",
       "   'an',\n",
       "   'average',\n",
       "   'accuracy',\n",
       "   'over',\n",
       "   '90',\n",
       "   '%',\n",
       "   'by',\n",
       "   'employing',\n",
       "   'Machine',\n",
       "   'Learning',\n",
       "   'algorithms',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Differently',\n",
       "   ',',\n",
       "   'we',\n",
       "   'have',\n",
       "   'experimented',\n",
       "   'REGL',\n",
       "   'with',\n",
       "   'varied',\n",
       "   'architectures',\n",
       "   '\\n',\n",
       "   'and',\n",
       "   'datasets',\n",
       "   'including',\n",
       "   'racial',\n",
       "   'factors',\n",
       "   '.',\n",
       "   'We',\n",
       "   'surpass',\n",
       "   'related',\n",
       "   'works',\n",
       "   '\\n',\n",
       "   'considering',\n",
       "   'the',\n",
       "   'following',\n",
       "   'contributions',\n",
       "   ':',\n",
       "   'the',\n",
       "   'REGL',\n",
       "   'method',\n",
       "   'represents',\n",
       "   'an',\n",
       "   'enhanced',\n",
       "   'approach',\n",
       "   'in',\n",
       "   'terms',\n",
       "   'of',\n",
       "   'hit',\n",
       "   'rate',\n",
       "   'and',\n",
       "   'response',\n",
       "   '\\n',\n",
       "   'time',\n",
       "   ',',\n",
       "   'and',\n",
       "   'REGL',\n",
       "   'generates',\n",
       "   'resilient',\n",
       "   'outcomes',\n",
       "   'by',\n",
       "   'demonstrating',\n",
       "   '\\n',\n",
       "   'reduced',\n",
       "   'reliance',\n",
       "   'on',\n",
       "   'both',\n",
       "   'the',\n",
       "   'training',\n",
       "   'set',\n",
       "   'and',\n",
       "   'classifier',\n",
       "   'architecture',\n",
       "   '.',\n",
       "   'Moreover',\n",
       "   ',',\n",
       "   'REGL',\n",
       "   'demonstrated',\n",
       "   'excellent',\n",
       "   'performance',\n",
       "   'in',\n",
       "   'terms',\n",
       "   '\\n',\n",
       "   'of',\n",
       "   'response',\n",
       "   'time',\n",
       "   'enabling',\n",
       "   'low',\n",
       "   '-',\n",
       "   'cost',\n",
       "   'and',\n",
       "   'real',\n",
       "   '-',\n",
       "   'time',\n",
       "   'processing',\n",
       "   ',',\n",
       "   'particularly',\n",
       "   'suitable',\n",
       "   'for',\n",
       "   'devices',\n",
       "   'with',\n",
       "   'limited',\n",
       "   'processing',\n",
       "   'capabilities',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'such',\n",
       "   'as',\n",
       "   'cellphones',\n",
       "   '.',\n",
       "   'We',\n",
       "   'intend',\n",
       "   'to',\n",
       "   'foster',\n",
       "   'the',\n",
       "   'advancement',\n",
       "   'of',\n",
       "   'robust',\n",
       "   '\\n',\n",
       "   'assistive',\n",
       "   'technologies',\n",
       "   ',',\n",
       "   'facilitate',\n",
       "   'enhancements',\n",
       "   'in',\n",
       "   'computational',\n",
       "   '\\n',\n",
       "   'synthesis',\n",
       "   'techniques',\n",
       "   ',',\n",
       "   'and',\n",
       "   'computational',\n",
       "   'resources',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'KEYWORDS',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Multimedia',\n",
       "   'Processing',\n",
       "   ',',\n",
       "   'Affective',\n",
       "   'Computing',\n",
       "   ',',\n",
       "   'Machine',\n",
       "   'LearningMultimodal',\n",
       "   'Interaction',\n",
       "   ',',\n",
       "   'Facial',\n",
       "   'Patterns',\n",
       "   ',',\n",
       "   'Image',\n",
       "   'Understanding',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   '1',\n",
       "   'INTRODUCTION',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'The',\n",
       "   'recognition',\n",
       "   'of',\n",
       "   'emotions',\n",
       "   'is',\n",
       "   'an',\n",
       "   'intrinsic',\n",
       "   'part',\n",
       "   'of',\n",
       "   'human',\n",
       "   'relationships',\n",
       "   '.',\n",
       "   'Even',\n",
       "   'in',\n",
       "   'early',\n",
       "   'childhood',\n",
       "   ',',\n",
       "   'children',\n",
       "   'learn',\n",
       "   'to',\n",
       "   'map',\n",
       "   'and',\n",
       "   'interpret',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   ':',\n",
       "   'Proceedings',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Web',\n",
       "   '(',\n",
       "   'WebMe',\n",
       "   '\\n',\n",
       "   'dia’2024',\n",
       "   ')',\n",
       "   '.',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '.',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   ':',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   ',',\n",
       "   '2024',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'SBC',\n",
       "   '–',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '-',\n",
       "   '2753',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Leandro',\n",
       "   'Persona',\n",
       "   'Fernando',\n",
       "   'Meloni',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'leandro.persona@alumni.usp.br',\n",
       "   'melonifernando@yahoo.com.br',\n",
       "   'University',\n",
       "   'of',\n",
       "   'São',\n",
       "   'Paulo',\n",
       "   ',',\n",
       "   'FFCLRP',\n",
       "   ',',\n",
       "   'DCM',\n",
       "   ',',\n",
       "   'PPG',\n",
       "   '-',\n",
       "   'CA',\n",
       "   'Ribeirão',\n",
       "   'Preto',\n",
       "   ',',\n",
       "   'São',\n",
       "   'Paulo',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '\\n\\n',\n",
       "   'the',\n",
       "   'emotions',\n",
       "   'of',\n",
       "   'others',\n",
       "   ',',\n",
       "   'utilizing',\n",
       "   'the',\n",
       "   'result',\n",
       "   'as',\n",
       "   'an',\n",
       "   'indicator',\n",
       "   'of',\n",
       "   'their',\n",
       "   '\\n',\n",
       "   'surrounding',\n",
       "   'context',\n",
       "   '[',\n",
       "   '11',\n",
       "   ',',\n",
       "   '23',\n",
       "   ',',\n",
       "   '27',\n",
       "   ']',\n",
       "   '.',\n",
       "   'The',\n",
       "   'mapping',\n",
       "   'is',\n",
       "   'possible',\n",
       "   'because',\n",
       "   '\\n',\n",
       "   'humans',\n",
       "   'typically',\n",
       "   'translate',\n",
       "   'their',\n",
       "   'emotions',\n",
       "   'into',\n",
       "   'detectable',\n",
       "   'physical',\n",
       "   '\\n',\n",
       "   'movements',\n",
       "   ',',\n",
       "   'particularly',\n",
       "   'facial',\n",
       "   'expressions',\n",
       "   ',',\n",
       "   'which',\n",
       "   'are',\n",
       "   'vital',\n",
       "   'for',\n",
       "   'social',\n",
       "   'interactions',\n",
       "   '.',\n",
       "   'Facial',\n",
       "   'expressions',\n",
       "   'are',\n",
       "   'ubiquitous',\n",
       "   'behaviors',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'exhibit',\n",
       "   'weak',\n",
       "   'dependence',\n",
       "   'on',\n",
       "   'cultural',\n",
       "   'factors',\n",
       "   'as',\n",
       "   'demonstrated',\n",
       "   'by',\n",
       "   '\\n\\n',\n",
       "   '[',\n",
       "   '13',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Thus',\n",
       "   ',',\n",
       "   'same',\n",
       "   'researchers',\n",
       "   'identified',\n",
       "   'the',\n",
       "   'following',\n",
       "   'seven',\n",
       "   'different',\n",
       "   '\\n',\n",
       "   'universal',\n",
       "   'emotions',\n",
       "   'across',\n",
       "   'cultures',\n",
       "   ':',\n",
       "   'fear',\n",
       "   ',',\n",
       "   'anger',\n",
       "   ',',\n",
       "   'sadness',\n",
       "   ',',\n",
       "   'happiness',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'surprise',\n",
       "   ',',\n",
       "   'disgust',\n",
       "   ',',\n",
       "   'along',\n",
       "   'with',\n",
       "   'the',\n",
       "   'neutral',\n",
       "   'emotion',\n",
       "   '[',\n",
       "   '13',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Due',\n",
       "   'to',\n",
       "   'their',\n",
       "   '\\n',\n",
       "   'distinct',\n",
       "   'and',\n",
       "   'stable',\n",
       "   'patterns',\n",
       "   ',',\n",
       "   'facial',\n",
       "   'expressions',\n",
       "   'can',\n",
       "   'be',\n",
       "   'recognized',\n",
       "   '\\n',\n",
       "   'even',\n",
       "   'in',\n",
       "   'unfamiliar',\n",
       "   'individuals',\n",
       "   '[',\n",
       "   '1',\n",
       "   ']',\n",
       "   '.',\n",
       "   'As',\n",
       "   'a',\n",
       "   'result',\n",
       "   ',',\n",
       "   'recognition',\n",
       "   'can',\n",
       "   '\\n',\n",
       "   'be',\n",
       "   'organized',\n",
       "   'with',\n",
       "   'significant',\n",
       "   'potential',\n",
       "   'for',\n",
       "   'automation',\n",
       "   ',',\n",
       "   'enabling',\n",
       "   '\\n',\n",
       "   'machines',\n",
       "   'to',\n",
       "   'interpret',\n",
       "   'a',\n",
       "   'select',\n",
       "   'range',\n",
       "   'of',\n",
       "   'human',\n",
       "   'emotions',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'In',\n",
       "   'recent',\n",
       "   'years',\n",
       "   ',',\n",
       "   'facial',\n",
       "   'expression',\n",
       "   '-',\n",
       "   'based',\n",
       "   'methods',\n",
       "   'for',\n",
       "   'emotion',\n",
       "   '\\n',\n",
       "   'recognition',\n",
       "   'have',\n",
       "   'witnessed',\n",
       "   'rapid',\n",
       "   'advancements',\n",
       "   'due',\n",
       "   'to',\n",
       "   'the',\n",
       "   'growing',\n",
       "   'scientific',\n",
       "   ',',\n",
       "   'medical',\n",
       "   ',',\n",
       "   'and',\n",
       "   'commercial',\n",
       "   'interest',\n",
       "   'in',\n",
       "   'the',\n",
       "   'field',\n",
       "   '[',\n",
       "   '22',\n",
       "   ',',\n",
       "   '26',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   '43',\n",
       "   ',',\n",
       "   '50',\n",
       "   ']',\n",
       "   '.',\n",
       "   'The',\n",
       "   'most',\n",
       "   'common',\n",
       "   'approaches',\n",
       "   'for',\n",
       "   'recognizing',\n",
       "   'individuals',\n",
       "   'and',\n",
       "   'facial',\n",
       "   'expressions',\n",
       "   'focus',\n",
       "   'on',\n",
       "   'automated',\n",
       "   'pattern',\n",
       "   'detection',\n",
       "   '\\n',\n",
       "   'in',\n",
       "   'digital',\n",
       "   'images',\n",
       "   '.',\n",
       "   'Notably',\n",
       "   ',',\n",
       "   'social',\n",
       "   'media',\n",
       "   'platforms',\n",
       "   ',',\n",
       "   'mobile',\n",
       "   'devices',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'and',\n",
       "   'digital',\n",
       "   'cameras',\n",
       "   ',',\n",
       "   'now',\n",
       "   'possess',\n",
       "   'the',\n",
       "   'capability',\n",
       "   'to',\n",
       "   'discern',\n",
       "   'whether',\n",
       "   '\\n',\n",
       "   'a',\n",
       "   'person',\n",
       "   'is',\n",
       "   'smiling',\n",
       "   'or',\n",
       "   'not',\n",
       "   '[',\n",
       "   '5',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Assistive',\n",
       "   'technologies',\n",
       "   'are',\n",
       "   'developed',\n",
       "   'to',\n",
       "   'aid',\n",
       "   'individuals',\n",
       "   'with',\n",
       "   'behavioral',\n",
       "   'syndromes',\n",
       "   '(',\n",
       "   'e.g.',\n",
       "   ',',\n",
       "   'autism',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'mood',\n",
       "   'disorders',\n",
       "   ')',\n",
       "   '[',\n",
       "   '6',\n",
       "   ',',\n",
       "   '26',\n",
       "   ',',\n",
       "   '36',\n",
       "   ']',\n",
       "   '.',\n",
       "   'By',\n",
       "   'enabling',\n",
       "   'individuals',\n",
       "   'with',\n",
       "   'disabilities',\n",
       "   'to',\n",
       "   'react',\n",
       "   'differently',\n",
       "   'upon',\n",
       "   'perceiving',\n",
       "   'expressed',\n",
       "   'emotions',\n",
       "   ',',\n",
       "   'these',\n",
       "   '\\n',\n",
       "   'technologies',\n",
       "   'contribute',\n",
       "   'to',\n",
       "   'enhancing',\n",
       "   'their',\n",
       "   'social',\n",
       "   'interactions',\n",
       "   '.',\n",
       "   'In',\n",
       "   '\\n',\n",
       "   'summary',\n",
       "   ',',\n",
       "   'automatic',\n",
       "   'facial',\n",
       "   'expression',\n",
       "   'recognition',\n",
       "   'offers',\n",
       "   'new',\n",
       "   'avenues',\n",
       "   'for',\n",
       "   'interaction',\n",
       "   'between',\n",
       "   'humans',\n",
       "   'and',\n",
       "   'machines',\n",
       "   ',',\n",
       "   'facilitated',\n",
       "   'by',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'detection',\n",
       "   'of',\n",
       "   'both',\n",
       "   'voluntary',\n",
       "   'and',\n",
       "   'involuntary',\n",
       "   'facial',\n",
       "   'movements',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'In',\n",
       "   'general',\n",
       "   ',',\n",
       "   'the',\n",
       "   'interpretation',\n",
       "   'of',\n",
       "   'information',\n",
       "   'from',\n",
       "   'digital',\n",
       "   'images',\n",
       "   '\\n',\n",
       "   'involves',\n",
       "   'automation',\n",
       "   'through',\n",
       "   'Machine',\n",
       "   'Learning',\n",
       "   '(',\n",
       "   'ML',\n",
       "   ')',\n",
       "   '[',\n",
       "   '7',\n",
       "   ']',\n",
       "   '.',\n",
       "   'However',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'image',\n",
       "   'manipulation',\n",
       "   'requires',\n",
       "   'initial',\n",
       "   'pre',\n",
       "   '-',\n",
       "   'processing',\n",
       "   'steps',\n",
       "   '[',\n",
       "   '44',\n",
       "   ']',\n",
       "   'to',\n",
       "   '\\n',\n",
       "   'enable',\n",
       "   'the',\n",
       "   'detection',\n",
       "   'of',\n",
       "   'specific',\n",
       "   'shapes',\n",
       "   'such',\n",
       "   'as',\n",
       "   'a',\n",
       "   'human',\n",
       "   'face',\n",
       "   '.',\n",
       "   'Next',\n",
       "   '\\n',\n",
       "   'pre',\n",
       "   '-',\n",
       "   'trained',\n",
       "   'models',\n",
       "   'scan',\n",
       "   'segments',\n",
       "   'of',\n",
       "   'the',\n",
       "   'image',\n",
       "   'and',\n",
       "   'use',\n",
       "   'probabilities',\n",
       "   '\\n',\n",
       "   'to',\n",
       "   'confirm',\n",
       "   'patterns',\n",
       "   '[',\n",
       "   '48',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Some',\n",
       "   'models',\n",
       "   'are',\n",
       "   'capable',\n",
       "   'of',\n",
       "   'detecting',\n",
       "   '\\n',\n",
       "   'human',\n",
       "   'faces',\n",
       "   'in',\n",
       "   'images',\n",
       "   'with',\n",
       "   'high',\n",
       "   'accuracy',\n",
       "   'rates',\n",
       "   ',',\n",
       "   'above',\n",
       "   '90',\n",
       "   '%',\n",
       "   '[',\n",
       "   '45',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   '48',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Once',\n",
       "   'the',\n",
       "   'face',\n",
       "   'is',\n",
       "   'identified',\n",
       "   ',',\n",
       "   'the',\n",
       "   'next',\n",
       "   'step',\n",
       "   'in',\n",
       "   'evaluating',\n",
       "   'facial',\n",
       "   '\\n',\n",
       "   'expressions',\n",
       "   'is',\n",
       "   'to',\n",
       "   'map',\n",
       "   'the',\n",
       "   'Regions',\n",
       "   'of',\n",
       "   'Interest',\n",
       "   '(',\n",
       "   'ROIs',\n",
       "   ')',\n",
       "   ',',\n",
       "   'which',\n",
       "   'are',\n",
       "   '\\n',\n",
       "   'specific',\n",
       "   'facial',\n",
       "   'structures',\n",
       "   'such',\n",
       "   'as',\n",
       "   'eyes',\n",
       "   ',',\n",
       "   'nose',\n",
       "   ',',\n",
       "   'mouth',\n",
       "   ',',\n",
       "   ...],\n",
       "  'pos_tagger': '',\n",
       "  'lema': ['recognition',\n",
       "   'of',\n",
       "   'Emotions',\n",
       "   'through',\n",
       "   'Facial',\n",
       "   'Geometry',\n",
       "   'with',\n",
       "   'normalized',\n",
       "   'Landmarks',\n",
       "   'Alessandra',\n",
       "   'Alaniz',\n",
       "   'Macedo',\n",
       "   'ale.alaniz@usp.br',\n",
       "   'University',\n",
       "   'of',\n",
       "   'São',\n",
       "   'Paulo',\n",
       "   'USP',\n",
       "   'ffclrp',\n",
       "   'DCM',\n",
       "   'PPG',\n",
       "   'CA',\n",
       "   'Ribeirão',\n",
       "   'Preto',\n",
       "   'São',\n",
       "   'Paulo',\n",
       "   'Brazil',\n",
       "   'ABSTRACT',\n",
       "   'Emotion',\n",
       "   'recognition',\n",
       "   'hold',\n",
       "   'pivotal',\n",
       "   'significance',\n",
       "   'in',\n",
       "   'human',\n",
       "   'social',\n",
       "   'interaction',\n",
       "   'as',\n",
       "   'it',\n",
       "   'entail',\n",
       "   'the',\n",
       "   'discernment',\n",
       "   'of',\n",
       "   'facial',\n",
       "   'pattern',\n",
       "   'intricately',\n",
       "   'link',\n",
       "   'to',\n",
       "   'diverse',\n",
       "   'emotional',\n",
       "   'state',\n",
       "   'the',\n",
       "   'scientific',\n",
       "   'artistic',\n",
       "   'medical',\n",
       "   'and',\n",
       "   'marketing',\n",
       "   'domain',\n",
       "   'have',\n",
       "   'all',\n",
       "   'demonstrate',\n",
       "   'substantial',\n",
       "   'interest',\n",
       "   'in',\n",
       "   'comprehending',\n",
       "   'emotion',\n",
       "   'result',\n",
       "   'in',\n",
       "   'the',\n",
       "   'emergence',\n",
       "   'and',\n",
       "   'refinement',\n",
       "   'of',\n",
       "   'technique',\n",
       "   'and',\n",
       "   'computational',\n",
       "   'methodology',\n",
       "   'to',\n",
       "   'facilitate',\n",
       "   'automate',\n",
       "   'emotion',\n",
       "   'recognition',\n",
       "   'in',\n",
       "   'this',\n",
       "   'study',\n",
       "   'we',\n",
       "   'introduce',\n",
       "   'a',\n",
       "   'novel',\n",
       "   'method',\n",
       "   'name',\n",
       "   'REGL',\n",
       "   'recognize',\n",
       "   'Emotions',\n",
       "   'through',\n",
       "   'Facial',\n",
       "   'Expression',\n",
       "   'and',\n",
       "   'Landmark',\n",
       "   'normalization',\n",
       "   'aim',\n",
       "   'at',\n",
       "   'recognize',\n",
       "   'facial',\n",
       "   'expression',\n",
       "   'and',\n",
       "   'human',\n",
       "   'emotion',\n",
       "   'depict',\n",
       "   'in',\n",
       "   'image',\n",
       "   'REGL',\n",
       "   'comprise',\n",
       "   'a',\n",
       "   'sequential',\n",
       "   'set',\n",
       "   'of',\n",
       "   'step',\n",
       "   'design',\n",
       "   'to',\n",
       "   'minimize',\n",
       "   'sample',\n",
       "   'variability',\n",
       "   'thereby',\n",
       "   'facilitate',\n",
       "   'a',\n",
       "   'fine',\n",
       "   'calibration',\n",
       "   'of',\n",
       "   'the',\n",
       "   'informative',\n",
       "   'aspect',\n",
       "   'that',\n",
       "   'delineate',\n",
       "   'facial',\n",
       "   'pattern',\n",
       "   'REGL',\n",
       "   'carry',\n",
       "   'out',\n",
       "   'the',\n",
       "   'normalization',\n",
       "   'of',\n",
       "   'facial',\n",
       "   'fiducial',\n",
       "   'point',\n",
       "   'call',\n",
       "   'landmark',\n",
       "   'through',\n",
       "   'the',\n",
       "   'use',\n",
       "   'of',\n",
       "   'landmark',\n",
       "   'position',\n",
       "   'the',\n",
       "   'reliability',\n",
       "   'of',\n",
       "   'the',\n",
       "   'emotion',\n",
       "   'recognition',\n",
       "   'process',\n",
       "   'be',\n",
       "   'significantly',\n",
       "   'improve',\n",
       "   'REGL',\n",
       "   'also',\n",
       "   'exploit',\n",
       "   'classifier',\n",
       "   'explicitly',\n",
       "   'tailor',\n",
       "   'for',\n",
       "   'the',\n",
       "   'accurate',\n",
       "   'identification',\n",
       "   'of',\n",
       "   'facial',\n",
       "   'emotion',\n",
       "   'as',\n",
       "   'relate',\n",
       "   'work',\n",
       "   'the',\n",
       "   'outcome',\n",
       "   'of',\n",
       "   'our',\n",
       "   'experimentation',\n",
       "   'yield',\n",
       "   'an',\n",
       "   'average',\n",
       "   'accuracy',\n",
       "   'over',\n",
       "   '90',\n",
       "   'by',\n",
       "   'employ',\n",
       "   'Machine',\n",
       "   'Learning',\n",
       "   'algorithm',\n",
       "   'differently',\n",
       "   'we',\n",
       "   'have',\n",
       "   'experiment',\n",
       "   'REGL',\n",
       "   'with',\n",
       "   'varied',\n",
       "   'architecture',\n",
       "   'and',\n",
       "   'dataset',\n",
       "   'include',\n",
       "   'racial',\n",
       "   'factor',\n",
       "   'we',\n",
       "   'surpass',\n",
       "   'related',\n",
       "   'work',\n",
       "   'consider',\n",
       "   'the',\n",
       "   'follow',\n",
       "   'contribution',\n",
       "   'the',\n",
       "   'REGL',\n",
       "   'method',\n",
       "   'represent',\n",
       "   'an',\n",
       "   'enhanced',\n",
       "   'approach',\n",
       "   'in',\n",
       "   'term',\n",
       "   'of',\n",
       "   'hit',\n",
       "   'rate',\n",
       "   'and',\n",
       "   'response',\n",
       "   'time',\n",
       "   'and',\n",
       "   'REGL',\n",
       "   'generate',\n",
       "   'resilient',\n",
       "   'outcome',\n",
       "   'by',\n",
       "   'demonstrate',\n",
       "   'reduce',\n",
       "   'reliance',\n",
       "   'on',\n",
       "   'both',\n",
       "   'the',\n",
       "   'training',\n",
       "   'set',\n",
       "   'and',\n",
       "   'classifier',\n",
       "   'architecture',\n",
       "   'moreover',\n",
       "   'REGL',\n",
       "   'demonstrate',\n",
       "   'excellent',\n",
       "   'performance',\n",
       "   'in',\n",
       "   'term',\n",
       "   'of',\n",
       "   'response',\n",
       "   'time',\n",
       "   'enable',\n",
       "   'low',\n",
       "   'cost',\n",
       "   'and',\n",
       "   'real',\n",
       "   'time',\n",
       "   'processing',\n",
       "   'particularly',\n",
       "   'suitable',\n",
       "   'for',\n",
       "   'device',\n",
       "   'with',\n",
       "   'limited',\n",
       "   'processing',\n",
       "   'capability',\n",
       "   'such',\n",
       "   'as',\n",
       "   'cellphone',\n",
       "   'we',\n",
       "   'intend',\n",
       "   'to',\n",
       "   'foster',\n",
       "   'the',\n",
       "   'advancement',\n",
       "   'of',\n",
       "   'robust',\n",
       "   'assistive',\n",
       "   'technology',\n",
       "   'facilitate',\n",
       "   'enhancement',\n",
       "   'in',\n",
       "   'computational',\n",
       "   'synthesis',\n",
       "   'technique',\n",
       "   'and',\n",
       "   'computational',\n",
       "   'resource',\n",
       "   'keyword',\n",
       "   'Multimedia',\n",
       "   'Processing',\n",
       "   'Affective',\n",
       "   'Computing',\n",
       "   'Machine',\n",
       "   'LearningMultimodal',\n",
       "   'Interaction',\n",
       "   'Facial',\n",
       "   'Patterns',\n",
       "   'Image',\n",
       "   'Understanding',\n",
       "   '1',\n",
       "   'introduction',\n",
       "   'the',\n",
       "   'recognition',\n",
       "   'of',\n",
       "   'emotion',\n",
       "   'be',\n",
       "   'an',\n",
       "   'intrinsic',\n",
       "   'part',\n",
       "   'of',\n",
       "   'human',\n",
       "   'relationship',\n",
       "   'even',\n",
       "   'in',\n",
       "   'early',\n",
       "   'childhood',\n",
       "   'child',\n",
       "   'learn',\n",
       "   'to',\n",
       "   'map',\n",
       "   'and',\n",
       "   'interpret',\n",
       "   'in',\n",
       "   'proceeding',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'web',\n",
       "   'WebMe',\n",
       "   'dia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   '2024',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'sbc',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '2753',\n",
       "   'Leandro',\n",
       "   'Persona',\n",
       "   'Fernando',\n",
       "   'Meloni',\n",
       "   'leandro.persona@alumni.usp.br',\n",
       "   'melonifernando@yahoo.com.br',\n",
       "   'University',\n",
       "   'of',\n",
       "   'São',\n",
       "   'Paulo',\n",
       "   'FFCLRP',\n",
       "   'DCM',\n",
       "   'PPG',\n",
       "   'CA',\n",
       "   'Ribeirão',\n",
       "   'Preto',\n",
       "   'São',\n",
       "   'Paulo',\n",
       "   'Brazil',\n",
       "   'the',\n",
       "   'emotion',\n",
       "   'of',\n",
       "   'other',\n",
       "   'utilize',\n",
       "   'the',\n",
       "   'result',\n",
       "   'as',\n",
       "   'an',\n",
       "   'indicator',\n",
       "   'of',\n",
       "   'their',\n",
       "   'surround',\n",
       "   'context',\n",
       "   '11',\n",
       "   '23',\n",
       "   '27',\n",
       "   'the',\n",
       "   'mapping',\n",
       "   'be',\n",
       "   'possible',\n",
       "   'because',\n",
       "   'human',\n",
       "   'typically',\n",
       "   'translate',\n",
       "   'their',\n",
       "   'emotion',\n",
       "   'into',\n",
       "   'detectable',\n",
       "   'physical',\n",
       "   'movement',\n",
       "   'particularly',\n",
       "   'facial',\n",
       "   'expression',\n",
       "   'which',\n",
       "   'be',\n",
       "   'vital',\n",
       "   'for',\n",
       "   'social',\n",
       "   'interaction',\n",
       "   'facial',\n",
       "   'expression',\n",
       "   'be',\n",
       "   'ubiquitous',\n",
       "   'behavior',\n",
       "   'and',\n",
       "   'exhibit',\n",
       "   'weak',\n",
       "   'dependence',\n",
       "   'on',\n",
       "   'cultural',\n",
       "   'factor',\n",
       "   'as',\n",
       "   'demonstrate',\n",
       "   'by',\n",
       "   '13',\n",
       "   'thus',\n",
       "   'same',\n",
       "   'researcher',\n",
       "   'identify',\n",
       "   'the',\n",
       "   'follow',\n",
       "   'seven',\n",
       "   'different',\n",
       "   'universal',\n",
       "   'emotion',\n",
       "   'across',\n",
       "   'culture',\n",
       "   'fear',\n",
       "   'anger',\n",
       "   'sadness',\n",
       "   'happiness',\n",
       "   'surprise',\n",
       "   'disgust',\n",
       "   'along',\n",
       "   'with',\n",
       "   'the',\n",
       "   'neutral',\n",
       "   'emotion',\n",
       "   '13',\n",
       "   'due',\n",
       "   'to',\n",
       "   'their',\n",
       "   'distinct',\n",
       "   'and',\n",
       "   'stable',\n",
       "   'pattern',\n",
       "   'facial',\n",
       "   'expression',\n",
       "   'can',\n",
       "   'be',\n",
       "   'recognize',\n",
       "   'even',\n",
       "   'in',\n",
       "   'unfamiliar',\n",
       "   'individual',\n",
       "   '1',\n",
       "   'as',\n",
       "   'a',\n",
       "   'result',\n",
       "   'recognition',\n",
       "   'can',\n",
       "   'be',\n",
       "   'organize',\n",
       "   'with',\n",
       "   'significant',\n",
       "   'potential',\n",
       "   'for',\n",
       "   'automation',\n",
       "   'enable',\n",
       "   'machine',\n",
       "   'to',\n",
       "   'interpret',\n",
       "   'a',\n",
       "   'select',\n",
       "   'range',\n",
       "   'of',\n",
       "   'human',\n",
       "   'emotion',\n",
       "   'in',\n",
       "   'recent',\n",
       "   'year',\n",
       "   'facial',\n",
       "   'expression',\n",
       "   'base',\n",
       "   'method',\n",
       "   'for',\n",
       "   'emotion',\n",
       "   'recognition',\n",
       "   'have',\n",
       "   'witness',\n",
       "   'rapid',\n",
       "   'advancement',\n",
       "   'due',\n",
       "   'to',\n",
       "   'the',\n",
       "   'grow',\n",
       "   'scientific',\n",
       "   'medical',\n",
       "   'and',\n",
       "   'commercial',\n",
       "   'interest',\n",
       "   'in',\n",
       "   'the',\n",
       "   'field',\n",
       "   '22',\n",
       "   '26',\n",
       "   '43',\n",
       "   '50',\n",
       "   'the',\n",
       "   'most',\n",
       "   'common',\n",
       "   'approach',\n",
       "   'for',\n",
       "   'recognize',\n",
       "   'individual',\n",
       "   'and',\n",
       "   'facial',\n",
       "   'expression',\n",
       "   'focus',\n",
       "   'on',\n",
       "   'automate',\n",
       "   'pattern',\n",
       "   'detection',\n",
       "   'in',\n",
       "   'digital',\n",
       "   'image',\n",
       "   'notably',\n",
       "   'social',\n",
       "   'medium',\n",
       "   'platform',\n",
       "   'mobile',\n",
       "   'device',\n",
       "   'and',\n",
       "   'digital',\n",
       "   'camera',\n",
       "   'now',\n",
       "   'possess',\n",
       "   'the',\n",
       "   'capability',\n",
       "   'to',\n",
       "   'discern',\n",
       "   'whether',\n",
       "   'a',\n",
       "   'person',\n",
       "   'be',\n",
       "   'smile',\n",
       "   'or',\n",
       "   'not',\n",
       "   '5',\n",
       "   'assistive',\n",
       "   'technology',\n",
       "   'be',\n",
       "   'develop',\n",
       "   'to',\n",
       "   'aid',\n",
       "   'individual',\n",
       "   'with',\n",
       "   'behavioral',\n",
       "   'syndrome',\n",
       "   'e.g.',\n",
       "   'autism',\n",
       "   'and',\n",
       "   'mood',\n",
       "   'disorder',\n",
       "   '6',\n",
       "   '26',\n",
       "   '36',\n",
       "   'by',\n",
       "   'enable',\n",
       "   'individual',\n",
       "   'with',\n",
       "   'disability',\n",
       "   'to',\n",
       "   'react',\n",
       "   'differently',\n",
       "   'upon',\n",
       "   'perceive',\n",
       "   'express',\n",
       "   'emotion',\n",
       "   'these',\n",
       "   'technology',\n",
       "   'contribute',\n",
       "   'to',\n",
       "   'enhance',\n",
       "   'their',\n",
       "   'social',\n",
       "   'interaction',\n",
       "   'in',\n",
       "   'summary',\n",
       "   'automatic',\n",
       "   'facial',\n",
       "   'expression',\n",
       "   'recognition',\n",
       "   'offer',\n",
       "   'new',\n",
       "   'avenue',\n",
       "   'for',\n",
       "   'interaction',\n",
       "   'between',\n",
       "   'human',\n",
       "   'and',\n",
       "   'machine',\n",
       "   'facilitate',\n",
       "   'by',\n",
       "   'the',\n",
       "   'detection',\n",
       "   'of',\n",
       "   'both',\n",
       "   'voluntary',\n",
       "   'and',\n",
       "   'involuntary',\n",
       "   'facial',\n",
       "   'movement',\n",
       "   'in',\n",
       "   'general',\n",
       "   'the',\n",
       "   'interpretation',\n",
       "   'of',\n",
       "   'information',\n",
       "   'from',\n",
       "   'digital',\n",
       "   'image',\n",
       "   'involve',\n",
       "   'automation',\n",
       "   'through',\n",
       "   'Machine',\n",
       "   'Learning',\n",
       "   'ML',\n",
       "   '7',\n",
       "   'however',\n",
       "   'image',\n",
       "   'manipulation',\n",
       "   'require',\n",
       "   'initial',\n",
       "   'pre',\n",
       "   'processing',\n",
       "   'step',\n",
       "   '44',\n",
       "   'to',\n",
       "   'enable',\n",
       "   'the',\n",
       "   'detection',\n",
       "   'of',\n",
       "   'specific',\n",
       "   'shape',\n",
       "   'such',\n",
       "   'as',\n",
       "   'a',\n",
       "   'human',\n",
       "   'face',\n",
       "   'next',\n",
       "   'pre',\n",
       "   'train',\n",
       "   'model',\n",
       "   'scan',\n",
       "   'segment',\n",
       "   'of',\n",
       "   'the',\n",
       "   'image',\n",
       "   'and',\n",
       "   'use',\n",
       "   'probability',\n",
       "   'to',\n",
       "   'confirm',\n",
       "   'pattern',\n",
       "   '48',\n",
       "   'some',\n",
       "   'model',\n",
       "   'be',\n",
       "   'capable',\n",
       "   'of',\n",
       "   'detect',\n",
       "   'human',\n",
       "   'face',\n",
       "   'in',\n",
       "   'image',\n",
       "   'with',\n",
       "   'high',\n",
       "   'accuracy',\n",
       "   'rate',\n",
       "   'above',\n",
       "   '90',\n",
       "   '45',\n",
       "   '48',\n",
       "   'once',\n",
       "   'the',\n",
       "   'face',\n",
       "   'be',\n",
       "   'identify',\n",
       "   'the',\n",
       "   'next',\n",
       "   'step',\n",
       "   'in',\n",
       "   'evaluate',\n",
       "   'facial',\n",
       "   'expression',\n",
       "   'be',\n",
       "   'to',\n",
       "   'map',\n",
       "   'the',\n",
       "   'Regions',\n",
       "   'of',\n",
       "   'Interest',\n",
       "   'ROIs',\n",
       "   'which',\n",
       "   'be',\n",
       "   'specific',\n",
       "   'facial',\n",
       "   'structure',\n",
       "   'such',\n",
       "   'as',\n",
       "   'eye',\n",
       "   'nose',\n",
       "   'mouth',\n",
       "   'chin',\n",
       "   'etc',\n",
       "   'the',\n",
       "   'relative',\n",
       "   'position',\n",
       "   'of',\n",
       "   'these',\n",
       "   'structure',\n",
       "   'be',\n",
       "   'then',\n",
       "   'mark',\n",
       "   'as',\n",
       "   'landmark',\n",
       "   'which',\n",
       "   'be',\n",
       "   'assign',\n",
       "   'bi',\n",
       "   'or',\n",
       "   'tridimensional',\n",
       "   'coordinate',\n",
       "   'add',\n",
       "   'an',\n",
       "   'additional',\n",
       "   'layer',\n",
       "   'of',\n",
       "   'information',\n",
       "   'the',\n",
       "   'use',\n",
       "   'of',\n",
       "   'landmark',\n",
       "   'provide',\n",
       "   'a',\n",
       "   'straightforward',\n",
       "   'and',\n",
       "   'objective',\n",
       "   'way',\n",
       "   'to',\n",
       "   'recognize',\n",
       "   'facial',\n",
       "   'expression',\n",
       "   'by',\n",
       "   'compare',\n",
       "   'pattern',\n",
       "   'for',\n",
       "   'both',\n",
       "   'facial',\n",
       "   'identification',\n",
       "   'facial',\n",
       "   'recognition',\n",
       "   'and',\n",
       "   'change',\n",
       "   'in',\n",
       "   'facial',\n",
       "   '257',\n",
       "   'WebMedia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Macedo',\n",
       "   'et',\n",
       "   'al',\n",
       "   'pattern',\n",
       "   'expression',\n",
       "   'and',\n",
       "   'emotion',\n",
       "   'recognition',\n",
       "   'although',\n",
       "   'implementation',\n",
       "   'may',\n",
       "   'vary',\n",
       "   'both',\n",
       "   'approach',\n",
       "   'consider',\n",
       "   'the',\n",
       "   'coordinate',\n",
       "   'of',\n",
       "   'landmark',\n",
       "   'as',\n",
       "   'problem',\n",
       "   'variable',\n",
       "   'reference',\n",
       "   'value',\n",
       "   'for',\n",
       "   'calculate',\n",
       "   'distance',\n",
       "   'e.g.',\n",
       "   'Euclidean',\n",
       "   'cosine',\n",
       "   'etc',\n",
       "   '15',\n",
       "   '29',\n",
       "   '32',\n",
       "   'in',\n",
       "   'the',\n",
       "   'case',\n",
       "   'of',\n",
       "   'emotion',\n",
       "   'the',\n",
       "   'muscle',\n",
       "   'movement',\n",
       "   'responsible',\n",
       "   'for',\n",
       "   'each',\n",
       "   'facial',\n",
       "   'expression',\n",
       "   'cause',\n",
       "   'geometric',\n",
       "   'change',\n",
       "   'in',\n",
       "   'the',\n",
       "   'relative',\n",
       "   'position',\n",
       "   'of',\n",
       "   'roi',\n",
       "   '49',\n",
       "   'these',\n",
       "   'change',\n",
       "   'alter',\n",
       "   'the',\n",
       "   'coordinate',\n",
       "   'of',\n",
       "   'landmark',\n",
       "   'and',\n",
       "   'the',\n",
       "   'value',\n",
       "   'of',\n",
       "   'distance',\n",
       "   'between',\n",
       "   'they',\n",
       "   'give',\n",
       "   'that',\n",
       "   'a',\n",
       "   'neutral',\n",
       "   'face',\n",
       "   'exhibit',\n",
       "   'different',\n",
       "   'relative',\n",
       "   'distance',\n",
       "   'from',\n",
       "   'a',\n",
       "   'fearful',\n",
       "   'or',\n",
       "   'smile',\n",
       "   'face',\n",
       "   'the',\n",
       "   'set',\n",
       "   'of',\n",
       "   'variation',\n",
       "   'can',\n",
       "   'be',\n",
       "   'utilize',\n",
       "   'to',\n",
       "   'identify',\n",
       "   'expression',\n",
       "   '32',\n",
       "   'the',\n",
       "   'generalizability',\n",
       "   'of',\n",
       "   'emotional',\n",
       "   'response',\n",
       "   'among',\n",
       "   'human',\n",
       "   'tend',\n",
       "   'to',\n",
       "   'simplify',\n",
       "   'the',\n",
       "   'problem',\n",
       "   'enable',\n",
       "   'the',\n",
       "   'identification',\n",
       "   'of',\n",
       "   'which',\n",
       "   'distance',\n",
       "   'be',\n",
       "   'most',\n",
       "   'affect',\n",
       "   'by',\n",
       "   'each',\n",
       "   'type',\n",
       "   'of',\n",
       "   'facial',\n",
       "   'expression',\n",
       "   'at',\n",
       "   'the',\n",
       "   'end',\n",
       "   'of',\n",
       "   'the',\n",
       "   'analysis',\n",
       "   'of',\n",
       "   'these',\n",
       "   'variation',\n",
       "   'ML',\n",
       "   'classifier',\n",
       "   'can',\n",
       "   'be',\n",
       "   'train',\n",
       "   'for',\n",
       "   'the',\n",
       "   'recognition',\n",
       "   'of',\n",
       "   'human',\n",
       "   'emotion',\n",
       "   '48',\n",
       "   'despite',\n",
       "   'the',\n",
       "   'current',\n",
       "   'method',\n",
       "   'yield',\n",
       "   'satisfactory',\n",
       "   'result',\n",
       "   'for',\n",
       "   'specific',\n",
       "   'contexts',\n",
       "   'such',\n",
       "   'as',\n",
       "   'photo',\n",
       "   'acquire',\n",
       "   'in',\n",
       "   'control',\n",
       "   'environment',\n",
       "   'there',\n",
       "   'be',\n",
       "   'still',\n",
       "   'significant',\n",
       "   'challenge',\n",
       "   'relate',\n",
       "   'to',\n",
       "   'the',\n",
       "   'subject',\n",
       "   'the',\n",
       "   'main',\n",
       "   'problem',\n",
       "   'involve',\n",
       "   'the',\n",
       "   'variability',\n",
       "   'of',\n",
       "   'pattern',\n",
       "   'find',\n",
       "   'in',\n",
       "   'facial',\n",
       "   'shape',\n",
       "   'pose',\n",
       "   'and',\n",
       "   'in',\n",
       "   'image',\n",
       "   'obtain',\n",
       "   'under',\n",
       "   'real',\n",
       "   'world',\n",
       "   'condition',\n",
       "   'uncontrolle',\n",
       "   'variable',\n",
       "   'such',\n",
       "   'as',\n",
       "   'lighting',\n",
       "   'brightness',\n",
       "   'distance',\n",
       "   'from',\n",
       "   'the',\n",
       "   'capture',\n",
       "   'device',\n",
       "   'etc',\n",
       "   '32',\n",
       "   ...]},\n",
       " {'titulo': 'Multi-Domain Spatio-Temporal Deformable Fusion model for video quality enhancementpdfauthor',\n",
       "  'informacoes_url': '',\n",
       "  'idioma': 'english',\n",
       "  'storage_key': '../articles/original/english/985-24762-1-10-20240923.pdf',\n",
       "  'author': '',\n",
       "  'data_publicacao': '11-09-2024',\n",
       "  'resumo': 'Lossy video compression introduces artifacts that can degrade the perceived visual quality of the video. Improving the quality of compressed videos involves mitigating these artifacts through filtering techniques. Deep neural network (DNN) models have emerged as powerful tools for this task, demonstrating effectiveness in artifact reduction. However, traditional approaches typically evaluate these models using videos compressed by a single coding standard, limiting their applicability across diverse codecs. To address this limitation, this study proposes a novel multi-domain architecture built upon the Spatio-Temporal Deformable Fusion technique. This innovative approach enables the development of models capable of enhancing videos compressed by various codecs, ensuring consistent performance across different standards. Experimental results showcase the efficacy of the proposed method, yielding significant improvements in average Peak Signal-to-Noise Ratio (PSNR) for videos compressed with HEVC, VVC, VP9, and AV1, with enhancements of 0.764 dB, 0.448 dB, 0.736 dB, and 0.228 dB, respectively. The code of our MD-STDF approach is available at *https://github.com/Espeto/md-stdf* ###',\n",
       "  'keywords': 'Redes neurais profundas, Melhoria de qualidade de vídeo, Codificação de vídeo, Aprendizado multi-domínio',\n",
       "  'referencias': ['[1] Aayushi Agarwal, Akshay Agarwal, Sayan Sinha, Mayank Vatsa, and Richa Singh.\\n2021. MD-CSDNetwork: Multi-domain cross stitched network for deepfake\\ndetection. In *2021 16th IEEE international conference on automatic face and gesture*\\n*recognition (FG 2021)* . IEEE, 1–8.',\n",
       "   '[2] Isis Bender, Daniel Palomino, Luciano Agostini, Guilherme Correa, and Marcelo\\nPorto. 2019. Compression efficiency and computational cost comparison between\\nAV1 and HEVC encoders. In *2019 27th European Signal Processing Conference*\\n*(EUSIPCO)* . IEEE, 1–5.',\n",
       "   '[3] J. Boyce, K. Suehring, and X. Li. 2018. JVET-J1010: JVET common test conditions\\nand software reference configurations. *JVET-J1010* (2018).',\n",
       "   '[4] Benjamin Bross, Ye-Kui Wang, Yan Ye, Shan Liu, Jianle Chen, Gary J Sullivan,\\nand Jens-Rainer Ohm. 2021. Overview of the versatile video coding (VVC)\\nstandard and its applications. *IEEE Transactions on Circuits and Systems for Video*\\n*Technology* 31, 10 (2021), 3736–3764.',\n",
       "   '[5] Jose Caballero, Christian Ledig, Andrew Aitken, Alejandro Acosta, Johannes\\nTotz, Zehan Wang, and Wenzhe Shi. 2017. Real-time video super-resolution with\\nspatio-temporal networks and motion compensation. In *Proceedings of the IEEE*\\n*conference on computer vision and pattern recognition* . 4778–4787.',\n",
       "   '[6] Yimin Chen, Rongrong Lu, Yibo Zou, and Yanhui Zhang. 2018. Branch-Activated\\nMulti-Domain Convolutional Neural Network for Visual Tracking. *Journal of*\\n*Shanghai Jiaotong University (Science)* 23 (2018), 360–367.',\n",
       "   '[7] Yue Chen, Debargha Murherjee, Jingning Han, Adrian Grange, Yaowu Xu, Zoe\\nLiu, Sarah Parker, Cheng Chen, Hui Su, Urvang Joshi, et al . 2018. An overview\\nof core coding tools in the AV1 video codec. In *2018 picture coding symposium*\\n*(PCS)* . IEEE, 41–45.',\n",
       "   '[8] V Cisco. 2020. Cisco visual networking index: Forecast and trends, 2018–2023.\\n*White Paper* 1 (2020).',\n",
       "   '[9] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen\\nWei. 2017. Deformable convolutional networks. In *Proceedings of the IEEE inter-*\\n*national conference on computer vision* . 764–773.',\n",
       "   '[10] Yuanying Dai, Dong Liu, and Feng Wu. 2017. A convolutional neural network\\napproach for post-processing in HEVC intra coding. In *MultiMedia Modeling:*\\n*23rd International Conference, MMM 2017, Reykjavik, Iceland, January 4-6, 2017,*\\n*Proceedings, Part I 23* . Springer, 28–39.',\n",
       "   '[11] Jianing Deng, Li Wang, Shiliang Pu, and Cheng Zhuo. 2020. Spatio-temporal deformable convolution for compressed video quality enhancement. In *Proceedings*\\n*of the AAAI conference on artificial intelligence*, Vol. 34. 10696–10703.',\n",
       "   '[12] Chao Dong, Yubin Deng, Chen Change Loy, and Xiaoou Tang. 2015. Compression\\nartifacts reduction by a deep convolutional network. In *Proceedings of the IEEE*\\n*International Conference on Computer Vision* . 576–584.',\n",
       "   '[13] Chih-Ming Fu, Elena Alshina, Alexander Alshin, Yu-Wen Huang, Ching-Yeh\\nChen, Chia-Yang Tsai, Chih-Wei Hsu, Shaw-Min Lei, Jeong-Hoon Park, and WooJin Han. 2012. Sample adaptive offset in the HEVC standard. *IEEE Transactions*\\n*on Circuits and Systems for Video technology* 22, 12 (2012), 1755–1764.',\n",
       "   '[14] Zhenyu Guan, Qunliang Xing, Mai Xu, Ren Yang, Tie Liu, and Zulin Wang. 2019.\\nMFQE 2.0: A new approach for multi-frame quality enhancement on compressed\\nvideo. *IEEE transactions on pattern analysis and machine intelligence* 43, 3 (2019),\\n949–963.',\n",
       "   '[15] Gilberto Kreisler, Garibaldi da Silveira Junior, Bruno Zatt, Daniel Palomino, and\\nGuilherme Correa. 2023. Modelo Multi-Codec Baseado em Spatio-Temporal\\n\\n\\nDeformable Fusion para Melhoria de Qualidade de Vídeos Comprimidos. In *Anais*\\n*do L Seminário Integrado de Software e Hardware* . SBC, 143–154.',\n",
       "   '[16] Shiba Kuanar, Christopher Conly, and KR Rao. 2018. Deep learning based HEVC\\nin-loop filtering for decoder quality enhancement. In *2018 Picture Coding Sympo-*\\n*sium (PCS)* . IEEE, 164–168.',\n",
       "   '[17] Tianyi Li, Mai Xu, Ce Zhu, Ren Yang, Zulin Wang, and Zhenyu Guan. 2019. A\\ndeep learning approach for multi-frame in-loop filter of HEVC. *IEEE Transactions*\\n*on Image Processing* 28, 11 (2019), 5663–5678.',\n",
       "   '[18] Zewen Li, Fan Liu, Wenjie Yang, Shouheng Peng, and Jun Zhou. 2021. A survey\\nof convolutional neural networks: analysis, applications, and prospects. *IEEE*\\n*transactions on neural networks and learning systems* 33, 12 (2021), 6999–7019.',\n",
       "   '[19] Ming Liang and Xiaolin Hu. 2015. Recurrent convolutional neural network for\\nobject recognition. In *Proceedings of the IEEE conference on computer vision and*\\n*pattern recognition* . 3367–3375.',\n",
       "   '[20] Xiandong Meng, Xuan Deng, Shuyuan Zhu, and Bing Zeng. 2019. Enhancing\\nquality for VVC compressed videos by jointly exploiting spatial details and\\ntemporal structure. In *2019 IEEE International Conference on Image Processing*\\n*(ICIP)* . IEEE, 1193–1197.',\n",
       "   '[21] Debargha Mukherjee, Jim Bankoski, Adrian Grange, Jingning Han, John Koleszar,\\nPaul Wilkins, Yaowu Xu, and Ronald Bultje. 2013. The latest open-source video\\ncodec VP9-an overview and preliminary results. In *2013 Picture Coding Symposium*\\n*(PCS)* . IEEE, 390–393.',\n",
       "   '[22] Seungjun Nah, Sanghyun Son, and Kyoung Mu Lee. 2019. Recurrent neural\\nnetworks with intra-frame iterations for video deblurring. In *Proceedings of the*\\n*IEEE/CVF conference on computer vision and pattern recognition* . 8102–8111.',\n",
       "   '[23] Hyeonseob Nam and Bohyung Han. 2016. Learning multi-domain convolutional\\nneural networks for visual tracking. In *Proceedings of the IEEE conference on*\\n*computer vision and pattern recognition* . 4293–4302.',\n",
       "   '[24] Andrey Norkin, Gisle Bjontegaard, Arild Fuldseth, Matthias Narroschke, Masaru\\nIkeda, Kenneth Andersson, Minhua Zhou, and Geert Van der Auwera. 2012. HEVC\\ndeblocking filter. *IEEE Transactions on Circuits and Systems for Video Technology*\\n22, 12 (2012), 1746–1754.',\n",
       "   '[25] Bo Peng, Renjie Chang, Zhaoqing Pan, Ge Li, Nam Ling, and Jianjun Lei. 2022.\\nDeep in-loop filtering via multi-domain correlation learning and partition constraint for multiview video coding. *IEEE Transactions on Circuits and Systems for*\\n*Video Technology* 33, 4 (2022), 1911–1921.',\n",
       "   '[26] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical image segmentation. In *Medical Image Computing*\\n*and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference,*\\n*Munich, Germany, October 5-9, 2015, Proceedings, Part III 18* . Springer, 234–241.',\n",
       "   '[27] Gary J Sullivan, Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand. 2012.\\nOverview of the high efficiency video coding (HEVC) standard. *IEEE Transactions*\\n*on circuits and systems for video technology* 22, 12 (2012), 1649–1668.',\n",
       "   '[28] Junchao Tong, Xilin Wu, Dandan Ding, Zheng Zhu, and Zoe Liu. 2019. Learningbased multi-frame video quality enhancement. In *2019 IEEE International Confer-*\\n*ence on Image Processing (ICIP)* . IEEE, 929–933.',\n",
       "   '[29] Chia-Yang Tsai, Ching-Yeh Chen, Tomoo Yamakage, In Suk Chong, Yu-Wen\\nHuang, Chih-Ming Fu, Takayuki Itoh, Takashi Watanabe, Takeshi Chujoh, Marta\\nKarczewicz, et al . 2013. Adaptive loop filtering for video coding. *IEEE Journal of*\\n*Selected Topics in Signal Processing* 7, 6 (2013), 934–945.',\n",
       "   '[30] Xintao Wang, Kelvin CK Chan, Ke Yu, Chao Dong, and Chen Change Loy. 2019.\\nEdvr: Video restoration with enhanced deformable convolutional networks. In\\n*Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*\\n*workshops* . 0–0.',\n",
       "   '[31] Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and William T Freeman. 2019.\\nVideo enhancement with task-oriented flow. *International Journal of Computer*\\n*Vision* 127 (2019), 1106–1125.',\n",
       "   '[32] Ren Yang and Radu Timofte. 2021. NTIRE 2021 Challenge on Quality Enhancement of Compressed Video: Dataset and Study. In *IEEE/CVF Conference on Com-*\\n*puter Vision and Pattern Recognition Workshops* .',\n",
       "   '[33] Ren Yang, Mai Xu, Tie Liu, Zulin Wang, and Zhenyu Guan. 2018. Enhancing\\nquality for HEVC compressed videos. *IEEE Transactions on Circuits and Systems*\\n*for Video Technology* 29, 7 (2018), 2039–2054.',\n",
       "   '[34] Ren Yang, Mai Xu, Zulin Wang, and Tianyi Li. 2018. Multi-frame quality enhancement for compressed video. In *Proceedings of the IEEE conference on computer*\\n*vision and pattern recognition* . 6664–6673.',\n",
       "   '[35] Chao Zhu, Hang Dong, Jinshan Pan, Boyang Liang, Yuhao Huang, Lean Fu, and\\nFei Wang. 2022. Deep recurrent neural network with multi-scale bi-directional\\npropagation for video deblurring. In *Proceedings of the AAAI conference on artifi-*\\n*cial intelligence*, Vol. 36. 3598–3607.',\n",
       "   '[36] Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. 2019. Deformable convnets\\nv2: More deformable, better results. In *Proceedings of the IEEE/CVF conference on*\\n*computer vision and pattern recognition* . 9308–9316.\\n\\n\\n230\\n\\n\\n-----'],\n",
       "  'text': '# **Multi-Domain Spatio-Temporal Deformable Fusion model for** **video quality enhancement**\\n\\n## Garibaldi da Silveira Júnior\\n#### garibaldi.dsj@inf.ufpel.edu.br ViTech - PPGC - UFPel Pelotas, Brazil\\n\\n## Gilberto Kreisler\\n#### gkfneto@inf.ufpel.edu.br ViTech - PPGC - UFPel Pelotas, Brazil\\n\\n## Bruno Zatt\\n#### zatt@inf.ufpel.edu.br ViTech - PPGC - UFPel Pelotas, Brazil\\n\\n## Daniel Palomino\\n#### dpalomino@inf.ufpel.edu.br ViTech - PPGC - UFPel Pelotas, Brazil\\n### **ABSTRACT**\\n\\nLossy video compression introduces artifacts that can degrade the\\nperceived visual quality of the video. Improving the quality of compressed videos involves mitigating these artifacts through filtering\\ntechniques. Deep neural network (DNN) models have emerged as\\npowerful tools for this task, demonstrating effectiveness in artifact reduction. However, traditional approaches typically evaluate\\nthese models using videos compressed by a single coding standard, limiting their applicability across diverse codecs. To address\\nthis limitation, this study proposes a novel multi-domain architecture built upon the Spatio-Temporal Deformable Fusion technique.\\nThis innovative approach enables the development of models capable of enhancing videos compressed by various codecs, ensuring\\nconsistent performance across different standards. Experimental\\nresults showcase the efficacy of the proposed method, yielding\\nsignificant improvements in average Peak Signal-to-Noise Ratio\\n(PSNR) for videos compressed with HEVC, VVC, VP9, and AV1,\\nwith enhancements of 0.764 dB, 0.448 dB, 0.736 dB, and 0.228 dB,\\nrespectively. The code of our MD-STDF approach is available at\\n*https://github.com/Espeto/md-stdf*\\n### **KEYWORDS**\\n\\nRedes neurais profundas, Melhoria de qualidade de vídeo, Codificação de vídeo, Aprendizado multi-domínio\\n### **1 INTRODUCTION**\\n\\nVideo compression plays a crucial role in services dealing with the\\ndistribution and storage of audiovisual content, becoming essential\\nfor the operation of companies like Netflix, TikTok, and YouTube.\\nDue to the high demand for this type of service, digital video represented the highest volume of data transmitted over the Internet\\nin recent years. It has been predicted that 4K videos represented\\n66% of Internet consumption on television devices by the end of\\n2023, surpassing the 2018 estimate [ 8 ]. Consequently, research efforts by both academia and industry are dedicated to improving\\nnot only compression efficiency but also reducing undesired visual\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n© 2024 SBC – Brazilian Computing Society.\\nISSN 2966-2753\\n\\n## Guilherme Correa\\n#### gcorrea@inf.ufpel.edu.br ViTech - PPGC - UFPel Pelotas, Brazil\\n\\neffects caused by this process. As video content continues to proliferate across various platforms and devices, maintaining high visual\\nquality while efficiently managing data transmission and storage\\nbecomes increasingly paramount.\\nCompressed videos suffer from visual effects such as blocking,\\nringing and blurring artifacts [ 12 ], which compromise the perceived\\nvideo quality for users. In Figure 1, the visual effects of these artifacts can be observed. The patches (c), (d) and (e) are separated\\nby the artifact that predominantly affects the selected part of the\\nimage. Generally, artifacts can blend with or appear next to others, making it difficult to generate a patch that isolate only one\\ncompression artifact. In (c), the division between blocks used in\\nthe compression process is perceptible in the middle of the blue\\npart of the ball, evidencing a blocking effect. In (d), we can see the\\nringing effect, noticeable as wave-like patterns along the edges of\\nthe orange rim. In (e), details from the original image, such as the\\nnails on the ground and the separations in the wooden floor, are\\nlost during compression, leading to a blurring effect.\\nFiltering algorithms like the Deblocking Filter (DF) [ 24 ], addressing blocking effects, Adaptive Loop Filter (ALF) [ 29 ], that minimize\\nthe distortion between the original and decoded samples, and the\\nSample Adaptive Offset (SAO) [ 13 ], focused on reducing banding effects, are standardized processes in formats such as High Efficiency\\nVideo Coding (HEVC) and Versatile Video Coding (VVC).\\nBoth DF and SAO are heuristic-based methods, devised based\\nupon statistical observations for reducing compression artifacts.\\nThese models are applied as filters that traverse all pixels in each\\nframe, aiming to enhance visual quality. While these heuristicbased approaches have demonstrated effectiveness in mitigating\\ncertain artifacts, they have inherent limitations. For example, DF\\nmay introduce a blurring effect on the image as it attempts to reduce blockiness, potentially sacrificing fine details and sharpness.\\nAdditionally, both DF and SAO may inadvertently amplify the presence of other compression artifacts, such as ringing or mosquito\\nnoise, particularly in regions with high contrast or intricate textures [ 17 ]. Despite that, heuristic-based methods remain valuable\\ntools of video compression techniques, especially when used in\\nconjunction with more sophisticated algorithms and Deep Neural\\nNetworks (DNN) to achieve comprehensive artifact reduction and\\nenhance overall visual quality.\\nCurrently, a significant amount of studies exploring the Video\\nQuality Enhancement (VQE) problem employ DNN models based on\\n\\n\\n223\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Silveira Júnior et al.\\n\\n**Figure 1: Compression Artifacts: (a) Original frame (RAW); (b) Compressed Frame; (c) Blocking Artifact; (d) Ringing Artifact; (e)**\\n**Blurring Artifact**\\n\\n\\nConvolutional Neural Networks (CNN) [9-27]. CNNs have emerged\\nas a powerful tool in image and video processing tasks due to\\ntheir ability to automatically learn hierarchical features from data.\\nUnlike traditional image processing techniques that operate on\\nindividual pixels, CNNs operate by convolving learned filters across\\nthe input image, enabling them to capture spatial hierarchies and\\ndependencies. By processing local image patches and learning from\\ntheir correlations, CNNs can effectively extract meaningful features\\nthat represent various visual patterns, such as edges, textures, and\\nobject shapes. This enables them to understand the contextual\\nconnection between neighboring pixels and learn complex patterns\\nin the data [ 18 ]. Therefore, CNN-based models are well-suited for\\nVQE tasks as they can identify and address overall image quality\\ndegradation caused by compression artifacts rather than focusing\\nsolely on specific types of artifacts.\\nIt has been observed that many DNN models for VQE are tested\\nusing videos compressed with the same codec and configuration as\\nthe training videos. The authors in [ 15 ] show that the VQE models tend to produce better results for videos compressed with the\\nsame codec and quantization parameter as those used for training.\\nOppositely, for videos compressed with different codecs and configurations the VQE models lead to little improvement in quality or\\neven quality degradation. Thus, considering the large number of\\nvideo codecs and encoding configurations available nowadays, it is\\ndesirable that the VQE model at the decoder side is generic enough\\nto be used for enhancing videos compressed in different scenarios.\\nTo address this issue, this work proposes the use of a multidomain training method [ 6 ] that allows identifying the video encoding scenario, generating a model that is adaptive to the video\\ncodec and its associated compression artifacts. The proposed MultiDomain Spatio-Temporal Deformable Fusion (MD-STDF) architecture explores multi-domain training for quality improvement of\\n\\n\\ncompressed videos, ensuring that a single model is efficient in enhancing the quality of videos originating from any of the domains\\ninvolved in training. In this work, the videos used in both the training and testing phases have been categorized into domains based\\non the video codec used for compression: HEVC, VVC, VP9, and\\nAV1.\\n\\nExperimental results demonstrate that the proposed architecture\\ngenerates a model that achieves a consistent increase in objective\\nquality for videos compressed with multiple codecs, reaching an\\naverage Δ PSNR value of up to 0.764 dB. To the best of the authors’\\nknowledge, this is the first DNN architecture to employ multidomain training for VQE and that has been trained and tested for\\nmultiple video codecs and formats.\\nThis paper is organized as follows. Section 2 presents previous\\nVQE works that are based on deep learning and other multi-domain\\nsolutions for other video-related tasks. Section 3 presents the proposed Multi-Domain Spatio-Temporal Deformable Fusion (MDSTDF) architecture. Section 4 presents and discusses the obtained\\nresults. Finally, Section 5 concludes this work.\\n### **2 RELATED WORK**\\n\\nIn recent years, different architectures have been proposed to investigate the VQE problem. In this section, we present the main studies\\nthat have contributed to the evolution of learning-based models for\\nVQE. Also, related works focusing on multi-domain learning for\\nvideo-related problems are discussed.\\n### **2.1 Video Quality Enhancement**\\n\\nThe problem of VQE emerged with the application of frame-byframe image processing algorithms in videos. These methods originated from linear algorithms based on heuristics, processing all\\npixels in the frame, disregarding spatial differences within the same\\n\\n\\n224\\n\\n\\n-----\\n\\nMulti-Domain Spatio-Temporal Deformable Fusion model for video quality enhancement WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\nframe, and applying the same equation to the entire image. While\\nthis approach effectively addressed degradation issues concentrated\\nin specific parts of the image, it often detrimentally affected other\\nareas that did not share the same problem.\\nThese algorithms have gradually given way to nonlinear machine learning models, which aim to comprehend a zone of pixels\\nand detect their characteristics, thereby determining which heuristic yields superior results in a given scenario. Consequently, the\\nside effects caused by these processes tend to diminish. Moreover,\\nthe advent of DNN has accompany in more robust architectures\\nfor nonlinear learning, enabling the detection of increasingly intricate characteristics as network depth increases. This evolution\\nhas significantly contributed to the development of more effective\\nmethods for Video Quality Enhancement.\\nOne of the first solutions for improving video quality based on\\ndeep learning is [ 12 ], which proposes an Artifact Reduction CNN\\n(ARCNN) that processes each frame individually, exploring only\\nthe spatial information within the image. Building on [ 12 ], other\\napplications for the model were explored, such as an in-loop filter\\nthat replaces DF and SAO filters [ 10 ] or as a post-processing filter\\n\\n[ 16 ], which performs the filtering after the frames are fully decoded.\\nSome studies emerged with the proposal to explore the existing\\ntemporal correlation between frames. Initially, models based on\\nmultiple frames [ 5, 11, 14, 34 ] proved effective for the VQE problem.\\nThese models define a temporal sliding window that processes a\\nfixed number of frames to improve the central one. This way, the\\nGroup of Pictures (GOP) structure present in most of the video\\ncoding standards can be explored, allowing information from highquality frames to be used to improve low-quality frames [28].\\nmodels based on multiple frames aim to synchronize past and\\nfuture frames with the currently processed frame. This alignment\\nprocess is typically achieved through motion compensation techniques, such as optical flow estimation [ 28, 31 ]. Optical flow helps in\\ndetermining the motion vectors between consecutive frames, facilitating the alignment process and enabling the model to understand\\nthe temporal relationships between frames.\\nFollowing alignment, the fusion of processed frames occurs, with\\nthe objective of incorporating the best quality characteristics from\\neach frame. Two common fusion approaches include direct fusion\\nand slow fusion. In direct fusion, all frames are fused simultaneously,\\nleveraging the information from each frame to enhance the overall\\nquality. On the other hand, slow fusion involves iteratively fusing\\npairs of frames until only one frame remains, gradually synthesizing\\nthe final enhanced frame [20].\\nThis alignment and fusion strategy allows models to exploit\\ntemporal information across multiple frames, leading to more comprehensive and effective video quality enhancement. By aligning\\nframes and fusing their features judiciously, these models can better capture and preserve the temporal coherence and visual details\\npresent in the video sequence.\\nAn alternative to the optical flow-based fusion is the use of\\ndeformable convolutions [ 11, 30 ]. This mechanism replaces the\\ntraditional convolution used by CNN layers with a deformable\\nconvolution, where, instead of using a fixed conventional matrix as\\na filter, a matrix with variably displaced points is used, learning the\\nprocess of displacing neighboring pixels relative to the processed\\npixel through information obtained from previous convolutions.\\n\\n\\nAnother strategy employed to harness temporal information in\\nvideo processing involves the utilization of recurrent models, particularly Recurrent Neural Networks (RNNs). RNNs are designed to\\nhandle sequential data by processing input sequences one step at\\na time, maintaining a hidden state that captures information from\\nprevious steps [ 19 ]. In the context of video quality enhancement,\\nRNNs operate by progressively analyzing each frame of the video sequence. As each frame is processed, its characteristics are extracted\\nand incorporated into the hidden state of the network. This hidden\\nstate serves as a condensed representation of the temporal information extracted from the video sequence so far, enabling the network\\nto understand the temporal dependencies and patterns present in\\nthe data. Subsequently, this aggregated temporal information can\\nbe utilized to enhance the quality of future frames through filtering or other enhancement techniques. Studies explore recurrent\\nnetworks in different ways, either in a unidirectional manner [ 22 ],\\nwhere the characteristics are propagated from past frames to the\\ncurrently processed frame, or using a bidirectional improvement\\n\\n[ 35 ], where both features from past and future frames are used to\\nimprove the quality of the current frame.\\n### **2.2 Multi-Domain Learning**\\n\\nThe advancement of machine learning algorithms has been made\\npossible due to the abundance of available data. However, the current training paradigms are limited in the variety of data they can\\nhandle. Most methods operate on data from specific domains, causing the model to learn the inherent bias of the dataset. As a result,\\nthe models perform well on tasks specific to the domains they\\nwere trained on, severely limiting their ability to generalize when\\nprocessing data from new and previously unseen domains. These\\nchallenges become more pronounced when dealing with data from\\nhighly variable domains, especially when there is a need to develop a single model capable of handling multiple distinct datasets.\\nTraining a single model to encompass this diversity of domains\\nprevents the capture of specific nuances from each one. The common approach to address this issue involves recreating a model for\\neach domain and applying each model to the corresponding data.\\nHowever, this methodology proves to be highly inefficient\\nVideos exhibit characteristics with a high variability, making it\\nchallenging for a model to generate a unique representation that can\\ncapture them all. Therefore, the Multi-Domain Network (MDNet)\\n\\n[ 23 ] emerged with the proposal of separating videos into annotated\\ndomains, so that each domain follows a distinct path in the final\\nlayers of the network. In this way, common features among all\\ndomains are extracted in the initial layers of the network, while\\nfeatures specific to the domain are extracted in the final layers.\\nFollowing the same proposal as the previous study, the BranchActivated Multi-Domain Convolutional Neural Network for Visual\\n\\nTracking (BAMCNN) [ 6 ] was developed. It leverages the concept of\\nmulti-domain training to create an architecture for visual tracking,\\nseparating videos into different domains based on their similarities.\\nThis involves creating branches in the network for each domain,\\nthus detecting their specific characteristics. During testing, the\\nbranch with the highest level of similarity to the processed video\\nsequence is identified and activated.\\n\\n\\n225\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Silveira Júnior et al.\\n\\n**Figure 2: The MD-STDF architecture consists of the STDF Alignment and Fusion Module, that is shared by all domains and**\\n***n*** **branches of QE multi-domain. The red-border frame represents the central one, under processing, whereas blue-border**\\n**neighboring frames are aligned and used in the enhancement of the central frame.**\\n\\n\\nRecently, other studies have employed multi-domain learning\\nfor different video-related problems. In [ 25 ], the authors addressed\\nthe quality enhancement of multiview video coding. The work [ 1 ]\\nexplores the detection of deepfake videos by combining features\\nfrom two distinct domains, spatial and frequency, in a discriminative\\nlearning model.\\nThis work aims to explore VQE under a generalized approach,\\nseeking to create a model that is not limited to improving the quality of videos in a specific domain but is generic enough to enhance\\nvideos belonging to other domains. No studies were found that addressed the multi-domain training method from such a perspective.\\n### **3 MULTI-DOMAIN STDF**\\n\\nThe proposed Multi-Domain Spatio-Temporal Deformable Fusion\\n(MD-STDF) architecture is based on the STDF architecture [ 11 ],\\nwhich employs the approach of using multiple frames to enhance\\na central frame. Additionally, the architecture applies the multidomain training strategy, with training data originated from different domains. The domain information is also incorporated in the\\ndataset to allow the model to learn the domain-specific characteristics along the training.\\nThe STDF architecture [ 11 ] is divided into two modules, with the\\nfirst one focusing on frame alignment and fusion, as well as shallow feature extraction. The second module is dedicated to quality\\nenhancement. The model receives as input the central frame to be\\nimproved ( *𝑡* 0 ), concatenated with temporally neighboring frames\\n( *𝑡* 1 for the future frames and *𝑡* −1 for past frames). The number of\\nneighboring frames to be concatenated with the central frame is determined by the Radius ( *R* ) parameter, which represents the number\\n\\n\\nof neighboring frames. Thus, with *R* defined as 1, the total number\\nof frames to be concatenated and introduced into the model is 3.\\n\\nAs depicted in Figure 2, in the development of MD-STDF, the\\nalignment and fusion module is dedicated to obtaining general\\nfeatures — i.e., those that are common to all videos regardless of the\\ndomain. The network starts with a shallow U-Net Model [ 26 ] that\\nextracts the features for offset prediction. This model is based on\\nStride 1 convolutions, which mantain the sample dimensionality,\\nStride 2 convolutions, which reduce sample dimensionality, and\\nStride 2 deconvolutions, which perform an upsampling. This part of\\nthe network captures the temporal characteristics of the sequence\\nand generates an offset field mask. The kernel size of the offset\\nmask is 2 *𝐾* [2] . This mask is used with the input 2R+1 sequence in\\nthe Spatio-Temporal Deformable Convolution (STDC) network,\\ngenerating a Fused Feature Map with 64 channels.\\nInstead of calculate the diference between two frames, that is the\\nmethod used in optical flow, in studies like [ 14 ], the STDC modules\\nuses a modulated deformable convolution layer [ 36 ], this layer\\nrealize the motion compensation of the entire input sequence at\\nonce. As an alternative of using a fixed grid of positions to apply the\\nconvolutional kernels (as in conventional convolutions), deformable\\nconvolution introduces additional offsets that are learned by the\\nnetwork during training. These offsets allow the convolutional\\nkernel to ’deform’ to better align with the input patterns, resulting\\nin improved capture of spatial and temporal variations [9].\\nThe multi-domain training is based on a label linking the video\\nto a specific domain (codec), which is employed in the transition\\nbetween the alignment and fusion module and the quality enhancement (QE) module. The branches created for each domain ensure\\nthat the QE module is updated with parameters specific to the codec.\\n\\n\\n226\\n\\n\\n-----\\n\\nMulti-Domain Spatio-Temporal Deformable Fusion model for video quality enhancement WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\nThe activated branch delivers the Fused Feature Map to the corresponding QE module according to the video label. The QE Modules\\nare composed by *L* convolutional layers with Stride 1, maintain\\nthe dimensionality of the samples and extract domain-related features. At last, the QE module generates a Residual Map, which is\\nadded to the central frame ( *𝑡* 0 ), generating an enhanced frame ( *𝑡* 0 ’).\\nThe procedure is repeated for each frame of the compressed video,\\ngenerating an enhanced video sequence.\\n### **3.1 Data Preparation**\\n\\nA survey of the main datasets used for VQE was conducted. This\\nsurvey did not include image datasets, only videos. Additionally,\\ndatasets with a specific purpose, such as screen content videos or\\nsports videos only, were not included, but rather those that cover\\na wide category range. Among the possible choices for datasets\\nare the MFQE dataset [ 14 ], the Large-scale Diverse Video (LDV)\\ndataset used in the challenge proposed at the New Trends in Image\\nRestoration and Enhancement workshop and challenges on image\\nand video processing (NTIRE) in 2021 [ 32 ], and the Vimeo-90K\\ndataset [31].\\nSince this work was not restricted to a specific video category,\\nthe videos from the chosen dataset encompass the broadest spectrum of possible categories (sports, screen content, face videos,\\nanimals, etc.), as well as differences related to brightness, video\\nenvironment, and camera angle. The MFQE dataset [ 33 ] was chosen, which contains 126 uncompressed videos (108 for training and\\n18 for testing) at different resolutions ranging from 352 × 240 to\\n\\n1920 × 1080.\\n\\nThe video sequences were organized according to the standards\\nused for compression, in order to split the dataset for the multidomain training. Four versions of the training dataset were generated, each corresponding to the addressed domains. Thus, the 108\\nvideos were encoded and decoded using the reference software for\\nthe four standards/formats, resulting in a total of 432 videos. The\\nfour distinct domains defined correspond to the High Efficiency\\nVideo Coding (HEVC) standard [ 27 ], the Versatile Video Coding\\n(VVC) standard [ 4 ], the AOMedia Video 1 (AV1) format [ 7 ], and the\\nVP9 [21] format.\\nFor HEVC encoding, the reference software HEVC Model (HM)\\nversion 16.5 was used, and for VVC, the VVC Test Model (VTM)\\nversion 13.0 was employed, both configured with *Low Delay P*\\ntemporal setting. For AV1 encoding, the reference software *libaom*,\\nhashcode3.3 was used, whereas for VP9 the *libvpx*, hashcode1.12.0,\\nwas used. For HEVC and VVC, the quantization parameter (QP) was\\nset to 37, while for VP9 and AV1, constant quality (CQ) parameter\\nwas set to 55.According to the [ 2 ] analysis, there is no correlation\\nbetween the QP and CP values that indicates the same quality loss\\nis occurring in both encoders. Thus, CQ and QP values were chosen\\nas those that lead to the highest level of degradation according to\\nthe recommended test conditions of HEVC/VVC and VP9/AV1. The\\nQP/CQ controls the level of quantization applied to the transforms\\nof video data blocks. A high QP/CQ value during compression\\ncauses information to be discarded during the quantization process,\\nleading to a loss of fine details and, consequently, lower image\\nquality. A lower CQ value means that less quantization is applied,\\nresulting in superior quality.\\n\\n### **3.2 Training Process**\\n\\nFor training, a computer with the following configuration was used:\\nAMD Ryzen 7 5700X processor, 32 GB RAM, Nvidia Geforce RTX\\n3070 GPU with 8 GB VRAM. The batch size and the number of\\n\\niterations were adjusted to achieve 10 epochs with one GPU (i.e.,\\na batch size of 32 and 1,200,000 iterations over the dataset). The\\ntraining was conducted using Adam as optimizer and a learning\\nrate of 0.0001.\\n\\nThe algorithm employed was Stochastic Gradient Descent (SGD)\\n\\n[ 23 ], where each training iteration was executed under a specific\\ndomain. In other words, the batch of videos used belongs to a single\\ndomain, activating only one branch of the network.\\nAfter a certain number of iterations, the model is updated based\\nsolely on the processed batch. Subsequently, a new batch from\\nanother domain is processed, causing the shared layers of the network to be updated while keeping the previously updated branch\\nunchanged. This process is repeated until the predefined number of\\niterations is reached. Following this approach, the generic features\\ncommon to all processed videos are obtained in the shared layers\\nof the network, while for each specific branch of each domain,\\nmodeling is done to acquire domain-specific characteristics.\\n### **4 EXPERIMENTAL RESULTS**\\n\\nThe obtained results offer a comprehensive view of the conducted\\nstudy and aid in assessing the effectiveness of the adopted multidomain training methodology. The results are presented in terms\\nof Delta Peak Signal-to-Noise Ratio ( Δ PSNR), which measures the\\nobjective difference between the enhanced and the low-quality\\ndecoded video. Positive numbers indicate and increase in objective\\nquality, whereas negative numbers indicate a quality decrease.\\nFor comparison purposes, three single-codec STDF models were\\nalso trained following the methodology presented in [ 11 ]: the first\\nwith a dataset containing only videos compressed with the HEVC\\ncodec; the second with videos compressed with the VVC codec; and\\nthe third with videos compressed with the AV1 codec. Additionally,\\nthe multi-codec, single-domain approach proposed in [ 15 ] is also\\npresented. The same encoder configurations and training setup\\nmentioned in the previous section were used.\\nTable 1 shows the comparison of different models trained for\\ncomparison purposes. The *Training Dataset* column represents the\\ndataset used to train each model. The subsequent columns represent\\nthe test dataset used to obtain the Δ PSNR values. The first three\\nrows (HEVC, VVC and AV1) present the VQE results obtained from\\ntraining models using the single-codec approach. In the fourth row,\\nthe results obtained from training using the multi-codec approach\\n\\n[ 15 ] are shown. Finally, the last row presents the results obtained\\nwith the model trained using the proposed multi-domain method.\\nAs observed, the model trained with videos compressed with\\nHEVC achieves the best results (0.755 dB) when tested with videos\\nencoded with the same standard. However, for videos encoded with\\nAV1 standard, the model yields a negative result for VQE (-0.506\\ndB). A similar trend is observed for the model trained with videos\\ncompressed with VVC, which performs poorly for AV1-compressed\\nvideos. The multi-codec model proposed in [ 15 ] presents more constant results (varying between 0.210 dB and 0.375 dB), but does not\\nperform as well as the single-codec models. Finally, the proposed\\n\\n\\n227\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Silveira Júnior et al.\\n\\n**Table 1: Comparison between single-codec, multi-codec and multi-domain approaches.**\\n\\n|STDF Model|PSNR (dB) Δ|Col3|Col4|Col5|\\n|---|---|---|---|---|\\n||HEVC|VVC|VP9|AV1|\\n||QP 37|QP 37|CQ 55|CQ 55|\\n\\n\\n\\nHEVC Q P 37 [ 11 ] 0.755 0.250 0.357 -0.506\\nVVC Q P 37 0.529 0.371 0.385 -0.016\\nAV1 C Q 55 0.285 0.144 0.389 0.286\\nMulti-Codec [ 15 ] 0.335 0.210 0.375 0.229\\n\\n**Multi-Domain** **0.764** **0.448** **0.736** **0.228**\\n\\n**Table 2: VQE results for the MD-STDF model (** Δ **PSNR and** Δ **SSIM).**\\n\\n|Test Dataset|Col2|PSNR (dB) and SSIM Δ Δ|Col4|Col5|Col6|Col7|Col8|Col9|Col10|\\n|---|---|---|---|---|---|---|---|---|---|\\n|||HEVC QP 37||VVC QP 37||VP9 CQ 55||AV1 CQ 55||\\n|||PSNR|SSIM|PSNR|SSIM|PSNR|SSIM|PSNR|SSIM|\\n|Class A|Trafcfi|0.662|0.011|0.420|0.006|0.727|0.009|0.120|0.003|\\n||People on Street|1.126|0.020|0.582|0.009|0.911|0.014|0.200|0.004|\\n|Class B|Kimono|0.831|0.016|0.547|0.008|0.462|0.007|0.184|0.004|\\n||ParkScene|0.551|0.013|0.426|0.011|0.487|0.008|0.152|0.005|\\n||Cactus|0.698|0.013|0.404|0.007|0.641|0.010|0.130|0.003|\\n||BQTerrace|0.543|0.009|0.217|0.005|0.402|0.006|-0.024|0.001|\\n||BasketballDrive|0.686|0.012|0.292|0.005|0.552|0.008|0.156|0.004|\\n|Class C|RaceHorses|0.447|0.011|0.220|0.005|0.473|0.011|0.153|0.003|\\n||BQMall|0.838|0.017|0.529|0.009|0.828|0.012|0.231|0.004|\\n||PartyScene|0.640|0.019|0.380|0.011|0.731|0.014|0.197|0.005|\\n||BasketballDrill|0.718|0.015|0.290|0.005|0.733|0.014|0.197|0.004|\\n|Class D|RaceHorses|0.691|0.018|0.436|0.011|0.661|0.015|0.344|0.008|\\n||BQSquare|1.020|0.015|0.667|0.009|1,234|0.010|0.488|0.004|\\n||BlowingBubbles|0.635|0.020|0.453|0.015|0.702|0.015|0.328|0.008|\\n||BasketballPass|0.971|0.019|0.716|0.015|0.860|0.015|0.496|0.008|\\n|Class E|FourPeople|0.913|0.012|0.548|0.006|1.046|0.008|0.325|0.002|\\n||Johnny|0.804|0.008|0.414|0.003|0.843|0.006|0.164|0.001|\\n||KristenAndSara|0.969|0.009|0.515|0.004|0.954|0.006|0.270|0.002|\\n|Average||0.764|0.014|0.448|0.008|0.736|0.010|0.228|0.004|\\n\\n\\nmulti-domain model is the one that presents the best results in all\\ncases (varying between 0.228 dB for AV1 and 0.764 dB for HEVC),\\nperforming even better than the single-codec models trained specifically for each standard.\\nTable 2 presents the results of objective quality variation for\\neach video sequence. The results are presented in terms of Δ PSNR\\nand Δ SSIM for each video sequence, with the last row showing the\\naverage results. The videos are grouped according to their Classs\\n\\n[ 3 ]: Class A (2560x1600), Class B (1920x1080), Class C (832x480),\\nClass D (416x240), Class E (1280x720). Due to the good results\\npresented in terms of Δ PSNR, it was decided to add the SSIM metric\\nto complement the analysis of the results.\\nAs can be observed in the table, most of the results are positive,\\nwith only one specific case showing a negative Δ PSNR value. However, in terms of Δ SSIM, the result remained positive. On average,\\nall the results were positive. The worst result achieved, in terms of\\nΔ PSNR, was -0.024 dB for AV1 in the *BQTerrace* sequence, but in\\nterms of Δ SSIM, the video showed a slight improvement. This is also\\nthe worst result in terms of Δ SSIM. The best result in Δ PSNR was\\n\\n\\n1.234 dB for VP9 in the *BQSquare* sequence; in Δ SSIM, it was 0.020\\nfor HEVC in the *People on Street* sequence. For average results of\\nΔ PSNR, the worst improvement achieved was 0.228 dB for videos\\nencoded with AV1, and the best improvement was 0.764 dB for\\nvideos encoded with HEVC. In terms of average Δ SSIM, the worst\\nimprovement was 0.004 for videos encoded with AV1, and the best\\nwas 0.014 for videos encoded with HEVC. Some specific cases, in\\naddition to the best case, showed results in terms of Δ PSNR above\\n1 dB, such as the *People on Street* sequence encoded by HEVC with\\nQP 37, which achieved a result of 1.126 dB; the *BQSquare* sequence\\nencoded by HEVC with QP 37 achieved a result of 1.020 dB; and\\nthe *FourPeople* sequence encoded by VP9 with CQ 55 achieved a\\nresult of 1.046 dB.\\n### **5 VISUAL QUALITY PERCEPTION**\\n\\nThis section presents an analysis of the perceived visual quality\\nimprovement of the multi-domain STDF solution. For this section,\\nthe selected frame of video sequence was the one in which the\\n\\n\\n228\\n\\n\\n-----\\n\\nMulti-Domain Spatio-Temporal Deformable Fusion model for video quality enhancement WebMedia’2024, Juiz de Fora, Brazil\\n\\n(a) Original Frame (RAW)\\n\\n**Figure 3: Frame number 14 of the BasketballDrill sequence: (a) Original frame (RAW); (b)(e)(h)(k) Cropped section of the**\\n**original frame; (c) HEVC compressed version; (d) Improved HEVC version; (f) VVC compressed version; (g) Improved VVC**\\n**version; (i) VP9 compressed version; (j) Improved VP9 version; (l) AV1 compressed version; (m) Improved AV1 version.**\\n\\n\\ngreatest difference in visual quality was observed after a series of\\nvisual analyses.\\nThe Figure 3 presents a composition based on frame number 14\\nof the *BasketballDrill* sequence. Observing the images in the second\\ncolumn, i.e., the images that went through the compression process,\\nmost of them exhibit a significant number of artifacts. The most\\ndeteriorated images are (c) and (i), which correspond to the HEVC\\nand VP9 codecs, respectively. In image (l), which corresponds to the\\nAV1 codec, some artifacts are still noticeable, although in smaller\\nquantities. On the other hand, image (f), corresponding to the VVC\\ncodec, has the fewest artifacts.\\nIn all the compressed images, the blurring effect is noticeable,\\nespecially in the background. The blocking effect can be observed\\nin images (c), (i), and (l). In image (c), this artifact creates a stair-step\\neffect on the edges of the ball. In image (i), it creates a mosaic effect.\\nIn image (l), the stair-step effect is also noticeable on the upper\\nright edge of the ball.\\nComparing the low-quality versions in the second column with\\ntheir respective enhanced versions in the third column, it is evident\\nthat images (d) and (j) show a significant improvement in visual\\nquality. Image (m) also demonstrates a smoothing of artifacts. Image\\n(g), on the other hand, is the one that most resembles its compressed\\nversion before the application of the enhancement filter, since image\\n(f) exhibits an extremely low incidence of compression artifacts.\\n\\n### **6 CONCLUSION**\\n\\nThis work proposed a novel architecture named Multi-Domain\\nSpatio-Temporal Deformable Fusion (MD-STDF), which employs\\nmulti-domain learning to enhance the quality of videos compressed\\nwith different codecs, overcoming limitations of previous approaches.\\nThe model was trained with videos generated by multiple video\\ncodecs, thus learning characteristics of different types and levels\\nof compression artifacts more effectively. In the conducted experiments, MD-STDF achieved promising results by providing significant improvements in VQE for videos compressed with HEVC,\\nVVC, AV1, and VP9. The multi-domain approach proved superior\\nto single-codec and single-domain techniques, consistently yielding gains across all tests. On average, the quality improvement\\nin terms of Δ PSNR ranged between 0.228 dB (AV1) and 0.764 dB\\n(HEVC),indicating that the model achieves a strong generalization\\ncapability for different video compression scenarios. This can be\\nexplained by the larger number of training samples, which may\\nhave led to a greater refinement of the alignment and fusion network, which is the network shared among all domains. The good\\nresults from the objective analysis of the multi-domain model are\\nalso reflected in the perception of subjective quality improvement.\\nAs shown in the visual quality analysis, the multi-domain model\\nwas able to satisfactorily remove the artifacts present in the frames,\\nsmoothing the images. Some details cannot be restored due to the\\n\\n\\n229\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Silveira Júnior et al.\\n\\n\\nlossy nature of the compression process; however, overall, the quality improvement in the frames is noticeable. Knowing that PSNR\\ndoes not have a strong correlation with subjective video quality, it\\nis intended that for future work, other evaluation metrics such as\\nVMAF and LPIPS will be adopted. Furthermore, the authors intend\\nto explore training models with an even more diverse set of codecs\\nand QP/CQ configurations, aiming for increased generalization\\nand effectiveness across different scenarios. Additionally, it is also\\nintended to conduct an ablation study and explore cost reduction\\ntechniques.\\n### **7 ACKNOWLEDGEMENTS**\\n\\nThis study was financed in part by the *Coordenação de Aperfeiçoa-*\\n*mento de Pessoal de Nível Superior* – Brasil (CAPES) – Finance Code\\n001, Foundation for Research Support of the State of Rio Grande\\ndo Sul (FAPERGS), and National Council for Scientific and Technological Development (CNPq).\\n### **REFERENCES**\\n\\n[1] Aayushi Agarwal, Akshay Agarwal, Sayan Sinha, Mayank Vatsa, and Richa Singh.\\n2021. MD-CSDNetwork: Multi-domain cross stitched network for deepfake\\ndetection. In *2021 16th IEEE international conference on automatic face and gesture*\\n*recognition (FG 2021)* . IEEE, 1–8.\\n\\n[2] Isis Bender, Daniel Palomino, Luciano Agostini, Guilherme Correa, and Marcelo\\nPorto. 2019. Compression efficiency and computational cost comparison between\\nAV1 and HEVC encoders. In *2019 27th European Signal Processing Conference*\\n*(EUSIPCO)* . IEEE, 1–5.\\n\\n[3] J. Boyce, K. Suehring, and X. Li. 2018. JVET-J1010: JVET common test conditions\\nand software reference configurations. *JVET-J1010* (2018).\\n\\n[4] Benjamin Bross, Ye-Kui Wang, Yan Ye, Shan Liu, Jianle Chen, Gary J Sullivan,\\nand Jens-Rainer Ohm. 2021. Overview of the versatile video coding (VVC)\\nstandard and its applications. *IEEE Transactions on Circuits and Systems for Video*\\n*Technology* 31, 10 (2021), 3736–3764.\\n\\n[5] Jose Caballero, Christian Ledig, Andrew Aitken, Alejandro Acosta, Johannes\\nTotz, Zehan Wang, and Wenzhe Shi. 2017. Real-time video super-resolution with\\nspatio-temporal networks and motion compensation. In *Proceedings of the IEEE*\\n*conference on computer vision and pattern recognition* . 4778–4787.\\n\\n[6] Yimin Chen, Rongrong Lu, Yibo Zou, and Yanhui Zhang. 2018. Branch-Activated\\nMulti-Domain Convolutional Neural Network for Visual Tracking. *Journal of*\\n*Shanghai Jiaotong University (Science)* 23 (2018), 360–367.\\n\\n[7] Yue Chen, Debargha Murherjee, Jingning Han, Adrian Grange, Yaowu Xu, Zoe\\nLiu, Sarah Parker, Cheng Chen, Hui Su, Urvang Joshi, et al . 2018. An overview\\nof core coding tools in the AV1 video codec. In *2018 picture coding symposium*\\n*(PCS)* . IEEE, 41–45.\\n\\n[8] V Cisco. 2020. Cisco visual networking index: Forecast and trends, 2018–2023.\\n*White Paper* 1 (2020).\\n\\n[9] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen\\nWei. 2017. Deformable convolutional networks. In *Proceedings of the IEEE inter-*\\n*national conference on computer vision* . 764–773.\\n\\n[10] Yuanying Dai, Dong Liu, and Feng Wu. 2017. A convolutional neural network\\napproach for post-processing in HEVC intra coding. In *MultiMedia Modeling:*\\n*23rd International Conference, MMM 2017, Reykjavik, Iceland, January 4-6, 2017,*\\n*Proceedings, Part I 23* . Springer, 28–39.\\n\\n[11] Jianing Deng, Li Wang, Shiliang Pu, and Cheng Zhuo. 2020. Spatio-temporal deformable convolution for compressed video quality enhancement. In *Proceedings*\\n*of the AAAI conference on artificial intelligence*, Vol. 34. 10696–10703.\\n\\n[12] Chao Dong, Yubin Deng, Chen Change Loy, and Xiaoou Tang. 2015. Compression\\nartifacts reduction by a deep convolutional network. In *Proceedings of the IEEE*\\n*International Conference on Computer Vision* . 576–584.\\n\\n[13] Chih-Ming Fu, Elena Alshina, Alexander Alshin, Yu-Wen Huang, Ching-Yeh\\nChen, Chia-Yang Tsai, Chih-Wei Hsu, Shaw-Min Lei, Jeong-Hoon Park, and WooJin Han. 2012. Sample adaptive offset in the HEVC standard. *IEEE Transactions*\\n*on Circuits and Systems for Video technology* 22, 12 (2012), 1755–1764.\\n\\n[14] Zhenyu Guan, Qunliang Xing, Mai Xu, Ren Yang, Tie Liu, and Zulin Wang. 2019.\\nMFQE 2.0: A new approach for multi-frame quality enhancement on compressed\\nvideo. *IEEE transactions on pattern analysis and machine intelligence* 43, 3 (2019),\\n949–963.\\n\\n[15] Gilberto Kreisler, Garibaldi da Silveira Junior, Bruno Zatt, Daniel Palomino, and\\nGuilherme Correa. 2023. Modelo Multi-Codec Baseado em Spatio-Temporal\\n\\n\\nDeformable Fusion para Melhoria de Qualidade de Vídeos Comprimidos. In *Anais*\\n*do L Seminário Integrado de Software e Hardware* . SBC, 143–154.\\n\\n[16] Shiba Kuanar, Christopher Conly, and KR Rao. 2018. Deep learning based HEVC\\nin-loop filtering for decoder quality enhancement. In *2018 Picture Coding Sympo-*\\n*sium (PCS)* . IEEE, 164–168.\\n\\n[17] Tianyi Li, Mai Xu, Ce Zhu, Ren Yang, Zulin Wang, and Zhenyu Guan. 2019. A\\ndeep learning approach for multi-frame in-loop filter of HEVC. *IEEE Transactions*\\n*on Image Processing* 28, 11 (2019), 5663–5678.\\n\\n[18] Zewen Li, Fan Liu, Wenjie Yang, Shouheng Peng, and Jun Zhou. 2021. A survey\\nof convolutional neural networks: analysis, applications, and prospects. *IEEE*\\n*transactions on neural networks and learning systems* 33, 12 (2021), 6999–7019.\\n\\n[19] Ming Liang and Xiaolin Hu. 2015. Recurrent convolutional neural network for\\nobject recognition. In *Proceedings of the IEEE conference on computer vision and*\\n*pattern recognition* . 3367–3375.\\n\\n[20] Xiandong Meng, Xuan Deng, Shuyuan Zhu, and Bing Zeng. 2019. Enhancing\\nquality for VVC compressed videos by jointly exploiting spatial details and\\ntemporal structure. In *2019 IEEE International Conference on Image Processing*\\n*(ICIP)* . IEEE, 1193–1197.\\n\\n[21] Debargha Mukherjee, Jim Bankoski, Adrian Grange, Jingning Han, John Koleszar,\\nPaul Wilkins, Yaowu Xu, and Ronald Bultje. 2013. The latest open-source video\\ncodec VP9-an overview and preliminary results. In *2013 Picture Coding Symposium*\\n*(PCS)* . IEEE, 390–393.\\n\\n[22] Seungjun Nah, Sanghyun Son, and Kyoung Mu Lee. 2019. Recurrent neural\\nnetworks with intra-frame iterations for video deblurring. In *Proceedings of the*\\n*IEEE/CVF conference on computer vision and pattern recognition* . 8102–8111.\\n\\n[23] Hyeonseob Nam and Bohyung Han. 2016. Learning multi-domain convolutional\\nneural networks for visual tracking. In *Proceedings of the IEEE conference on*\\n*computer vision and pattern recognition* . 4293–4302.\\n\\n[24] Andrey Norkin, Gisle Bjontegaard, Arild Fuldseth, Matthias Narroschke, Masaru\\nIkeda, Kenneth Andersson, Minhua Zhou, and Geert Van der Auwera. 2012. HEVC\\ndeblocking filter. *IEEE Transactions on Circuits and Systems for Video Technology*\\n22, 12 (2012), 1746–1754.\\n\\n[25] Bo Peng, Renjie Chang, Zhaoqing Pan, Ge Li, Nam Ling, and Jianjun Lei. 2022.\\nDeep in-loop filtering via multi-domain correlation learning and partition constraint for multiview video coding. *IEEE Transactions on Circuits and Systems for*\\n*Video Technology* 33, 4 (2022), 1911–1921.\\n\\n[26] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical image segmentation. In *Medical Image Computing*\\n*and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference,*\\n*Munich, Germany, October 5-9, 2015, Proceedings, Part III 18* . Springer, 234–241.\\n\\n[27] Gary J Sullivan, Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand. 2012.\\nOverview of the high efficiency video coding (HEVC) standard. *IEEE Transactions*\\n*on circuits and systems for video technology* 22, 12 (2012), 1649–1668.\\n\\n[28] Junchao Tong, Xilin Wu, Dandan Ding, Zheng Zhu, and Zoe Liu. 2019. Learningbased multi-frame video quality enhancement. In *2019 IEEE International Confer-*\\n*ence on Image Processing (ICIP)* . IEEE, 929–933.\\n\\n[29] Chia-Yang Tsai, Ching-Yeh Chen, Tomoo Yamakage, In Suk Chong, Yu-Wen\\nHuang, Chih-Ming Fu, Takayuki Itoh, Takashi Watanabe, Takeshi Chujoh, Marta\\nKarczewicz, et al . 2013. Adaptive loop filtering for video coding. *IEEE Journal of*\\n*Selected Topics in Signal Processing* 7, 6 (2013), 934–945.\\n\\n[30] Xintao Wang, Kelvin CK Chan, Ke Yu, Chao Dong, and Chen Change Loy. 2019.\\nEdvr: Video restoration with enhanced deformable convolutional networks. In\\n*Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*\\n*workshops* . 0–0.\\n\\n[31] Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and William T Freeman. 2019.\\nVideo enhancement with task-oriented flow. *International Journal of Computer*\\n*Vision* 127 (2019), 1106–1125.\\n\\n[32] Ren Yang and Radu Timofte. 2021. NTIRE 2021 Challenge on Quality Enhancement of Compressed Video: Dataset and Study. In *IEEE/CVF Conference on Com-*\\n*puter Vision and Pattern Recognition Workshops* .\\n\\n[33] Ren Yang, Mai Xu, Tie Liu, Zulin Wang, and Zhenyu Guan. 2018. Enhancing\\nquality for HEVC compressed videos. *IEEE Transactions on Circuits and Systems*\\n*for Video Technology* 29, 7 (2018), 2039–2054.\\n\\n[34] Ren Yang, Mai Xu, Zulin Wang, and Tianyi Li. 2018. Multi-frame quality enhancement for compressed video. In *Proceedings of the IEEE conference on computer*\\n*vision and pattern recognition* . 6664–6673.\\n\\n[35] Chao Zhu, Hang Dong, Jinshan Pan, Boyang Liang, Yuhao Huang, Lean Fu, and\\nFei Wang. 2022. Deep recurrent neural network with multi-scale bi-directional\\npropagation for video deblurring. In *Proceedings of the AAAI conference on artifi-*\\n*cial intelligence*, Vol. 36. 3598–3607.\\n\\n[36] Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. 2019. Deformable convnets\\nv2: More deformable, better results. In *Proceedings of the IEEE/CVF conference on*\\n*computer vision and pattern recognition* . 9308–9316.\\n\\n\\n230\\n\\n\\n-----\\n\\n',\n",
       "  'artigo_tokenizado': ['#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Multi',\n",
       "   '-',\n",
       "   'Domain',\n",
       "   'Spatio',\n",
       "   '-',\n",
       "   'Temporal',\n",
       "   'Deformable',\n",
       "   'Fusion',\n",
       "   'model',\n",
       "   'for',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   'video',\n",
       "   'quality',\n",
       "   'enhancement',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Garibaldi',\n",
       "   'da',\n",
       "   'Silveira',\n",
       "   'Júnior',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'garibaldi.dsj@inf.ufpel.edu.br',\n",
       "   'ViTech',\n",
       "   '-',\n",
       "   'PPGC',\n",
       "   '-',\n",
       "   'UFPel',\n",
       "   'Pelotas',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Gilberto',\n",
       "   'Kreisler',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'gkfneto@inf.ufpel.edu.br',\n",
       "   'ViTech',\n",
       "   '-',\n",
       "   'PPGC',\n",
       "   '-',\n",
       "   'UFPel',\n",
       "   'Pelotas',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Bruno',\n",
       "   'Zatt',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'zatt@inf.ufpel.edu.br',\n",
       "   'ViTech',\n",
       "   '-',\n",
       "   'PPGC',\n",
       "   '-',\n",
       "   'UFPel',\n",
       "   'Pelotas',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Daniel',\n",
       "   'Palomino',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'dpalomino@inf.ufpel.edu.br',\n",
       "   'ViTech',\n",
       "   '-',\n",
       "   'PPGC',\n",
       "   '-',\n",
       "   'UFPel',\n",
       "   'Pelotas',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'ABSTRACT',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Lossy',\n",
       "   'video',\n",
       "   'compression',\n",
       "   'introduces',\n",
       "   'artifacts',\n",
       "   'that',\n",
       "   'can',\n",
       "   'degrade',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'perceived',\n",
       "   'visual',\n",
       "   'quality',\n",
       "   'of',\n",
       "   'the',\n",
       "   'video',\n",
       "   '.',\n",
       "   'Improving',\n",
       "   'the',\n",
       "   'quality',\n",
       "   'of',\n",
       "   'compressed',\n",
       "   'videos',\n",
       "   'involves',\n",
       "   'mitigating',\n",
       "   'these',\n",
       "   'artifacts',\n",
       "   'through',\n",
       "   'filtering',\n",
       "   '\\n',\n",
       "   'techniques',\n",
       "   '.',\n",
       "   'Deep',\n",
       "   'neural',\n",
       "   'network',\n",
       "   '(',\n",
       "   'DNN',\n",
       "   ')',\n",
       "   'models',\n",
       "   'have',\n",
       "   'emerged',\n",
       "   'as',\n",
       "   '\\n',\n",
       "   'powerful',\n",
       "   'tools',\n",
       "   'for',\n",
       "   'this',\n",
       "   'task',\n",
       "   ',',\n",
       "   'demonstrating',\n",
       "   'effectiveness',\n",
       "   'in',\n",
       "   'artifact',\n",
       "   'reduction',\n",
       "   '.',\n",
       "   'However',\n",
       "   ',',\n",
       "   'traditional',\n",
       "   'approaches',\n",
       "   'typically',\n",
       "   'evaluate',\n",
       "   '\\n',\n",
       "   'these',\n",
       "   'models',\n",
       "   'using',\n",
       "   'videos',\n",
       "   'compressed',\n",
       "   'by',\n",
       "   'a',\n",
       "   'single',\n",
       "   'coding',\n",
       "   'standard',\n",
       "   ',',\n",
       "   'limiting',\n",
       "   'their',\n",
       "   'applicability',\n",
       "   'across',\n",
       "   'diverse',\n",
       "   'codecs',\n",
       "   '.',\n",
       "   'To',\n",
       "   'address',\n",
       "   '\\n',\n",
       "   'this',\n",
       "   'limitation',\n",
       "   ',',\n",
       "   'this',\n",
       "   'study',\n",
       "   'proposes',\n",
       "   'a',\n",
       "   'novel',\n",
       "   'multi',\n",
       "   '-',\n",
       "   'domain',\n",
       "   'architecture',\n",
       "   'built',\n",
       "   'upon',\n",
       "   'the',\n",
       "   'Spatio',\n",
       "   '-',\n",
       "   'Temporal',\n",
       "   'Deformable',\n",
       "   'Fusion',\n",
       "   'technique',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'This',\n",
       "   'innovative',\n",
       "   'approach',\n",
       "   'enables',\n",
       "   'the',\n",
       "   'development',\n",
       "   'of',\n",
       "   'models',\n",
       "   'capable',\n",
       "   'of',\n",
       "   'enhancing',\n",
       "   'videos',\n",
       "   'compressed',\n",
       "   'by',\n",
       "   'various',\n",
       "   'codecs',\n",
       "   ',',\n",
       "   'ensuring',\n",
       "   '\\n',\n",
       "   'consistent',\n",
       "   'performance',\n",
       "   'across',\n",
       "   'different',\n",
       "   'standards',\n",
       "   '.',\n",
       "   'Experimental',\n",
       "   '\\n',\n",
       "   'results',\n",
       "   'showcase',\n",
       "   'the',\n",
       "   'efficacy',\n",
       "   'of',\n",
       "   'the',\n",
       "   'proposed',\n",
       "   'method',\n",
       "   ',',\n",
       "   'yielding',\n",
       "   '\\n',\n",
       "   'significant',\n",
       "   'improvements',\n",
       "   'in',\n",
       "   'average',\n",
       "   'Peak',\n",
       "   'Signal',\n",
       "   '-',\n",
       "   'to',\n",
       "   '-',\n",
       "   'Noise',\n",
       "   'Ratio',\n",
       "   '\\n',\n",
       "   '(',\n",
       "   'PSNR',\n",
       "   ')',\n",
       "   'for',\n",
       "   'videos',\n",
       "   'compressed',\n",
       "   'with',\n",
       "   'HEVC',\n",
       "   ',',\n",
       "   'VVC',\n",
       "   ',',\n",
       "   'VP9',\n",
       "   ',',\n",
       "   'and',\n",
       "   'AV1',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'with',\n",
       "   'enhancements',\n",
       "   'of',\n",
       "   '0.764',\n",
       "   'dB',\n",
       "   ',',\n",
       "   '0.448',\n",
       "   'dB',\n",
       "   ',',\n",
       "   '0.736',\n",
       "   'dB',\n",
       "   ',',\n",
       "   'and',\n",
       "   '0.228',\n",
       "   'dB',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'respectively',\n",
       "   '.',\n",
       "   'The',\n",
       "   'code',\n",
       "   'of',\n",
       "   'our',\n",
       "   'MD',\n",
       "   '-',\n",
       "   'STDF',\n",
       "   'approach',\n",
       "   'is',\n",
       "   'available',\n",
       "   'at',\n",
       "   '\\n',\n",
       "   '*',\n",
       "   'https://github.com/Espeto/md-stdf',\n",
       "   '*',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'KEYWORDS',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Redes',\n",
       "   'neurais',\n",
       "   'profundas',\n",
       "   ',',\n",
       "   'Melhoria',\n",
       "   'de',\n",
       "   'qualidade',\n",
       "   'de',\n",
       "   'vídeo',\n",
       "   ',',\n",
       "   'Codificação',\n",
       "   'de',\n",
       "   'vídeo',\n",
       "   ',',\n",
       "   'Aprendizado',\n",
       "   'multi',\n",
       "   '-',\n",
       "   'domínio',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   '1',\n",
       "   'INTRODUCTION',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Video',\n",
       "   'compression',\n",
       "   'plays',\n",
       "   'a',\n",
       "   'crucial',\n",
       "   'role',\n",
       "   'in',\n",
       "   'services',\n",
       "   'dealing',\n",
       "   'with',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'distribution',\n",
       "   'and',\n",
       "   'storage',\n",
       "   'of',\n",
       "   'audiovisual',\n",
       "   'content',\n",
       "   ',',\n",
       "   'becoming',\n",
       "   'essential',\n",
       "   '\\n',\n",
       "   'for',\n",
       "   'the',\n",
       "   'operation',\n",
       "   'of',\n",
       "   'companies',\n",
       "   'like',\n",
       "   'Netflix',\n",
       "   ',',\n",
       "   'TikTok',\n",
       "   ',',\n",
       "   'and',\n",
       "   'YouTube',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Due',\n",
       "   'to',\n",
       "   'the',\n",
       "   'high',\n",
       "   'demand',\n",
       "   'for',\n",
       "   'this',\n",
       "   'type',\n",
       "   'of',\n",
       "   'service',\n",
       "   ',',\n",
       "   'digital',\n",
       "   'video',\n",
       "   'represented',\n",
       "   'the',\n",
       "   'highest',\n",
       "   'volume',\n",
       "   'of',\n",
       "   'data',\n",
       "   'transmitted',\n",
       "   'over',\n",
       "   'the',\n",
       "   'Internet',\n",
       "   '\\n',\n",
       "   'in',\n",
       "   'recent',\n",
       "   'years',\n",
       "   '.',\n",
       "   'It',\n",
       "   'has',\n",
       "   'been',\n",
       "   'predicted',\n",
       "   'that',\n",
       "   '4',\n",
       "   'K',\n",
       "   'videos',\n",
       "   'represented',\n",
       "   '\\n',\n",
       "   '66',\n",
       "   '%',\n",
       "   'of',\n",
       "   'Internet',\n",
       "   'consumption',\n",
       "   'on',\n",
       "   'television',\n",
       "   'devices',\n",
       "   'by',\n",
       "   'the',\n",
       "   'end',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   '2023',\n",
       "   ',',\n",
       "   'surpassing',\n",
       "   'the',\n",
       "   '2018',\n",
       "   'estimate',\n",
       "   '[',\n",
       "   '8',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Consequently',\n",
       "   ',',\n",
       "   'research',\n",
       "   'efforts',\n",
       "   'by',\n",
       "   'both',\n",
       "   'academia',\n",
       "   'and',\n",
       "   'industry',\n",
       "   'are',\n",
       "   'dedicated',\n",
       "   'to',\n",
       "   'improving',\n",
       "   '\\n',\n",
       "   'not',\n",
       "   'only',\n",
       "   'compression',\n",
       "   'efficiency',\n",
       "   'but',\n",
       "   'also',\n",
       "   'reducing',\n",
       "   'undesired',\n",
       "   'visual',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   ':',\n",
       "   'Proceedings',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Web',\n",
       "   '(',\n",
       "   'WebMe',\n",
       "   '\\n',\n",
       "   'dia’2024',\n",
       "   ')',\n",
       "   '.',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '.',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   ':',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   ',',\n",
       "   '2024',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'SBC',\n",
       "   '–',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '-',\n",
       "   '2753',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Guilherme',\n",
       "   'Correa',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'gcorrea@inf.ufpel.edu.br',\n",
       "   'ViTech',\n",
       "   '-',\n",
       "   'PPGC',\n",
       "   '-',\n",
       "   'UFPel',\n",
       "   'Pelotas',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '\\n\\n',\n",
       "   'effects',\n",
       "   'caused',\n",
       "   'by',\n",
       "   'this',\n",
       "   'process',\n",
       "   '.',\n",
       "   'As',\n",
       "   'video',\n",
       "   'content',\n",
       "   'continues',\n",
       "   'to',\n",
       "   'proliferate',\n",
       "   'across',\n",
       "   'various',\n",
       "   'platforms',\n",
       "   'and',\n",
       "   'devices',\n",
       "   ',',\n",
       "   'maintaining',\n",
       "   'high',\n",
       "   'visual',\n",
       "   '\\n',\n",
       "   'quality',\n",
       "   'while',\n",
       "   'efficiently',\n",
       "   'managing',\n",
       "   'data',\n",
       "   'transmission',\n",
       "   'and',\n",
       "   'storage',\n",
       "   '\\n',\n",
       "   'becomes',\n",
       "   'increasingly',\n",
       "   'paramount',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Compressed',\n",
       "   'videos',\n",
       "   'suffer',\n",
       "   'from',\n",
       "   'visual',\n",
       "   'effects',\n",
       "   'such',\n",
       "   'as',\n",
       "   'blocking',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'ringing',\n",
       "   'and',\n",
       "   'blurring',\n",
       "   'artifacts',\n",
       "   '[',\n",
       "   '12',\n",
       "   ']',\n",
       "   ',',\n",
       "   'which',\n",
       "   'compromise',\n",
       "   'the',\n",
       "   'perceived',\n",
       "   '\\n',\n",
       "   'video',\n",
       "   'quality',\n",
       "   'for',\n",
       "   'users',\n",
       "   '.',\n",
       "   'In',\n",
       "   'Figure',\n",
       "   '1',\n",
       "   ',',\n",
       "   'the',\n",
       "   'visual',\n",
       "   'effects',\n",
       "   'of',\n",
       "   'these',\n",
       "   'artifacts',\n",
       "   'can',\n",
       "   'be',\n",
       "   'observed',\n",
       "   '.',\n",
       "   'The',\n",
       "   'patches',\n",
       "   '(',\n",
       "   'c',\n",
       "   ')',\n",
       "   ',',\n",
       "   '(',\n",
       "   'd',\n",
       "   ')',\n",
       "   'and',\n",
       "   '(',\n",
       "   'e',\n",
       "   ')',\n",
       "   'are',\n",
       "   'separated',\n",
       "   '\\n',\n",
       "   'by',\n",
       "   'the',\n",
       "   'artifact',\n",
       "   'that',\n",
       "   'predominantly',\n",
       "   'affects',\n",
       "   'the',\n",
       "   'selected',\n",
       "   'part',\n",
       "   'of',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'image',\n",
       "   '.',\n",
       "   'Generally',\n",
       "   ',',\n",
       "   'artifacts',\n",
       "   'can',\n",
       "   'blend',\n",
       "   'with',\n",
       "   'or',\n",
       "   'appear',\n",
       "   'next',\n",
       "   'to',\n",
       "   'others',\n",
       "   ',',\n",
       "   'making',\n",
       "   'it',\n",
       "   'difficult',\n",
       "   'to',\n",
       "   'generate',\n",
       "   'a',\n",
       "   'patch',\n",
       "   'that',\n",
       "   'isolate',\n",
       "   'only',\n",
       "   'one',\n",
       "   '\\n',\n",
       "   'compression',\n",
       "   'artifact',\n",
       "   '.',\n",
       "   'In',\n",
       "   '(',\n",
       "   'c',\n",
       "   ')',\n",
       "   ',',\n",
       "   'the',\n",
       "   'division',\n",
       "   'between',\n",
       "   'blocks',\n",
       "   'used',\n",
       "   'in',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'compression',\n",
       "   'process',\n",
       "   'is',\n",
       "   'perceptible',\n",
       "   'in',\n",
       "   'the',\n",
       "   'middle',\n",
       "   'of',\n",
       "   'the',\n",
       "   'blue',\n",
       "   '\\n',\n",
       "   'part',\n",
       "   'of',\n",
       "   'the',\n",
       "   'ball',\n",
       "   ',',\n",
       "   'evidencing',\n",
       "   'a',\n",
       "   'blocking',\n",
       "   'effect',\n",
       "   '.',\n",
       "   'In',\n",
       "   '(',\n",
       "   'd',\n",
       "   ')',\n",
       "   ',',\n",
       "   'we',\n",
       "   'can',\n",
       "   'see',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'ringing',\n",
       "   'effect',\n",
       "   ',',\n",
       "   'noticeable',\n",
       "   'as',\n",
       "   'wave',\n",
       "   '-',\n",
       "   'like',\n",
       "   'patterns',\n",
       "   'along',\n",
       "   'the',\n",
       "   'edges',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'orange',\n",
       "   'rim',\n",
       "   '.',\n",
       "   'In',\n",
       "   '(',\n",
       "   'e',\n",
       "   ')',\n",
       "   ',',\n",
       "   'details',\n",
       "   'from',\n",
       "   'the',\n",
       "   'original',\n",
       "   'image',\n",
       "   ',',\n",
       "   'such',\n",
       "   'as',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'nails',\n",
       "   'on',\n",
       "   'the',\n",
       "   'ground',\n",
       "   'and',\n",
       "   'the',\n",
       "   'separations',\n",
       "   'in',\n",
       "   'the',\n",
       "   'wooden',\n",
       "   'floor',\n",
       "   ',',\n",
       "   'are',\n",
       "   '\\n',\n",
       "   'lost',\n",
       "   'during',\n",
       "   'compression',\n",
       "   ',',\n",
       "   'leading',\n",
       "   'to',\n",
       "   'a',\n",
       "   'blurring',\n",
       "   'effect',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Filtering',\n",
       "   'algorithms',\n",
       "   'like',\n",
       "   'the',\n",
       "   'Deblocking',\n",
       "   'Filter',\n",
       "   '(',\n",
       "   'DF',\n",
       "   ')',\n",
       "   '[',\n",
       "   '24',\n",
       "   ']',\n",
       "   ',',\n",
       "   'addressing',\n",
       "   'blocking',\n",
       "   'effects',\n",
       "   ',',\n",
       "   'Adaptive',\n",
       "   'Loop',\n",
       "   'Filter',\n",
       "   '(',\n",
       "   'ALF',\n",
       "   ')',\n",
       "   '[',\n",
       "   '29',\n",
       "   ']',\n",
       "   ',',\n",
       "   'that',\n",
       "   'minimize',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'distortion',\n",
       "   'between',\n",
       "   'the',\n",
       "   'original',\n",
       "   'and',\n",
       "   'decoded',\n",
       "   'samples',\n",
       "   ',',\n",
       "   'and',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'Sample',\n",
       "   'Adaptive',\n",
       "   'Offset',\n",
       "   '(',\n",
       "   'SAO',\n",
       "   ')',\n",
       "   '[',\n",
       "   '13',\n",
       "   ']',\n",
       "   ',',\n",
       "   'focused',\n",
       "   'on',\n",
       "   'reducing',\n",
       "   'banding',\n",
       "   'effects',\n",
       "   ',',\n",
       "   'are',\n",
       "   'standardized',\n",
       "   'processes',\n",
       "   'in',\n",
       "   'formats',\n",
       "   'such',\n",
       "   'as',\n",
       "   'High',\n",
       "   'Efficiency',\n",
       "   '\\n',\n",
       "   'Video',\n",
       "   'Coding',\n",
       "   '(',\n",
       "   'HEVC',\n",
       "   ')',\n",
       "   'and',\n",
       "   'Versatile',\n",
       "   'Video',\n",
       "   'Coding',\n",
       "   '(',\n",
       "   'VVC',\n",
       "   ')',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Both',\n",
       "   'DF',\n",
       "   'and',\n",
       "   'SAO',\n",
       "   'are',\n",
       "   'heuristic',\n",
       "   '-',\n",
       "   'based',\n",
       "   'methods',\n",
       "   ',',\n",
       "   'devised',\n",
       "   'based',\n",
       "   '\\n',\n",
       "   'upon',\n",
       "   'statistical',\n",
       "   'observations',\n",
       "   'for',\n",
       "   'reducing',\n",
       "   'compression',\n",
       "   'artifacts',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'These',\n",
       "   'models',\n",
       "   'are',\n",
       "   'applied',\n",
       "   'as',\n",
       "   'filters',\n",
       "   'that',\n",
       "   'traverse',\n",
       "   'all',\n",
       "   'pixels',\n",
       "   'in',\n",
       "   'each',\n",
       "   '\\n',\n",
       "   'frame',\n",
       "   ',',\n",
       "   'aiming',\n",
       "   'to',\n",
       "   'enhance',\n",
       "   'visual',\n",
       "   'quality',\n",
       "   '.',\n",
       "   'While',\n",
       "   'these',\n",
       "   'heuristicbased',\n",
       "   'approaches',\n",
       "   'have',\n",
       "   'demonstrated',\n",
       "   'effectiveness',\n",
       "   'in',\n",
       "   'mitigating',\n",
       "   '\\n',\n",
       "   'certain',\n",
       "   'artifacts',\n",
       "   ',',\n",
       "   'they',\n",
       "   'have',\n",
       "   'inherent',\n",
       "   'limitations',\n",
       "   '.',\n",
       "   'For',\n",
       "   'example',\n",
       "   ',',\n",
       "   'DF',\n",
       "   '\\n',\n",
       "   'may',\n",
       "   'introduce',\n",
       "   'a',\n",
       "   'blurring',\n",
       "   'effect',\n",
       "   'on',\n",
       "   'the',\n",
       "   'image',\n",
       "   'as',\n",
       "   'it',\n",
       "   'attempts',\n",
       "   'to',\n",
       "   'reduce',\n",
       "   'blockiness',\n",
       "   ',',\n",
       "   'potentially',\n",
       "   'sacrificing',\n",
       "   'fine',\n",
       "   'details',\n",
       "   'and',\n",
       "   'sharpness',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Additionally',\n",
       "   ',',\n",
       "   'both',\n",
       "   'DF',\n",
       "   'and',\n",
       "   'SAO',\n",
       "   'may',\n",
       "   'inadvertently',\n",
       "   'amplify',\n",
       "   'the',\n",
       "   'presence',\n",
       "   'of',\n",
       "   'other',\n",
       "   'compression',\n",
       "   'artifacts',\n",
       "   ',',\n",
       "   'such',\n",
       "   'as',\n",
       "   'ringing',\n",
       "   'or',\n",
       "   'mosquito',\n",
       "   '\\n',\n",
       "   'noise',\n",
       "   ',',\n",
       "   'particularly',\n",
       "   'in',\n",
       "   'regions',\n",
       "   'with',\n",
       "   'high',\n",
       "   'contrast',\n",
       "   'or',\n",
       "   'intricate',\n",
       "   'textures',\n",
       "   '[',\n",
       "   '17',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Despite',\n",
       "   'that',\n",
       "   ',',\n",
       "   'heuristic',\n",
       "   '-',\n",
       "   'based',\n",
       "   'methods',\n",
       "   'remain',\n",
       "   'valuable',\n",
       "   '\\n',\n",
       "   ...],\n",
       "  'pos_tagger': '',\n",
       "  'lema': ['multi',\n",
       "   'domain',\n",
       "   'spatio',\n",
       "   'Temporal',\n",
       "   'Deformable',\n",
       "   'Fusion',\n",
       "   'model',\n",
       "   'for',\n",
       "   'video',\n",
       "   'quality',\n",
       "   'enhancement',\n",
       "   'Garibaldi',\n",
       "   'da',\n",
       "   'Silveira',\n",
       "   'Júnior',\n",
       "   'garibaldi.dsj@inf.ufpel.edu.br',\n",
       "   'ViTech',\n",
       "   'PPGC',\n",
       "   'UFPel',\n",
       "   'Pelotas',\n",
       "   'Brazil',\n",
       "   'Gilberto',\n",
       "   'Kreisler',\n",
       "   'gkfneto@inf.ufpel.edu.br',\n",
       "   'ViTech',\n",
       "   'PPGC',\n",
       "   'UFPel',\n",
       "   'Pelotas',\n",
       "   'Brazil',\n",
       "   'Bruno',\n",
       "   'Zatt',\n",
       "   'zatt@inf.ufpel.edu.br',\n",
       "   'ViTech',\n",
       "   'PPGC',\n",
       "   'UFPel',\n",
       "   'Pelotas',\n",
       "   'Brazil',\n",
       "   'Daniel',\n",
       "   'Palomino',\n",
       "   'dpalomino@inf.ufpel.edu.br',\n",
       "   'ViTech',\n",
       "   'PPGC',\n",
       "   'UFPel',\n",
       "   'Pelotas',\n",
       "   'Brazil',\n",
       "   'ABSTRACT',\n",
       "   'lossy',\n",
       "   'video',\n",
       "   'compression',\n",
       "   'introduce',\n",
       "   'artifact',\n",
       "   'that',\n",
       "   'can',\n",
       "   'degrade',\n",
       "   'the',\n",
       "   'perceive',\n",
       "   'visual',\n",
       "   'quality',\n",
       "   'of',\n",
       "   'the',\n",
       "   'video',\n",
       "   'improve',\n",
       "   'the',\n",
       "   'quality',\n",
       "   'of',\n",
       "   'compress',\n",
       "   'video',\n",
       "   'involve',\n",
       "   'mitigate',\n",
       "   'these',\n",
       "   'artifact',\n",
       "   'through',\n",
       "   'filtering',\n",
       "   'technique',\n",
       "   'deep',\n",
       "   'neural',\n",
       "   'network',\n",
       "   'DNN',\n",
       "   'model',\n",
       "   'have',\n",
       "   'emerge',\n",
       "   'as',\n",
       "   'powerful',\n",
       "   'tool',\n",
       "   'for',\n",
       "   'this',\n",
       "   'task',\n",
       "   'demonstrate',\n",
       "   'effectiveness',\n",
       "   'in',\n",
       "   'artifact',\n",
       "   'reduction',\n",
       "   'however',\n",
       "   'traditional',\n",
       "   'approach',\n",
       "   'typically',\n",
       "   'evaluate',\n",
       "   'these',\n",
       "   'model',\n",
       "   'use',\n",
       "   'video',\n",
       "   'compress',\n",
       "   'by',\n",
       "   'a',\n",
       "   'single',\n",
       "   'code',\n",
       "   'standard',\n",
       "   'limit',\n",
       "   'their',\n",
       "   'applicability',\n",
       "   'across',\n",
       "   'diverse',\n",
       "   'codec',\n",
       "   'to',\n",
       "   'address',\n",
       "   'this',\n",
       "   'limitation',\n",
       "   'this',\n",
       "   'study',\n",
       "   'propose',\n",
       "   'a',\n",
       "   'novel',\n",
       "   'multi',\n",
       "   'domain',\n",
       "   'architecture',\n",
       "   'build',\n",
       "   'upon',\n",
       "   'the',\n",
       "   'Spatio',\n",
       "   'Temporal',\n",
       "   'Deformable',\n",
       "   'Fusion',\n",
       "   'technique',\n",
       "   'this',\n",
       "   'innovative',\n",
       "   'approach',\n",
       "   'enable',\n",
       "   'the',\n",
       "   'development',\n",
       "   'of',\n",
       "   'model',\n",
       "   'capable',\n",
       "   'of',\n",
       "   'enhance',\n",
       "   'video',\n",
       "   'compress',\n",
       "   'by',\n",
       "   'various',\n",
       "   'codec',\n",
       "   'ensure',\n",
       "   'consistent',\n",
       "   'performance',\n",
       "   'across',\n",
       "   'different',\n",
       "   'standard',\n",
       "   'experimental',\n",
       "   'result',\n",
       "   'showcase',\n",
       "   'the',\n",
       "   'efficacy',\n",
       "   'of',\n",
       "   'the',\n",
       "   'propose',\n",
       "   'method',\n",
       "   'yield',\n",
       "   'significant',\n",
       "   'improvement',\n",
       "   'in',\n",
       "   'average',\n",
       "   'Peak',\n",
       "   'Signal',\n",
       "   'to',\n",
       "   'Noise',\n",
       "   'Ratio',\n",
       "   'PSNR',\n",
       "   'for',\n",
       "   'video',\n",
       "   'compress',\n",
       "   'with',\n",
       "   'HEVC',\n",
       "   'VVC',\n",
       "   'VP9',\n",
       "   'and',\n",
       "   'AV1',\n",
       "   'with',\n",
       "   'enhancement',\n",
       "   'of',\n",
       "   '0.764',\n",
       "   'dB',\n",
       "   '0.448',\n",
       "   'dB',\n",
       "   '0.736',\n",
       "   'db',\n",
       "   'and',\n",
       "   '0.228',\n",
       "   'dB',\n",
       "   'respectively',\n",
       "   'the',\n",
       "   'code',\n",
       "   'of',\n",
       "   'our',\n",
       "   'MD',\n",
       "   'STDF',\n",
       "   'approach',\n",
       "   'be',\n",
       "   'available',\n",
       "   'at',\n",
       "   'https://github.com/Espeto/md-stdf',\n",
       "   'keyword',\n",
       "   'Redes',\n",
       "   'neurais',\n",
       "   'profunda',\n",
       "   'Melhoria',\n",
       "   'de',\n",
       "   'qualidade',\n",
       "   'de',\n",
       "   'vídeo',\n",
       "   'Codificação',\n",
       "   'de',\n",
       "   'vídeo',\n",
       "   'Aprendizado',\n",
       "   'multi',\n",
       "   'domínio',\n",
       "   '1',\n",
       "   'introduction',\n",
       "   'video',\n",
       "   'compression',\n",
       "   'play',\n",
       "   'a',\n",
       "   'crucial',\n",
       "   'role',\n",
       "   'in',\n",
       "   'service',\n",
       "   'deal',\n",
       "   'with',\n",
       "   'the',\n",
       "   'distribution',\n",
       "   'and',\n",
       "   'storage',\n",
       "   'of',\n",
       "   'audiovisual',\n",
       "   'content',\n",
       "   'become',\n",
       "   'essential',\n",
       "   'for',\n",
       "   'the',\n",
       "   'operation',\n",
       "   'of',\n",
       "   'company',\n",
       "   'like',\n",
       "   'Netflix',\n",
       "   'TikTok',\n",
       "   'and',\n",
       "   'YouTube',\n",
       "   'due',\n",
       "   'to',\n",
       "   'the',\n",
       "   'high',\n",
       "   'demand',\n",
       "   'for',\n",
       "   'this',\n",
       "   'type',\n",
       "   'of',\n",
       "   'service',\n",
       "   'digital',\n",
       "   'video',\n",
       "   'represent',\n",
       "   'the',\n",
       "   'high',\n",
       "   'volume',\n",
       "   'of',\n",
       "   'datum',\n",
       "   'transmit',\n",
       "   'over',\n",
       "   'the',\n",
       "   'internet',\n",
       "   'in',\n",
       "   'recent',\n",
       "   'year',\n",
       "   'it',\n",
       "   'have',\n",
       "   'be',\n",
       "   'predict',\n",
       "   'that',\n",
       "   '4',\n",
       "   'k',\n",
       "   'video',\n",
       "   'represent',\n",
       "   '66',\n",
       "   'of',\n",
       "   'internet',\n",
       "   'consumption',\n",
       "   'on',\n",
       "   'television',\n",
       "   'device',\n",
       "   'by',\n",
       "   'the',\n",
       "   'end',\n",
       "   'of',\n",
       "   '2023',\n",
       "   'surpass',\n",
       "   'the',\n",
       "   '2018',\n",
       "   'estimate',\n",
       "   '8',\n",
       "   'consequently',\n",
       "   'research',\n",
       "   'effort',\n",
       "   'by',\n",
       "   'both',\n",
       "   'academia',\n",
       "   'and',\n",
       "   'industry',\n",
       "   'be',\n",
       "   'dedicate',\n",
       "   'to',\n",
       "   'improve',\n",
       "   'not',\n",
       "   'only',\n",
       "   'compression',\n",
       "   'efficiency',\n",
       "   'but',\n",
       "   'also',\n",
       "   'reduce',\n",
       "   'undesired',\n",
       "   'visual',\n",
       "   'in',\n",
       "   'proceeding',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'web',\n",
       "   'WebMe',\n",
       "   'dia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   '2024',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'sbc',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '2753',\n",
       "   'Guilherme',\n",
       "   'Correa',\n",
       "   'gcorrea@inf.ufpel.edu.br',\n",
       "   'ViTech',\n",
       "   'PPGC',\n",
       "   'UFPel',\n",
       "   'Pelotas',\n",
       "   'Brazil',\n",
       "   'effect',\n",
       "   'cause',\n",
       "   'by',\n",
       "   'this',\n",
       "   'process',\n",
       "   'as',\n",
       "   'video',\n",
       "   'content',\n",
       "   'continue',\n",
       "   'to',\n",
       "   'proliferate',\n",
       "   'across',\n",
       "   'various',\n",
       "   'platform',\n",
       "   'and',\n",
       "   'device',\n",
       "   'maintain',\n",
       "   'high',\n",
       "   'visual',\n",
       "   'quality',\n",
       "   'while',\n",
       "   'efficiently',\n",
       "   'manage',\n",
       "   'datum',\n",
       "   'transmission',\n",
       "   'and',\n",
       "   'storage',\n",
       "   'become',\n",
       "   'increasingly',\n",
       "   'paramount',\n",
       "   'compressed',\n",
       "   'video',\n",
       "   'suffer',\n",
       "   'from',\n",
       "   'visual',\n",
       "   'effect',\n",
       "   'such',\n",
       "   'as',\n",
       "   'block',\n",
       "   'ring',\n",
       "   'and',\n",
       "   'blur',\n",
       "   'artifact',\n",
       "   '12',\n",
       "   'which',\n",
       "   'compromise',\n",
       "   'the',\n",
       "   'perceive',\n",
       "   'video',\n",
       "   'quality',\n",
       "   'for',\n",
       "   'user',\n",
       "   'in',\n",
       "   'Figure',\n",
       "   '1',\n",
       "   'the',\n",
       "   'visual',\n",
       "   'effect',\n",
       "   'of',\n",
       "   'these',\n",
       "   'artifact',\n",
       "   'can',\n",
       "   'be',\n",
       "   'observe',\n",
       "   'the',\n",
       "   'patch',\n",
       "   'c',\n",
       "   'd',\n",
       "   'and',\n",
       "   'e',\n",
       "   'be',\n",
       "   'separate',\n",
       "   'by',\n",
       "   'the',\n",
       "   'artifact',\n",
       "   'that',\n",
       "   'predominantly',\n",
       "   'affect',\n",
       "   'the',\n",
       "   'select',\n",
       "   'part',\n",
       "   'of',\n",
       "   'the',\n",
       "   'image',\n",
       "   'generally',\n",
       "   'artifact',\n",
       "   'can',\n",
       "   'blend',\n",
       "   'with',\n",
       "   'or',\n",
       "   'appear',\n",
       "   'next',\n",
       "   'to',\n",
       "   'other',\n",
       "   'make',\n",
       "   'it',\n",
       "   'difficult',\n",
       "   'to',\n",
       "   'generate',\n",
       "   'a',\n",
       "   'patch',\n",
       "   'that',\n",
       "   'isolate',\n",
       "   'only',\n",
       "   'one',\n",
       "   'compression',\n",
       "   'artifact',\n",
       "   'in',\n",
       "   'c',\n",
       "   'the',\n",
       "   'division',\n",
       "   'between',\n",
       "   'block',\n",
       "   'use',\n",
       "   'in',\n",
       "   'the',\n",
       "   'compression',\n",
       "   'process',\n",
       "   'be',\n",
       "   'perceptible',\n",
       "   'in',\n",
       "   'the',\n",
       "   'middle',\n",
       "   'of',\n",
       "   'the',\n",
       "   'blue',\n",
       "   'part',\n",
       "   'of',\n",
       "   'the',\n",
       "   'ball',\n",
       "   'evidence',\n",
       "   'a',\n",
       "   'block',\n",
       "   'effect',\n",
       "   'in',\n",
       "   'd',\n",
       "   'we',\n",
       "   'can',\n",
       "   'see',\n",
       "   'the',\n",
       "   'ringing',\n",
       "   'effect',\n",
       "   'noticeable',\n",
       "   'as',\n",
       "   'wave',\n",
       "   'like',\n",
       "   'pattern',\n",
       "   'along',\n",
       "   'the',\n",
       "   'edge',\n",
       "   'of',\n",
       "   'the',\n",
       "   'orange',\n",
       "   'rim',\n",
       "   'in',\n",
       "   'e',\n",
       "   'detail',\n",
       "   'from',\n",
       "   'the',\n",
       "   'original',\n",
       "   'image',\n",
       "   'such',\n",
       "   'as',\n",
       "   'the',\n",
       "   'nail',\n",
       "   'on',\n",
       "   'the',\n",
       "   'ground',\n",
       "   'and',\n",
       "   'the',\n",
       "   'separation',\n",
       "   'in',\n",
       "   'the',\n",
       "   'wooden',\n",
       "   'floor',\n",
       "   'be',\n",
       "   'lose',\n",
       "   'during',\n",
       "   'compression',\n",
       "   'lead',\n",
       "   'to',\n",
       "   'a',\n",
       "   'blur',\n",
       "   'effect',\n",
       "   'filtering',\n",
       "   'algorithm',\n",
       "   'like',\n",
       "   'the',\n",
       "   'Deblocking',\n",
       "   'Filter',\n",
       "   'DF',\n",
       "   '24',\n",
       "   'address',\n",
       "   'block',\n",
       "   'effect',\n",
       "   'Adaptive',\n",
       "   'Loop',\n",
       "   'Filter',\n",
       "   'ALF',\n",
       "   '29',\n",
       "   'that',\n",
       "   'minimize',\n",
       "   'the',\n",
       "   'distortion',\n",
       "   'between',\n",
       "   'the',\n",
       "   'original',\n",
       "   'and',\n",
       "   'decoded',\n",
       "   'sample',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Sample',\n",
       "   'Adaptive',\n",
       "   'Offset',\n",
       "   'SAO',\n",
       "   '13',\n",
       "   'focus',\n",
       "   'on',\n",
       "   'reduce',\n",
       "   'banding',\n",
       "   'effect',\n",
       "   'be',\n",
       "   'standardize',\n",
       "   'process',\n",
       "   'in',\n",
       "   'format',\n",
       "   'such',\n",
       "   'as',\n",
       "   'High',\n",
       "   'Efficiency',\n",
       "   'Video',\n",
       "   'Coding',\n",
       "   'HEVC',\n",
       "   'and',\n",
       "   'Versatile',\n",
       "   'Video',\n",
       "   'Coding',\n",
       "   'VVC',\n",
       "   'both',\n",
       "   'DF',\n",
       "   'and',\n",
       "   'SAO',\n",
       "   'be',\n",
       "   'heuristic',\n",
       "   'base',\n",
       "   'method',\n",
       "   'devise',\n",
       "   'base',\n",
       "   'upon',\n",
       "   'statistical',\n",
       "   'observation',\n",
       "   'for',\n",
       "   'reduce',\n",
       "   'compression',\n",
       "   'artifact',\n",
       "   'these',\n",
       "   'model',\n",
       "   'be',\n",
       "   'apply',\n",
       "   'as',\n",
       "   'filter',\n",
       "   'that',\n",
       "   'traverse',\n",
       "   'all',\n",
       "   'pixel',\n",
       "   'in',\n",
       "   'each',\n",
       "   'frame',\n",
       "   'aim',\n",
       "   'to',\n",
       "   'enhance',\n",
       "   'visual',\n",
       "   'quality',\n",
       "   'while',\n",
       "   'these',\n",
       "   'heuristicbase',\n",
       "   'approach',\n",
       "   'have',\n",
       "   'demonstrate',\n",
       "   'effectiveness',\n",
       "   'in',\n",
       "   'mitigate',\n",
       "   'certain',\n",
       "   'artifact',\n",
       "   'they',\n",
       "   'have',\n",
       "   'inherent',\n",
       "   'limitation',\n",
       "   'for',\n",
       "   'example',\n",
       "   'DF',\n",
       "   'may',\n",
       "   'introduce',\n",
       "   'a',\n",
       "   'blur',\n",
       "   'effect',\n",
       "   'on',\n",
       "   'the',\n",
       "   'image',\n",
       "   'as',\n",
       "   'it',\n",
       "   'attempt',\n",
       "   'to',\n",
       "   'reduce',\n",
       "   'blockiness',\n",
       "   'potentially',\n",
       "   'sacrifice',\n",
       "   'fine',\n",
       "   'detail',\n",
       "   'and',\n",
       "   'sharpness',\n",
       "   'additionally',\n",
       "   'both',\n",
       "   'DF',\n",
       "   'and',\n",
       "   'SAO',\n",
       "   'may',\n",
       "   'inadvertently',\n",
       "   'amplify',\n",
       "   'the',\n",
       "   'presence',\n",
       "   'of',\n",
       "   'other',\n",
       "   'compression',\n",
       "   'artifact',\n",
       "   'such',\n",
       "   'as',\n",
       "   'ring',\n",
       "   'or',\n",
       "   'mosquito',\n",
       "   'noise',\n",
       "   'particularly',\n",
       "   'in',\n",
       "   'region',\n",
       "   'with',\n",
       "   'high',\n",
       "   'contrast',\n",
       "   'or',\n",
       "   'intricate',\n",
       "   'texture',\n",
       "   '17',\n",
       "   'despite',\n",
       "   'that',\n",
       "   'heuristic',\n",
       "   'base',\n",
       "   'method',\n",
       "   'remain',\n",
       "   'valuable',\n",
       "   'tool',\n",
       "   'of',\n",
       "   'video',\n",
       "   'compression',\n",
       "   'technique',\n",
       "   'especially',\n",
       "   'when',\n",
       "   'use',\n",
       "   'in',\n",
       "   'conjunction',\n",
       "   'with',\n",
       "   'more',\n",
       "   'sophisticated',\n",
       "   'algorithm',\n",
       "   'and',\n",
       "   'Deep',\n",
       "   'Neural',\n",
       "   'Networks',\n",
       "   'DNN',\n",
       "   'to',\n",
       "   'achieve',\n",
       "   'comprehensive',\n",
       "   'artifact',\n",
       "   'reduction',\n",
       "   'and',\n",
       "   'enhance',\n",
       "   'overall',\n",
       "   'visual',\n",
       "   'quality',\n",
       "   'currently',\n",
       "   'a',\n",
       "   'significant',\n",
       "   'amount',\n",
       "   'of',\n",
       "   'study',\n",
       "   'explore',\n",
       "   'the',\n",
       "   'Video',\n",
       "   'Quality',\n",
       "   'Enhancement',\n",
       "   'VQE',\n",
       "   'problem',\n",
       "   'employ',\n",
       "   'DNN',\n",
       "   'model',\n",
       "   'base',\n",
       "   'on',\n",
       "   '223',\n",
       "   'WebMedia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Silveira',\n",
       "   'Júnior',\n",
       "   'et',\n",
       "   'al',\n",
       "   'figure',\n",
       "   '1',\n",
       "   'compression',\n",
       "   'artifact',\n",
       "   'a',\n",
       "   'original',\n",
       "   'frame',\n",
       "   'RAW',\n",
       "   'b',\n",
       "   'Compressed',\n",
       "   'Frame',\n",
       "   'c',\n",
       "   'Blocking',\n",
       "   'Artifact',\n",
       "   'd',\n",
       "   'Ringing',\n",
       "   'Artifact',\n",
       "   'e',\n",
       "   'blur',\n",
       "   'Artifact',\n",
       "   'Convolutional',\n",
       "   'Neural',\n",
       "   'Networks',\n",
       "   'CNN',\n",
       "   '9',\n",
       "   '27',\n",
       "   'cnn',\n",
       "   'have',\n",
       "   'emerge',\n",
       "   'as',\n",
       "   'a',\n",
       "   'powerful',\n",
       "   'tool',\n",
       "   'in',\n",
       "   'image',\n",
       "   'and',\n",
       "   'video',\n",
       "   'processing',\n",
       "   'task',\n",
       "   'due',\n",
       "   'to',\n",
       "   'their',\n",
       "   'ability',\n",
       "   'to',\n",
       "   'automatically',\n",
       "   'learn',\n",
       "   'hierarchical',\n",
       "   'feature',\n",
       "   'from',\n",
       "   'datum',\n",
       "   'unlike',\n",
       "   'traditional',\n",
       "   'image',\n",
       "   'processing',\n",
       "   'technique',\n",
       "   'that',\n",
       "   'operate',\n",
       "   'on',\n",
       "   'individual',\n",
       "   'pixel',\n",
       "   'cnn',\n",
       "   'operate',\n",
       "   'by',\n",
       "   'convolve',\n",
       "   'learn',\n",
       "   'filter',\n",
       "   'across',\n",
       "   'the',\n",
       "   'input',\n",
       "   'image',\n",
       "   'enable',\n",
       "   'they',\n",
       "   'to',\n",
       "   'capture',\n",
       "   'spatial',\n",
       "   'hierarchy',\n",
       "   'and',\n",
       "   'dependency',\n",
       "   'by',\n",
       "   'process',\n",
       "   'local',\n",
       "   'image',\n",
       "   'patch',\n",
       "   'and',\n",
       "   'learn',\n",
       "   'from',\n",
       "   'their',\n",
       "   'correlation',\n",
       "   'cnn',\n",
       "   'can',\n",
       "   'effectively',\n",
       "   'extract',\n",
       "   'meaningful',\n",
       "   'feature',\n",
       "   'that',\n",
       "   'represent',\n",
       "   'various',\n",
       "   'visual',\n",
       "   'pattern',\n",
       "   'such',\n",
       "   'as',\n",
       "   'edge',\n",
       "   'texture',\n",
       "   'and',\n",
       "   'object',\n",
       "   'shape',\n",
       "   'this',\n",
       "   'enable',\n",
       "   'they',\n",
       "   'to',\n",
       "   'understand',\n",
       "   'the',\n",
       "   'contextual',\n",
       "   'connection',\n",
       "   'between',\n",
       "   'neighboring',\n",
       "   'pixel',\n",
       "   'and',\n",
       "   'learn',\n",
       "   'complex',\n",
       "   'pattern',\n",
       "   'in',\n",
       "   'the',\n",
       "   'datum',\n",
       "   '18',\n",
       "   'therefore',\n",
       "   'CNN',\n",
       "   'base',\n",
       "   'model',\n",
       "   'be',\n",
       "   'well',\n",
       "   'suit',\n",
       "   'for',\n",
       "   'VQE',\n",
       "   'task',\n",
       "   'as',\n",
       "   'they',\n",
       "   'can',\n",
       "   'identify',\n",
       "   'and',\n",
       "   'address',\n",
       "   'overall',\n",
       "   'image',\n",
       "   'quality',\n",
       "   'degradation',\n",
       "   'cause',\n",
       "   'by',\n",
       "   'compression',\n",
       "   'artifact',\n",
       "   'rather',\n",
       "   'than',\n",
       "   'focus',\n",
       "   'solely',\n",
       "   'on',\n",
       "   'specific',\n",
       "   'type',\n",
       "   'of',\n",
       "   'artifact',\n",
       "   'it',\n",
       "   'have',\n",
       "   'be',\n",
       "   'observe',\n",
       "   'that',\n",
       "   'many',\n",
       "   'dnn',\n",
       "   'model',\n",
       "   'for',\n",
       "   'VQE',\n",
       "   'be',\n",
       "   'test',\n",
       "   'use',\n",
       "   'video',\n",
       "   'compress',\n",
       "   'with',\n",
       "   'the',\n",
       "   'same',\n",
       "   'codec',\n",
       "   'and',\n",
       "   'configuration',\n",
       "   'as',\n",
       "   'the',\n",
       "   'training',\n",
       "   'video',\n",
       "   'the',\n",
       "   'author',\n",
       "   'in',\n",
       "   '15',\n",
       "   'show',\n",
       "   'that',\n",
       "   'the',\n",
       "   'VQE',\n",
       "   'model',\n",
       "   'tend',\n",
       "   'to',\n",
       "   'produce',\n",
       "   'well',\n",
       "   'result',\n",
       "   'for',\n",
       "   'video',\n",
       "   'compress',\n",
       "   'with',\n",
       "   'the',\n",
       "   'same',\n",
       "   'codec',\n",
       "   'and',\n",
       "   'quantization',\n",
       "   'parameter',\n",
       "   'as',\n",
       "   'those',\n",
       "   'use',\n",
       "   'for',\n",
       "   'training',\n",
       "   'oppositely',\n",
       "   'for',\n",
       "   'video',\n",
       "   'compress',\n",
       "   'with',\n",
       "   'different',\n",
       "   'codec',\n",
       "   'and',\n",
       "   'configuration',\n",
       "   'the',\n",
       "   'VQE',\n",
       "   'model',\n",
       "   'lead',\n",
       "   'to',\n",
       "   'little',\n",
       "   'improvement',\n",
       "   ...]},\n",
       " {'titulo': 'Tagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies',\n",
       "  'informacoes_url': '',\n",
       "  'idioma': 'english',\n",
       "  'storage_key': '../articles/original/english/985-24767-1-10-20240923.pdf',\n",
       "  'author': 'Daniel de S. Moraes; Polyana B. da Costa; Pedro T. C. Santos; Ivan de J. P. Pinto; Sérgio Colcher; Antonio J. G. Busson; Matheus A. S. Pinto; Rafael H. Rocha; Rennan Gaio; Gabriela Tourinho; Marcos Rabaioli; and David Favaro',\n",
       "  'data_publicacao': '11-09-2024',\n",
       "  'resumo': 'assessing the quality of the taxonomies and the tags assigned to merchants. The evaluation revealed a coherence rate exceeding 90% for the chosen taxonomies. Additionally, taxonomy expansion using LLMs demonstrated promising results for parent node prediction, with F1-scores of 89% and 70% for *Food* and *Shopping* taxonomies, respectively. ###',\n",
       "  'keywords': 'Large Language Models, Natural Language Processing, Web Scrapping, Topic Modeling',\n",
       "  'referencias': ['[1] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,\\nRadu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al . 2023. Gemini:\\na family of highly capable multimodal models.',\n",
       "   '[2] David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation.\\n*Journal of machine Learning research* 3, Jan (2003), 993–1022.',\n",
       "   '[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al . 2020. Language models are few-shot learners. *Advances in neural*\\n*information processing systems* 33 (2020), 1877–1901.',\n",
       "   '[4] Antonio J. G. Busson, Rafael Rocha, Rennan Gaio, Rafael Miceli, Ivan Pereira,\\nDaniel de S. Moraes, Sérgio Colcher, Alvaro Veiga, Bruno Rizzi, Francisco Evangelista, Leandro Santos, Fellipe Marques, Marcos Rabaioli, Diego Feldberg, Debora\\nMattos, João Pasqua, and Diogo Dias. 2023. Hierarchical Classification of Financial Transactions Through Context-Fusion of Transformer-based Embeddings\\nand Taxonomy-aware Attention Layer. In *Anais do II Brazilian Workshop on Arti-*\\n*ficial Intelligence in Finance (BWAIF 2023) (BWAIF 2023)* . Sociedade Brasileira de\\nComputação. https://doi.org/10.5753/bwaif.2023.229322',\n",
       "   '[5] Ricardo Campos, Vítor Mangaravite, Arian Pasquali, Alípio Jorge, Célia Nunes,\\nand Adam Jatowt. 2020. YAKE! Keyword extraction from single documents using\\nmultiple local features. *Information Sciences* 509 (2020), 257–289.',\n",
       "   '[6] Boqi Chen, Fandi Yi, and Dániel Varró. 2023. Prompting or Fine-tuning? A\\nComparative Study of Large Language Models for Taxonomy Construction. In\\n*2023 ACM/IEEE International Conference on Model Driven Engineering Languages*\\n*and Systems Companion (MODELS-C)* . IEEE, 588–596.',\n",
       "   '[7] Sabit Ekin. 2023. Prompt engineering for ChatGPT: a quick guide to techniques,\\ntips, and best practices. *Authorea Preprints* (2023).',\n",
       "   '[8] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\\nWang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large\\nlanguage models. *arXiv preprint arXiv:2106.09685* (2021).',\n",
       "   '[9] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche\\nSavary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou\\nHanna, Florian Bressand, et al . 2024. Mixtral of experts. *arXiv preprint*\\n*arXiv:2401.04088* (2024).',\n",
       "   '[10] Dongha Lee, Jiaming Shen, SeongKu Kang, Susik Yoon, Jiawei Han, and Hwanjo\\nYu. 2022. TaxoCom: Topic Taxonomy Completion with Hierarchical Discovery of\\nNovel Topic Clusters. In *Proceedings of the ACM Web Conference 2022* . 2819–2829.',\n",
       "   '[11] Yinheng Li. 2023. A Practical Survey on Zero-Shot Prompt Design for In-Context\\nLearning. In *Proceedings of the 14th International Conference on Recent Advances*\\n*in Natural Language Processing* . 641–647.',\n",
       "   '[12] Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun\\nMa, and Zhoujun Li. 2023. Unleashing Infinite-Length Input Capacity for Largescale Language Models with Self-Controlled Memory System. *arXiv preprint*\\n*arXiv:2304.13343* (2023).',\n",
       "   '[13] Irina Nikishina, Varvara Logacheva, Alexander Panchenko, and Natalia\\nLoukachevitch. 2020. RUSSE’2020: Findings of the First Taxonomy Enrichment\\n\\n\\n273\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Moraes et al.\\n\\n\\nTask for the Russian language. *arXiv preprint arXiv:2005.11176* (2020).',\n",
       "   '[14] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]',\n",
       "   '[15] Octavian Popescu and Carlo Strapparava. 2015. Semeval 2015, task 7: Diachronic\\ntext evaluation. In *Proceedings of the 9th International Workshop on Semantic*\\n*Evaluation (SemEval 2015)* . 870–878.',\n",
       "   '[16] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of\\ntransfer learning with a unified text-to-text transformer. *The Journal of Machine*\\n*Learning Research* 21, 1 (2020), 5485–5551.',\n",
       "   '[17] Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language\\nmodels: Beyond the few-shot paradigm. In *Extended abstracts of the 2021 CHI*\\n*conference on human factors in computing systems* . 1–7.',\n",
       "   '[18] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal,\\nand Aman Chadha. 2024. A systematic survey of prompt engineering in large\\nlanguage models: Techniques and applications. *arXiv preprint arXiv:2402.07927*\\n(2024).',\n",
       "   '[19] Rion Snow, Daniel Jurafsky, and Andrew Ng. 2004. Learning syntactic patterns\\nfor automatic hypernym discovery. *Advances in neural information processing*\\n*systems* 17 (2004).',\n",
       "   '[20] Kunihiro Takeoka, Kosuke Akimoto, and Masafumi Oyamada. 2021. Low-resource\\ntaxonomy enrichment with pretrained language models. In *Proceedings of the*\\n*2021 Conference on Empirical Methods in Natural Language Processing* . 2747–2758.',\n",
       "   '[21] Adrian Tam. [n. d.]. What Are Zero-Shot Prompting and Few-Shot Prompting. https://machinelearningmastery.com/what-are-zero-shot-prompting-andfew-shot-prompting/. Accessed: 2024-07-02.',\n",
       "   '[22] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, et al . 2023. Llama: Open and efficient foundation language models. *arXiv*\\n*preprint arXiv:2302.13971* (2023).',\n",
       "   '[23] Erlend Vollset, Eirik Folkestad, Marius Rise Gallala, and Jon Atle Gulla. 2017.\\nMaking use of external company data to improve the classification of bank transactions. In *Advanced Data Mining and Applications: 13th International Conference,*\\n*ADMA 2017, Singapore, November 5–6, 2017, Proceedings 13* . Springer, 767–780.',\n",
       "   '[24] Hanna Wallach, David Mimno, and Andrew McCallum. 2009. Rethinking LDA:\\nWhy priors matter. *Advances in neural information processing systems* 22 (2009).',\n",
       "   '[25] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang,\\nAakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain\\nof thought reasoning in language models. *arXiv preprint arXiv:2203.11171* (2022).',\n",
       "   '[26] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei\\nXia, H Chi, Quoc V Le, and Denny Zhou. [n. d.]. Chain-of-Thought Prompting\\nElicits Reasoning in Large Language Models. ([n. d.]).',\n",
       "   '[27] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,\\nand Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting in Language\\nModels. In *International Conference on Learning Representations (ICLR)* .',\n",
       "   '[28] Chao Zhang, Fangbo Tao, Xiusi Chen, Jiaming Shen, Meng Jiang, Brian Sadler,\\nMichelle Vanni, and Jiawei Han. 2018. TaxoGen: Unsupervised Topic Taxonomy\\nConstruction by Adaptive Term Embedding and Clustering. In *Proceedings of the*\\n*24th ACM SIGKDD International Conference on Knowledge Discovery & Data Min-*\\n*ing* (London, United Kingdom) *(KDD ’18)* . Association for Computing Machinery,\\nNew York, NY, USA, 2701–2709. https://doi.org/10.1145/3219819.3220064\\n\\n\\n274\\n\\n\\n-----'],\n",
       "  'text': '# **Tagging Enriched Bank Transactions Using LLM-Generated Topic** **Taxonomies**\\n\\n## Daniel de S. Moraes\\n#### TeleMídia Lab - PUC-Rio danielmoraes@telemidia.puc-rio.br\\n## Ivan de J. P. Pinto\\n#### TeleMídia Lab - PUC-Rio ivan@telemidia.puc-rio.br\\n## Matheus A. S. Pinto\\n#### BTG Pactual matheus.adler@btgpactual.com\\n## Gabriela Tourinho\\n#### BTG Pactual gabriela.tourinho@btgpactual.com\\n\\n## Polyana B. da Costa\\n#### TeleMídia Lab - PUC-Rio polyana@telemidia.puc-rio.br\\n## Sérgio Colcher\\n#### TeleMídia Lab - PUC-Rio colcher@inf.puc-rio.br\\n## Rafael H. Rocha\\n#### BTG Pactual rafael-h.rocha@btgpactual.com\\n## Marcos Rabaioli\\n#### BTG Pactual marcos.rabaioli@btgpactual.com\\n\\n## Pedro T. C. Santos\\n#### TeleMídia Lab - PUC-Rio thiagocutrim@telemidia.puc-rio.br\\n## Antonio J. G. Busson\\n#### BTG Pactual antonio.busson@btgpactual.com\\n## Rennan Gaio\\n#### BTG Pactual rennan.gaio@btgpactual.com\\n## David Favaro\\n#### BTG Pactual david.favaro@btgpactual.com\\n\\n**Taxonomies**\\n\\n\\n**Transaction: Purchase**\\n\\n**at PAG*distr_nona on**\\n**19/02/2008 at 19:20:05**\\n\\n**Italian Cuisine**\\n\\n**Pasta**\\n\\n**Sauce**\\n\\n**Pizza**\\n\\n**Wine**\\n\\n\\n\\n\\n\\n|name|macrocategory|microcategory|\\n|---|---|---|\\n|Nona Ristorante|Food|Restaurant|\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n**Figure 1: Overview of our method. (Note: Merchant and transaction data are fabricated for demonstration only).**\\n\\n### **ABSTRACT**\\n\\nThis work presents an unsupervised method for tagging banking\\nconsumers’ transactions using automatically constructed and expanded topic taxonomies. Initially, we enrich the bank transactions\\nvia web scraping to collect relevant descriptions, which are then\\npreprocessed using NLP techniques to generate candidate terms.\\nTopic taxonomies are created using instruction-based fine-tuned\\nLLMs (Large Language Models). To expand existing taxonomies\\nwith new terms, we use zero-shot prompting to determine where\\nto add new nodes. The resulting taxonomies are used to assign\\ndescriptive tags that characterize the transactions in the retail bank\\ndataset. For evaluation, 12 volunteers completed a two-part form\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n© 2024 SBC – Brazilian Computing Society.\\nISSN 2966-2753\\n\\n\\nassessing the quality of the taxonomies and the tags assigned to\\nmerchants. The evaluation revealed a coherence rate exceeding 90%\\nfor the chosen taxonomies. Additionally, taxonomy expansion using\\nLLMs demonstrated promising results for parent node prediction,\\nwith F1-scores of 89% and 70% for *Food* and *Shopping* taxonomies,\\nrespectively.\\n### **KEYWORDS**\\n\\nLarge Language Models, Natural Language Processing, Web Scrapping, Topic Modeling\\n### **1 INTRODUCTION**\\n\\nMany recent studies have focused on the application of Machine\\nLearning-based methods for the classification and characterization of financial transactions. For example, Vollset et al . [23] and\\nBusson et al . [4] explored an approach to hierarchically classifying\\n\\n\\n267\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Moraes et al.\\n\\n\\nfinancial transactions, employing a predefined set of categories/subcategories that describe purchase types and transactions. However,\\nthese methods apply a limited, predefined set of static classes, which\\nrestricts the ability to extend classifications based on user experiences when encountering new, previously undefined categories.\\nIn this context, to expand the possible set of classes/tags to label\\na transaction, we developed an unsupervised method based on\\n*topic taxonomies* . Taxonomies are very useful in the structural and\\nsemantic analyses of topics and textual data. However, creating and\\nmaintaining them is often costly and challenging to scale manually.\\nTherefore, recent works have tackled the automatic creation and\\nexpansion of *topic taxonomies*, in which each node in a hierarchy\\nrepresents a conceptual topic composed of semantically coherent\\n\\nterms.\\n\\nWe present an unsupervised method for automatically constructing and expanding topic taxonomies with instruction-based LLMs\\n(Large Language Models), in a Zero-Shot manner. Candidate terms\\nfor the initial version of the taxonomy are obtained using topic\\nmodeling and keyword extraction techniques. Then we apply LLMs\\nto post-process the resulting terms, create a hierarchy, and add new\\nterms to an existing taxonomy. Since the taxonomies are derived\\nfrom a corpus of unstructured texts describing niches of consuming\\nhabits, we opted to investigate the use of LLMs in our approach.\\nLLMs are often pre-trained on a large corpus of text, allowing them\\nto learn contextual representations that capture the intricacies of\\nhuman language.\\nWe applied our method to a private dataset of transactions of\\na retail bank, enriched with scraped data from food and shopping\\ncompanies, and evaluated the resulting taxonomies quantitatively.\\nThe generated tags of our topic taxonomies are then assigned to the\\nbank transactions characterizing the companies in each transaction,\\nas shown on Figure 1. In total, 58 topic taxonomies were created\\nfor the *Food* category and 6 for the *Shopping* category.\\nA two-step quantitative evaluation was conducted on a subset of the taxonomies. For this evaluation, we selected the topic\\ntaxonomies with the highest number of terms in each category:\\n\"Brazilian Cuisine\" from *Food* and \"Clothing and Accessories\" from\\n*Shopping* . Taxonomies with more terms are most likely to result\\nin a deeper hierarchy, which gives more data for evaluation. We\\nasked 12 volunteers to answer a two-part form, which assessed\\nthe quality of the created taxonomies and the quality of the tags\\nassigned to label transactions. The evaluation showed an average\\ncoherence of tags to transactions above 90%.\\nAs more scraped data from food and shopping companies are\\nadded to the retail bank’s dataset, the topic taxonomies will need\\nto be updated to include new terms. We used LLMs for this task as\\nwell, employing commercial LLMs like Gemini Pro [ 1 ] and GPT4 [ 14 ], alongside open-source LLM options such as LLaMA-Alpaca\\n(7B) [ 22 ], Phi-2 [1], and Mixtral 8x7B [ 9 ]. We showcase their results in\\nboth taxonomy creation and expansion. For the expansion part, we\\nalso compared our method to existing ones (a BERT-based method\\nand Musubu[ 20 ]) on the SemEval dataset and our generated taxonomies as well. Gemini Pro achieved the best results, with F1scores of 89% and 70% for parent node prediction on the *Food* and\\n*Shopping* taxonomies, respectively.\\n\\n1 https://huggingface.co/microsoft/phi-2\\n\\n\\nThe remainder of the paper is structured as follows: Section 2,\\nreviews the related work, highlighting existing approaches. Section 3 provides the necessary background, laying the foundation\\nfor our methodologies and contextualizing our contributions. Section 4 details the dataset construction process, explaining how we\\nenriched and prepared the data for the taxonomies’ construction. In\\nSection 5, we describe the creation of the taxonomies, outlining the\\nmethods used to generate them. Section 6 discusses the expansion\\nof the taxonomies, demonstrating how they can be dynamically\\nextended to accommodate new categories. Section 7 focuses on the\\nevaluation of these taxonomies, presenting the metrics and results\\nthat validate their accuracy and also the quality of the tags assigned\\nto the transactions. Finally, Section 8 concludes the paper, summarizing our findings, discussing their implications, and suggesting\\ndirections for future research.\\n### **2 RELATED WORK**\\n\\nTaxonomies represent the structure behind a collection of documents, organizing the hierarchical relationships between terms in a\\ntree structure [ 13 ]. They play an essential part in the structural and\\nsemantic analysis of textual data, providing valuable content for\\nmany applications that involve information retrieval and filtering,\\nsuch as web searching, recommendation systems, classification,\\nand question answering.\\nSince creating and maintaining taxonomies is a costly task, often difficult to scale if done manually, methods that automatically\\nconstruct and update them are desirable. Early works on automatic\\ntaxonomy creation focused on building hypernym-hyponym taxonomies, where each pair of terms expresses an ‘is-a’ relationship\\n\\n[ 19 ]. More recent works have tackled the automatic creation of\\nother taxonomies, such as topic taxonomies. In a topic taxonomy,\\neach node represents a conceptual topic composed of semantically\\ncoherent terms.\\n\\nIn this context, Zhang et al . [28] developed TaxoGen, an unsupervised method for constructing topic taxonomies. Taxogen uses\\nthe SkipGram model from an input text corpus to embed all the\\nconcept terms into a latent space that captures their semantics. In\\nthis space, the authors applied a clustering method to construct a\\nhierarchy recursively based on a variation of the spherical K-means\\nalgorithm.\\nAnother work that focuses on topic taxonomies is TaxoCom\\n\\n[ 10 ], a framework for automatic taxonomy expansion. TaxoCom is\\na hierarchical topic discovery framework that recursively expands\\nan initial taxonomy by discovering new sub-topics. It uses locally\\ndiscriminative embeddings and adaptive clustering, resulting in\\na low-dimensional embedding space that effectively encodes the\\ntextual similarity between terms. One main disadvantage of TaxoCom is that it requires a large set of quality phrases in the target\\nlanguage, and curating these phrases can be costly. The quality of\\nthe output taxonomy is highly dependent on those phrases.\\nRegarding the automatic expansion of taxonomies, an important related example is Musubu [ 20 ], a framework for low-resource\\ntaxonomy enrichment that uses a Language Model (LM) as a knowledge base to infer term relationships. For the taxonomy expansion\\npart of out method, we used Musubu as a baseline for comparison.\\n\\n\\n268\\n\\n\\n-----\\n\\nTagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\nAs to using Large Language Models for taxonomy tasks, Chen\\net al . [6] investigated how LLMs, like GPT-3, perform in taxonomy\\nconstruction tasks. The authors compared two approaches: finetuning, which involves training the LLM on a specific dataset to\\nadapt it for taxonomy tasks, and prompt techniques, where the\\nLLM receives instructions and examples to perform a task without\\nbeing explicitly trained for it. Their findings showed that prompt\\ntechniques such as few-shot learning generally outperform finetuning, particularly with smaller datasets. Based on these findings,\\nwe applied prompting techniques, specifically zero-shot prompting,\\nacross various LLMs to assess their effectiveness in constructing and\\nexpanding taxonomies. Section 7 shows the results of our approach,\\nas well as the results of applying Musubu[20] as baseline.\\n### **3 BACKGROUND**\\n\\nIn this section, we provide a comprehensive background on Large\\nLanguage Models (LLMs), and the concept of Prompt-tuning. These\\nconcepts are essential to understanding the construction and editing\\nof taxonomies utilizing LLMs.\\n### **3.1 Large Language Models**\\n\\nLately, Large Language Models (LLMs) have garnered significant\\nattention for their exceptional performance in various NLP tasks.\\nLLMs, such as GPT-3[ 3 ] and LLAMA[ 22 ], are characterized by their\\nmassive scale, comprising billions of parameters and being trained\\non vast amounts of data. These models are often pre-trained in\\nan unsupervised manner on large corpora of textual data, such as\\nbooks, articles, and web pages, allowing them to learn contextual\\nrepresentations that capture the intricacies of human language.\\nTo use LLMs for specific purposes, a highly effective approach is\\nto fine-tune them on task-specific data. Fine-tuning enables LLMs\\nto adapt to specific domains or tasks with minimal labeled data,\\nsignificantly reducing the need for large annotated datasets. However, in scenarios where labeled data is scarce or difficult to obtain,\\nLLMs can also be used without specific training or additional data,\\nin a Zero-Shot manner [ 21 ]. Given the scale of these models and\\nthe data they are trained on, LLMs embed vast knowledge that enables them to achieve high generalization capabilities and perform\\ntasks in diverse contexts, even without specific training for those\\ntasks[16].\\nIn our experiments, we tested several types of language models,\\nfrom private LLMs (GPT 4 [ 14 ], Gemini Pro [2] ), to open-source LLMs\\n(Llama 2 [ 22 ]), to a Mixture of Experts LLM (Mixtral [ 9 ], and a Small\\nLanguage Model (SLM), Phi 2 [3] .\\n### **3.2 Prompt Engineering**\\n\\nPrompt Engineering is a fundamental technique used to enhance\\nthe performance and adaptability of Large Language Models (LLMs)\\nin specific tasks or domains [ 7 ]. It involves optimizing and crafting\\nprompts to efficiently use language models (LMs) [ 3 ]. This approach\\nallows researchers and practitioners to tailor the behavior and output of LLMs, making them more suitable for targeted applications.\\n\\n2 https://deepmind.google/technologies/gemini/pro/\\n3 https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-ofsmall-language-models/\\n\\n\\nTechniques such as Zero-shot prompting, Few-shot prompting,\\nChain of Thought, ReAct, Self-Consistency etc. have been explored\\nto guide LLMs toward desired responses [ 18, 21, 25 – 27 ]. The effectiveness of prompt tuning has been demonstrated in various\\napplications, including question-answering, summarization, and\\ndialogue generation. The choice of prompt greatly influences the\\ngenerated output, and by carefully crafting prompts, researchers\\ncan guide the model’s responses toward desired behaviors. For example, in language translation, a prompt can specify the source\\nlanguage and desired target language to ensure accurate and fluent translations. In our method, we used the Zero-Shot prompting\\ntechnique.\\n### **3.3 Zero-Shot Prompting**\\n\\nSince LLMs (Large Language Models) are trained on vast amounts\\nof data, they can follow instructions and perform tasks in contexts\\nwhere they were not specifically trained, in a Zero-Shot (ZS) manner.\\nThis prompting style allows the model to adapt, making it versatile.\\nA Zero-Shot (ZS) prompt directly instructs the model to perform\\na task without additional examples or demonstrations to guide\\nthe LLM’s response, which is why they are also known as task\\ninstructions [21].\\nIn a study by Li [11], the authors highlighted several advantages\\nof using ZS prompts, such as the ability to craft highly interpretable\\nprompts, requiring fewer training data or examples, a more straightforward prompt design process, and a flexible prompt structure.\\nAdditionally, Reynolds and McDonell [17] noted that carefully engineered zero-shot prompts can outperform few-shot prompts in\\ncertain scenarios, as examples can sometimes be interpreted as part\\nof a narrative rather than as a guiding mechanism. This finding also\\ninfluenced our decision to use zero-shot prompting in our method.\\n### **4 DATASET CONSTRUCTION**\\n\\nThis work uses a proprietary dataset consisting of consumer transactions from a retail bank. Each transaction includes only a merchant\\nname indicating the business where that purchase occurred along\\nwith macro and micro categories as illustrated in Figure 1 The macro\\nand micro are originally assigned by [ 4 ] using the information from\\nthe business activities and products.\\nWe focus on two macro-categories from this dataset: *Food* and\\n*Shopping*, selecting the top 50,000 businesses with the highest number of transactions for each category.\\nWith the limited initial information, assigning detailed tags to\\ntransactions is challenging. To address this, we augment the dataset\\nthrough a data enrichment process involving web scraping. Using\\ntools such as Selenium [4] and Beautiful Soup [5], we gathered activity\\ndescriptions for companies in each macro category. For the *Food*\\nmacro category, the search was conducted on specialized platforms\\nfor restaurants and food delivery services. For the *Shopping* macro\\ncategory, we obtained establishment descriptions directly from\\ninternet indexing and search tools.\\nIn the context of enrichment for the *Food* macro category, web\\nscraping was conducted as follows: (1) the centers of all Brazilian\\nstate capitals and the Federal District were used as base locations\\n\\n4 https://www.selenium.dev/about/\\n5 https://readthedocs.org/projects/beautiful-soup-4/downloads/pdf/latest/\\n\\n\\n269\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Moraes et al.\\n\\n\\nfor restaurant searching; (2) for each location, restaurants listed on\\nthe first one hundred pages of the platform were extracted. After\\ncompleting these steps, the information was combined with the\\nmerchant database using the merchant’s name and micro categories.\\nFor the *Shopping* macro category, the description of each merchant was obtained using Google Search Engine [6], selecting descriptions from the first ten search results. The final description\\nconsists of a concatenation of all the obtained descriptions. The\\nsearch queries were constructed using the merchant names combined with their micro categories.\\n### **5 TAXONOMY CONSTRUCTION**\\n\\nTo automatically create topic taxonomies for *Food* and *Shopping*\\nbusinesses, we developed a 3-step method. First, we preprocess the\\ndescriptions in our enriched dataset to retain only the relevant parts\\nof the text. Next, we apply two techniques to select candidate terms\\nfor the topic taxonomies: keyword extraction and topic modeling.\\nIn the post-processing phase, we use large language models\\n(LLMs) to refine the results of each step, filtering out unrelated\\nterms. Finally, we use LLMs again to organize the final terms into\\nhierarchies, forming the topic taxonomies.\\n### **5.1 Preprocessing**\\n\\nWe applied a few NLP techniques to refine the businesses’ descriptions in our dataset. At first, we remove stop words to eliminate\\ncommonly used words that do not carry significant meaning in\\nour contexts. Then, to retain only the most relevant portions of the\\ndescriptions, we employ part-of-speech (POS) tagging to identify\\nand exclude words that belong to specific POS categories. The list\\nof POS tag categories that were removed includes ADV, CCONJ,\\nADP, AUX, CONJ, DET, INTJ, PART, PRON, PUNCT, SYM, SCONJ,\\nADJ, VERB, PROPN. [7]\\n\\nAfter this initial preprocessing step, we run the first iteration\\nof the candidate term selection part to build a filter of generic\\nwords, not to create topic taxonomies yet. For this step, we use the\\nentire corpus of descriptions for each macro category, resulting in\\ntwo corpora ( *Food* and *Shopping* ). For each micro category in the\\nmacro categories’ corpora, we use Keyword Extraction and Topic\\nModeling to gather candidate terms for the filter, combining the\\nresults of both techniques in a list. Then, We use an LLM to remove\\nthe terms it identifies as unrelated to the main topic (each micro\\ncategory) from the list. The prompt that we used for requesting this\\nseparation is illustrated below.\\n\\nprompt= \"Given the terms in the following list: \"+\\n<wordsList> +\". Separate them into two groups. In\\ngroup 1 the terms with no relation to the topic \"+\\n<type> +\". And in group 2 the terms that are related.\"\\n\\n**Listing 1: Prompt for separating candidate terms related to**\\n**the type of establishment**\\n\\nBy using this prompt, we try to ensure that the model’s response\\nis consistently formatted according to the pattern described in it,\\nfacilitating the processing of the resulting string, although, some\\n\\n6 https://www.google.com\\n7 https://spacy.io/usage/linguistic-features#pos-tagging\\n\\n\\nof the LLMs we tested did not output the response in the requested\\nformat. Once we complete one iteration of this method for each\\nmacro category in our dataset, we add the words of group 2 to the\\ncorresponding list of generic words. We apply the corresponding\\nfilter of generic words for each macro category corpus, resulting in\\nthe final preprocessed corpus.\\n### **5.2 Candidate Terms Selection**\\n\\nFor this part of our method, we use each preprocessed corpus\\nseparately. For the *Food* corpus, we group the descriptions based\\non their micro-categories, creating 58 sub-corpus specific to that\\ndomain. We have six micro categories for the *Shopping* corpus,\\nresulting in 6 specific sub-corpus. The candidate terms selection\\nmethods are applied to each sub-corpus, creating topic taxonomies\\nwhere the main topic is the micro category.\\n\\n*5.2.1* *Keyword Extraction.* The first approach to candidate term\\nselection was to use an unsupervised keyword selection method\\ncalled Yake! [ 5 ]. This method is based on statistical text features\\nextracted from single documents to select the most relevant keywords from that text. It does not require training on a document set\\nand is not dependent on dictionaries, text size, language, domain,\\nor external corpora.\\nYake! allows for the specification of parameters such as the\\nlanguage of the text, the maximum size of the n-grams being sought,\\nand others. In our method, we customized only the language to\\nPortuguese, and the maximum number of keywords sought for each\\nset of descriptions was 30 words.\\nAfter extracting the keywords from each group of descriptions,\\nwe obtained a total set of *𝑁* candidate terms. However, these terms\\nare further filtered using an LLM, where we ask it to separate the\\nterms related to the main topic from those unrelated, as explained\\nearlier in subsection 5.1.\\n\\n*5.2.2* *Topic Modeling.* Our second approach to collecting initial\\ntopics and candidate terms was Topic Modeling. We applied the\\nLatent Dirichlet Allocation algorithm [ 2 ], available at the Gensim\\nLibrary [8] .\\nWe construct a dictionary for each macro-category corpus in\\nour macro-categories corpora by extracting unique tokens and\\nbigrams. After a few empirical tests, we set the minimum frequency\\nof a bigram to 20 occurrences. Since some corpora have a minimal\\nnumber of tokens (the micro category \"Greek Cuisine\" from the *Food*\\nmacro category has only five stores marked as such, with a corpus\\nof only 127 tokens), we had to set a reasonably small number so that\\nsmaller corpora could also have a few bigrams. With the resulting\\ndictionary of tokens, the LDA algorithm was applied. Three main\\nparameters are to be defined in an LDA algorithm: number of topics,\\n*alpha*, and *beta* .\\nThe number of topics defines the latent topics to be extracted\\nfrom the corpus. The parameter *alpha* is *a priori* belief in documenttopic distribution, while *beta* is *a priori* belief in topic-word distribution.\\nTo define the number of topics for each micro category corpus,\\nwe tried numbers from 1 to 5, constantly checking which configuration would result in the best average topic coherence for that\\n\\n8 https://pypi.org/project/gensim/\\n\\n\\n270\\n\\n\\n-----\\n\\nTagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\ncorpus. Small corpora would have 1 or 2 topics, while bigger ones\\nwould have 5. To correctly define the *alpha* and *beta* priors, we\\nwould have to analyze the distribution for each category corpus\\n\\n[ 24 ]. Since this would be rather difficult, we set those priors to be\\nauto-defined by the LDA algorithm, which learns these parameters\\nbased on the corpus. We select the terms with the highest coherence\\nwith the resulting topics. Each topic returns 20 words with their\\ncoherence scores, but we do not use all of them as some have very\\nlow coherence. After testing a few configurations, for each topic,\\nwe select 60% of the terms with the highest coherence within that\\ntopic.\\nWith initial terms for each topic taxonomy, we ask an LLM\\nto separate the ones closely related to the main topic from those\\nunrelated, as mentioned earlier.\\n### **5.3 Hierarchy Construction**\\n\\nOnce we have the post-processed lists of candidate terms obtained\\nby each technique mentioned in subsection 5.2, we merge them and\\nremove repetitions. After the merge, for each macro category, we\\nhave lists of terms for each micro category, representing each topic\\ntaxonomy. However, they do not have any hierarchy level between\\nthe terms configuring the taxonomy.\\nTo tackle this problem, we use an LLM again, this time with a\\nprompt that searches for sub-categories within the terms of a topic\\nto create these hierarchies. The prompt is illustrated below:\\n\\nprompt=\"Create a dictionary by hierarchically arranging the\\n\\nfollowing words:\" + <wordsList> +.\" Use JSON format as\\nthe output such as the following: {\\\\\"key\\\\\": [\\\\\" list\\nof words\\\\\"]}\"\\n\\n**Listing 2: Prompt for creating a hierarchy for each list of**\\n**tags.**\\n\\nWith this prompt, we seek to ensure that the LLM response has a\\nconsistent pattern and facilitates handling the returned string. After\\nthis step, we have a hierarchy of terms in each topic taxonomy in\\nthe *Food* and *Shopping* macro categories.\\n### **5.4 Merchant Tagging**\\n\\nWith the topic taxonomies for both *Food* and *Shopping* macrocategories, we can now assign tags to merchants/establishments.\\nTo do so, we use the descriptions attached to these establishments,\\nand we see which terms from a taxonomy are mentioned in their\\ndescriptions with a reverse index algorithm. We employ the taxonomy whose topic is the same as the establishment’s micro category,\\nas shown in Figure 2.\\n### **6 TAXONOMY EXPANSION**\\n\\nAnother essential part of our method is the automatic expansion of\\nexisting taxonomies as new terms arrive, derived from additional\\nmerchant scrapped data, as shown in Section 4. In this section, we\\npresent our approach to taxonomy expansion by using instructionbased LLMs.\\n\\nAs new transactions may include new businesses, new terms\\ncan emerge from the descriptions obtained through the scraping\\nprocess. Therefore, we need to update the taxonomies with these\\n\\n\\nnew terms maintaining and enriching the created hierarchies with\\nthe potential new terms.\\nAfter completing the transaction enrichment process, including\\nthe search for business descriptions and the selection of candidate\\nterms, if relevant terms not included in the current hierarchies are\\ndetected, we initiate the expansion process.\\n### **6.1 Prompt engineering instruction for** **taxonomy representation**\\n\\nFirst, we represent our topic taxonomies in a format that can be\\ninterpreted by an LLM. We employed a generic prompt, illustrated\\nbelow, across all tested methods to convert topics into root nodes\\nand their terms into child nodes.\\n\\nChilds of [ROOT]: [CHILD1,CHILD2,CHILD3]\\nChilds of [CHILD1]: [CHILD4,CHILD5]\\n\\nChilds of [CHILD2]: [CHILD6]\\n\\n...\\n\\n**Listing 3: Prompt for representation of taxonomy**\\n### **6.2 Predicting the parent of a node**\\n\\nTo experiment with taxonomies expansion, we used two datasets:\\nour *Food* and *Shopping* topic taxonomies and the taxonomies from\\nSemEval-2015 Task 17 [ 15 ]. Those are low-resource taxonomies,\\nwith thousands of nodes or less, which are appropriate for the current prompt size of LLMs. We used the SemEval dataset to compare\\nthe results with well-established methods for taxonomy expansion,\\nsuch as Musubu [ 20 ]. Similar to their experiments, we hid 20% of\\nthe terms (chosen randomically) in the taxonomies to predict their\\nrespective parent nodes. To verify the parent/root of a new term,\\nwe used the following prompt:\\n\\n**Listin** **g** **4: Prom** **p** **t for searchin** **g** **for a node’s** **p** **arent**\\n\\nprompt=\"Who is the father of \"+<new_term>+\"?\"\\n\\nIn Table 1, we see the F1-Scores for parent node prediction. Equation 1 showcases how to calculate the F1-Score. TP is the number\\n\\nof true positives, nodes that were correctly assigned as parents of\\nchild nodes. FP is the number of false positives, nodes that were\\nincorrectly assigned as a parent to a child node. FN is the number\\nof false negatives, nodes that should have been assigned as parent\\nnodes but were not.\\n\\n2 ∗ *𝑇𝑃*\\n*𝐹* 1 = (1)\\n2 ∗ *𝑇𝑃* + *𝐹𝑃* + *𝐹𝑁*\\n\\nFor baseline models, we used Bert and Musubu; for commercial\\nLLMs, Gemini Pro and GPT-4; and for open-source LLMs, LLamaAlpaca(7B), Phi-2, and Mixtral 8x7B. We evaluate them in 4 taxonomies from the SemEval dataset and our taxonomies. For each\\ntaxonomy, the LLMs perform significantly better than Musubu,\\nwith GPT-4 and Gemini Pro having the highest F1-Scores, with the\\nlatter beating the former by a few points. However, the most recent\\nopen-source options (Phi-2 and Mixtral 8x7B) are getting close in\\nperformance.\\n\\n\\n271\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Moraes et al.\\n\\n\\n\\n\\n**Tags assigned to establishments from the**\\n\\n***\"Clothing & Accessories\"*** **micro category**\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n**Figure 2: Assigning tags to establishments based on a topic**\\n**taxonomy.**\\n\\nIt is important to note that while SemEval taxonomies have\\nthousands of nodes, ours have only a few hundred, which we can\\nassume is a significant reason for the degrading performance of\\nMusubu and Bert (LMs or LM-based methods). In contrast, the LLMs\\nhave a robust performance in such low-resource settings. This also\\nshows that LLMs have a remarkable understanding of questions\\nand zero-shot performance, generalizing well even for datasets in\\ndifferent languages.\\n### **7 TAXONOMY EVALUATION**\\n\\nTo properly evaluate the topic taxonomies that we created in this\\nwork, we developed a two-step qualitative evaluation of a limited\\npart of the results.\\nIn total, 58 topic taxonomies were created for the *Food* set and\\n6 for the *Shopping* set. For our evaluation, we selected the topic\\ntaxonomies with the highest number of terms in each part (the\\n\"Brazilian Cuisine\" taxonomy for the *Food* part and the \"Clothing\\nand Accessories\" taxonomy for the *Shopping* one). First, we assess\\nthe quality of removing generic terms from each taxonomy, and\\nthen, we evaluate the tags assigned to establishments based on that\\ntaxonomy. We asked 12 volunteers to answer a two-part form.\\n*Part 1 - Accuracy of the terms that were selected as related to the*\\n*topic* : In this part, we evaluate if the LLMs could correctly group\\nthe relevant and non-relevant terms, removing the generic terms.\\nTo do so, we defined a ground truth with the relevant terms as\\n\\n\\ntrue positives and the non-relevant terms as true negatives. Table 3\\nshows the results.\\n\\nGPT-4 was the best model, followed by Gemini Pro, both scoring\\nover 60% accuracy for the Brazilian Cuisine taxonomy and over\\n86% accuracy for the Clothing and Accessories taxonomy. Smaller\\nlanguage models such as Phi 2 and Llama 2 7B performed poorly\\nboth in removing generic terms and in formatting the response\\naccordingly, with Phi 2 being particularly verbose.\\n*Part 2 - Human Evaluation of the Quality of the Tagging Process* :\\nIn this part, the volunteers were asked to examine if the tags assigned to an establishment were appropriate and coherent to that\\nestablishment’s description. We selected the top 5 establishments\\nwith the highest transactions for each micro category. We asked our\\nevaluators to analyze the tags assigned to describe that establishment and choose the ones that were not appropriate. This way, we\\nhave a coherence ratio for each establishment based on the number\\n\\nof proper tags divided by the total number of tags. We average the\\nresults of our 12 evaluators and present them in Table 2. Figure 2\\nshows the \"Clothing & Accessories\" taxonomy that was evaluated\\nand 2 of the merchants and the tags assigned to them that were\\nincluded in the evaluation.\\n### **8 CONCLUSION**\\n\\nIn this work, we presented an unsupervised method for automatically creating and expanding topic taxonomies using LLMs. We\\nevaluated some of the generated taxonomies and applied them\\nin transaction tagging in a retailer’s bank dataset. The evaluation\\nshowed promising results, with average coherence scores above 90%\\nfor the two selected taxonomies. The taxonomies’ expansion with\\nGemini Pro also showed exciting results for parent node prediction,\\nwith F1-scores of 89% and 70% for *Food* and *Shopping* taxonomies,\\nrespectively.\\nFor future work on taxonomy construction, we plan to test\\nmore robust term selection methods, such as embedding-based\\napproaches. We also plan on conducting ablation studies to validate whether the keyword extraction and topic modeling parts\\nhelp improve the quality of the taxonomies created, by using a\\nbaseline prompt to ask the LLM to generate child nodes given a\\nparent node. In terms of taxonomy expansion, there are several\\ntasks to explore, ranging from node-level operations to generating\\nentire sub-trees and identifying similar structures. Additionally, we\\nintend to enhance our instruction-tuned LLM for taxonomy tasks\\n\\n\\n|Method|SemEval-2015 Task 17 Chemical Equipment Food Science|Our taxonomies Food Shopping|\\n|---|---|---|\\n|Gemini Pro|0.68 0.80 0.91 0.72|0.89 0.73|\\n|GPT-4|0.65 0.78 0.89 0.70|0.87 0.71|\\n|Mixtral-8x7B|0.59 0.63 0.80 0.57|0.74 0.60|\\n|Phi-2|0.56 0.52 0.68 0.56|0.64 0.54|\\n|LLama 7B|0.51 0.42 0.58 0.46|0.60 0.49|\\n|Musubu|0.35 0.46 0.37 0.42|0.21 0.13|\\n|Bert-Base|0.13 0.14 0.12 0.16|0.11 0.06|\\n\\n\\n**Table 1: F1-score for parent node prediction.**\\n\\n272\\n\\n\\n-----\\n\\nTagging Enriched Bank Transactions Using LLM-Generated Topic Taxonomies WebMedia’2024, Juiz de Fora, Brazil\\n\\n|Col1|Brazilian Cuisine Taxonomy Average Coherence Number of Tags|Col3|Clothing & Accessories Taxonomy Average Coherence Number of Tags|Col5|\\n|---|---|---|---|---|\\n|Merchant 1|92.30%|10|97.11%|8|\\n|Merchant 2|94.23%|8|83.07%|5|\\n|Merchant 3|89.23%|5|94.38%|5|\\n|Merchant 4|87.17%|6|93.84%|5|\\n|Merchant 5|93.40%|7|97.43%|6|\\n\\n\\n\\n**Table 2: Results of evaluating the tags assigned to each merchant/establishment.**\\n\\n\\n|Col1|Brazilian Cuisine|Clothing & Accessories|\\n|---|---|---|\\n|Llama 2 7B|29.54%|52.78%|\\n|Phi 2|40.90|73.68%|\\n|Mixtral 8x7B v0.1|46.93%|70.27%|\\n|Gemini Pro|61.36%|86.11%|\\n|GPT 4|68.08%|86.84%|\\n\\n\\n**Table 3: Accuracy of using each LLM to remove generic words**\\n**from each topic taxonomy.**\\n\\nby fine-tuning or employing more efficient methods such as LoRA\\n\\n[8].\\n### **LIMITATIONS**\\n\\nTo address the limitations of our work, we begin with the taxonomy construction component. In this phase, we relied on topic\\nmodeling and keywords extraction to select candidate terms for our\\ntaxonomies. The LDA algorithm used for topic modeling performs\\nsuboptimally when the base corpus is small. Some of our topics had\\ncorpora with vocabularies of fewer than 100 words, which can result\\nin topics containing irrelevant or incoherent terms. Additionally,\\nwe could have further experimented with the LDA hyperparameters\\nfor each micro-category corpus.\\nRegarding the evaluation of the generated taxonomies, we did not\\nassess topic completeness. Without a ground truth, it is challenging\\nto quantify how comprehensively the terms in a taxonomy cover the\\nmain topic. Furthermore, we evaluated only 2 of the 64 taxonomies\\ngenerated by our method, leaving a substantial portion unexamined.\\nIn the taxonomy expansion experiment, we evaluated only a lowresource setting with fewer than a thousand nodes. Most studies\\nfocus on taxonomies with hundreds of thousands or more nodes.\\n\\nThis presents a challenge for LLMs due to their limited context.\\nAddressing this contextual limitation could benefit from insights\\nfound in other works that tackle similar issues [12].\\n### **ETHICS STATEMENT**\\n\\nIn this work, we ensure the utmost protection of customers and\\nstore sensitive data by exclusively using non-sensitive information\\nin our dataset. Our prompts solely rely on selected words from store\\ndescriptions, thus avoiding any direct usage of personal or sensitive\\ninformation. No customer-specific data or store-sensitive details\\nare integrated into the system, upholding privacy and security as\\ntop priorities.\\n\\n\\nMoreover, we strictly adhere to ethical guidelines during our\\nexperiments involving volunteers, and no personal data is collected\\nfrom them. Our focus lies solely on analyzing the results of our\\nproposed approach. Participants’ anonymity and confidentiality are\\nmaintained throughout the research process, ensuring a responsible\\nand trustworthy approach to data handling.\\n### **ACKNOWLEDGMENTS**\\n\\nThe authors would like to acknowledge BTG Pactual for the partnership and financial support to this research.\\n### **REFERENCES**\\n\\n[1] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,\\nRadu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al . 2023. Gemini:\\na family of highly capable multimodal models.\\n\\n[2] David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation.\\n*Journal of machine Learning research* 3, Jan (2003), 993–1022.\\n\\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al . 2020. Language models are few-shot learners. *Advances in neural*\\n*information processing systems* 33 (2020), 1877–1901.\\n\\n[4] Antonio J. G. Busson, Rafael Rocha, Rennan Gaio, Rafael Miceli, Ivan Pereira,\\nDaniel de S. Moraes, Sérgio Colcher, Alvaro Veiga, Bruno Rizzi, Francisco Evangelista, Leandro Santos, Fellipe Marques, Marcos Rabaioli, Diego Feldberg, Debora\\nMattos, João Pasqua, and Diogo Dias. 2023. Hierarchical Classification of Financial Transactions Through Context-Fusion of Transformer-based Embeddings\\nand Taxonomy-aware Attention Layer. In *Anais do II Brazilian Workshop on Arti-*\\n*ficial Intelligence in Finance (BWAIF 2023) (BWAIF 2023)* . Sociedade Brasileira de\\nComputação. https://doi.org/10.5753/bwaif.2023.229322\\n\\n[5] Ricardo Campos, Vítor Mangaravite, Arian Pasquali, Alípio Jorge, Célia Nunes,\\nand Adam Jatowt. 2020. YAKE! Keyword extraction from single documents using\\nmultiple local features. *Information Sciences* 509 (2020), 257–289.\\n\\n[6] Boqi Chen, Fandi Yi, and Dániel Varró. 2023. Prompting or Fine-tuning? A\\nComparative Study of Large Language Models for Taxonomy Construction. In\\n*2023 ACM/IEEE International Conference on Model Driven Engineering Languages*\\n*and Systems Companion (MODELS-C)* . IEEE, 588–596.\\n\\n[7] Sabit Ekin. 2023. Prompt engineering for ChatGPT: a quick guide to techniques,\\ntips, and best practices. *Authorea Preprints* (2023).\\n\\n[8] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\\nWang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large\\nlanguage models. *arXiv preprint arXiv:2106.09685* (2021).\\n\\n[9] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche\\nSavary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou\\nHanna, Florian Bressand, et al . 2024. Mixtral of experts. *arXiv preprint*\\n*arXiv:2401.04088* (2024).\\n\\n[10] Dongha Lee, Jiaming Shen, SeongKu Kang, Susik Yoon, Jiawei Han, and Hwanjo\\nYu. 2022. TaxoCom: Topic Taxonomy Completion with Hierarchical Discovery of\\nNovel Topic Clusters. In *Proceedings of the ACM Web Conference 2022* . 2819–2829.\\n\\n[11] Yinheng Li. 2023. A Practical Survey on Zero-Shot Prompt Design for In-Context\\nLearning. In *Proceedings of the 14th International Conference on Recent Advances*\\n*in Natural Language Processing* . 641–647.\\n\\n[12] Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun\\nMa, and Zhoujun Li. 2023. Unleashing Infinite-Length Input Capacity for Largescale Language Models with Self-Controlled Memory System. *arXiv preprint*\\n*arXiv:2304.13343* (2023).\\n\\n[13] Irina Nikishina, Varvara Logacheva, Alexander Panchenko, and Natalia\\nLoukachevitch. 2020. RUSSE’2020: Findings of the First Taxonomy Enrichment\\n\\n\\n273\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Moraes et al.\\n\\n\\nTask for the Russian language. *arXiv preprint arXiv:2005.11176* (2020).\\n\\n[14] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\\n\\n[15] Octavian Popescu and Carlo Strapparava. 2015. Semeval 2015, task 7: Diachronic\\ntext evaluation. In *Proceedings of the 9th International Workshop on Semantic*\\n*Evaluation (SemEval 2015)* . 870–878.\\n\\n[16] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of\\ntransfer learning with a unified text-to-text transformer. *The Journal of Machine*\\n*Learning Research* 21, 1 (2020), 5485–5551.\\n\\n[17] Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language\\nmodels: Beyond the few-shot paradigm. In *Extended abstracts of the 2021 CHI*\\n*conference on human factors in computing systems* . 1–7.\\n\\n[18] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal,\\nand Aman Chadha. 2024. A systematic survey of prompt engineering in large\\nlanguage models: Techniques and applications. *arXiv preprint arXiv:2402.07927*\\n(2024).\\n\\n[19] Rion Snow, Daniel Jurafsky, and Andrew Ng. 2004. Learning syntactic patterns\\nfor automatic hypernym discovery. *Advances in neural information processing*\\n*systems* 17 (2004).\\n\\n[20] Kunihiro Takeoka, Kosuke Akimoto, and Masafumi Oyamada. 2021. Low-resource\\ntaxonomy enrichment with pretrained language models. In *Proceedings of the*\\n*2021 Conference on Empirical Methods in Natural Language Processing* . 2747–2758.\\n\\n[21] Adrian Tam. [n. d.]. What Are Zero-Shot Prompting and Few-Shot Prompting. https://machinelearningmastery.com/what-are-zero-shot-prompting-andfew-shot-prompting/. Accessed: 2024-07-02.\\n\\n\\n\\n[22] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, et al . 2023. Llama: Open and efficient foundation language models. *arXiv*\\n*preprint arXiv:2302.13971* (2023).\\n\\n[23] Erlend Vollset, Eirik Folkestad, Marius Rise Gallala, and Jon Atle Gulla. 2017.\\nMaking use of external company data to improve the classification of bank transactions. In *Advanced Data Mining and Applications: 13th International Conference,*\\n*ADMA 2017, Singapore, November 5–6, 2017, Proceedings 13* . Springer, 767–780.\\n\\n[24] Hanna Wallach, David Mimno, and Andrew McCallum. 2009. Rethinking LDA:\\nWhy priors matter. *Advances in neural information processing systems* 22 (2009).\\n\\n[25] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang,\\nAakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain\\nof thought reasoning in language models. *arXiv preprint arXiv:2203.11171* (2022).\\n\\n[26] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei\\nXia, H Chi, Quoc V Le, and Denny Zhou. [n. d.]. Chain-of-Thought Prompting\\nElicits Reasoning in Large Language Models. ([n. d.]).\\n\\n[27] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,\\nand Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting in Language\\nModels. In *International Conference on Learning Representations (ICLR)* .\\n\\n[28] Chao Zhang, Fangbo Tao, Xiusi Chen, Jiaming Shen, Meng Jiang, Brian Sadler,\\nMichelle Vanni, and Jiawei Han. 2018. TaxoGen: Unsupervised Topic Taxonomy\\nConstruction by Adaptive Term Embedding and Clustering. In *Proceedings of the*\\n*24th ACM SIGKDD International Conference on Knowledge Discovery & Data Min-*\\n*ing* (London, United Kingdom) *(KDD ’18)* . Association for Computing Machinery,\\nNew York, NY, USA, 2701–2709. https://doi.org/10.1145/3219819.3220064\\n\\n\\n274\\n\\n\\n-----\\n\\n',\n",
       "  'artigo_tokenizado': ['#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Tagging',\n",
       "   'Enriched',\n",
       "   'Bank',\n",
       "   'Transactions',\n",
       "   'Using',\n",
       "   'LLM',\n",
       "   '-',\n",
       "   'Generated',\n",
       "   'Topic',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Taxonomies',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Daniel',\n",
       "   'de',\n",
       "   'S.',\n",
       "   'Moraes',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'TeleMídia',\n",
       "   'Lab',\n",
       "   '-',\n",
       "   'PUC',\n",
       "   '-',\n",
       "   'Rio',\n",
       "   'danielmoraes@telemidia.puc-rio.br',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Ivan',\n",
       "   'de',\n",
       "   'J.',\n",
       "   'P.',\n",
       "   'Pinto',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'TeleMídia',\n",
       "   'Lab',\n",
       "   '-',\n",
       "   'PUC',\n",
       "   '-',\n",
       "   'Rio',\n",
       "   'ivan@telemidia.puc-rio.br',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Matheus',\n",
       "   'A.',\n",
       "   'S.',\n",
       "   'Pinto',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'BTG',\n",
       "   'Pactual',\n",
       "   'matheus.adler@btgpactual.com',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Gabriela',\n",
       "   'Tourinho',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'BTG',\n",
       "   'Pactual',\n",
       "   'gabriela.tourinho@btgpactual.com',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Polyana',\n",
       "   'B.',\n",
       "   'da',\n",
       "   'Costa',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'TeleMídia',\n",
       "   'Lab',\n",
       "   '-',\n",
       "   'PUC',\n",
       "   '-',\n",
       "   'Rio',\n",
       "   'polyana@telemidia.puc-rio.br',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Sérgio',\n",
       "   'Colcher',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'TeleMídia',\n",
       "   'Lab',\n",
       "   '-',\n",
       "   'PUC',\n",
       "   '-',\n",
       "   'Rio',\n",
       "   'colcher@inf.puc-rio.br',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Rafael',\n",
       "   'H.',\n",
       "   'Rocha',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'BTG',\n",
       "   'Pactual',\n",
       "   'rafael-h.rocha@btgpactual.com',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Marcos',\n",
       "   'Rabaioli',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'BTG',\n",
       "   'Pactual',\n",
       "   'marcos.rabaioli@btgpactual.com',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Pedro',\n",
       "   'T.',\n",
       "   'C.',\n",
       "   'Santos',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'TeleMídia',\n",
       "   'Lab',\n",
       "   '-',\n",
       "   'PUC',\n",
       "   '-',\n",
       "   'Rio',\n",
       "   'thiagocutrim@telemidia.puc-rio.br',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Antonio',\n",
       "   'J.',\n",
       "   'G.',\n",
       "   'Busson',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'BTG',\n",
       "   'Pactual',\n",
       "   'antonio.busson@btgpactual.com',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Rennan',\n",
       "   'Gaio',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'BTG',\n",
       "   'Pactual',\n",
       "   'rennan.gaio@btgpactual.com',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'David',\n",
       "   'Favaro',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'BTG',\n",
       "   'Pactual',\n",
       "   'david.favaro@btgpactual.com',\n",
       "   '\\n\\n',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Taxonomies',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n\\n',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Transaction',\n",
       "   ':',\n",
       "   'Purchase',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '*',\n",
       "   '*',\n",
       "   'at',\n",
       "   'PAG*distr_nona',\n",
       "   'on',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n',\n",
       "   '*',\n",
       "   '*',\n",
       "   '19/02/2008',\n",
       "   'at',\n",
       "   '19:20:05',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Italian',\n",
       "   'Cuisine',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Pasta',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Sauce',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Pizza',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Wine',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n\\n\\n\\n\\n',\n",
       "   '|name|macrocategory|microcategory|',\n",
       "   '\\n',\n",
       "   '|---|---|---|',\n",
       "   '\\n',\n",
       "   '|Nona',\n",
       "   'Ristorante|Food|Restaurant|',\n",
       "   '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Figure',\n",
       "   '1',\n",
       "   ':',\n",
       "   'Overview',\n",
       "   'of',\n",
       "   'our',\n",
       "   'method',\n",
       "   '.',\n",
       "   '(',\n",
       "   'Note',\n",
       "   ':',\n",
       "   'Merchant',\n",
       "   'and',\n",
       "   'transaction',\n",
       "   'data',\n",
       "   'are',\n",
       "   'fabricated',\n",
       "   'for',\n",
       "   'demonstration',\n",
       "   'only',\n",
       "   ')',\n",
       "   '.',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'ABSTRACT',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'This',\n",
       "   'work',\n",
       "   'presents',\n",
       "   'an',\n",
       "   'unsupervised',\n",
       "   'method',\n",
       "   'for',\n",
       "   'tagging',\n",
       "   'banking',\n",
       "   '\\n',\n",
       "   'consumers',\n",
       "   '’',\n",
       "   'transactions',\n",
       "   'using',\n",
       "   'automatically',\n",
       "   'constructed',\n",
       "   'and',\n",
       "   'expanded',\n",
       "   'topic',\n",
       "   'taxonomies',\n",
       "   '.',\n",
       "   'Initially',\n",
       "   ',',\n",
       "   'we',\n",
       "   'enrich',\n",
       "   'the',\n",
       "   'bank',\n",
       "   'transactions',\n",
       "   '\\n',\n",
       "   'via',\n",
       "   'web',\n",
       "   'scraping',\n",
       "   'to',\n",
       "   'collect',\n",
       "   'relevant',\n",
       "   'descriptions',\n",
       "   ',',\n",
       "   'which',\n",
       "   'are',\n",
       "   'then',\n",
       "   '\\n',\n",
       "   'preprocessed',\n",
       "   'using',\n",
       "   'NLP',\n",
       "   'techniques',\n",
       "   'to',\n",
       "   'generate',\n",
       "   'candidate',\n",
       "   'terms',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Topic',\n",
       "   'taxonomies',\n",
       "   'are',\n",
       "   'created',\n",
       "   'using',\n",
       "   'instruction',\n",
       "   '-',\n",
       "   'based',\n",
       "   'fine',\n",
       "   '-',\n",
       "   'tuned',\n",
       "   '\\n',\n",
       "   'LLMs',\n",
       "   '(',\n",
       "   'Large',\n",
       "   'Language',\n",
       "   'Models',\n",
       "   ')',\n",
       "   '.',\n",
       "   'To',\n",
       "   'expand',\n",
       "   'existing',\n",
       "   'taxonomies',\n",
       "   '\\n',\n",
       "   'with',\n",
       "   'new',\n",
       "   'terms',\n",
       "   ',',\n",
       "   'we',\n",
       "   'use',\n",
       "   'zero',\n",
       "   '-',\n",
       "   'shot',\n",
       "   'prompting',\n",
       "   'to',\n",
       "   'determine',\n",
       "   'where',\n",
       "   '\\n',\n",
       "   'to',\n",
       "   'add',\n",
       "   'new',\n",
       "   'nodes',\n",
       "   '.',\n",
       "   'The',\n",
       "   'resulting',\n",
       "   'taxonomies',\n",
       "   'are',\n",
       "   'used',\n",
       "   'to',\n",
       "   'assign',\n",
       "   '\\n',\n",
       "   'descriptive',\n",
       "   'tags',\n",
       "   'that',\n",
       "   'characterize',\n",
       "   'the',\n",
       "   'transactions',\n",
       "   'in',\n",
       "   'the',\n",
       "   'retail',\n",
       "   'bank',\n",
       "   '\\n',\n",
       "   'dataset',\n",
       "   '.',\n",
       "   'For',\n",
       "   'evaluation',\n",
       "   ',',\n",
       "   '12',\n",
       "   'volunteers',\n",
       "   'completed',\n",
       "   'a',\n",
       "   'two',\n",
       "   '-',\n",
       "   'part',\n",
       "   'form',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   ':',\n",
       "   'Proceedings',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Web',\n",
       "   '(',\n",
       "   'WebMe',\n",
       "   '\\n',\n",
       "   'dia’2024',\n",
       "   ')',\n",
       "   '.',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '.',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   ':',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   ',',\n",
       "   '2024',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'SBC',\n",
       "   '–',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '-',\n",
       "   '2753',\n",
       "   '\\n\\n\\n',\n",
       "   'assessing',\n",
       "   'the',\n",
       "   'quality',\n",
       "   'of',\n",
       "   'the',\n",
       "   'taxonomies',\n",
       "   'and',\n",
       "   'the',\n",
       "   'tags',\n",
       "   'assigned',\n",
       "   'to',\n",
       "   '\\n',\n",
       "   'merchants',\n",
       "   '.',\n",
       "   'The',\n",
       "   'evaluation',\n",
       "   'revealed',\n",
       "   'a',\n",
       "   'coherence',\n",
       "   'rate',\n",
       "   'exceeding',\n",
       "   '90',\n",
       "   '%',\n",
       "   '\\n',\n",
       "   'for',\n",
       "   'the',\n",
       "   'chosen',\n",
       "   'taxonomies',\n",
       "   '.',\n",
       "   'Additionally',\n",
       "   ',',\n",
       "   'taxonomy',\n",
       "   'expansion',\n",
       "   'using',\n",
       "   '\\n',\n",
       "   'LLMs',\n",
       "   'demonstrated',\n",
       "   'promising',\n",
       "   'results',\n",
       "   'for',\n",
       "   'parent',\n",
       "   'node',\n",
       "   'prediction',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'with',\n",
       "   'F1',\n",
       "   '-',\n",
       "   'scores',\n",
       "   'of',\n",
       "   '89',\n",
       "   '%',\n",
       "   'and',\n",
       "   '70',\n",
       "   '%',\n",
       "   'for',\n",
       "   '*',\n",
       "   'Food',\n",
       "   '*',\n",
       "   'and',\n",
       "   '*',\n",
       "   'Shopping',\n",
       "   '*',\n",
       "   'taxonomies',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'respectively',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'KEYWORDS',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Large',\n",
       "   'Language',\n",
       "   'Models',\n",
       "   ',',\n",
       "   'Natural',\n",
       "   'Language',\n",
       "   'Processing',\n",
       "   ',',\n",
       "   'Web',\n",
       "   'Scrapping',\n",
       "   ',',\n",
       "   'Topic',\n",
       "   'Modeling',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   '1',\n",
       "   'INTRODUCTION',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Many',\n",
       "   'recent',\n",
       "   'studies',\n",
       "   'have',\n",
       "   'focused',\n",
       "   'on',\n",
       "   'the',\n",
       "   'application',\n",
       "   'of',\n",
       "   'Machine',\n",
       "   '\\n',\n",
       "   'Learning',\n",
       "   '-',\n",
       "   'based',\n",
       "   'methods',\n",
       "   'for',\n",
       "   'the',\n",
       "   'classification',\n",
       "   'and',\n",
       "   'characterization',\n",
       "   'of',\n",
       "   'financial',\n",
       "   'transactions',\n",
       "   '.',\n",
       "   'For',\n",
       "   'example',\n",
       "   ',',\n",
       "   'Vollset',\n",
       "   'et',\n",
       "   'al',\n",
       "   '.',\n",
       "   '[',\n",
       "   '23',\n",
       "   ']',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'Busson',\n",
       "   'et',\n",
       "   'al',\n",
       "   '.',\n",
       "   '[',\n",
       "   '4',\n",
       "   ']',\n",
       "   'explored',\n",
       "   'an',\n",
       "   'approach',\n",
       "   'to',\n",
       "   'hierarchically',\n",
       "   'classifying',\n",
       "   '\\n\\n\\n',\n",
       "   '267',\n",
       "   '\\n\\n\\n',\n",
       "   '-----',\n",
       "   '\\n\\n',\n",
       "   'WebMedia’2024',\n",
       "   ',',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   'Moraes',\n",
       "   'et',\n",
       "   'al',\n",
       "   '.',\n",
       "   '\\n\\n\\n',\n",
       "   'financial',\n",
       "   'transactions',\n",
       "   ',',\n",
       "   'employing',\n",
       "   'a',\n",
       "   'predefined',\n",
       "   'set',\n",
       "   'of',\n",
       "   'categories',\n",
       "   '/',\n",
       "   'subcategories',\n",
       "   'that',\n",
       "   'describe',\n",
       "   'purchase',\n",
       "   'types',\n",
       "   'and',\n",
       "   'transactions',\n",
       "   '.',\n",
       "   'However',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'these',\n",
       "   'methods',\n",
       "   'apply',\n",
       "   'a',\n",
       "   'limited',\n",
       "   ',',\n",
       "   'predefined',\n",
       "   'set',\n",
       "   'of',\n",
       "   'static',\n",
       "   'classes',\n",
       "   ',',\n",
       "   'which',\n",
       "   '\\n',\n",
       "   'restricts',\n",
       "   'the',\n",
       "   'ability',\n",
       "   'to',\n",
       "   'extend',\n",
       "   'classifications',\n",
       "   'based',\n",
       "   'on',\n",
       "   'user',\n",
       "   'experiences',\n",
       "   'when',\n",
       "   'encountering',\n",
       "   'new',\n",
       "   ',',\n",
       "   'previously',\n",
       "   'undefined',\n",
       "   'categories',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'In',\n",
       "   'this',\n",
       "   'context',\n",
       "   ',',\n",
       "   'to',\n",
       "   'expand',\n",
       "   'the',\n",
       "   'possible',\n",
       "   'set',\n",
       "   'of',\n",
       "   'classes',\n",
       "   '/',\n",
       "   'tags',\n",
       "   'to',\n",
       "   'label',\n",
       "   '\\n',\n",
       "   'a',\n",
       "   'transaction',\n",
       "   ',',\n",
       "   'we',\n",
       "   'developed',\n",
       "   'an',\n",
       "   'unsupervised',\n",
       "   'method',\n",
       "   'based',\n",
       "   'on',\n",
       "   '\\n',\n",
       "   '*',\n",
       "   'topic',\n",
       "   'taxonomies',\n",
       "   '*',\n",
       "   '.',\n",
       "   'Taxonomies',\n",
       "   'are',\n",
       "   'very',\n",
       "   'useful',\n",
       "   'in',\n",
       "   'the',\n",
       "   'structural',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'semantic',\n",
       "   'analyses',\n",
       "   'of',\n",
       "   'topics',\n",
       "   'and',\n",
       "   'textual',\n",
       "   'data',\n",
       "   '.',\n",
       "   'However',\n",
       "   ',',\n",
       "   'creating',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'maintaining',\n",
       "   'them',\n",
       "   'is',\n",
       "   'often',\n",
       "   'costly',\n",
       "   'and',\n",
       "   'challenging',\n",
       "   'to',\n",
       "   'scale',\n",
       "   'manually',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Therefore',\n",
       "   ',',\n",
       "   'recent',\n",
       "   'works',\n",
       "   'have',\n",
       "   'tackled',\n",
       "   'the',\n",
       "   'automatic',\n",
       "   'creation',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'expansion',\n",
       "   'of',\n",
       "   '*',\n",
       "   'topic',\n",
       "   'taxonomies',\n",
       "   '*',\n",
       "   ',',\n",
       "   'in',\n",
       "   'which',\n",
       "   'each',\n",
       "   'node',\n",
       "   'in',\n",
       "   'a',\n",
       "   'hierarchy',\n",
       "   '\\n',\n",
       "   'represents',\n",
       "   'a',\n",
       "   'conceptual',\n",
       "   'topic',\n",
       "   'composed',\n",
       "   'of',\n",
       "   'semantically',\n",
       "   'coherent',\n",
       "   '\\n\\n',\n",
       "   'terms',\n",
       "   '.',\n",
       "   '\\n\\n',\n",
       "   'We',\n",
       "   'present',\n",
       "   'an',\n",
       "   'unsupervised',\n",
       "   'method',\n",
       "   'for',\n",
       "   'automatically',\n",
       "   'constructing',\n",
       "   'and',\n",
       "   'expanding',\n",
       "   'topic',\n",
       "   'taxonomies',\n",
       "   'with',\n",
       "   'instruction',\n",
       "   '-',\n",
       "   'based',\n",
       "   'LLMs',\n",
       "   '\\n',\n",
       "   '(',\n",
       "   'Large',\n",
       "   'Language',\n",
       "   'Models',\n",
       "   ')',\n",
       "   ',',\n",
       "   'in',\n",
       "   'a',\n",
       "   'Zero',\n",
       "   '-',\n",
       "   'Shot',\n",
       "   'manner',\n",
       "   '.',\n",
       "   'Candidate',\n",
       "   'terms',\n",
       "   '\\n',\n",
       "   'for',\n",
       "   'the',\n",
       "   'initial',\n",
       "   'version',\n",
       "   'of',\n",
       "   'the',\n",
       "   'taxonomy',\n",
       "   'are',\n",
       "   'obtained',\n",
       "   'using',\n",
       "   'topic',\n",
       "   '\\n',\n",
       "   'modeling',\n",
       "   'and',\n",
       "   'keyword',\n",
       "   'extraction',\n",
       "   'techniques',\n",
       "   '.',\n",
       "   'Then',\n",
       "   'we',\n",
       "   'apply',\n",
       "   'LLMs',\n",
       "   '\\n',\n",
       "   'to',\n",
       "   'post',\n",
       "   '-',\n",
       "   'process',\n",
       "   'the',\n",
       "   'resulting',\n",
       "   'terms',\n",
       "   ',',\n",
       "   'create',\n",
       "   'a',\n",
       "   'hierarchy',\n",
       "   ',',\n",
       "   'and',\n",
       "   'add',\n",
       "   'new',\n",
       "   '\\n',\n",
       "   'terms',\n",
       "   'to',\n",
       "   'an',\n",
       "   'existing',\n",
       "   'taxonomy',\n",
       "   '.',\n",
       "   'Since',\n",
       "   'the',\n",
       "   'taxonomies',\n",
       "   'are',\n",
       "   'derived',\n",
       "   '\\n',\n",
       "   'from',\n",
       "   'a',\n",
       "   'corpus',\n",
       "   'of',\n",
       "   'unstructured',\n",
       "   'texts',\n",
       "   'describing',\n",
       "   'niches',\n",
       "   'of',\n",
       "   'consuming',\n",
       "   '\\n',\n",
       "   'habits',\n",
       "   ',',\n",
       "   'we',\n",
       "   'opted',\n",
       "   'to',\n",
       "   'investigate',\n",
       "   'the',\n",
       "   'use',\n",
       "   'of',\n",
       "   'LLMs',\n",
       "   'in',\n",
       "   'our',\n",
       "   'approach',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'LLMs',\n",
       "   'are',\n",
       "   'often',\n",
       "   'pre',\n",
       "   '-',\n",
       "   'trained',\n",
       "   'on',\n",
       "   'a',\n",
       "   'large',\n",
       "   'corpus',\n",
       "   'of',\n",
       "   'text',\n",
       "   ',',\n",
       "   'allowing',\n",
       "   'them',\n",
       "   '\\n',\n",
       "   'to',\n",
       "   'learn',\n",
       "   'contextual',\n",
       "   'representations',\n",
       "   'that',\n",
       "   'capture',\n",
       "   'the',\n",
       "   'intricacies',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'human',\n",
       "   'language',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'We',\n",
       "   'applied',\n",
       "   'our',\n",
       "   'method',\n",
       "   'to',\n",
       "   'a',\n",
       "   'private',\n",
       "   'dataset',\n",
       "   'of',\n",
       "   'transactions',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'a',\n",
       "   'retail',\n",
       "   'bank',\n",
       "   ',',\n",
       "   'enriched',\n",
       "   'with',\n",
       "   'scraped',\n",
       "   'data',\n",
       "   'from',\n",
       "   'food',\n",
       "   'and',\n",
       "   'shopping',\n",
       "   '\\n',\n",
       "   'companies',\n",
       "   ',',\n",
       "   'and',\n",
       "   'evaluated',\n",
       "   'the',\n",
       "   'resulting',\n",
       "   'taxonomies',\n",
       "   'quantitatively',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'The',\n",
       "   'generated',\n",
       "   'tags',\n",
       "   'of',\n",
       "   'our',\n",
       "   'topic',\n",
       "   'taxonomies',\n",
       "   'are',\n",
       "   'then',\n",
       "   ...],\n",
       "  'pos_tagger': '',\n",
       "  'lema': ['Tagging',\n",
       "   'Enriched',\n",
       "   'Bank',\n",
       "   'Transactions',\n",
       "   'use',\n",
       "   'LLM',\n",
       "   'generate',\n",
       "   'Topic',\n",
       "   'taxonomy',\n",
       "   'Daniel',\n",
       "   'de',\n",
       "   'S.',\n",
       "   'Moraes',\n",
       "   'TeleMídia',\n",
       "   'Lab',\n",
       "   'PUC',\n",
       "   'Rio',\n",
       "   'danielmoraes@telemidia.puc-rio.br',\n",
       "   'Ivan',\n",
       "   'de',\n",
       "   'J.',\n",
       "   'P.',\n",
       "   'Pinto',\n",
       "   'TeleMídia',\n",
       "   'Lab',\n",
       "   'PUC',\n",
       "   'Rio',\n",
       "   'ivan@telemidia.puc-rio.br',\n",
       "   'Matheus',\n",
       "   'a.',\n",
       "   'S.',\n",
       "   'Pinto',\n",
       "   'btg',\n",
       "   'Pactual',\n",
       "   'matheus.adler@btgpactual.com',\n",
       "   'Gabriela',\n",
       "   'Tourinho',\n",
       "   'btg',\n",
       "   'Pactual',\n",
       "   'gabriela.tourinho@btgpactual.com',\n",
       "   'Polyana',\n",
       "   'B.',\n",
       "   'da',\n",
       "   'Costa',\n",
       "   'TeleMídia',\n",
       "   'Lab',\n",
       "   'PUC',\n",
       "   'Rio',\n",
       "   'polyana@telemidia.puc-rio.br',\n",
       "   'Sérgio',\n",
       "   'Colcher',\n",
       "   'TeleMídia',\n",
       "   'Lab',\n",
       "   'PUC',\n",
       "   'Rio',\n",
       "   'colcher@inf.puc-rio.br',\n",
       "   'Rafael',\n",
       "   'H.',\n",
       "   'Rocha',\n",
       "   'btg',\n",
       "   'Pactual',\n",
       "   'rafael-h.rocha@btgpactual.com',\n",
       "   'Marcos',\n",
       "   'Rabaioli',\n",
       "   'btg',\n",
       "   'pactual',\n",
       "   'marcos.rabaioli@btgpactual.com',\n",
       "   'Pedro',\n",
       "   'T.',\n",
       "   'C.',\n",
       "   'Santos',\n",
       "   'TeleMídia',\n",
       "   'Lab',\n",
       "   'PUC',\n",
       "   'Rio',\n",
       "   'thiagocutrim@telemidia.puc-rio.br',\n",
       "   'Antonio',\n",
       "   'J.',\n",
       "   'G.',\n",
       "   'Busson',\n",
       "   'btg',\n",
       "   'Pactual',\n",
       "   'antonio.busson@btgpactual.com',\n",
       "   'Rennan',\n",
       "   'Gaio',\n",
       "   'btg',\n",
       "   'Pactual',\n",
       "   'rennan.gaio@btgpactual.com',\n",
       "   'David',\n",
       "   'Favaro',\n",
       "   'btg',\n",
       "   'Pactual',\n",
       "   'david.favaro@btgpactual.com',\n",
       "   'taxonomy',\n",
       "   'transaction',\n",
       "   'purchase',\n",
       "   'at',\n",
       "   'PAG*distr_nona',\n",
       "   'on',\n",
       "   '19/02/2008',\n",
       "   'at',\n",
       "   '19:20:05',\n",
       "   'italian',\n",
       "   'Cuisine',\n",
       "   'Pasta',\n",
       "   'sauce',\n",
       "   'Pizza',\n",
       "   'wine',\n",
       "   '|name|macrocategory|microcategory|',\n",
       "   '|---|---|---|',\n",
       "   '|nona',\n",
       "   'ristorante|food|restaurant|',\n",
       "   'figure',\n",
       "   '1',\n",
       "   'overview',\n",
       "   'of',\n",
       "   'our',\n",
       "   'method',\n",
       "   'note',\n",
       "   'merchant',\n",
       "   'and',\n",
       "   'transaction',\n",
       "   'datum',\n",
       "   'be',\n",
       "   'fabricate',\n",
       "   'for',\n",
       "   'demonstration',\n",
       "   'only',\n",
       "   'ABSTRACT',\n",
       "   'this',\n",
       "   'work',\n",
       "   'present',\n",
       "   'an',\n",
       "   'unsupervised',\n",
       "   'method',\n",
       "   'for',\n",
       "   'tag',\n",
       "   'banking',\n",
       "   'consumer',\n",
       "   'transaction',\n",
       "   'use',\n",
       "   'automatically',\n",
       "   'construct',\n",
       "   'and',\n",
       "   'expand',\n",
       "   'topic',\n",
       "   'taxonomy',\n",
       "   'initially',\n",
       "   'we',\n",
       "   'enrich',\n",
       "   'the',\n",
       "   'bank',\n",
       "   'transaction',\n",
       "   'via',\n",
       "   'web',\n",
       "   'scraping',\n",
       "   'to',\n",
       "   'collect',\n",
       "   'relevant',\n",
       "   'description',\n",
       "   'which',\n",
       "   'be',\n",
       "   'then',\n",
       "   'preprocesse',\n",
       "   'use',\n",
       "   'NLP',\n",
       "   'technique',\n",
       "   'to',\n",
       "   'generate',\n",
       "   'candidate',\n",
       "   'term',\n",
       "   'topic',\n",
       "   'taxonomy',\n",
       "   'be',\n",
       "   'create',\n",
       "   'use',\n",
       "   'instruction',\n",
       "   'base',\n",
       "   'fine',\n",
       "   'tune',\n",
       "   'LLMs',\n",
       "   'large',\n",
       "   'Language',\n",
       "   'model',\n",
       "   'to',\n",
       "   'expand',\n",
       "   'exist',\n",
       "   'taxonomy',\n",
       "   'with',\n",
       "   'new',\n",
       "   'term',\n",
       "   'we',\n",
       "   'use',\n",
       "   'zero',\n",
       "   'shot',\n",
       "   'prompt',\n",
       "   'to',\n",
       "   'determine',\n",
       "   'where',\n",
       "   'to',\n",
       "   'add',\n",
       "   'new',\n",
       "   'node',\n",
       "   'the',\n",
       "   'result',\n",
       "   'taxonomy',\n",
       "   'be',\n",
       "   'use',\n",
       "   'to',\n",
       "   'assign',\n",
       "   'descriptive',\n",
       "   'tag',\n",
       "   'that',\n",
       "   'characterize',\n",
       "   'the',\n",
       "   'transaction',\n",
       "   'in',\n",
       "   'the',\n",
       "   'retail',\n",
       "   'bank',\n",
       "   'dataset',\n",
       "   'for',\n",
       "   'evaluation',\n",
       "   '12',\n",
       "   'volunteer',\n",
       "   'complete',\n",
       "   'a',\n",
       "   'two',\n",
       "   'part',\n",
       "   'form',\n",
       "   'in',\n",
       "   'proceeding',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'web',\n",
       "   'WebMe',\n",
       "   'dia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   '2024',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'sbc',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '2753',\n",
       "   'assess',\n",
       "   'the',\n",
       "   'quality',\n",
       "   'of',\n",
       "   'the',\n",
       "   'taxonomy',\n",
       "   'and',\n",
       "   'the',\n",
       "   'tag',\n",
       "   'assign',\n",
       "   'to',\n",
       "   'merchant',\n",
       "   'the',\n",
       "   'evaluation',\n",
       "   'reveal',\n",
       "   'a',\n",
       "   'coherence',\n",
       "   'rate',\n",
       "   'exceed',\n",
       "   '90',\n",
       "   'for',\n",
       "   'the',\n",
       "   'choose',\n",
       "   'taxonomy',\n",
       "   'additionally',\n",
       "   'taxonomy',\n",
       "   'expansion',\n",
       "   'use',\n",
       "   'llm',\n",
       "   'demonstrate',\n",
       "   'promise',\n",
       "   'result',\n",
       "   'for',\n",
       "   'parent',\n",
       "   'node',\n",
       "   'prediction',\n",
       "   'with',\n",
       "   'f1',\n",
       "   'score',\n",
       "   'of',\n",
       "   '89',\n",
       "   'and',\n",
       "   '70',\n",
       "   'for',\n",
       "   'Food',\n",
       "   'and',\n",
       "   'shopping',\n",
       "   'taxonomy',\n",
       "   'respectively',\n",
       "   'keyword',\n",
       "   'large',\n",
       "   'Language',\n",
       "   'model',\n",
       "   'Natural',\n",
       "   'Language',\n",
       "   'Processing',\n",
       "   'web',\n",
       "   'Scrapping',\n",
       "   'Topic',\n",
       "   'modeling',\n",
       "   '1',\n",
       "   'introduction',\n",
       "   'many',\n",
       "   'recent',\n",
       "   'study',\n",
       "   'have',\n",
       "   'focus',\n",
       "   'on',\n",
       "   'the',\n",
       "   'application',\n",
       "   'of',\n",
       "   'machine',\n",
       "   'Learning',\n",
       "   'base',\n",
       "   'method',\n",
       "   'for',\n",
       "   'the',\n",
       "   'classification',\n",
       "   'and',\n",
       "   'characterization',\n",
       "   'of',\n",
       "   'financial',\n",
       "   'transaction',\n",
       "   'for',\n",
       "   'example',\n",
       "   'Vollset',\n",
       "   'et',\n",
       "   'al',\n",
       "   '23',\n",
       "   'and',\n",
       "   'Busson',\n",
       "   'et',\n",
       "   'al',\n",
       "   '4',\n",
       "   'explore',\n",
       "   'an',\n",
       "   'approach',\n",
       "   'to',\n",
       "   'hierarchically',\n",
       "   'classify',\n",
       "   '267',\n",
       "   'WebMedia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Moraes',\n",
       "   'et',\n",
       "   'al',\n",
       "   'financial',\n",
       "   'transaction',\n",
       "   'employ',\n",
       "   'a',\n",
       "   'predefine',\n",
       "   'set',\n",
       "   'of',\n",
       "   'category',\n",
       "   'subcategorie',\n",
       "   'that',\n",
       "   'describe',\n",
       "   'purchase',\n",
       "   'type',\n",
       "   'and',\n",
       "   'transaction',\n",
       "   'however',\n",
       "   'these',\n",
       "   'method',\n",
       "   'apply',\n",
       "   'a',\n",
       "   'limited',\n",
       "   'predefine',\n",
       "   'set',\n",
       "   'of',\n",
       "   'static',\n",
       "   'class',\n",
       "   'which',\n",
       "   'restrict',\n",
       "   'the',\n",
       "   'ability',\n",
       "   'to',\n",
       "   'extend',\n",
       "   'classification',\n",
       "   'base',\n",
       "   'on',\n",
       "   'user',\n",
       "   'experience',\n",
       "   'when',\n",
       "   'encounter',\n",
       "   'new',\n",
       "   'previously',\n",
       "   'undefined',\n",
       "   'category',\n",
       "   'in',\n",
       "   'this',\n",
       "   'context',\n",
       "   'to',\n",
       "   'expand',\n",
       "   'the',\n",
       "   'possible',\n",
       "   'set',\n",
       "   'of',\n",
       "   'class',\n",
       "   'tag',\n",
       "   'to',\n",
       "   'label',\n",
       "   'a',\n",
       "   'transaction',\n",
       "   'we',\n",
       "   'develop',\n",
       "   'an',\n",
       "   'unsupervised',\n",
       "   'method',\n",
       "   'base',\n",
       "   'on',\n",
       "   'topic',\n",
       "   'taxonomy',\n",
       "   'taxonomy',\n",
       "   'be',\n",
       "   'very',\n",
       "   'useful',\n",
       "   'in',\n",
       "   'the',\n",
       "   'structural',\n",
       "   'and',\n",
       "   'semantic',\n",
       "   'analysis',\n",
       "   'of',\n",
       "   'topic',\n",
       "   'and',\n",
       "   'textual',\n",
       "   'datum',\n",
       "   'however',\n",
       "   'create',\n",
       "   'and',\n",
       "   'maintain',\n",
       "   'they',\n",
       "   'be',\n",
       "   'often',\n",
       "   'costly',\n",
       "   'and',\n",
       "   'challenging',\n",
       "   'to',\n",
       "   'scale',\n",
       "   'manually',\n",
       "   'therefore',\n",
       "   'recent',\n",
       "   'work',\n",
       "   'have',\n",
       "   'tackle',\n",
       "   'the',\n",
       "   'automatic',\n",
       "   'creation',\n",
       "   'and',\n",
       "   'expansion',\n",
       "   'of',\n",
       "   'topic',\n",
       "   'taxonomy',\n",
       "   'in',\n",
       "   'which',\n",
       "   'each',\n",
       "   'node',\n",
       "   'in',\n",
       "   'a',\n",
       "   'hierarchy',\n",
       "   'represent',\n",
       "   'a',\n",
       "   'conceptual',\n",
       "   'topic',\n",
       "   'compose',\n",
       "   'of',\n",
       "   'semantically',\n",
       "   'coherent',\n",
       "   'term',\n",
       "   'we',\n",
       "   'present',\n",
       "   'an',\n",
       "   'unsupervised',\n",
       "   'method',\n",
       "   'for',\n",
       "   'automatically',\n",
       "   'construct',\n",
       "   'and',\n",
       "   'expand',\n",
       "   'topic',\n",
       "   'taxonomy',\n",
       "   'with',\n",
       "   'instruction',\n",
       "   'base',\n",
       "   'LLMs',\n",
       "   'large',\n",
       "   'Language',\n",
       "   'model',\n",
       "   'in',\n",
       "   'a',\n",
       "   'Zero',\n",
       "   'Shot',\n",
       "   'manner',\n",
       "   'candidate',\n",
       "   'term',\n",
       "   'for',\n",
       "   'the',\n",
       "   'initial',\n",
       "   'version',\n",
       "   'of',\n",
       "   'the',\n",
       "   'taxonomy',\n",
       "   'be',\n",
       "   'obtain',\n",
       "   'use',\n",
       "   'topic',\n",
       "   'modeling',\n",
       "   'and',\n",
       "   'keyword',\n",
       "   'extraction',\n",
       "   'technique',\n",
       "   'then',\n",
       "   'we',\n",
       "   'apply',\n",
       "   'LLMs',\n",
       "   'to',\n",
       "   'post',\n",
       "   'process',\n",
       "   'the',\n",
       "   'result',\n",
       "   'term',\n",
       "   'create',\n",
       "   'a',\n",
       "   'hierarchy',\n",
       "   'and',\n",
       "   'add',\n",
       "   'new',\n",
       "   'term',\n",
       "   'to',\n",
       "   'an',\n",
       "   'exist',\n",
       "   'taxonomy',\n",
       "   'since',\n",
       "   'the',\n",
       "   'taxonomy',\n",
       "   'be',\n",
       "   'derive',\n",
       "   'from',\n",
       "   'a',\n",
       "   'corpus',\n",
       "   'of',\n",
       "   'unstructured',\n",
       "   'text',\n",
       "   'describe',\n",
       "   'niche',\n",
       "   'of',\n",
       "   'consume',\n",
       "   'habit',\n",
       "   'we',\n",
       "   'opt',\n",
       "   'to',\n",
       "   'investigate',\n",
       "   'the',\n",
       "   'use',\n",
       "   'of',\n",
       "   'LLMs',\n",
       "   'in',\n",
       "   'our',\n",
       "   'approach',\n",
       "   'llm',\n",
       "   'be',\n",
       "   'often',\n",
       "   'pre',\n",
       "   'train',\n",
       "   'on',\n",
       "   'a',\n",
       "   'large',\n",
       "   'corpus',\n",
       "   'of',\n",
       "   'text',\n",
       "   'allow',\n",
       "   'they',\n",
       "   'to',\n",
       "   'learn',\n",
       "   'contextual',\n",
       "   'representation',\n",
       "   'that',\n",
       "   'capture',\n",
       "   'the',\n",
       "   'intricacy',\n",
       "   'of',\n",
       "   'human',\n",
       "   'language',\n",
       "   'we',\n",
       "   'apply',\n",
       "   'our',\n",
       "   'method',\n",
       "   'to',\n",
       "   'a',\n",
       "   'private',\n",
       "   'dataset',\n",
       "   'of',\n",
       "   'transaction',\n",
       "   'of',\n",
       "   'a',\n",
       "   'retail',\n",
       "   'bank',\n",
       "   'enrich',\n",
       "   'with',\n",
       "   'scrape',\n",
       "   'datum',\n",
       "   'from',\n",
       "   'food',\n",
       "   'and',\n",
       "   'shopping',\n",
       "   'company',\n",
       "   'and',\n",
       "   'evaluate',\n",
       "   'the',\n",
       "   'result',\n",
       "   'taxonomy',\n",
       "   'quantitatively',\n",
       "   'the',\n",
       "   'generate',\n",
       "   'tag',\n",
       "   'of',\n",
       "   'our',\n",
       "   'topic',\n",
       "   'taxonomy',\n",
       "   'be',\n",
       "   'then',\n",
       "   'assign',\n",
       "   'to',\n",
       "   'the',\n",
       "   'bank',\n",
       "   'transaction',\n",
       "   'characterize',\n",
       "   'the',\n",
       "   'company',\n",
       "   'in',\n",
       "   'each',\n",
       "   'transaction',\n",
       "   'as',\n",
       "   'show',\n",
       "   'on',\n",
       "   'Figure',\n",
       "   '1',\n",
       "   'in',\n",
       "   'total',\n",
       "   '58',\n",
       "   'topic',\n",
       "   'taxonomy',\n",
       "   'be',\n",
       "   'create',\n",
       "   'for',\n",
       "   'the',\n",
       "   'food',\n",
       "   'category',\n",
       "   'and',\n",
       "   '6',\n",
       "   'for',\n",
       "   'the',\n",
       "   'shopping',\n",
       "   'category',\n",
       "   'a',\n",
       "   'two',\n",
       "   'step',\n",
       "   'quantitative',\n",
       "   'evaluation',\n",
       "   'be',\n",
       "   'conduct',\n",
       "   'on',\n",
       "   'a',\n",
       "   'subset',\n",
       "   'of',\n",
       "   'the',\n",
       "   'taxonomy',\n",
       "   'for',\n",
       "   'this',\n",
       "   'evaluation',\n",
       "   'we',\n",
       "   'select',\n",
       "   'the',\n",
       "   'topic',\n",
       "   'taxonomy',\n",
       "   'with',\n",
       "   'the',\n",
       "   'high',\n",
       "   'number',\n",
       "   'of',\n",
       "   'term',\n",
       "   'in',\n",
       "   'each',\n",
       "   'category',\n",
       "   'Brazilian',\n",
       "   'Cuisine',\n",
       "   'from',\n",
       "   'Food',\n",
       "   'and',\n",
       "   'Clothing',\n",
       "   'and',\n",
       "   'accessory',\n",
       "   'from',\n",
       "   'shopping',\n",
       "   'taxonomy',\n",
       "   'with',\n",
       "   'more',\n",
       "   'term',\n",
       "   'be',\n",
       "   'most',\n",
       "   'likely',\n",
       "   'to',\n",
       "   'result',\n",
       "   'in',\n",
       "   'a',\n",
       "   'deep',\n",
       "   'hierarchy',\n",
       "   'which',\n",
       "   'give',\n",
       "   'more',\n",
       "   'datum',\n",
       "   'for',\n",
       "   'evaluation',\n",
       "   'we',\n",
       "   'ask',\n",
       "   '12',\n",
       "   'volunteer',\n",
       "   'to',\n",
       "   'answer',\n",
       "   'a',\n",
       "   'two',\n",
       "   'part',\n",
       "   'form',\n",
       "   'which',\n",
       "   'assess',\n",
       "   'the',\n",
       "   'quality',\n",
       "   'of',\n",
       "   'the',\n",
       "   'create',\n",
       "   'taxonomy',\n",
       "   'and',\n",
       "   'the',\n",
       "   'quality',\n",
       "   'of',\n",
       "   'the',\n",
       "   'tag',\n",
       "   'assign',\n",
       "   'to',\n",
       "   'label',\n",
       "   'transaction',\n",
       "   'the',\n",
       "   'evaluation',\n",
       "   'show',\n",
       "   'an',\n",
       "   'average',\n",
       "   'coherence',\n",
       "   'of',\n",
       "   'tag',\n",
       "   'to',\n",
       "   'transaction',\n",
       "   'above',\n",
       "   '90',\n",
       "   'as',\n",
       "   'more',\n",
       "   'scrape',\n",
       "   'datum',\n",
       "   'from',\n",
       "   'food',\n",
       "   'and',\n",
       "   'shopping',\n",
       "   'company',\n",
       "   'be',\n",
       "   'add',\n",
       "   'to',\n",
       "   'the',\n",
       "   'retail',\n",
       "   'bank',\n",
       "   '’s',\n",
       "   'dataset',\n",
       "   'the',\n",
       "   'topic',\n",
       "   'taxonomy',\n",
       "   'will',\n",
       "   'need',\n",
       "   'to',\n",
       "   'be',\n",
       "   'update',\n",
       "   'to',\n",
       "   'include',\n",
       "   'new',\n",
       "   'term',\n",
       "   'we',\n",
       "   'use',\n",
       "   'LLMs',\n",
       "   'for',\n",
       "   'this',\n",
       "   'task',\n",
       "   'as',\n",
       "   'well',\n",
       "   'employ',\n",
       "   'commercial',\n",
       "   'llm',\n",
       "   'like',\n",
       "   'Gemini',\n",
       "   'Pro',\n",
       "   '1',\n",
       "   'and',\n",
       "   'GPT4',\n",
       "   '14',\n",
       "   'alongside',\n",
       "   'open',\n",
       "   'source',\n",
       "   'llm',\n",
       "   'option',\n",
       "   'such',\n",
       "   'as',\n",
       "   'LLaMA',\n",
       "   'Alpaca',\n",
       "   '7b',\n",
       "   '22',\n",
       "   'Phi-2',\n",
       "   '1',\n",
       "   'and',\n",
       "   'Mixtral',\n",
       "   '8x7B',\n",
       "   '9',\n",
       "   'we',\n",
       "   'showcase',\n",
       "   'their',\n",
       "   'result',\n",
       "   'in',\n",
       "   'both',\n",
       "   'taxonomy',\n",
       "   'creation',\n",
       "   'and',\n",
       "   'expansion',\n",
       "   'for',\n",
       "   'the',\n",
       "   'expansion',\n",
       "   'part',\n",
       "   'we',\n",
       "   'also',\n",
       "   'compare',\n",
       "   'our',\n",
       "   'method',\n",
       "   'to',\n",
       "   'exist',\n",
       "   'one',\n",
       "   'a',\n",
       "   'BERT',\n",
       "   'base',\n",
       "   'method',\n",
       "   'and',\n",
       "   'Musubu',\n",
       "   '20',\n",
       "   'on',\n",
       "   'the',\n",
       "   'SemEval',\n",
       "   'dataset',\n",
       "   'and',\n",
       "   'our',\n",
       "   'generate',\n",
       "   'taxonomy',\n",
       "   'as',\n",
       "   'well',\n",
       "   'Gemini',\n",
       "   'Pro',\n",
       "   'achieve',\n",
       "   'the',\n",
       "   'good',\n",
       "   'result',\n",
       "   'with',\n",
       "   'f1score',\n",
       "   'of',\n",
       "   '89',\n",
       "   'and',\n",
       "   '70',\n",
       "   'for',\n",
       "   'parent',\n",
       "   'node',\n",
       "   'prediction',\n",
       "   'on',\n",
       "   'the',\n",
       "   'Food',\n",
       "   'and',\n",
       "   'shopping',\n",
       "   'taxonomy',\n",
       "   'respectively',\n",
       "   '1',\n",
       "   'https://huggingface.co/microsoft/phi-2',\n",
       "   'the',\n",
       "   'remainder',\n",
       "   'of',\n",
       "   'the',\n",
       "   'paper',\n",
       "   'be',\n",
       "   'structure',\n",
       "   'as',\n",
       "   'follow',\n",
       "   'section',\n",
       "   '2',\n",
       "   'review',\n",
       "   'the',\n",
       "   'related',\n",
       "   'work',\n",
       "   'highlight',\n",
       "   'exist',\n",
       "   'approach',\n",
       "   'section',\n",
       "   '3',\n",
       "   'provide',\n",
       "   'the',\n",
       "   'necessary',\n",
       "   'background',\n",
       "   'lay',\n",
       "   'the',\n",
       "   'foundation',\n",
       "   'for',\n",
       "   'our',\n",
       "   'methodology',\n",
       "   'and',\n",
       "   'contextualize',\n",
       "   'our',\n",
       "   'contribution',\n",
       "   'section',\n",
       "   '4',\n",
       "   'detail',\n",
       "   'the',\n",
       "   'dataset',\n",
       "   'construction',\n",
       "   'process',\n",
       "   'explain',\n",
       "   'how',\n",
       "   'we',\n",
       "   'enrich',\n",
       "   'and',\n",
       "   'prepare',\n",
       "   'the',\n",
       "   'datum',\n",
       "   'for',\n",
       "   'the',\n",
       "   'taxonomy',\n",
       "   'construction',\n",
       "   'in',\n",
       "   'Section',\n",
       "   '5',\n",
       "   'we',\n",
       "   'describe',\n",
       "   'the',\n",
       "   'creation',\n",
       "   'of',\n",
       "   'the',\n",
       "   'taxonomy',\n",
       "   'outline',\n",
       "   'the',\n",
       "   'method',\n",
       "   'use',\n",
       "   'to',\n",
       "   'generate',\n",
       "   'they',\n",
       "   'section',\n",
       "   '6',\n",
       "   'discuss',\n",
       "   'the',\n",
       "   'expansion',\n",
       "   'of',\n",
       "   'the',\n",
       "   'taxonomy',\n",
       "   'demonstrate',\n",
       "   'how',\n",
       "   'they',\n",
       "   'can',\n",
       "   'be',\n",
       "   'dynamically',\n",
       "   'extend',\n",
       "   'to',\n",
       "   'accommodate',\n",
       "   'new',\n",
       "   'category',\n",
       "   'section',\n",
       "   '7',\n",
       "   'focus',\n",
       "   'on',\n",
       "   ...]},\n",
       " {'titulo': 'Enhancing widget recognition for automated Android testing using Computer Vision techniques',\n",
       "  'informacoes_url': '',\n",
       "  'idioma': 'english',\n",
       "  'storage_key': '../articles/original/english/985-24752-1-10-20240923.pdf',\n",
       "  'author': 'Yadini Pérez López; Laís Dib Albuquerque; Gilmar J. F. Costa Júnior; Daniel Lopes Xavier; Leticia Balbi; Juan G. Colonna; and Richand Degaki',\n",
       "  'data_publicacao': '11-09-2024',\n",
       "  'resumo': 'Widget recognition is crucial for automated Android black box testing. Over the past ten years, different industrial tools and academic works have been available for identifying Graphical User Interface (GUI) components in Android screens. Traditional identification methods, like GUI hierarchy parsing, often struggle with dynamic content and complex structures. In contrast, Computer Vision (CV) techniques provide greater robustness and flexibility to adapt to different screen resolutions, design specifications, and patterns. However, the CV-based solutions available are still limited concerning the variety of widgets that can be recognized. Moreover, the current identification of GUI components mainly relies on classification, which can lead to ambiguous lists with repeated elements. In this paper, we combine different CV-based techniques to extract contextbased descriptions for each widget, to enhance the identification process by going beyond class recognition for describing widgets. We also implemented two primary CV-based approaches for widget recognition: Object Detection combined with Classification, and a One-Stage Recognition method. We trained and evaluated the approaches on a custom 105 classes widget dataset. Moreover, we present a Computer Vision-based method for describing widgets using their contextual meaning on Android screen captures. ###',\n",
       "  'keywords': 'Automated Android Testing, Test Portability, Object Recognition, Object Classification, Optical Character Recognition, Graphical User Interface component, Widget Recognition',\n",
       "  'referencias': ['[1] A. A. Abdelhamid, S. Alotaibi, and A. Mousa. 2020. Deep learning-based prototyping of android GUI from hand-drawn mockups. *IET Software* 14 (2020),\\n816–824. Issue 7. https://doi.org/10.1049/iet-sen.2019.0378',\n",
       "   '[2] David Adamo, Md Khorrom Khan, Sreedevi Koppula, and Renée Bryce. 2018.\\nReinforcement learning for Android GUI testing. In *Proceedings of the 9th ACM*\\n*SIGSOFT International Workshop on Automating TEST Case Design, Selection, and*\\n*Evaluation* (Lake Buena Vista, FL, USA) *(A-TEST 2018)* . Association for Computing\\nMachinery, New York, NY, USA, 2–8. https://doi.org/10.1145/3278186.3278187',\n",
       "   '[3] Domenico Amalfitano, Anna Rita Fasolino, Porfirio Tramontana, Salvatore\\nDe Carmine, and Atif M. Memon. 2012. Using GUI ripping for automated\\ntesting of Android applications. In *Proceedings of the 27th IEEE/ACM Interna-*\\n*tional Conference on Automated Software Engineering* (Essen, Germany) *(ASE*\\n*’12)* . Association for Computing Machinery, New York, NY, USA, 258–261.\\nhttps://doi.org/10.1145/2351676.2351717',\n",
       "   '[4] Domenico Amalfitano, Vincenzo Riccio, Nicola Amatucci, Vincenzo De Simone,\\nand Anna Rita Fasolino. 2019. Combining Automated GUI Exploration of Android\\napps with Capture and Replay through Machine Learning. *Information and*\\n*Software Technology* 105 (2019), 95–116. https://doi.org/10.1016/j.infsof.2018.08.\\n007',\n",
       "   '[5] Luca Ardito, Riccardo Coppola, Simone Leonardi, Maurizio Morisio, and Ugo Buy.\\n2020. Automated Test Selection for Android Apps Based on APK and Activity\\nClassification. *IEEE Access* 8 (2020), 187648–187670. https://doi.org/10.1109/\\nACCESS.2020.3029735',\n",
       "   '[6] Tanzirul Azim and Iulian Neamtiu. 2013. Targeted and depth-first exploration\\nfor systematic testing of android apps. In *Proceedings of the 2013 ACM SIGPLAN*\\n*International Conference on Object Oriented Programming Systems Languages*\\n*&amp; Applications* (Indianapolis, Indiana, USA) *(OOPSLA ’13)* . Association for\\nComputing Machinery, New York, NY, USA, 641–660. https://doi.org/10.1145/\\n2509136.2509549',\n",
       "   '[7] Young-Min Baek and Doo-Hwan Bae. 2016. Automated model-based Android\\nGUI testing using multi-level GUI comparison criteria. In *Proceedings of the 31st*\\n*IEEE/ACM International Conference on Automated Software Engineering* (Singapore, Singapore) *(ASE ’16)* . Association for Computing Machinery, New York,\\nNY, USA, 238–249. https://doi.org/10.1145/2970276.2970313',\n",
       "   '[8] Sara Banian, Kai Li, Chaima Jemmali, Casper Harteveld, Yun Fu, and Magy\\nEl-Nasr. 2021. *VINS: Visual Search for Mobile User Interface Design* . https:\\n//github.com/sbunian/VINS',\n",
       "   '[9] Sara Banian, Kai Li, Chaima Jemmali, Casper Harteveld, Yun Fu, and Magy\\nEl-Nasr. 2021. VINS: Visual Search for Mobile User Interface Design. 1–14.\\nhttps://doi.org/10.1145/3411764.3445762',\n",
       "   '[10] Manuele Barraco, Matteo Stefanini, Marcella Cornia, Silvia Cascianelli, Lorenzo\\nBaraldi, and Rita Cucchiara. 2022. CaMEL: Mean Teacher Learning for Image\\nCaptioning. arXiv:2202.10492\\n\\n\\n141\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil López et al.',\n",
       "   '[11] Jieshan Chen, Chunyang Chen, Zhenchang Xing, Xiwei Xu, Liming Zhu, Guoqiang Li, and Jinshui Wang. 2020. Unblind your apps: predicting naturallanguage labels for mobile GUI components by deep learning. In *Proceedings*\\n*of the ACM/IEEE 42nd International Conference on Software Engineering* . ACM.\\nhttps://doi.org/10.1145/3377811.3380327',\n",
       "   '[12] Jing Cheng, Dingmei Tan, Tao Zhang, Aodi Wei, Jingyi Chen, and Ali Khattak\\nHasan. 2022. YOLOv5-MGC: GUI Element Identification for Mobile Applications\\nBased on Improved YOLOv5. *Mob. Inf. Syst.* 2022 (jan 2022), 9 pages. https:\\n//doi.org/10.1155/2022/8900734',\n",
       "   '[13] Eliane Collins, Arilo Neto, Auri Vincenzi, and José Maldonado. 2021. Deep\\nReinforcement Learning based Android Application GUI Testing. In *Proceedings*\\n*of the XXXV Brazilian Symposium on Software Engineering* (Joinville, Brazil)\\n*(SBES ’21)* . Association for Computing Machinery, New York, NY, USA, 186–194.\\nhttps://doi.org/10.1145/3474624.3474634',\n",
       "   '[14] Richard Hada Degaki, Juan Gabriel Colonna, Yadini Lopez, José Reginaldo Carvalho, and Edson Silva. 2022. Real Time Detection of Mobile Graphical User\\nInterface Elements Using Convolutional Neural Networks. In *Proceedings of*\\n*the Brazilian Symposium on Multimedia and the Web* (Curitiba, Brazil) *(WebMe-*\\n*dia ’22)* . Association for Computing Machinery, New York, NY, USA, 159–167.\\nhttps://doi.org/10.1145/3539637.3558044',\n",
       "   '[15] Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan,\\nYang Li, Jeffrey Nichols, and Ranjitha Kumar. 2017. Rico: A Mobile App Dataset\\nfor Building Data-Driven Design Applications. In *Proceedings of the 30th Annual*\\n*ACM Symposium on User Interface Software and Technology* (Québec City, QC,\\nCanada) *(UIST ’17)* . Association for Computing Machinery, New York, NY, USA,\\n845–854. https://doi.org/10.1145/3126594.3126651',\n",
       "   '[16] Giovanni Denaro, Luca Guglielmo, Leonardo Mariani, and Oliviero Riganelli.\\n2019. GUI testing in production: challenges and opportunities. In *Companion*\\n*Proceedings of the 3rd International Conference on the Art, Science, and Engineering*\\n*of Programming* (Genova, Italy) *(Programming ’19)* . Association for Computing\\nMachinery, New York, NY, USA, Article 18, 3 pages. https://doi.org/10.1145/\\n3328433.3328452',\n",
       "   '[17] Android Developers. 2024. *Espresso* . https://developer.android.com/training/\\ntesting/espresso',\n",
       "   '[18] Android Developers. 2024. *UI Automator* . https://developer.android.com/training/\\ntesting/other-components/ui-automator',\n",
       "   '[19] Juha Eskonen, Julen Kahles, and Joel Reijonen. 2020. Automating GUI Testing\\nwith Image-Based Deep Reinforcement Learning. In *2020 IEEE International*\\n*Conference on Autonomic Computing and Self-Organizing Systems (ACSOS)* . 160–\\n167. https://doi.org/10.1109/ACSOS49614.2020.00038',\n",
       "   '[20] Anna Esparcia-Alcazar, F. (Francisco) Almenar, Mirella Martınez, U. (Urko)\\nRueda, and T.E.J. Vos. 2016. Q-learning strategies for action selection in the\\nTESTAR automated testing tool. In *Proceedings of the 6TH International Con-*\\n*ference on Metaheuristics and Nature Inspired Computing* . 174–180. https:\\n//meta2016.sciencesconf.org/ 6th International Conference on Metaheuristic and\\nNature inspired Computing, META 2016; Conference date: 27-10-2016 Through\\n31-10-2016.',\n",
       "   '[21] OpenJS Foundation. 2012. *Appium* . https://appium.io/',\n",
       "   '[22] Jerry Gao, ShiTing Li, Chuanqi Tao, Yejun He, Amrutha Pavani Anumalasetty,\\nErica Wilson Joseph, Akshata Hatwar Kumbashi Sripathi, and Himabindu Nayani.\\n2022. An Approach to GUI Test Scenario Generation Using Machine Learning.\\nIn *2022 IEEE International Conference On Artificial Intelligence Testing (AITest)* .\\n79–86. https://doi.org/10.1109/AITest55621.2022.00020',\n",
       "   '[23] Xin Gao, Sundaresh Ram, and Jeffrey J. Rodríguez. 2020. Object sieving and\\nmorphological closing to reduce false detections in wide-area aerial imagery.\\n*CoRR* abs/2010.15260 (2020). arXiv:2010.15260 https://arxiv.org/abs/2010.15260',\n",
       "   '[24] Data Driven Design Group. 2024. *Rico: A Mobile App Dataset for Building Data-*\\n*Driven Design Applications* . http://www.interactionmining.org/rico.html',\n",
       "   '[25] Shuai Hao, Bin Liu, Suman Nath, William G.J. Halfond, and Ramesh Govindan.\\n2014. PUMA: programmable UI-automation for large-scale dynamic analysis\\nof mobile apps. In *Proceedings of the 12th Annual International Conference on*\\n*Mobile Systems, Applications, and Services* (Bretton Woods, New Hampshire,\\nUSA) *(MobiSys ’14)* . Association for Computing Machinery, New York, NY, USA,\\n204–217. https://doi.org/10.1145/2594368.2594390',\n",
       "   '[26] L. V. Haoyin. 2017. Automatic android application GUI testing—-A random walk\\napproach. In *2017 International Conference on Wireless Communications, Signal*\\n*Processing and Networking (WiSPNET)* . 72–76. https://doi.org/10.1109/WiSPNET.\\n2017.8299722',\n",
       "   '[27] Saad Hassan, Manan Arya, Ujjwal Bhardwaj, and Silica Kole. 2018. Extraction\\nand classification of user interface components from an image. *International*\\n*Journal of Pure and Applied Mathematics* 118, 24 (2018), 1–16.',\n",
       "   '[28] Gang Hu, Linjie Zhu, and Junfeng Yang. 2018. AppFlow: using machine learning\\nto synthesize robust, reusable UI tests. In *Proceedings of the 2018 26th ACM Joint*\\n*Meeting on European Software Engineering Conference and Symposium on the*\\n*Foundations of Software Engineering* (Lake Buena Vista, FL, USA) *(ESEC/FSE*\\n*2018)* . Association for Computing Machinery, New York, NY, USA, 269–282.\\nhttps://doi.org/10.1145/3236024.3236055',\n",
       "   '[29] Jianjun Huang, Zhichun Li, Xusheng Xiao, Zhenyu Wu, Kangjie Lu, Xiangyu\\nZhang, and Guofei Jiang. 2015. SUPOR: Precise and scalable sensitive user input\\ndetection for android apps. In *Proceedings of the 24th USENIX Conference on*\\n*Security Symposium* (Washington, D.C.) *(SEC’15)* . USENIX Association, USA,\\n977–992.',\n",
       "   '[30] IBM. 2021. *Rational Functional Tester* . https://www.ibm.com/support/pages/\\nrational-functional-tester-v1013',\n",
       "   '[31] K. Jaganeshwari and S. Djodilatchoumy. 2021. A Novel approach of GUI Mapping\\nwith image based widget detection and classification. In *2021 2nd International*\\n*Conference on Intelligent Engineering and Management (ICIEM)* . 342–346. https:\\n//doi.org/10.1109/ICIEM51511.2021.9445281',\n",
       "   '[32] Jinseong Jeon and Jeffrey Foster. 2012. Troyd: Integration Testing for Android.\\n*Technical Report CS-TR-5013, Department of Computer Science, University of Mary-*\\n*land, College Park* (2012).',\n",
       "   '[33] Yavuz Koroglu, Alper Sen, Ozlem Muslu, Yunus Mete, Ceyda Ulker, Tolga Tanriverdi, and Yunus Donmez. 2018. QBE: QLearning-Based Exploration of Android\\nApplications. In *2018 IEEE 11th International Conference on Software Testing, Veri-*\\n*fication and Validation (ICST)* . 105–115. https://doi.org/10.1109/ICST.2018.00020',\n",
       "   '[34] Wing Lam, Zhengkai Wu, Dengfeng Li, Wenyu Wang, Haibing Zheng, Hui Luo,\\nPeng Yan, Yuetang Deng, and Tao Xie. 2017. Record and replay for Android: are\\nwe there yet in industrial cases?. In *Proceedings of the 2017 11th Joint Meeting*\\n*on Foundations of Software Engineering* (Paderborn, Germany) *(ESEC/FSE 2017)* .\\nAssociation for Computing Machinery, New York, NY, USA, 854–859. https:\\n//doi.org/10.1145/3106237.3117769',\n",
       "   '[35] Yang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, and Zhiwei Guan. 2020.\\nWidget Captioning: Generating Natural Language Description for Mobile User\\nInterface Elements. arXiv:2010.04295',\n",
       "   '[36] Mario Linares-Vásquez, Kevin Moran, and Denys Poshyvanyk. 2017. Continuous,\\nEvolutionary and Large-Scale: A New Perspective for Automated Mobile App\\nTesting. 399–410. https://doi.org/10.1109/ICSME.2017.27',\n",
       "   '[37] Yadini Pérez López, Laís Dib Albuquerque, Gilmar Jóia de F. Costa Júnior,\\nDaniel Lopes Xavier, Juan David Ochoa, and Denizard Dimitri Camargo. 2023.\\nAdapting RICO Dataset for Boosting Graphical User Interface Component Classification for Automated Android Testing. In *2023 10th International Conference on*\\n*Soft Computing Machine Intelligence (ISCMI)* . 118–123. https://doi.org/10.1109/\\nISCMI59957.2023.10458576',\n",
       "   '[38] Yadini Pérez López, Juan G. Colonna, Edson De Araujo Silva, Richard Hada\\nDegaki, and Javier Martinez Silva. 2022. Q-funcT: A Reinforcement Learning\\nApproach for Automated Black Box Functionality Testing. In *2022 IEEE 2nd*\\n*International Conference on Software Engineering and Artificial Intelligence (SEAI)* .\\n119–123. https://doi.org/10.1109/SEAI55746.2022.9832177',\n",
       "   '[39] Ying Ma, ChuYi Yu, and Ming Yan. 2022. Icon Label Generation for Mobile Applications by Mean Teacher Learning. https://doi.org/10.21203/rs.3.rs-1888657/v1',\n",
       "   '[40] Leonardo Mariani, Mauro Pezzè, Oliviero Riganelli, and Mauro Santoro. 2011.\\nAutoBlackTest: a tool for automatic black-box testing. In *2011 33rd International*\\n*Conference on Software Engineering (ICSE)* . 1013–1015. https://doi.org/10.1145/\\n1985793.1985979',\n",
       "   '[41] Forough Mehralian, Navid Salehnamadi, and Sam Malek. 2021. Data-driven\\naccessibility repair revisited: on the effectiveness of generating labels for icons in\\nAndroid apps. In *Proceedings of the 29th ACM Joint Meeting on European Software*\\n*Engineering Conference and Symposium on the Foundations of Software Engineering*\\n(Athens, Greece) *(ESEC/FSE 2021)* . Association for Computing Machinery, New\\nYork, NY, USA, 107–118. https://doi.org/10.1145/3468264.3468604',\n",
       "   '[42] Kevin Moran, Carlos Bernal-Cárdenas, Michael Curcio, Richard Bonett, and\\nDenys Poshyvanyk. 2018. *The ReDraw Dataset: A Set of Android Screenshots, GUI*\\n*Metadata, and Labeled Images of GUI Components* . https://zenodo.org/records/\\n2530277',\n",
       "   '[43] Kevin Moran, Carlos Bernal-Cárdenas, Michael Curcio, Richard Bonett, and\\nDenys Poshyvanyk. 2020. Machine Learning-Based Prototyping of Graphical\\nUser Interfaces for Mobile Apps. *IEEE Transactions on Software Engineering* 46, 2\\n(2020), 196–221. https://doi.org/10.1109/TSE.2018.2844788',\n",
       "   '[44] Seong-Guk Nam and Yeong-Seok Seo. 2023. GUI Component Detection-Based\\nAutomated Software Crash Diagnosis. *Electronics* 12, 11 (2023). https://doi.org/\\n10.3390/electronics12112382',\n",
       "   '[45] Clemens Neudecker, Konstantin Baierer, Mike Gerber, Christian Clausner, Apostolos Antonacopoulos, and Stefan Pletschacher. 2021. A survey of OCR evaluation tools and metrics. In *Proceedings of the 6th International Workshop on*\\n*Historical Document Imaging and Processing* (Lausanne, Switzerland) *(HIP ’21)* .\\nAssociation for Computing Machinery, New York, NY, USA, 13–18. https:\\n//doi.org/10.1145/3476887.3476888',\n",
       "   '[46] Tuan Anh Nguyen and Christoph Csallner. 2015. Reverse Engineering Mobile Application User Interfaces with REMAUI (T). In *2015 30th IEEE/ACM*\\n*International Conference on Automated Software Engineering (ASE)* . 248–259.\\nhttps://doi.org/10.1109/ASE.2015.32',\n",
       "   '[47] Rafael Padilla, Sergio L. Netto, and Eduardo A. B. da Silva. 2020. A Survey\\non Performance Metrics for Object-Detection Algorithms. In *2020 International*\\n*Conference on Systems, Signals and Image Processing (IWSSIP)* . 237–242. https:\\n//doi.org/10.1109/IWSSIP48289.2020.9145130\\n\\n\\n142\\n\\n\\n-----\\n\\nEnhancing widget recognition for automated Android testing using Computer Vision techniques WebMedia’2024, Juiz de Fora, Brazil',\n",
       "   '[48] Minxue Pan, An Huang, Guoxin Wang, Tian Zhang, and Xuandong Li. 2020. Reinforcement learning based curiosity-driven testing of Android applications. In\\n*Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing*\\n*and Analysis* (Virtual Event, USA) *(ISSTA 2020)* . Association for Computing Machinery, New York, NY, USA, 153–164. https://doi.org/10.1145/3395363.3397354',\n",
       "   '[49] Mahmood R, Mirzaei N, and Malek S. 2014. Automated Test Generation for Graphical User Interfaces: Challenges and Opportunities. *ACM Computing Surveys*\\n*(CSUR)* (2014), 45.',\n",
       "   '[50] Dillon Reis, Jordan Kupec, Jacqueline Hong, and Ahmad Daoudi. 2024. Real-Time\\nFlying Object Detection with YOLOv8. (2024). https://doi.org/10.48550/arXiv.\\n2305.09972',\n",
       "   '[51] Ariel Rosenfeld, Odaya Kardashov, and Orel Zang. 2018. Automation of Android\\nApplications Functional Testing Using Machine Learning Activities Classification.\\nIn *2018 IEEE/ACM 5th International Conference on Mobile Software Engineering*\\n*and Systems (MOBILESoft)* . 122–132.',\n",
       "   '[52] Xiaolei Sun, Tongyu Li, and Jianfeng Xu. 2020. UI Components Recognition\\nSystem Based On Image Understanding. In *2020 IEEE 20th International Conference*\\n*on Software Quality, Reliability and Security Companion (QRS-C)* . 65–71. https:\\n//doi.org/10.1109/QRS-C51114.2020.00022',\n",
       "   '[53] Amrita Vazirani. 2022. *What is Android UI Testing?* https://www.browserstack.\\ncom/guide/what-is-android-ui-testing',\n",
       "   '[54] Tuyet Vuong and Shingo Takada. 2019. Semantic analysis for deep Q-network\\nin android GUI testing. In *Proceedings - SEKE 2019 (Proceedings of the Interna-*\\n*tional Conference on Software Engineering and Knowledge Engineering, SEKE)* .\\nKnowledge Systems Institute Graduate School, 123–128. https://doi.org/10.\\n18293/SEKE2019-080 31st International Conference on Software Engineering\\nand Knowledge Engineering, SEKE 2019 ; Conference date: 10-07-2019 Through\\n12-07-2019.',\n",
       "   '[55] Thi Anh Tuyet Vuong and Shingo Takada. 2018. A reinforcement learning\\nbased approach to automated testing of Android applications. In *Proceedings of*\\n*the 9th ACM SIGSOFT International Workshop on Automating TEST Case Design,*\\n*Selection, and Evaluation* (Lake Buena Vista, FL, USA) *(A-TEST 2018)* . Association\\n\\n\\nfor Computing Machinery, New York, NY, USA, 31–37. https://doi.org/10.1145/\\n3278186.3278191',\n",
       "   '[56] Bryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman, and Yang\\nLi. 2021. Screen2Words: Automatic Mobile UI Summarization with Multimodal\\n\\nLearning. arXiv:2108.03353',\n",
       "   '[57] Thomas D. White, Gordon Fraser, and Guy J. Brown. 2019. Improving random\\nGUI testing with image-based widget detection. In *Proceedings of the 28th ACM*\\n*SIGSOFT International Symposium on Software Testing and Analysis* (Beijing,\\nChina) *(ISSTA 2019)* . Association for Computing Machinery, New York, NY, USA,\\n307–317. https://doi.org/10.1145/3293882.3330551',\n",
       "   '[58] Xusheng Xiao, Xiaoyin Wang, Zhihao Cao, Hanlin Wang, and Peng Gao. 2019.\\nIconIntent: Automatic Identification of Sensitive UI Widgets Based on Icon Classification for Android Apps. In *2019 IEEE/ACM 41st International Conference on*\\n*Software Engineering (ICSE)* . 257–268. https://doi.org/10.1109/ICSE.2019.00041',\n",
       "   '[59] Mulong Xie, Sidong Feng, Zhenchang Xing, Jieshan Chen, and Chunyang Chen.\\n2020. UIED: a hybrid tool for GUI element detection. In *Proceedings of the 28th*\\n*ACM Joint Meeting on European Software Engineering Conference and Symposium*\\n*on the Foundations of Software Engineering* (Virtual Event, USA) *(ESEC/FSE 2020)* .\\nAssociation for Computing Machinery, New York, NY, USA, 1655–1659. https:\\n//doi.org/10.1145/3368089.3417940',\n",
       "   '[60] Feng Xue. 2020. Automated mobile apps testing from visual perspective. In *Pro-*\\n*ceedings of the 29th ACM SIGSOFT International Symposium on Software Testing*\\n*and Analysis* (Virtual Event, USA) *(ISSTA 2020)* . Association for Computing Machinery, New York, NY, USA, 577–581. https://doi.org/10.1145/3395363.3402644',\n",
       "   '[61] Shengcheng Yu, Chunrong Fang, Tongyu Li, Mingzhe Du, Xuan Li, Jing Zhang,\\nYexiao Yun, Xu Wang, and Zhenyu Chen. 2021. Automated Mobile App Test Script\\nIntent Generation via Image and Code Understanding. arXiv:2107.05165 [cs.SE]',\n",
       "   '[62] Xiaoyi Zhang, Lilian de Greef, Amanda Swearngin, Samuel White, Kyle Murray,\\nLisa Yu, Qi Shan, Jeffrey Nichols, Jason Wu, Chris Fleizach, Aaron Everitt, and\\nJeffrey P. Bigham. 2021. Screen Recognition: Creating Accessibility Metadata for\\nMobile Applications from Pixels. arXiv:2101.04893\\n\\n\\n143\\n\\n\\n-----'],\n",
       "  'text': '# **Enhancing widget recognition for automated Android testing** **using Computer Vision techniques**\\n\\n## Yadini Pérez López [∗]\\n#### yadini.lopez@sidia.com Sidia Research and Development Institute Manaus, AM\\n## Daniel Lopes Xavier\\n#### daniel.xavier@sidia.com Sidia Research and Development Institute Manaus, AM\\n### **ABSTRACT**\\n\\n## Laís Dib Albuquerque\\n#### lais.albuquerque@sidia.com Sidia Research and Development Institute Manaus, AM\\n## Leticia Balbi [†]\\n#### leticia.balbi@sidia.com Sidia Research and Development Institute Manaus, AM\\n## Richand Degaki\\n#### rhd@icomp.ufam.edu.br Federal University of Amazonas Manaus, AM\\n\\n## Gilmar J. F. Costa Júnior\\n#### gilmar.junior@sidia.com Sidia Research and Development Institute Manaus, AM\\n## Juan G. Colonna\\n#### juancolonna@icomp.ufam.edu.br Federal University of Amazonas Manaus, AM\\n\\n\\nWidget recognition is crucial for automated Android black box testing. Over the past ten years, different industrial tools and academic\\nworks have been available for identifying Graphical User Interface\\n(GUI) components in Android screens. Traditional identification\\nmethods, like GUI hierarchy parsing, often struggle with dynamic\\ncontent and complex structures. In contrast, Computer Vision (CV)\\ntechniques provide greater robustness and flexibility to adapt to different screen resolutions, design specifications, and patterns. However, the CV-based solutions available are still limited concerning\\nthe variety of widgets that can be recognized. Moreover, the current\\nidentification of GUI components mainly relies on classification,\\nwhich can lead to ambiguous lists with repeated elements. In this\\npaper, we combine different CV-based techniques to extract contextbased descriptions for each widget, to enhance the identification\\nprocess by going beyond class recognition for describing widgets.\\nWe also implemented two primary CV-based approaches for widget\\nrecognition: Object Detection combined with Classification, and\\na One-Stage Recognition method. We trained and evaluated the\\napproaches on a custom 105 classes widget dataset. Moreover, we\\npresent a Computer Vision-based method for describing widgets\\nusing their contextual meaning on Android screen captures.\\n### **KEYWORDS**\\n\\nAutomated Android Testing, Test Portability, Object Recognition,\\nObject Classification, Optical Character Recognition, Graphical\\nUser Interface component, Widget Recognition\\n\\n∗ Correspondence author.\\n\\n- Current e-mail: leticiabalbisv@gmail.com.\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n© 2024 SBC – Brazilian Computing Society.\\nISSN 2966-2753\\n\\n### **1 INTRODUCTION**\\n\\nAutomated testing of Android applications is a critical component\\nin ensuring the quality and reliability of software. One key aspect\\nof this process is widget recognition, which involves identifying\\nand interacting with various elements of a Graphical User Interface\\n(GUI) [ 36 ]. Effective widget recognition allows testing frameworks\\nto simulate user actions, verify the presence and state of User Interface (UI) components, and ensure that applications respond correctly to user inputs. This is essential for detecting and diagnosing\\nissues that might not be apparent through manual testing alone,\\nultimately leading to higher-quality software [34].\\nCurrent approaches to widget recognition primarily fall into\\ntwo categories: GUI hierarchy parsing and Computer Vision (CV)\\ntechniques. The first one involves extracting and analyzing the\\nstructural information of the application’s UI, while CV techniques\\nrely on image processing to recognize widgets directly from screenshots [ 4, 17, 31, 33, 40 ]. While GUI hierarchy parsing is intuitive\\nand straightforward, it often struggles with dynamic content and\\ncomplex structures, leading to incomplete or inaccurate results.\\nOn the other hand, CV-based methods offer greater flexibility and\\nrobustness, particularly in environments with frequent interface\\nchanges or non-standardized structures [37].\\nIn this paper, we explore and compare two primary approaches\\nto widget recognition: Object Detection combined with Classification, and a One-Stage Recognition method. We also address how\\nwe generated two datasets tailored for deep learning widget classification and recognition. Our data and recognition approaches\\nfocus on identifying a wide variety of widgets, up to 105 different classes, and generating UI context-based descriptions for each\\nwidget, attempting to expand the scope of automated black box\\nAndroid testing [37].\\nThe main contributions of our work are:\\n\\n\\n133\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil López et al.\\n\\n**Figure 1:** Two main approaches identified in the literature and industrial tools.\\n\\n\\n(1) We have trained two widget recognition approaches using\\na dataset of 105 classes of GUI components and compared\\ntheir performance.\\n(2) We have trained and evaluated a widget identification method\\nthat is not limited to class recognition but generates contextbased information for complementing the widget descrip\\ntion.\\n\\nTo the best of our knowledge, this is the first widget identification\\napproach (recognition + smart description) that focus on identifying\\na larger variety of widgets, up to 105 different classes, and generates\\na context-based description for each widget identified.\\nThe rest of the paper is organized as follows: section 2 reviews\\nthe main knowledge and related works with this research. Section\\n3 explains our methodology, detailing each stage of the widget\\nidentification process, including recognition, smart description,\\nand the datasets used. Section 4 presents the evaluation of our\\nmethods, comparing the two recognition techniques and assessing\\nthe performance of our smart descriptor method. We also address\\nfive research questions to demonstrate our contributions. Finally,\\nsections 5 and 6 provide our final considerations and conclusions,\\nrespectively.\\n### **2 BACKGROUND AND RELATED WORKS**\\n\\nWidget recognition is crucial for Android testing automation tasks\\nsuch as GUI and functional verification. It allows the testing framework to interact with and validate the behavior of the application’s\\nUI elements [ 53 ]. By recognizing widgets, automated tests can simulate user actions, verify the presence and state of UI components,\\nand ensure that the app responds correctly to user inputs. This level\\nof interaction is essential for issues detecting and diagnosing that\\nmight not be apparent through manual testing alone, leading to\\nhigher quality and more reliable software. Moreover, widget recognition facilitates the creation of more robust and maintainable test\\n\\nscripts, as the tests can adapt to cross-device-release changes in the\\nUI layout or design without requiring extensive manual updates\\n\\n[16].\\nFurthermore, maximizing the number of widget types that a\\nGUI recognizer can identify is paramount for automated Android\\ntesting, as it directly influences the comprehensiveness and accuracy of the testing process. Diverse widget recognition ensures the\\ntesting framework can interact with all elements of an application’s\\ninterface, from basic buttons and text fields to complex custom\\nwidgets and dynamic content. Limitations in recognizing certain\\n\\n\\nwidget types can lead to incomplete testing and overlooked bugs\\n\\n[ 3 ]. By expanding the range of recognized widget types, developers\\ncan ensure that their automated tests cover a wider spectrum of UI\\nelements, thereby improving the overall quality and stability of the\\napplication [49].\\nNowadays, several industrial and academic solutions are available for automated GUI testing. These solutions were implemented\\nfollowing two main approaches: GUI hierarchy parsing and Computer Vision (CV) techniques for widget recognition. Moreover,\\nsome of these approaches extract the contextual meaning of the\\nGUI elements, resulting in more detailed and complete identifiers.\\n### **2.1 GUI hierarchy parsing for widget** **identification**\\n\\nComponent recognition through metadata, such as dump hierarchies, leverages the structural information of the application’s UI\\nto identify and interact with various widgets. This method relies on\\nparsing the UI’s XML or other hierarchical representations to extract widget properties and relationships, allowing precise targeting\\nand manipulation during testing. This can be done using software\\n\\n[ 18, 21, 30, 32 ] or employed by frameworks and tools responsible\\nfor assisting in Android testing [ 2, 4 – 7, 13, 19, 20, 25, 26, 28, 33, 38,\\n40, 46, 48, 51, 54, 55, 58].\\nAlthough GUI hierarchy parsing for widget recognition is an\\nintuitive and easy-to-implement solution, it has several notable limitations. First, it often struggles with dynamic content, and complex\\nand deeply nested structures, leading to incomplete or inaccurate\\nparsing results [ 36 ]. Moreover, this approach can be significantly\\nhindered by variations in widget naming conventions and inconsistencies in hierarchical organization across different applications\\n\\n[ 34 ]. Additionally, it tends to perform poorly when encountering\\ncustom widgets or those that deviate from standard GUI frameworks, as these elements may not be accurately represented in the\\nhierarchical data [ 36 ]. Furthermore, the reliance on a static representation of the GUI state can limit the technique’s effectiveness in\\nreal-time or interactive environments where the GUI can change\\nfrequently. This can be translated into maintenance costs and poor\\nportability of automation scripts. Finally, GUI hierarchy parsing\\ncan be computationally expensive, making it less suitable for applications that require real-time performance or those operating on\\nresource-constrained devices [36].\\n\\n\\n134\\n\\n\\n-----\\n\\nEnhancing widget recognition for automated Android testing using Computer Vision techniques WebMedia’2024, Juiz de Fora, Brazil\\n\\n**Figure 2:** Research steps for *widget recognition* and *description* using Computer Vision techniques.\\n\\n**(a)** **(b)**\\n\\n**Figure 4:** Example of “Multi-Tab” non-atomic widget (a) and “Text Button” atomic\\nwidget (b). Non-atomic widgets contain different atomic components, and their center\\ncoordinates (x,y) may have no effect during automated interactions (a).\\n\\n**Figure 3:** Number of classes (a) and samples (b) of Rico, Redraw, and VINS original\\ndatasets.\\n\\n### **2.2 Computer Vision-based widget recognition**\\n\\nCV-based solutions offer distinct advantages over GUI hierarchy\\nparsing for widget recognition in software applications. Primarily,\\nComputer Vision techniques provide a more robust and flexible\\napproach, as they can recognize and adapt to a wide variety of\\nvisual styles, themes, and customizations that GUIs often employ\\n\\n[ 37 ]. This adaptability is crucial in environments where interfaces\\nfrequently change or lack standardized structures [ 34 ]. Furthermore, CV methods do not rely on the underlying code structure or\\naccess to the application’s source code, making them applicable to\\na broader range of scenarios, including black-box applications [ 14 ].\\nThey also enhance cross-platform compatibility since the visual\\nelements remain consistent regardless of the operating system or\\nthe framework used to build the GUI [36].\\nCV-based widget identification have been addressed through two\\nmain stages in the academic and industrial solutions [ 43 ]. Figure\\n1 illustrates them: *widget recognition* and *description* . The first one\\naims to localize and classify widgets on screen capture images.\\nFor this recognition task, we identified two recurrent approaches:\\nObject Detection + Widget Classification [ 27, 31, 43, 52, 59, 61 ] and\\nOne-Stage Recognition [ 1, 12, 14, 22, 44, 57, 60 ]. Sparsely, some\\nworks also included the second stage, the *widget description* stage,\\nwhose goal is to generate more detailed information about the GUI\\nelement [ 10, 11, 29, 35, 39, 41, 43, 56, 62 ]. In other words, this stage\\nseeks to differentiate GUI elements of the same class according to\\ntheir contextual meaning on the screen. Figure 1 shows a screen\\ncapture fragment of the “Settings” application that contains three\\nGUI elements: an “Icon arrow backward”, a text label “Display”, and\\nan “Icon search”. The *widget recognition* stage localizes and classifies\\nthe elements according to a set of class names, in this example,\\n\\n\\n“Icon”, “Text”, and “Icon”. Then, with the *widget description* stage,\\nthe class information is complemented with the icons sub-category,\\n“Icon-arrow backward” and “Icon search”, and the actual content of\\nthe text element “Display”.\\nIn automated Android testing scenarios, like functionality tests,\\nperforming a stage one + stage two pipeline will generate better widget representations for maximizing the reproducibility of test steps.\\nMoreover, GUI tests will improve the precision and also readability\\nof bug reports, since problematic widgets will be fully described\\nand easy to localize, which is desirable by Quality Assurance (QA)\\nand Development teams [36].\\nIt is important to highlight that according to our study, only a few\\nsolutions attempt to go beyond class-based widget identification [ 11,\\n39, 41 ]. Also, most of these works use the UI metadata information\\nfor locating and classifying the GUI elements, which is not scalable\\nand leads to incompletely described widgets. Moreover, these works\\nare limited concerning the variety of widgets that can be recognized,\\nfrom 10 to 15 different classes [35, 43, 62].\\nTo address the above limitations, we developed a methodology\\nwith two main goal: to expand the recognition capacity in terms\\nof widget classes and to enhance the contextual description of\\nthe GUI components identified. Our solution is based on state-ofthe-art Object Recognition techniques for widget localization and\\nclassification. Novelty, we present a combination of CV techniques,\\ncalled Smart Descriptor, for contextual-based widget description.\\n### **3 METHODOLOGY**\\n\\nFigure 2 illustrates our research methodology. Firstly, we selected\\na dataset to work with Computer Vision techniques for widget\\nrecognition and description. Then, we implemented the most recurrent approaches in the literature and industrial solutions for\\n\\n\\n135\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil López et al.\\n\\n\\nwidget recognition: Object Detection + Classification, and OneStage Widget Recognition. After that, we studied how to extract\\ncontextual information about text, images, and “context-dependent”\\nwidgets to generate descriptions for those GUI components. Finally,\\nwe evaluated the *widget recognition* and *description* methods using\\nappropriate metrics for each technique.\\n### **3.1 Mobile GUI dataset**\\n\\nRico [ 15, 24 ], ReDraw [ 42, 43 ] and VINS [ 8, 9 ] datasets are the most\\nrecurrently used collections for widget recognition, publicly available. Figure 3 shows the number of classes (a) and samples (b) for\\neach one of the three datasets. Rico is the largest, with 782 *,* 589\\nRegion of Interest (RoI) of widgets in different screenshots mined\\nover 9 *.* 3k free Android apps from 27 categories [ 24 ]. His extension\\nmakes Rico ideal for working with deep-learning algorithms. However, Rico is noisy, presenting wrong and missing GUI component\\nbounds in the semantic annotations [ 14, 37 ]. Still, to our knowledge,\\nRico is the Android screen capture dataset with a greater variety of\\nwidgets, specifically 115 classes, and 98 icon types [24]. Moreover,\\nRico includes the GUI hierarchy for each screen capture, which\\ncontains valuable widget information such as the actual “text” for\\ncomponents like “labels” and “text buttons”, which serve as ground\\ntruth data for evaluating GUI recognition methods [37].\\nThis work seeks to recognize and describe widgets while maximizing their variety. Regarding this, we decided to work with the\\nRico dataset because it is publicly available and has the largest\\nnumber of annotated classes and Android screenshots [ 37 ]. Nevertheless, we did not work with the total of 213 types of widgets\\nannotated on the original Rico dataset.\\nWe analyzed the Rico classes and grouped them into categories:\\n\\n  - 8 **Atomic classes** [1] ;\\n\\n  - 13 **Container classes** [2] ;\\n\\n  - 98 **“Icon” types** [3] ;\\n\\n  - 27 **“List Item” sub-types** [4] ;\\n\\n  - 3 **“Pager Indicator” sub-types** [5] ;\\n\\n  - 7 **“Text” sub-types** [6] ;\\n\\n1 Atomic classes: “Checkbox”, “Input”, “Number Stepper”, “On/Off Switch”, “Pager\\nIndicator”, “Radio Button”, “Slider”, “Text Button”\\n2 Container classes: “Advertisement”, “Background Image”, “Bottom Navigation”, “Button Bar”, “Card”, “Date Picker”, “Drawer”, “Map View”, “Modal”, “Multi-Tab”, “Toolbar”,\\n“Video”, “Web View”\\n3 “Icon” types: “add”, “arrow_backward”, “arrow_downward”, “arrow_forward”, “arrow_upward”, “attach_file”, “av_forward”, “av_rewind”, “avatar”, “bluetooth”, “book”,\\n“bookmark”, “build”, “call”, “cart”, “chat”, “check”, “close”, “compare”, “copy”, “dashboard”, “date_range”, “delete”, “description”, “dialpad”, “edit”, “email”, “emoji”, “expand_less”, “expand_more”, “explore”, “facebook”, “favorite”, “file_download”, “filter”, “filter_list”, “flash”, “flight”, “folder”, “follow”, “font”, “fullscreen”, “gift”, “globe”,\\n“group”, “help”, “history”, “home”, “info”, “label”, “launch”, “layers”, “list”, “location”,\\n“location_crosshair”, “lock”, “menu”, “microphone”, “minus”, “more”, “music”, “national_flag”, “navigation”, “network_wifi”, “notifications”, “pause”, “photo”, “play”,\\n“playlist”, “power”, “redo”, “refresh”, “repeat”, “reply”, “save”, “search”, “send”, “settings”, “share”, “shop”, “skip_next”, “skip_previous”, “sliders”, “star”, “swap”, “switcher”,\\n“thumbs_down”, “thumbs_up”, “time”, “twitter”, “undo”, “videocam”, “visibility”, “volume”, “volume”, “warning”, “weather”, “zoom_out”\\n4 “List-Item” sub-types: “add”, “avatar”, “book”, “bookmark”, “call”, “chat”, “check”,\\n“close”, “date_range”, “delete”, “email”, “emoji”, “facebook”, “favorite”, “flash”, “folder”,\\n“font”, “fullscreen”, “location”, “more”, “national_flag”, “notifications”, “playlist”, “settings”, “sliders”, “star”, “warning”\\n5 “Pager Indicator” sub-types: “menu”, “more”, “sliders”\\n6 “Text” sub-types: “check”, “close”, “date_range”, “favorite”, “font”, “photo”, “wallpaper”\\n\\n\\n\\n  - 54 **“Text Button” sub-types** [7] ;\\n\\n  - 3 **Confusing classes** [8] which samples have an identical appearance with atomic samples.\\n\\nIn this work, we created our dataset by extracting all samples\\nfrom the 8 **Atomic classes** and the 97 **Icon types** .\\nWe decided to exclude **Container** classes since they accommodate other clickable elements inside, which may demand more\\ncomplex interaction flows in test automation tools and lead to incorrect interactions. For example, as shown in Figure 4(a), interacting\\nwith a “Multi-Tab” widget is impossible through a single tap on the\\ncenter coordinate. Instead, it requires a second routine for recognizing the contained widgets, and then, selecting one of them to\\ninteract with. In this figure, the red lines indicate that the center of\\nthe “Multi-Tab” element does not correspond to a valid clickable\\ncoordinate. On the other hand, as shown in Figure 4(b), “Text Buttons” can be triggered with a single tap. Green lines show that the\\ncenter of this widget leads to activating the “Log in” action.\\nMoreover, we also excluded classes from the following **sub-type**\\ngroups: **“List Item”, “Pager Indicator”, “Text” and “Text But-**\\n**ton”** . Samples of these classes are originally annotated in Rico as\\nbelonging to more than one class. However, visually they have\\nthe same appearance, i.e., “Text Button-delete” and “Icon-delete”,\\nhave the same “image pattern” but are annotated as belonging to\\nmore than one class. This is considered noisy information that may\\nhinder the convergence of machine learning algorithms. According this, the **Confusing** classes was not considered as well, and\\nanalyzing images of **Icon types**, we found that the “Icon-switcher”\\nclass is composed of 1046 identical instances, which also look very\\nsimilar to other types of icons. To avoid this noise information, we\\nalso excluded this class.\\n\\nAfter discarding the mentioned classes of widgets, we kept 105\\ncategories: 8 **Atomic classes** and 97 **Icon types** .\\n\\n*3.1.1* *Rico Annotations.*\\nWe created two different datasets for implementing the widget\\nrecognition approaches: (1) Object Detection + Classification and\\n(2) One-Stage Recognition. These datasets included the 105 classes\\nof widgets previously selected.\\nThe first dataset was generated by cropping the RoIs from the\\nRico screenshots. This was done by traversing the GUI graph available on the semantic annotations files [ 24 ] and saving the labeled\\ncrops corresponding to the bounds of each “componentLabel” and\\n“iconClass” key. This dataset was used for training the classification\\nalgorithm of the Object Detection + Classification approach. In total, this dataset resulted in 355 *,* 999 images that we split for training,\\ntesting, and validation in the proportion of 50:30:20 respectively.\\nThe other dataset was created to train the One-stage recognition\\napproach. Here, we used the Rico semantic annotations files to\\ngenerate the class and bounding box ground truth annotations,\\nresulting in a dataset of 44 *,* 873 screenshots. We also used the same\\n\\n7 “Text Button” sub-types: “add”, “arrow_backward”, “arrow_forward”, “av_forward”,\\n“av_rewind”, “avatar”, “bookmark”, “cart”, “chat”, “check”, “close”, “copy”, “date_range”,\\n“delete”, “edit”, “emoji”, “expand_more”, “facebook”, “favorite”, “file_download”, “filter,\\n“flash”, “folder”, “follow”, “font”, “gift”, “globe”, “group”, “help”, “home”, “info”, “launch”,\\n“layers”, “list”, “menu”, “minus”, “more”, “notifications”, “pause”, “photo”, “play”, “refresh”, “search”, “send”, “settings”, “share”, “sliders”, “star”, “swap”, “time”, “twitter”,\\n“videocam”, “volume”, “warning”\\n8 Confusing classes: “Image”, “List Item”, “Text”\\n\\n\\n136\\n\\n\\n-----\\n\\nEnhancing widget recognition for automated Android testing using Computer Vision techniques WebMedia’2024, Juiz de Fora, Brazil\\n\\n**Figure 5:** The two approaches implemented for Widget Recognition. The first one uses Digital Image Processing (DIP) techniques for Object Detection (edge filters, binarization,\\nand contour detection) and a Convolutional Neural Network (CNN) model for classification. The second approach uses YOLOv8 pre-trained model to detect and classify the widgets\\nin a single stage.\\n\\n\\n**Table 1:** Algorithms and parameterization used in Object Detection stage.\\n\\n**Algorithm** **Parameters**\\n\\n1st Hysteresis threshold (threshold1): 100\\nCanny Filter\\n2st Hysteresis threshold (threshold2): 200\\n\\n\\n**Table 2:** Details of the 3-depth CNN classifier architecture and the training\\nhyper-parameters.\\n\\n**Configurations**\\n\\n\\nConv2D(64, (3, 3))\\nConv2D(32, (3, 3))\\nConv2D(16, (3, 3))\\nDense(128))\\n\\nFor all layers, was used ReLU as the\\nactivation function, HeUniform as\\nkernel and bias initializers, and L1L2\\nas kernel and bias regularizers. Between then, a group of layers was\\nused, consisting of:\\nBatchNormalization()\\nAveragePooling2D((2, 2))\\nDropout(0.5)\\n\\nAt the end of the network, was used:\\n\\nDropout(0.3)\\nDense(105, activation=\"softmax\")\\n\\n\\n**Layers**\\n\\n**Parameters**\\n\\n\\nMorphological\\nClosing\\n\\nContour\\n\\nDetection\\n\\n\\nMorphological Operation (op):\\ncv2.MORPH_CLOSE\\n\\nKernel: np.ones((35, 35), np.uint8\\n\\nContour Retrieval mode (mode):\\n\\ncv2.RETR_EXTERNAL\\nContour approximation method (method):\\ncv2.CHAIN_APPROX_SIMPLE\\n\\n\\nproportion of 50:30:20 for splitting the data into training, testing,\\nand validation subsets.\\n### **3.2 Computer Vision-based widget recognition**\\n\\nThe goal of the recognition stage is to localize widgets in Android\\nscreenshots and classify them. For this, we implemented two approaches, where the first one uses more traditional techniques,\\nspecifically, Digital Image Processing (DIP) for object detection\\nand Convolutional Neural Networks (CNN) for classification. The\\nsecond approach uses the state-of-the-art “You Only Look Once”\\n(YOLO) object detection algorithm [9] .\\n\\n*3.2.1* *Approach 1: Object Detection + Classification.*\\nAccording to shown in Figure 5, the “traditional” approach for\\nwidget recognition has two stages, object detection and classification. The object detection stage was implemented by, firstly, locating\\nedges using the Canny filter [10] [ 43 ], followed by a morphological\\n\\n9 Real-time object detection and image segmentation model, built on cutting-edge\\nadvancements in deep learning and CV, offering unparalleled performance according\\nto speed and accuracy [50].\\n10 Was used the OpenCV implementation of Canny filter available on: https://docs.\\nopencv.org/4.x/da/d22/tutorial_py_canny.html\\n\\n\\nOptimizer:\\n**Hyper-parameters** Adam(learning_rate=1e-4)\\nLoss: \"categorical_crossentropy\"\\n\\nclosing [11] to prevent object splitting [ 23 ] and finally, the find contours technique [12] [ 43 ] to get the potential RoI on the screenshots.\\nTable 1 shows the parameterization used in each function of the\\nobject detection stage.\\nThe Object Detector’s output is a set of bounding boxes of all\\nRoIs with a high probability of being widgets. The next step is\\n\\n11 Was used the OpenCV implementation of morphological closing available on https:\\n//docs.opencv.org/4.x/d9/d61/tutorial_py_morphological_ops.html\\n12 Was used the find contour OpenCV implementation available on: https://docs.opencv.\\norg/4.x/d4/d73/tutorial_py_contours_begin.html\\n\\n\\n137\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil López et al.\\n\\n**Figure 6:** Flowchart of the *Widget Description* stage.\\n\\n\\nclassifying each RoI according to the 105 GUI component classes.\\nFor this, we trained a CNN model with three convolutional layers of depth (CNN3). This classifier architecture was proposed by\\n\\n[8]. In the training process, the training dataset was fed to the\\nmodel in Grayscale format, since early experimentation showed\\nbetter results when compared with the Red, Green, and Blue (RGB)\\noriginal format. The input dimensions were 50 × 50. The model\\nwas trained using Tensorflow [13] 2 *.* 4 on a server with Ubuntu 18 *.* 04\\noperating system and GPU NVIDIA GeForce RTX 360 TI. Table 2\\ngives the configuration details about the model and the training\\nhyper-parameters.\\n\\n*3.2.2* *Approach 2: One-Stage Recognition.*\\nWe implemented the one-stage widget recognition approach\\nusing YOLOv8 as an alternative solution for Approach 1 described\\nabove. YOLOv8 improves the detection accuracy and speed compared with the previous version, making it highly suitable for realtime applications. Although real-time recognition is not the focus\\nof this research, we wanted to explore how this model will perform in our scope, due to the potential application in automated\\ntests where the instant action-motion replay is valuable. To train\\nYOLOv8, we used the Ultralytics [14] framework with the default\\ntraining parameters.\\n### **3.3 Widget Description**\\n\\nIn this work, we propose to go beyond widget recognition and\\ndescribe the RoIs according to the contextual information on the\\nscreenshot. For this, we use three main techniques, Optical Character Recognition (OCR), Image Classification, and distance-based\\nheuristic rules. The Figure 6 shows in a flowchart our proposal for\\ndescribing widget crops according to the class recognized. If the\\nRoI is a “Text Button” or an “Input Text” the content is extracted\\nusing Keras-OCR [15] or a Python wrapper of Tesseract [16] . We experimented with both OCRs to verify which performs best in our scope.\\nDifferent from the “Text Button” class, “Input Text” components\\nare empty sometimes. Hence, if the OCR does not recognize any\\n\\n13 Available at https://www.tensorflow.org/\\n14 Available at https://github.com/ultralytics/ultralytics\\n15 Available at https://keras.io/\\n16 Avaiable at https://github.com/tesseract-ocr/tesseract\\n\\n\\ntext, the “Input text” crop gets the description of the nearest textual\\nelement on the screenshot. This logic is applied also, to “Checkbox”, “Number Stepper”, “On/Off Switch”, “Pager Indicator”, “Radio\\nButton” and “Slider” classes. We calculate the Euclidean distance\\n\\nof the RoI and all textual elements (which are described using the\\nOCR technique) and use the description of the nearest one. Finally,\\nif the approached techniques fail to generate a description, we use\\nthe InceptionV3 [17] model to extract any semantic information from\\nthe RoI.\\nIn the case of “Icon” types classes, we considered the class information explicit enough to describe a RoI. In this work, we focused\\non recognizing a nice variety of icons (97 classes). However, there\\nare still different icon types unknown to our models. Regarding this,\\nif a RoI is classified as any “Icon” type with a confidence below 0 *.* 5,\\nthen we use the InceptionV3 model to extract semantic information\\nof the area and validate icon classes that do not exist in the 97 types\\nin our dataset.\\n### **4 STUDY**\\n\\nIn this work, we have two main objectives. The first is to study\\nhow the two state-of-the-art approaches for widget recognition\\nperform in an extended-class dataset (105 classes). The second goal\\nis to propose a method to describe widgets beyond the type or class\\nby including contextual information about what that component\\nrepresents on screen. We implemented the methods described in\\nthe previous sections to attain these goals. Then, to evaluate our\\nproposal, we addressed the following research questions.\\n### **4.1 Research Questions**\\n\\n**RQ1. Effectiveness of widget recognition:** How accurate are\\nthe implemented approaches for widget recognition to localize and\\nclassify 105 classes of widgets?\\n**RQ2. Analysis of miss-classifications:** Which classes are\\nwrongly recognized by the implemented methods?\\n**RQ3. Effectiveness of widget description:** How accurate is\\nour proposal for describing widgets?\\n**RQ4. Analysis of uncovered descriptions:** What are the major\\nreasons for our widget description method to fail?\\n\\n17 Available at https://keras.io/api/applications/inceptionv3/\\n\\n\\n138\\n\\n\\n-----\\n\\nEnhancing widget recognition for automated Android testing using Computer Vision techniques WebMedia’2024, Juiz de Fora, Brazil\\n\\n**(a)** **(b)**\\n\\n**Figure 8:** Examples of classes wrongly predicted by the *widget recognition*\\napproaches, (a) shows “on” and “off” switch samples, where “off” instances are\\nunder-represented on the training data, and (b) shows a “versatile” appearance widget\\nthat fits in two classes, an unselected checkbox, and an icon-app-switch.\\n\\n\\n**(a)** **(b)**\\n\\n**Figure 7:** Rico screenshot with the widgets annotations enclosed in green boxes (a)\\nand the Object Detector + Classifier predictions also enclosed in green boxes (b).\\n\\n**RQ5. Impact of approached CV-based methods for recog-**\\n**nizing and describing widgets:** When integrated into a mobile\\ntest automation tool, the approached CV-based methods for recognizing and describing widgets contribute to generating more\\ncomprehensive steps for Test/Script Case generation or Bug Review?\\n### **4.2 Measurement**\\n\\nTo evaluate the performance of the “traditional” and one-stage widget recognition approaches, we studied the Precision, Recall, and\\nmean Average Precision (mAP). The mAP is a key metric for evaluating object detection models [ 47 ]. It combines both Precision and\\nRecall to offer a comprehensive assessment of a model’s accuracy.\\nThe Precision (P) is the ratio of True Positive detection to the sum of\\nTrue Positive and False Positive detection. The Recall (R) is the ratio\\nof True Positive detection to the sum of True Positive and False\\n\\nNegative detection [ 47 ]. These can be mathematically expressed as:\\n\\n\\n\\n[1]\\n\\n*𝑁* [·]\\n\\n*𝑁*\\n\\n∑︁\\n\\n\\n(1)\\n�\\n\\n\\nPrecision = [1]\\n\\n\\n*𝑁*\\n\\n\\nTrue Positives\\n� True Positives + True Negatives\\n\\n\\n*𝑁*\\n\\n[1]\\n\\n*𝑁* [·]\\n\\n*𝑁*\\n\\n∑︁\\n\\n\\n(2)\\n�\\n\\n\\nRecall = [1]\\n\\n\\n*𝑁*\\n\\n\\nTrue Positives\\n� True Positives + False Negatives\\n\\n\\n\\n[1]\\n\\n*𝑁* [·]\\n\\n*𝑁*\\n\\n∑︁\\n\\n\\nmAP = [1]\\n\\n\\n(R *𝑁* − R *𝑁* −1 ) · P *𝑁* (3)\\n\\n*𝑁*\\n\\n∑︁\\n\\n\\nwhere *𝑁* is the number of classes. mAP metric provides a single\\nvalue that summarizes the model’s overall detection accuracy, making it essential for comparing the performance of different object\\ndetection models [47].\\nMoreover, we assess the *widget description* method using the\\nCharacter Error Rate (CER). This metric is crucial to evaluate text\\nrecognition algorithms[ 45 ]. The CER measures the number of characterlevel errors made by the OCR process and is calculated using the\\nformula:\\nCER = *[𝑆]* [+] *[ 𝐷]* [+] *[ 𝐼]* (4)\\n\\n\\nwhere *𝑆* is the number of substitutions, *𝐷* is the number of deletions,\\n*𝐼* is the number of insertions, and *𝑁* is the total number of characters\\n\\nin the reference text. A lower CER indicates a more accurate OCR\\n\\nsystem. This metric provides a granular view of OCR performance,\\nenabling developers to identify specific types of errors and improve\\nsystem robustness by refining algorithms to reduce substitutions,\\ndeletions, and insertions. Thus, CER is vital for applications requiring high precision in text recognition, such as digital archiving and\\nautomated data entry [45].\\n### **5 DISCUSSION** **5.1 Widget Recognition**\\n\\n**RQ1. Effectiveness of widget recognition:** We studied the performance of the two approaches for *widget recognition* on the test\\nset compound by 17 *,* 808 images and their respective annotations.\\nWe measured the methods using the mean Average Precision (mAP)\\nmetric and attained a low value, where the first approach achieved\\n0 *.* 065, while for YOLOv8, the result was 0 *.* 690. To understand these\\nmetrics, we did an exhaustive manual inspection of the Rico annotations and the predictions of the widget recognition methods\\nimplemented in this work. Then, we find out that Rico has a very\\nhigh number of missed widget annotations, representing most of\\nthe False Positives that impacted the precision of the recognition\\nmethods. Figure 7 illustrates this problem: (a) is an annotated screenshot of the Rico dataset, and (b) is the result of executing the Object\\nDetector + Classifier approach for widget recognition on the same\\nimage. In (a), more than 80% of the RoI are not annotated, but our\\nrecognition approach can detect those areas, unfairly counting as\\nFalse Positives. Furthermore, the object detector used in the first\\napproach is sensitive to capturing extreme details of the interface,\\nhelping to explain the extremely low value performed for the first\\napproach.\\n**RQ2. Analysis of miss-classifications:** Regardless of the mentioned Rico annotation problems, we detected some false predictions for both of the widget recognition approaches. During the\\nmanual inspection of the methods’ predictions, we observed that the\\n“off” samples belonging to the class “On/Off Switch” were always\\nmisclassified. Then, we revised the training samples and discovered\\nthat only 2 *.* 5% of the “On/Off Switch” RoIs were “off” instances, see\\nFigure 8(a). This justifies the miss-predictions of the YOLOv8 recognition model and the CNN3 classifier. To overcome this problem,\\nthe “off” switch samples must be augmented to balance with the\\n“on” ones.\\n\\n\\n(4)\\n*𝑁*\\n\\n\\n139\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil López et al.\\n\\n**(a)** **(b)**\\n\\n**Figure 10:** Example of “number-only” (a) and “noisy-background” (b) “Text\\nButton”.\\n\\n**Figure 9:** Selection of “Text Button” samples of the dataset created for evaluating\\nthe smart descriptor and a fragment of the respective UI hierarchy metadata\\nhighlighting the *bounds* and *text attributes* .\\n\\n**Table 3:** Character Error Rate (CER) and Response time per request in seconds for\\nTesseract, Keras-OCR, and the combination of both, in 1 *,* 021 “Text Button” crops\\nextracted from manually selected screenshots from Rico testing set.\\n\\n\\n**Method** **Mean CER** **Response Time (sec)**\\n\\nTesseract 0 *.* 204 2 *.* 1\\n\\nKeras-OCR 0 *.* 172 2 *.* 28\\n\\nTesseract + Keras-OCR 0 *.* 171 1 *.* 98\\n\\nMoreover, we found wrong predictions caused by widgets having\\na “versatile” appearance, meaning that the same image belongs to\\nmore than one class. Figure 8(b) shows an example of this, where\\nan unselected “Checkbox” is identical to the “app-switch” icon of\\nthe navigation bar.\\n### **5.2 Widget Description**\\n\\nTo evaluate our Widget Descriptor proposal, we needed the contextual meaning annotations of Rico images. However, the Rico\\nview hierarchies files [ 24 ] only have this information for textual elements like “Text Buttons”, which is contained in the “text” attribute.\\nThis information sparsely appears for other widget types in the\\n“content-desc” attribute. Regarding this, we managed to evaluate\\nquantitatively, only the description generated for “Text Buttons”.\\nDue to the mentioned ground truth limitations for the other classes,\\nwe did a manual inspection to identify problems in the descriptions\\ngenerated and have an overall notion of the method’s performance.\\nTo evaluate the Widget Descriptor’s capacity for recognizing\\ntextual components we created a dataset of 1 *,* 021 “Text Button”\\nRoIs randomly extracted from the 17 *,* 808 samples of the test Rico\\ndataset. For this, we used the UI hierarchy *json* files from Rico [ 24 ]\\nto extract the *bounds*, *class* and *text* attributes for each node. Then,\\nwe filtered the nodes belonging to the “Text Button” class, used the\\n*bounds* for cropping the RoIs on the full screenshot, and saved the\\n*text* information to generate the RoIs description ground truth.\\nFigure 9 shows a representative selection of this dataset and a\\nfragment of a UI hierarchy metadata highlighting the *bounds* and\\n*text* attributes.\\n\\n\\n**Figure 11:** Percentage of Issues reported by the automated exploratory test tool\\nusing our widget identification (recognition+description) method for mapping the\\ndevice under test (DUT) UI at each timestamp.\\n\\n**RQ3 Effectiveness of widget description:** The results of evaluating the textual elements are shown in Table 3. Keras-OCR reached\\na mean CER of 0 *.* 172 and a mean execution time per request of 2 *.* 28\\nseconds. Tesseract showed a higher mean CER, 0 *.* 204 but better\\nperformance with a mean response time per request of 2 *.* 1 seconds.\\nBased on these results, we combined the two OCRs sequentially to\\ndescribe text-based components. Firstly Tesseract is used. Then, if\\nit fails to recognize the text, Keras-OCR should be called. This OCR\\ncombination demanded in mean 1 *.* 98 seconds per request with a\\nmean CER of 0 *.* 171.\\n\\n**RQ4. Analysis of uncovered descriptions:** While evaluating\\nthe widget descriptor for textual elements, we collected the samples\\nwith high CER for manual analysis. Here, we detected that while\\nTesseract could not recognize “Text Button” samples containing\\njust numbers (Figure 10a), Keras-OCR made accurate predictions\\nfor these cases. We also noticed that images with non-solid backgrounds (Figure 10b) had poor results for both OCRs, Tesseract, and\\n\\nKeras.\\nTo have an overall idea of how effective is the *widget descrip-*\\n*tion* method for predicting non-textual classes, we ran it on the\\n17 *,* 808 test portion of the Rico images, saved the screenshots with\\nthe bounding boxes, and the description generated for each RoI.\\nThen, we manually inspected these results, finding some situations\\nwhere our method failed to retrieve a valid description for some\\nGUI components. Mainly, these fails occur in “context-dependent”\\nclasses when the widget is not side-to-side with the textual widget,\\nbut above or below. For example, in some applications is a common design practice to have “Radio Buttons” below a text label. In\\nthose cases, our method distance-based method does not perform\\naccurately.\\n\\n\\n140\\n\\n\\n-----\\n\\nEnhancing widget recognition for automated Android testing using Computer Vision techniques WebMedia’2024, Juiz de Fora, Brazil\\n\\n### **5.3** **Applying our widget identification method** **on an exploratory test tool**\\n\\n**RQ5. Impact of approached CV-based methods for recogniz-**\\n**ing and describing widgets:** As a study case, we implemented\\nan API REST service to make available our widget identification\\napproach (recognition + description) for being consumed by an\\nautomated Android testing tool focused on exploratory testing. Figure 11 shows the Issues reported by the exploratory tool using our\\nmethod for six months of 2021–2022. In total 41% of the Issues were\\nstated as being non-reproducible or having insufficient information\\nfor reproduction (red and orange portions), 29% were fixed (green)\\nand 29% were reported to third-party owners (blue). From this, we\\ncan conclude that the development team effectively processed more\\nthan half of the issues reported using our widget identification\\nmethod to describe steps.\\n### **5.4 Limitations and future works**\\n\\nThis study investigates two principal approaches for enhancing\\nwidget recognition in automated black box Android testing: Object\\nDetection combined with Classification, and a One-Stage Recognition method. Our analysis was supported by two newly introduced\\ndatasets, tailored to the demands of deep learning applications in\\nwidget recognition and classification. By targeting a comprehensive\\nset of 105 different widget classes and incorporating context-based\\ndescriptions, we aimed to improve the scalability of automated\\ntesting frameworks.\\nDespite these advancements, several challenges remain. The\\ndiversity and complexity of Android UIs continue to pose significant\\nhurdles. Variations in widget design, dynamic content updates, and\\nthe presence of custom widgets that do not adhere to standard\\nconventions all impact the effectiveness of recognition methods.\\nFuture work should focus on further refining recognition algorithms\\nto handle these challenges, potentially incorporating additional data\\nsources or leveraging more advanced machine learning techniques.\\nEven though the One-Stage Recognition Approach has proven to\\nbe better, the two approaches implemented for widget recognition\\npresented poor mAP, due to the noisy nature of the data, according\\nto our manual inspection of the two datasets created from Rico. We\\npropose future work to improve the quality of the datasets using\\ntechniques and data proposed in a previous work [ 37 ], in addition\\nto extending the number of samples for our 105 with smaller but\\nconsistent datasets like VINS [9].\\nTo improve our widget description method, we propose to review the distance-based algorithm for “content-dependent” widgets,\\nspecifically for those cases where the element is positioned below\\nor above the label. To analyze the feasibility of using the proposed\\npipeline in real time testing applications, we propose as future\\nwork to test the best widget recognition approach of this research\\ncombined with widget description component in an environment\\nprepared for this purpose, in order to collect metrics so that improvements and studies can be made.\\n### **6 CONCLUSIONS**\\n\\nThis paper presents a study of using two advanced approaches to\\nwidget recognition for automated Android testing: Object Detection\\ncombined with Classification, and a One-Stage Recognition method.\\n\\n\\nWe created two datasets of widgets from Rico, one for classification and another one for recognition. We filtered atomic widget\\ntypes resulting in 105 classes including 97 icon types. We also presented a method for describing widgets beyond just using the class,\\ninstead, we extract contextual information about what those components represent on screenshots. Our evaluation of YOLOv8 and\\nthe OpenCV Object Detector + CNN Classifier approaches exposed\\nseveral annotation problems of the Rico dataset, concluding that\\nis not recommended to work with this Rico without performing\\nnoise removal routines. Furthermore, the proposed combination\\nof Keras and Tesseract OCRs for extracting information of “Text\\nButton” RoIs presented a low mean CER of 0 *.* 172.\\n### **ACKNOWLEDGMENTS**\\n\\nThis article is a result of the Research and Development project\\nSTAR, carried out by the Sidia Institute of Science and Technology,\\nin partnership with Samsung Electronics of Amazônia Ltda. Advertising by the provisions of article 39 of Decree No. 10 *,* 521 / 2020. This\\nstudy was financed in part by the Coordenação de Aperfeiçoamento\\nde Pessoal de Nível Superior — Brasil (CAPES) — Finance Code 001.\\nThis work was partially supported by Amazonas State Research\\nSupport Foundation — FAPEAM — through the POSGRAD project.\\n### **REFERENCES**\\n\\n[1] A. A. Abdelhamid, S. Alotaibi, and A. Mousa. 2020. Deep learning-based prototyping of android GUI from hand-drawn mockups. *IET Software* 14 (2020),\\n816–824. Issue 7. https://doi.org/10.1049/iet-sen.2019.0378\\n\\n[2] David Adamo, Md Khorrom Khan, Sreedevi Koppula, and Renée Bryce. 2018.\\nReinforcement learning for Android GUI testing. In *Proceedings of the 9th ACM*\\n*SIGSOFT International Workshop on Automating TEST Case Design, Selection, and*\\n*Evaluation* (Lake Buena Vista, FL, USA) *(A-TEST 2018)* . Association for Computing\\nMachinery, New York, NY, USA, 2–8. https://doi.org/10.1145/3278186.3278187\\n\\n[3] Domenico Amalfitano, Anna Rita Fasolino, Porfirio Tramontana, Salvatore\\nDe Carmine, and Atif M. Memon. 2012. Using GUI ripping for automated\\ntesting of Android applications. In *Proceedings of the 27th IEEE/ACM Interna-*\\n*tional Conference on Automated Software Engineering* (Essen, Germany) *(ASE*\\n*’12)* . Association for Computing Machinery, New York, NY, USA, 258–261.\\nhttps://doi.org/10.1145/2351676.2351717\\n\\n[4] Domenico Amalfitano, Vincenzo Riccio, Nicola Amatucci, Vincenzo De Simone,\\nand Anna Rita Fasolino. 2019. Combining Automated GUI Exploration of Android\\napps with Capture and Replay through Machine Learning. *Information and*\\n*Software Technology* 105 (2019), 95–116. https://doi.org/10.1016/j.infsof.2018.08.\\n007\\n\\n[5] Luca Ardito, Riccardo Coppola, Simone Leonardi, Maurizio Morisio, and Ugo Buy.\\n2020. Automated Test Selection for Android Apps Based on APK and Activity\\nClassification. *IEEE Access* 8 (2020), 187648–187670. https://doi.org/10.1109/\\nACCESS.2020.3029735\\n\\n[6] Tanzirul Azim and Iulian Neamtiu. 2013. Targeted and depth-first exploration\\nfor systematic testing of android apps. In *Proceedings of the 2013 ACM SIGPLAN*\\n*International Conference on Object Oriented Programming Systems Languages*\\n*&amp; Applications* (Indianapolis, Indiana, USA) *(OOPSLA ’13)* . Association for\\nComputing Machinery, New York, NY, USA, 641–660. https://doi.org/10.1145/\\n2509136.2509549\\n\\n[7] Young-Min Baek and Doo-Hwan Bae. 2016. Automated model-based Android\\nGUI testing using multi-level GUI comparison criteria. In *Proceedings of the 31st*\\n*IEEE/ACM International Conference on Automated Software Engineering* (Singapore, Singapore) *(ASE ’16)* . Association for Computing Machinery, New York,\\nNY, USA, 238–249. https://doi.org/10.1145/2970276.2970313\\n\\n[8] Sara Banian, Kai Li, Chaima Jemmali, Casper Harteveld, Yun Fu, and Magy\\nEl-Nasr. 2021. *VINS: Visual Search for Mobile User Interface Design* . https:\\n//github.com/sbunian/VINS\\n\\n[9] Sara Banian, Kai Li, Chaima Jemmali, Casper Harteveld, Yun Fu, and Magy\\nEl-Nasr. 2021. VINS: Visual Search for Mobile User Interface Design. 1–14.\\nhttps://doi.org/10.1145/3411764.3445762\\n\\n[10] Manuele Barraco, Matteo Stefanini, Marcella Cornia, Silvia Cascianelli, Lorenzo\\nBaraldi, and Rita Cucchiara. 2022. CaMEL: Mean Teacher Learning for Image\\nCaptioning. arXiv:2202.10492\\n\\n\\n141\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil López et al.\\n\\n\\n\\n[11] Jieshan Chen, Chunyang Chen, Zhenchang Xing, Xiwei Xu, Liming Zhu, Guoqiang Li, and Jinshui Wang. 2020. Unblind your apps: predicting naturallanguage labels for mobile GUI components by deep learning. In *Proceedings*\\n*of the ACM/IEEE 42nd International Conference on Software Engineering* . ACM.\\nhttps://doi.org/10.1145/3377811.3380327\\n\\n[12] Jing Cheng, Dingmei Tan, Tao Zhang, Aodi Wei, Jingyi Chen, and Ali Khattak\\nHasan. 2022. YOLOv5-MGC: GUI Element Identification for Mobile Applications\\nBased on Improved YOLOv5. *Mob. Inf. Syst.* 2022 (jan 2022), 9 pages. https:\\n//doi.org/10.1155/2022/8900734\\n\\n[13] Eliane Collins, Arilo Neto, Auri Vincenzi, and José Maldonado. 2021. Deep\\nReinforcement Learning based Android Application GUI Testing. In *Proceedings*\\n*of the XXXV Brazilian Symposium on Software Engineering* (Joinville, Brazil)\\n*(SBES ’21)* . Association for Computing Machinery, New York, NY, USA, 186–194.\\nhttps://doi.org/10.1145/3474624.3474634\\n\\n[14] Richard Hada Degaki, Juan Gabriel Colonna, Yadini Lopez, José Reginaldo Carvalho, and Edson Silva. 2022. Real Time Detection of Mobile Graphical User\\nInterface Elements Using Convolutional Neural Networks. In *Proceedings of*\\n*the Brazilian Symposium on Multimedia and the Web* (Curitiba, Brazil) *(WebMe-*\\n*dia ’22)* . Association for Computing Machinery, New York, NY, USA, 159–167.\\nhttps://doi.org/10.1145/3539637.3558044\\n\\n[15] Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan,\\nYang Li, Jeffrey Nichols, and Ranjitha Kumar. 2017. Rico: A Mobile App Dataset\\nfor Building Data-Driven Design Applications. In *Proceedings of the 30th Annual*\\n*ACM Symposium on User Interface Software and Technology* (Québec City, QC,\\nCanada) *(UIST ’17)* . Association for Computing Machinery, New York, NY, USA,\\n845–854. https://doi.org/10.1145/3126594.3126651\\n\\n[16] Giovanni Denaro, Luca Guglielmo, Leonardo Mariani, and Oliviero Riganelli.\\n2019. GUI testing in production: challenges and opportunities. In *Companion*\\n*Proceedings of the 3rd International Conference on the Art, Science, and Engineering*\\n*of Programming* (Genova, Italy) *(Programming ’19)* . Association for Computing\\nMachinery, New York, NY, USA, Article 18, 3 pages. https://doi.org/10.1145/\\n3328433.3328452\\n\\n[17] Android Developers. 2024. *Espresso* . https://developer.android.com/training/\\ntesting/espresso\\n\\n[18] Android Developers. 2024. *UI Automator* . https://developer.android.com/training/\\ntesting/other-components/ui-automator\\n\\n[19] Juha Eskonen, Julen Kahles, and Joel Reijonen. 2020. Automating GUI Testing\\nwith Image-Based Deep Reinforcement Learning. In *2020 IEEE International*\\n*Conference on Autonomic Computing and Self-Organizing Systems (ACSOS)* . 160–\\n167. https://doi.org/10.1109/ACSOS49614.2020.00038\\n\\n[20] Anna Esparcia-Alcazar, F. (Francisco) Almenar, Mirella Martınez, U. (Urko)\\nRueda, and T.E.J. Vos. 2016. Q-learning strategies for action selection in the\\nTESTAR automated testing tool. In *Proceedings of the 6TH International Con-*\\n*ference on Metaheuristics and Nature Inspired Computing* . 174–180. https:\\n//meta2016.sciencesconf.org/ 6th International Conference on Metaheuristic and\\nNature inspired Computing, META 2016; Conference date: 27-10-2016 Through\\n31-10-2016.\\n\\n[21] OpenJS Foundation. 2012. *Appium* . https://appium.io/\\n\\n[22] Jerry Gao, ShiTing Li, Chuanqi Tao, Yejun He, Amrutha Pavani Anumalasetty,\\nErica Wilson Joseph, Akshata Hatwar Kumbashi Sripathi, and Himabindu Nayani.\\n2022. An Approach to GUI Test Scenario Generation Using Machine Learning.\\nIn *2022 IEEE International Conference On Artificial Intelligence Testing (AITest)* .\\n79–86. https://doi.org/10.1109/AITest55621.2022.00020\\n\\n[23] Xin Gao, Sundaresh Ram, and Jeffrey J. Rodríguez. 2020. Object sieving and\\nmorphological closing to reduce false detections in wide-area aerial imagery.\\n*CoRR* abs/2010.15260 (2020). arXiv:2010.15260 https://arxiv.org/abs/2010.15260\\n\\n[24] Data Driven Design Group. 2024. *Rico: A Mobile App Dataset for Building Data-*\\n*Driven Design Applications* . http://www.interactionmining.org/rico.html\\n\\n[25] Shuai Hao, Bin Liu, Suman Nath, William G.J. Halfond, and Ramesh Govindan.\\n2014. PUMA: programmable UI-automation for large-scale dynamic analysis\\nof mobile apps. In *Proceedings of the 12th Annual International Conference on*\\n*Mobile Systems, Applications, and Services* (Bretton Woods, New Hampshire,\\nUSA) *(MobiSys ’14)* . Association for Computing Machinery, New York, NY, USA,\\n204–217. https://doi.org/10.1145/2594368.2594390\\n\\n[26] L. V. Haoyin. 2017. Automatic android application GUI testing—-A random walk\\napproach. In *2017 International Conference on Wireless Communications, Signal*\\n*Processing and Networking (WiSPNET)* . 72–76. https://doi.org/10.1109/WiSPNET.\\n2017.8299722\\n\\n[27] Saad Hassan, Manan Arya, Ujjwal Bhardwaj, and Silica Kole. 2018. Extraction\\nand classification of user interface components from an image. *International*\\n*Journal of Pure and Applied Mathematics* 118, 24 (2018), 1–16.\\n\\n[28] Gang Hu, Linjie Zhu, and Junfeng Yang. 2018. AppFlow: using machine learning\\nto synthesize robust, reusable UI tests. In *Proceedings of the 2018 26th ACM Joint*\\n*Meeting on European Software Engineering Conference and Symposium on the*\\n*Foundations of Software Engineering* (Lake Buena Vista, FL, USA) *(ESEC/FSE*\\n*2018)* . Association for Computing Machinery, New York, NY, USA, 269–282.\\nhttps://doi.org/10.1145/3236024.3236055\\n\\n\\n\\n[29] Jianjun Huang, Zhichun Li, Xusheng Xiao, Zhenyu Wu, Kangjie Lu, Xiangyu\\nZhang, and Guofei Jiang. 2015. SUPOR: Precise and scalable sensitive user input\\ndetection for android apps. In *Proceedings of the 24th USENIX Conference on*\\n*Security Symposium* (Washington, D.C.) *(SEC’15)* . USENIX Association, USA,\\n977–992.\\n\\n[30] IBM. 2021. *Rational Functional Tester* . https://www.ibm.com/support/pages/\\nrational-functional-tester-v1013\\n\\n[31] K. Jaganeshwari and S. Djodilatchoumy. 2021. A Novel approach of GUI Mapping\\nwith image based widget detection and classification. In *2021 2nd International*\\n*Conference on Intelligent Engineering and Management (ICIEM)* . 342–346. https:\\n//doi.org/10.1109/ICIEM51511.2021.9445281\\n\\n[32] Jinseong Jeon and Jeffrey Foster. 2012. Troyd: Integration Testing for Android.\\n*Technical Report CS-TR-5013, Department of Computer Science, University of Mary-*\\n*land, College Park* (2012).\\n\\n[33] Yavuz Koroglu, Alper Sen, Ozlem Muslu, Yunus Mete, Ceyda Ulker, Tolga Tanriverdi, and Yunus Donmez. 2018. QBE: QLearning-Based Exploration of Android\\nApplications. In *2018 IEEE 11th International Conference on Software Testing, Veri-*\\n*fication and Validation (ICST)* . 105–115. https://doi.org/10.1109/ICST.2018.00020\\n\\n[34] Wing Lam, Zhengkai Wu, Dengfeng Li, Wenyu Wang, Haibing Zheng, Hui Luo,\\nPeng Yan, Yuetang Deng, and Tao Xie. 2017. Record and replay for Android: are\\nwe there yet in industrial cases?. In *Proceedings of the 2017 11th Joint Meeting*\\n*on Foundations of Software Engineering* (Paderborn, Germany) *(ESEC/FSE 2017)* .\\nAssociation for Computing Machinery, New York, NY, USA, 854–859. https:\\n//doi.org/10.1145/3106237.3117769\\n\\n[35] Yang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, and Zhiwei Guan. 2020.\\nWidget Captioning: Generating Natural Language Description for Mobile User\\nInterface Elements. arXiv:2010.04295\\n\\n[36] Mario Linares-Vásquez, Kevin Moran, and Denys Poshyvanyk. 2017. Continuous,\\nEvolutionary and Large-Scale: A New Perspective for Automated Mobile App\\nTesting. 399–410. https://doi.org/10.1109/ICSME.2017.27\\n\\n[37] Yadini Pérez López, Laís Dib Albuquerque, Gilmar Jóia de F. Costa Júnior,\\nDaniel Lopes Xavier, Juan David Ochoa, and Denizard Dimitri Camargo. 2023.\\nAdapting RICO Dataset for Boosting Graphical User Interface Component Classification for Automated Android Testing. In *2023 10th International Conference on*\\n*Soft Computing Machine Intelligence (ISCMI)* . 118–123. https://doi.org/10.1109/\\nISCMI59957.2023.10458576\\n\\n[38] Yadini Pérez López, Juan G. Colonna, Edson De Araujo Silva, Richard Hada\\nDegaki, and Javier Martinez Silva. 2022. Q-funcT: A Reinforcement Learning\\nApproach for Automated Black Box Functionality Testing. In *2022 IEEE 2nd*\\n*International Conference on Software Engineering and Artificial Intelligence (SEAI)* .\\n119–123. https://doi.org/10.1109/SEAI55746.2022.9832177\\n\\n[39] Ying Ma, ChuYi Yu, and Ming Yan. 2022. Icon Label Generation for Mobile Applications by Mean Teacher Learning. https://doi.org/10.21203/rs.3.rs-1888657/v1\\n\\n[40] Leonardo Mariani, Mauro Pezzè, Oliviero Riganelli, and Mauro Santoro. 2011.\\nAutoBlackTest: a tool for automatic black-box testing. In *2011 33rd International*\\n*Conference on Software Engineering (ICSE)* . 1013–1015. https://doi.org/10.1145/\\n1985793.1985979\\n\\n[41] Forough Mehralian, Navid Salehnamadi, and Sam Malek. 2021. Data-driven\\naccessibility repair revisited: on the effectiveness of generating labels for icons in\\nAndroid apps. In *Proceedings of the 29th ACM Joint Meeting on European Software*\\n*Engineering Conference and Symposium on the Foundations of Software Engineering*\\n(Athens, Greece) *(ESEC/FSE 2021)* . Association for Computing Machinery, New\\nYork, NY, USA, 107–118. https://doi.org/10.1145/3468264.3468604\\n\\n[42] Kevin Moran, Carlos Bernal-Cárdenas, Michael Curcio, Richard Bonett, and\\nDenys Poshyvanyk. 2018. *The ReDraw Dataset: A Set of Android Screenshots, GUI*\\n*Metadata, and Labeled Images of GUI Components* . https://zenodo.org/records/\\n2530277\\n\\n[43] Kevin Moran, Carlos Bernal-Cárdenas, Michael Curcio, Richard Bonett, and\\nDenys Poshyvanyk. 2020. Machine Learning-Based Prototyping of Graphical\\nUser Interfaces for Mobile Apps. *IEEE Transactions on Software Engineering* 46, 2\\n(2020), 196–221. https://doi.org/10.1109/TSE.2018.2844788\\n\\n[44] Seong-Guk Nam and Yeong-Seok Seo. 2023. GUI Component Detection-Based\\nAutomated Software Crash Diagnosis. *Electronics* 12, 11 (2023). https://doi.org/\\n10.3390/electronics12112382\\n\\n[45] Clemens Neudecker, Konstantin Baierer, Mike Gerber, Christian Clausner, Apostolos Antonacopoulos, and Stefan Pletschacher. 2021. A survey of OCR evaluation tools and metrics. In *Proceedings of the 6th International Workshop on*\\n*Historical Document Imaging and Processing* (Lausanne, Switzerland) *(HIP ’21)* .\\nAssociation for Computing Machinery, New York, NY, USA, 13–18. https:\\n//doi.org/10.1145/3476887.3476888\\n\\n[46] Tuan Anh Nguyen and Christoph Csallner. 2015. Reverse Engineering Mobile Application User Interfaces with REMAUI (T). In *2015 30th IEEE/ACM*\\n*International Conference on Automated Software Engineering (ASE)* . 248–259.\\nhttps://doi.org/10.1109/ASE.2015.32\\n\\n[47] Rafael Padilla, Sergio L. Netto, and Eduardo A. B. da Silva. 2020. A Survey\\non Performance Metrics for Object-Detection Algorithms. In *2020 International*\\n*Conference on Systems, Signals and Image Processing (IWSSIP)* . 237–242. https:\\n//doi.org/10.1109/IWSSIP48289.2020.9145130\\n\\n\\n142\\n\\n\\n-----\\n\\nEnhancing widget recognition for automated Android testing using Computer Vision techniques WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\n\\n[48] Minxue Pan, An Huang, Guoxin Wang, Tian Zhang, and Xuandong Li. 2020. Reinforcement learning based curiosity-driven testing of Android applications. In\\n*Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing*\\n*and Analysis* (Virtual Event, USA) *(ISSTA 2020)* . Association for Computing Machinery, New York, NY, USA, 153–164. https://doi.org/10.1145/3395363.3397354\\n\\n[49] Mahmood R, Mirzaei N, and Malek S. 2014. Automated Test Generation for Graphical User Interfaces: Challenges and Opportunities. *ACM Computing Surveys*\\n*(CSUR)* (2014), 45.\\n\\n[50] Dillon Reis, Jordan Kupec, Jacqueline Hong, and Ahmad Daoudi. 2024. Real-Time\\nFlying Object Detection with YOLOv8. (2024). https://doi.org/10.48550/arXiv.\\n2305.09972\\n\\n[51] Ariel Rosenfeld, Odaya Kardashov, and Orel Zang. 2018. Automation of Android\\nApplications Functional Testing Using Machine Learning Activities Classification.\\nIn *2018 IEEE/ACM 5th International Conference on Mobile Software Engineering*\\n*and Systems (MOBILESoft)* . 122–132.\\n\\n[52] Xiaolei Sun, Tongyu Li, and Jianfeng Xu. 2020. UI Components Recognition\\nSystem Based On Image Understanding. In *2020 IEEE 20th International Conference*\\n*on Software Quality, Reliability and Security Companion (QRS-C)* . 65–71. https:\\n//doi.org/10.1109/QRS-C51114.2020.00022\\n\\n[53] Amrita Vazirani. 2022. *What is Android UI Testing?* https://www.browserstack.\\ncom/guide/what-is-android-ui-testing\\n\\n[54] Tuyet Vuong and Shingo Takada. 2019. Semantic analysis for deep Q-network\\nin android GUI testing. In *Proceedings - SEKE 2019 (Proceedings of the Interna-*\\n*tional Conference on Software Engineering and Knowledge Engineering, SEKE)* .\\nKnowledge Systems Institute Graduate School, 123–128. https://doi.org/10.\\n18293/SEKE2019-080 31st International Conference on Software Engineering\\nand Knowledge Engineering, SEKE 2019 ; Conference date: 10-07-2019 Through\\n12-07-2019.\\n\\n[55] Thi Anh Tuyet Vuong and Shingo Takada. 2018. A reinforcement learning\\nbased approach to automated testing of Android applications. In *Proceedings of*\\n*the 9th ACM SIGSOFT International Workshop on Automating TEST Case Design,*\\n*Selection, and Evaluation* (Lake Buena Vista, FL, USA) *(A-TEST 2018)* . Association\\n\\n\\nfor Computing Machinery, New York, NY, USA, 31–37. https://doi.org/10.1145/\\n3278186.3278191\\n\\n[56] Bryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman, and Yang\\nLi. 2021. Screen2Words: Automatic Mobile UI Summarization with Multimodal\\n\\nLearning. arXiv:2108.03353\\n\\n[57] Thomas D. White, Gordon Fraser, and Guy J. Brown. 2019. Improving random\\nGUI testing with image-based widget detection. In *Proceedings of the 28th ACM*\\n*SIGSOFT International Symposium on Software Testing and Analysis* (Beijing,\\nChina) *(ISSTA 2019)* . Association for Computing Machinery, New York, NY, USA,\\n307–317. https://doi.org/10.1145/3293882.3330551\\n\\n[58] Xusheng Xiao, Xiaoyin Wang, Zhihao Cao, Hanlin Wang, and Peng Gao. 2019.\\nIconIntent: Automatic Identification of Sensitive UI Widgets Based on Icon Classification for Android Apps. In *2019 IEEE/ACM 41st International Conference on*\\n*Software Engineering (ICSE)* . 257–268. https://doi.org/10.1109/ICSE.2019.00041\\n\\n[59] Mulong Xie, Sidong Feng, Zhenchang Xing, Jieshan Chen, and Chunyang Chen.\\n2020. UIED: a hybrid tool for GUI element detection. In *Proceedings of the 28th*\\n*ACM Joint Meeting on European Software Engineering Conference and Symposium*\\n*on the Foundations of Software Engineering* (Virtual Event, USA) *(ESEC/FSE 2020)* .\\nAssociation for Computing Machinery, New York, NY, USA, 1655–1659. https:\\n//doi.org/10.1145/3368089.3417940\\n\\n[60] Feng Xue. 2020. Automated mobile apps testing from visual perspective. In *Pro-*\\n*ceedings of the 29th ACM SIGSOFT International Symposium on Software Testing*\\n*and Analysis* (Virtual Event, USA) *(ISSTA 2020)* . Association for Computing Machinery, New York, NY, USA, 577–581. https://doi.org/10.1145/3395363.3402644\\n\\n[61] Shengcheng Yu, Chunrong Fang, Tongyu Li, Mingzhe Du, Xuan Li, Jing Zhang,\\nYexiao Yun, Xu Wang, and Zhenyu Chen. 2021. Automated Mobile App Test Script\\nIntent Generation via Image and Code Understanding. arXiv:2107.05165 [cs.SE]\\n\\n[62] Xiaoyi Zhang, Lilian de Greef, Amanda Swearngin, Samuel White, Kyle Murray,\\nLisa Yu, Qi Shan, Jeffrey Nichols, Jason Wu, Chris Fleizach, Aaron Everitt, and\\nJeffrey P. Bigham. 2021. Screen Recognition: Creating Accessibility Metadata for\\nMobile Applications from Pixels. arXiv:2101.04893\\n\\n\\n143\\n\\n\\n-----\\n\\n',\n",
       "  'artigo_tokenizado': ['#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Enhancing',\n",
       "   'widget',\n",
       "   'recognition',\n",
       "   'for',\n",
       "   'automated',\n",
       "   'Android',\n",
       "   'testing',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   'using',\n",
       "   'Computer',\n",
       "   'Vision',\n",
       "   'techniques',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Yadini',\n",
       "   'Pérez',\n",
       "   'López',\n",
       "   '[',\n",
       "   '∗',\n",
       "   ']',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'yadini.lopez@sidia.com',\n",
       "   'Sidia',\n",
       "   'Research',\n",
       "   'and',\n",
       "   'Development',\n",
       "   'Institute',\n",
       "   'Manaus',\n",
       "   ',',\n",
       "   'AM',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Daniel',\n",
       "   'Lopes',\n",
       "   'Xavier',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'daniel.xavier@sidia.com',\n",
       "   'Sidia',\n",
       "   'Research',\n",
       "   'and',\n",
       "   'Development',\n",
       "   'Institute',\n",
       "   'Manaus',\n",
       "   ',',\n",
       "   'AM',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'ABSTRACT',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Laís',\n",
       "   'Dib',\n",
       "   'Albuquerque',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'lais.albuquerque@sidia.com',\n",
       "   'Sidia',\n",
       "   'Research',\n",
       "   'and',\n",
       "   'Development',\n",
       "   'Institute',\n",
       "   'Manaus',\n",
       "   ',',\n",
       "   'AM',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Leticia',\n",
       "   'Balbi',\n",
       "   '[',\n",
       "   '†',\n",
       "   ']',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'leticia.balbi@sidia.com',\n",
       "   'Sidia',\n",
       "   'Research',\n",
       "   'and',\n",
       "   'Development',\n",
       "   'Institute',\n",
       "   'Manaus',\n",
       "   ',',\n",
       "   'AM',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Richand',\n",
       "   'Degaki',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'rhd@icomp.ufam.edu.br',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Amazonas',\n",
       "   'Manaus',\n",
       "   ',',\n",
       "   'AM',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Gilmar',\n",
       "   'J.',\n",
       "   'F.',\n",
       "   'Costa',\n",
       "   'Júnior',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'gilmar.junior@sidia.com',\n",
       "   'Sidia',\n",
       "   'Research',\n",
       "   'and',\n",
       "   'Development',\n",
       "   'Institute',\n",
       "   'Manaus',\n",
       "   ',',\n",
       "   'AM',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Juan',\n",
       "   'G.',\n",
       "   'Colonna',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'juancolonna@icomp.ufam.edu.br',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Amazonas',\n",
       "   'Manaus',\n",
       "   ',',\n",
       "   'AM',\n",
       "   '\\n\\n\\n',\n",
       "   'Widget',\n",
       "   'recognition',\n",
       "   'is',\n",
       "   'crucial',\n",
       "   'for',\n",
       "   'automated',\n",
       "   'Android',\n",
       "   'black',\n",
       "   'box',\n",
       "   'testing',\n",
       "   '.',\n",
       "   'Over',\n",
       "   'the',\n",
       "   'past',\n",
       "   'ten',\n",
       "   'years',\n",
       "   ',',\n",
       "   'different',\n",
       "   'industrial',\n",
       "   'tools',\n",
       "   'and',\n",
       "   'academic',\n",
       "   '\\n',\n",
       "   'works',\n",
       "   'have',\n",
       "   'been',\n",
       "   'available',\n",
       "   'for',\n",
       "   'identifying',\n",
       "   'Graphical',\n",
       "   'User',\n",
       "   'Interface',\n",
       "   '\\n',\n",
       "   '(',\n",
       "   'GUI',\n",
       "   ')',\n",
       "   'components',\n",
       "   'in',\n",
       "   'Android',\n",
       "   'screens',\n",
       "   '.',\n",
       "   'Traditional',\n",
       "   'identification',\n",
       "   '\\n',\n",
       "   'methods',\n",
       "   ',',\n",
       "   'like',\n",
       "   'GUI',\n",
       "   'hierarchy',\n",
       "   'parsing',\n",
       "   ',',\n",
       "   'often',\n",
       "   'struggle',\n",
       "   'with',\n",
       "   'dynamic',\n",
       "   '\\n',\n",
       "   'content',\n",
       "   'and',\n",
       "   'complex',\n",
       "   'structures',\n",
       "   '.',\n",
       "   'In',\n",
       "   'contrast',\n",
       "   ',',\n",
       "   'Computer',\n",
       "   'Vision',\n",
       "   '(',\n",
       "   'CV',\n",
       "   ')',\n",
       "   '\\n',\n",
       "   'techniques',\n",
       "   'provide',\n",
       "   'greater',\n",
       "   'robustness',\n",
       "   'and',\n",
       "   'flexibility',\n",
       "   'to',\n",
       "   'adapt',\n",
       "   'to',\n",
       "   'different',\n",
       "   'screen',\n",
       "   'resolutions',\n",
       "   ',',\n",
       "   'design',\n",
       "   'specifications',\n",
       "   ',',\n",
       "   'and',\n",
       "   'patterns',\n",
       "   '.',\n",
       "   'However',\n",
       "   ',',\n",
       "   'the',\n",
       "   'CV',\n",
       "   '-',\n",
       "   'based',\n",
       "   'solutions',\n",
       "   'available',\n",
       "   'are',\n",
       "   'still',\n",
       "   'limited',\n",
       "   'concerning',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'variety',\n",
       "   'of',\n",
       "   'widgets',\n",
       "   'that',\n",
       "   'can',\n",
       "   'be',\n",
       "   'recognized',\n",
       "   '.',\n",
       "   'Moreover',\n",
       "   ',',\n",
       "   'the',\n",
       "   'current',\n",
       "   '\\n',\n",
       "   'identification',\n",
       "   'of',\n",
       "   'GUI',\n",
       "   'components',\n",
       "   'mainly',\n",
       "   'relies',\n",
       "   'on',\n",
       "   'classification',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'which',\n",
       "   'can',\n",
       "   'lead',\n",
       "   'to',\n",
       "   'ambiguous',\n",
       "   'lists',\n",
       "   'with',\n",
       "   'repeated',\n",
       "   'elements',\n",
       "   '.',\n",
       "   'In',\n",
       "   'this',\n",
       "   '\\n',\n",
       "   'paper',\n",
       "   ',',\n",
       "   'we',\n",
       "   'combine',\n",
       "   'different',\n",
       "   'CV',\n",
       "   '-',\n",
       "   'based',\n",
       "   'techniques',\n",
       "   'to',\n",
       "   'extract',\n",
       "   'contextbased',\n",
       "   'descriptions',\n",
       "   'for',\n",
       "   'each',\n",
       "   'widget',\n",
       "   ',',\n",
       "   'to',\n",
       "   'enhance',\n",
       "   'the',\n",
       "   'identification',\n",
       "   '\\n',\n",
       "   'process',\n",
       "   'by',\n",
       "   'going',\n",
       "   'beyond',\n",
       "   'class',\n",
       "   'recognition',\n",
       "   'for',\n",
       "   'describing',\n",
       "   'widgets',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'We',\n",
       "   'also',\n",
       "   'implemented',\n",
       "   'two',\n",
       "   'primary',\n",
       "   'CV',\n",
       "   '-',\n",
       "   'based',\n",
       "   'approaches',\n",
       "   'for',\n",
       "   'widget',\n",
       "   '\\n',\n",
       "   'recognition',\n",
       "   ':',\n",
       "   'Object',\n",
       "   'Detection',\n",
       "   'combined',\n",
       "   'with',\n",
       "   'Classification',\n",
       "   ',',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'a',\n",
       "   'One',\n",
       "   '-',\n",
       "   'Stage',\n",
       "   'Recognition',\n",
       "   'method',\n",
       "   '.',\n",
       "   'We',\n",
       "   'trained',\n",
       "   'and',\n",
       "   'evaluated',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'approaches',\n",
       "   'on',\n",
       "   'a',\n",
       "   'custom',\n",
       "   '105',\n",
       "   'classes',\n",
       "   'widget',\n",
       "   'dataset',\n",
       "   '.',\n",
       "   'Moreover',\n",
       "   ',',\n",
       "   'we',\n",
       "   '\\n',\n",
       "   'present',\n",
       "   'a',\n",
       "   'Computer',\n",
       "   'Vision',\n",
       "   '-',\n",
       "   'based',\n",
       "   'method',\n",
       "   'for',\n",
       "   'describing',\n",
       "   'widgets',\n",
       "   '\\n',\n",
       "   'using',\n",
       "   'their',\n",
       "   'contextual',\n",
       "   'meaning',\n",
       "   'on',\n",
       "   'Android',\n",
       "   'screen',\n",
       "   'captures',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'KEYWORDS',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Automated',\n",
       "   'Android',\n",
       "   'Testing',\n",
       "   ',',\n",
       "   'Test',\n",
       "   'Portability',\n",
       "   ',',\n",
       "   'Object',\n",
       "   'Recognition',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'Object',\n",
       "   'Classification',\n",
       "   ',',\n",
       "   'Optical',\n",
       "   'Character',\n",
       "   'Recognition',\n",
       "   ',',\n",
       "   'Graphical',\n",
       "   '\\n',\n",
       "   'User',\n",
       "   'Interface',\n",
       "   'component',\n",
       "   ',',\n",
       "   'Widget',\n",
       "   'Recognition',\n",
       "   '\\n\\n',\n",
       "   '∗',\n",
       "   'Correspondence',\n",
       "   'author',\n",
       "   '.',\n",
       "   '\\n\\n',\n",
       "   '-',\n",
       "   'Current',\n",
       "   'e',\n",
       "   '-',\n",
       "   'mail',\n",
       "   ':',\n",
       "   'leticiabalbisv@gmail.com',\n",
       "   '.',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   ':',\n",
       "   'Proceedings',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Web',\n",
       "   '(',\n",
       "   'WebMe',\n",
       "   '\\n',\n",
       "   'dia’2024',\n",
       "   ')',\n",
       "   '.',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '.',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   ':',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   ',',\n",
       "   '2024',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'SBC',\n",
       "   '–',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '-',\n",
       "   '2753',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   '1',\n",
       "   'INTRODUCTION',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Automated',\n",
       "   'testing',\n",
       "   'of',\n",
       "   'Android',\n",
       "   'applications',\n",
       "   'is',\n",
       "   'a',\n",
       "   'critical',\n",
       "   'component',\n",
       "   '\\n',\n",
       "   'in',\n",
       "   'ensuring',\n",
       "   'the',\n",
       "   'quality',\n",
       "   'and',\n",
       "   'reliability',\n",
       "   'of',\n",
       "   'software',\n",
       "   '.',\n",
       "   'One',\n",
       "   'key',\n",
       "   'aspect',\n",
       "   '\\n',\n",
       "   'of',\n",
       "   'this',\n",
       "   'process',\n",
       "   'is',\n",
       "   'widget',\n",
       "   'recognition',\n",
       "   ',',\n",
       "   'which',\n",
       "   'involves',\n",
       "   'identifying',\n",
       "   '\\n',\n",
       "   'and',\n",
       "   'interacting',\n",
       "   'with',\n",
       "   'various',\n",
       "   'elements',\n",
       "   'of',\n",
       "   'a',\n",
       "   'Graphical',\n",
       "   'User',\n",
       "   'Interface',\n",
       "   '\\n',\n",
       "   '(',\n",
       "   'GUI',\n",
       "   ')',\n",
       "   '[',\n",
       "   '36',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Effective',\n",
       "   'widget',\n",
       "   'recognition',\n",
       "   'allows',\n",
       "   'testing',\n",
       "   'frameworks',\n",
       "   '\\n',\n",
       "   'to',\n",
       "   'simulate',\n",
       "   'user',\n",
       "   'actions',\n",
       "   ',',\n",
       "   'verify',\n",
       "   'the',\n",
       "   'presence',\n",
       "   'and',\n",
       "   'state',\n",
       "   'of',\n",
       "   'User',\n",
       "   'Interface',\n",
       "   '(',\n",
       "   'UI',\n",
       "   ')',\n",
       "   'components',\n",
       "   ',',\n",
       "   'and',\n",
       "   'ensure',\n",
       "   'that',\n",
       "   'applications',\n",
       "   'respond',\n",
       "   'correctly',\n",
       "   'to',\n",
       "   'user',\n",
       "   'inputs',\n",
       "   '.',\n",
       "   'This',\n",
       "   'is',\n",
       "   'essential',\n",
       "   'for',\n",
       "   'detecting',\n",
       "   'and',\n",
       "   'diagnosing',\n",
       "   '\\n',\n",
       "   'issues',\n",
       "   'that',\n",
       "   'might',\n",
       "   'not',\n",
       "   'be',\n",
       "   'apparent',\n",
       "   'through',\n",
       "   'manual',\n",
       "   'testing',\n",
       "   'alone',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'ultimately',\n",
       "   'leading',\n",
       "   'to',\n",
       "   'higher',\n",
       "   '-',\n",
       "   'quality',\n",
       "   'software',\n",
       "   '[',\n",
       "   '34',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Current',\n",
       "   'approaches',\n",
       "   'to',\n",
       "   'widget',\n",
       "   'recognition',\n",
       "   'primarily',\n",
       "   'fall',\n",
       "   'into',\n",
       "   '\\n',\n",
       "   'two',\n",
       "   'categories',\n",
       "   ':',\n",
       "   'GUI',\n",
       "   'hierarchy',\n",
       "   'parsing',\n",
       "   'and',\n",
       "   'Computer',\n",
       "   'Vision',\n",
       "   '(',\n",
       "   'CV',\n",
       "   ')',\n",
       "   '\\n',\n",
       "   'techniques',\n",
       "   '.',\n",
       "   'The',\n",
       "   'first',\n",
       "   'one',\n",
       "   'involves',\n",
       "   'extracting',\n",
       "   'and',\n",
       "   'analyzing',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'structural',\n",
       "   'information',\n",
       "   'of',\n",
       "   'the',\n",
       "   'application',\n",
       "   '’s',\n",
       "   'UI',\n",
       "   ',',\n",
       "   'while',\n",
       "   'CV',\n",
       "   'techniques',\n",
       "   '\\n',\n",
       "   'rely',\n",
       "   'on',\n",
       "   'image',\n",
       "   'processing',\n",
       "   'to',\n",
       "   'recognize',\n",
       "   'widgets',\n",
       "   'directly',\n",
       "   'from',\n",
       "   'screenshots',\n",
       "   '[',\n",
       "   '4',\n",
       "   ',',\n",
       "   '17',\n",
       "   ',',\n",
       "   '31',\n",
       "   ',',\n",
       "   '33',\n",
       "   ',',\n",
       "   '40',\n",
       "   ']',\n",
       "   '.',\n",
       "   'While',\n",
       "   'GUI',\n",
       "   'hierarchy',\n",
       "   'parsing',\n",
       "   'is',\n",
       "   'intuitive',\n",
       "   '\\n',\n",
       "   'and',\n",
       "   'straightforward',\n",
       "   ',',\n",
       "   'it',\n",
       "   'often',\n",
       "   'struggles',\n",
       "   'with',\n",
       "   'dynamic',\n",
       "   'content',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'complex',\n",
       "   'structures',\n",
       "   ',',\n",
       "   'leading',\n",
       "   'to',\n",
       "   'incomplete',\n",
       "   'or',\n",
       "   'inaccurate',\n",
       "   'results',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'On',\n",
       "   'the',\n",
       "   'other',\n",
       "   'hand',\n",
       "   ',',\n",
       "   'CV',\n",
       "   '-',\n",
       "   'based',\n",
       "   'methods',\n",
       "   'offer',\n",
       "   'greater',\n",
       "   'flexibility',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'robustness',\n",
       "   ',',\n",
       "   'particularly',\n",
       "   'in',\n",
       "   'environments',\n",
       "   'with',\n",
       "   'frequent',\n",
       "   'interface',\n",
       "   '\\n',\n",
       "   'changes',\n",
       "   'or',\n",
       "   'non',\n",
       "   '-',\n",
       "   'standardized',\n",
       "   'structures',\n",
       "   '[',\n",
       "   '37',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'In',\n",
       "   'this',\n",
       "   'paper',\n",
       "   ',',\n",
       "   'we',\n",
       "   'explore',\n",
       "   'and',\n",
       "   'compare',\n",
       "   'two',\n",
       "   'primary',\n",
       "   'approaches',\n",
       "   '\\n',\n",
       "   'to',\n",
       "   'widget',\n",
       "   'recognition',\n",
       "   ':',\n",
       "   'Object',\n",
       "   'Detection',\n",
       "   'combined',\n",
       "   'with',\n",
       "   'Classification',\n",
       "   ',',\n",
       "   'and',\n",
       "   'a',\n",
       "   'One',\n",
       "   '-',\n",
       "   'Stage',\n",
       "   'Recognition',\n",
       "   'method',\n",
       "   '.',\n",
       "   'We',\n",
       "   'also',\n",
       "   'address',\n",
       "   'how',\n",
       "   '\\n',\n",
       "   'we',\n",
       "   'generated',\n",
       "   'two',\n",
       "   'datasets',\n",
       "   'tailored',\n",
       "   'for',\n",
       "   'deep',\n",
       "   'learning',\n",
       "   'widget',\n",
       "   'classification',\n",
       "   'and',\n",
       "   'recognition',\n",
       "   '.',\n",
       "   'Our',\n",
       "   'data',\n",
       "   'and',\n",
       "   'recognition',\n",
       "   'approaches',\n",
       "   '\\n',\n",
       "   'focus',\n",
       "   'on',\n",
       "   'identifying',\n",
       "   'a',\n",
       "   'wide',\n",
       "   'variety',\n",
       "   'of',\n",
       "   'widgets',\n",
       "   ',',\n",
       "   'up',\n",
       "   'to',\n",
       "   '105',\n",
       "   'different',\n",
       "   'classes',\n",
       "   ',',\n",
       "   'and',\n",
       "   'generating',\n",
       "   'UI',\n",
       "   'context',\n",
       "   '-',\n",
       "   'based',\n",
       "   'descriptions',\n",
       "   'for',\n",
       "   'each',\n",
       "   '\\n',\n",
       "   'widget',\n",
       "   ',',\n",
       "   'attempting',\n",
       "   'to',\n",
       "   'expand',\n",
       "   'the',\n",
       "   'scope',\n",
       "   'of',\n",
       "   'automated',\n",
       "   'black',\n",
       "   'box',\n",
       "   '\\n',\n",
       "   'Android',\n",
       "   'testing',\n",
       "   '[',\n",
       "   '37',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'The',\n",
       "   'main',\n",
       "   'contributions',\n",
       "   'of',\n",
       "   'our',\n",
       "   'work',\n",
       "   'are',\n",
       "   ':',\n",
       "   '\\n\\n\\n',\n",
       "   '133',\n",
       "   '\\n\\n\\n',\n",
       "   '-----',\n",
       "   '\\n\\n',\n",
       "   'WebMedia’2024',\n",
       "   ',',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   'López',\n",
       "   'et',\n",
       "   'al',\n",
       "   '.',\n",
       "   '\\n\\n',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Figure',\n",
       "   '1',\n",
       "   ':*',\n",
       "   '*',\n",
       "   'Two',\n",
       "   'main',\n",
       "   'approaches',\n",
       "   'identified',\n",
       "   'in',\n",
       "   'the',\n",
       "   'literature',\n",
       "   'and',\n",
       "   'industrial',\n",
       "   'tools',\n",
       "   '.',\n",
       "   '\\n\\n\\n',\n",
       "   '(',\n",
       "   '1',\n",
       "   ')',\n",
       "   'We',\n",
       "   'have',\n",
       "   'trained',\n",
       "   'two',\n",
       "   'widget',\n",
       "   'recognition',\n",
       "   'approaches',\n",
       "   'using',\n",
       "   '\\n',\n",
       "   'a',\n",
       "   'dataset',\n",
       "   'of',\n",
       "   '105',\n",
       "   'classes',\n",
       "   'of',\n",
       "   'GUI',\n",
       "   'components',\n",
       "   'and',\n",
       "   'compared',\n",
       "   '\\n',\n",
       "   'their',\n",
       "   'performance',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '(',\n",
       "   '2',\n",
       "   ')',\n",
       "   'We',\n",
       "   'have',\n",
       "   'trained',\n",
       "   'and',\n",
       "   'evaluated',\n",
       "   'a',\n",
       "   'widget',\n",
       "   'identification',\n",
       "   'method',\n",
       "   '\\n',\n",
       "   'that',\n",
       "   'is',\n",
       "   'not',\n",
       "   'limited',\n",
       "   'to',\n",
       "   'class',\n",
       "   'recognition',\n",
       "   'but',\n",
       "   'generates',\n",
       "   'contextbased',\n",
       "   'information',\n",
       "   'for',\n",
       "   'complementing',\n",
       "   'the',\n",
       "   'widget',\n",
       "   'descrip',\n",
       "   '\\n',\n",
       "   'tion',\n",
       "   '.',\n",
       "   '\\n\\n',\n",
       "   'To',\n",
       "   'the',\n",
       "   'best',\n",
       "   'of',\n",
       "   'our',\n",
       "   'knowledge',\n",
       "   ',',\n",
       "   'this',\n",
       "   'is',\n",
       "   'the',\n",
       "   'first',\n",
       "   'widget',\n",
       "   'identification',\n",
       "   '\\n',\n",
       "   'approach',\n",
       "   '(',\n",
       "   'recognition',\n",
       "   '+',\n",
       "   'smart',\n",
       "   'description',\n",
       "   ')',\n",
       "   'that',\n",
       "   'focus',\n",
       "   'on',\n",
       "   'identifying',\n",
       "   '\\n',\n",
       "   'a',\n",
       "   ...],\n",
       "  'pos_tagger': '',\n",
       "  'lema': ['enhance',\n",
       "   'widget',\n",
       "   'recognition',\n",
       "   'for',\n",
       "   'automated',\n",
       "   'Android',\n",
       "   'testing',\n",
       "   'use',\n",
       "   'Computer',\n",
       "   'Vision',\n",
       "   'technique',\n",
       "   'Yadini',\n",
       "   'Pérez',\n",
       "   'López',\n",
       "   '∗',\n",
       "   'yadini.lopez@sidia.com',\n",
       "   'Sidia',\n",
       "   'Research',\n",
       "   'and',\n",
       "   'Development',\n",
       "   'Institute',\n",
       "   'Manaus',\n",
       "   'AM',\n",
       "   'Daniel',\n",
       "   'Lopes',\n",
       "   'xavi',\n",
       "   'daniel.xavier@sidia.com',\n",
       "   'Sidia',\n",
       "   'Research',\n",
       "   'and',\n",
       "   'Development',\n",
       "   'Institute',\n",
       "   'Manaus',\n",
       "   'AM',\n",
       "   'ABSTRACT',\n",
       "   'Laís',\n",
       "   'Dib',\n",
       "   'Albuquerque',\n",
       "   'lais.albuquerque@sidia.com',\n",
       "   'Sidia',\n",
       "   'Research',\n",
       "   'and',\n",
       "   'Development',\n",
       "   'Institute',\n",
       "   'Manaus',\n",
       "   'AM',\n",
       "   'Leticia',\n",
       "   'Balbi',\n",
       "   'leticia.balbi@sidia.com',\n",
       "   'Sidia',\n",
       "   'Research',\n",
       "   'and',\n",
       "   'Development',\n",
       "   'Institute',\n",
       "   'Manaus',\n",
       "   'AM',\n",
       "   'Richand',\n",
       "   'Degaki',\n",
       "   'rhd@icomp.ufam.edu.br',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Amazonas',\n",
       "   'Manaus',\n",
       "   'AM',\n",
       "   'Gilmar',\n",
       "   'J.',\n",
       "   'F.',\n",
       "   'Costa',\n",
       "   'Júnior',\n",
       "   'gilmar.junior@sidia.com',\n",
       "   'Sidia',\n",
       "   'Research',\n",
       "   'and',\n",
       "   'Development',\n",
       "   'Institute',\n",
       "   'Manaus',\n",
       "   'AM',\n",
       "   'Juan',\n",
       "   'G.',\n",
       "   'Colonna',\n",
       "   'juancolonna@icomp.ufam.edu.br',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Amazonas',\n",
       "   'Manaus',\n",
       "   'AM',\n",
       "   'Widget',\n",
       "   'recognition',\n",
       "   'be',\n",
       "   'crucial',\n",
       "   'for',\n",
       "   'automated',\n",
       "   'Android',\n",
       "   'black',\n",
       "   'box',\n",
       "   'testing',\n",
       "   'over',\n",
       "   'the',\n",
       "   'past',\n",
       "   'ten',\n",
       "   'year',\n",
       "   'different',\n",
       "   'industrial',\n",
       "   'tool',\n",
       "   'and',\n",
       "   'academic',\n",
       "   'work',\n",
       "   'have',\n",
       "   'be',\n",
       "   'available',\n",
       "   'for',\n",
       "   'identify',\n",
       "   'Graphical',\n",
       "   'User',\n",
       "   'Interface',\n",
       "   'GUI',\n",
       "   'component',\n",
       "   'in',\n",
       "   'Android',\n",
       "   'screen',\n",
       "   'traditional',\n",
       "   'identification',\n",
       "   'method',\n",
       "   'like',\n",
       "   'GUI',\n",
       "   'hierarchy',\n",
       "   'parsing',\n",
       "   'often',\n",
       "   'struggle',\n",
       "   'with',\n",
       "   'dynamic',\n",
       "   'content',\n",
       "   'and',\n",
       "   'complex',\n",
       "   'structure',\n",
       "   'in',\n",
       "   'contrast',\n",
       "   'Computer',\n",
       "   'Vision',\n",
       "   'CV',\n",
       "   'technique',\n",
       "   'provide',\n",
       "   'great',\n",
       "   'robustness',\n",
       "   'and',\n",
       "   'flexibility',\n",
       "   'to',\n",
       "   'adapt',\n",
       "   'to',\n",
       "   'different',\n",
       "   'screen',\n",
       "   'resolution',\n",
       "   'design',\n",
       "   'specification',\n",
       "   'and',\n",
       "   'pattern',\n",
       "   'however',\n",
       "   'the',\n",
       "   'CV',\n",
       "   'base',\n",
       "   'solution',\n",
       "   'available',\n",
       "   'be',\n",
       "   'still',\n",
       "   'limited',\n",
       "   'concern',\n",
       "   'the',\n",
       "   'variety',\n",
       "   'of',\n",
       "   'widget',\n",
       "   'that',\n",
       "   'can',\n",
       "   'be',\n",
       "   'recognize',\n",
       "   'moreover',\n",
       "   'the',\n",
       "   'current',\n",
       "   'identification',\n",
       "   'of',\n",
       "   'GUI',\n",
       "   'component',\n",
       "   'mainly',\n",
       "   'rely',\n",
       "   'on',\n",
       "   'classification',\n",
       "   'which',\n",
       "   'can',\n",
       "   'lead',\n",
       "   'to',\n",
       "   'ambiguous',\n",
       "   'list',\n",
       "   'with',\n",
       "   'repeat',\n",
       "   'element',\n",
       "   'in',\n",
       "   'this',\n",
       "   'paper',\n",
       "   'we',\n",
       "   'combine',\n",
       "   'different',\n",
       "   'CV',\n",
       "   'base',\n",
       "   'technique',\n",
       "   'to',\n",
       "   'extract',\n",
       "   'contextbase',\n",
       "   'description',\n",
       "   'for',\n",
       "   'each',\n",
       "   'widget',\n",
       "   'to',\n",
       "   'enhance',\n",
       "   'the',\n",
       "   'identification',\n",
       "   'process',\n",
       "   'by',\n",
       "   'go',\n",
       "   'beyond',\n",
       "   'class',\n",
       "   'recognition',\n",
       "   'for',\n",
       "   'describe',\n",
       "   'widget',\n",
       "   'we',\n",
       "   'also',\n",
       "   'implement',\n",
       "   'two',\n",
       "   'primary',\n",
       "   'cv',\n",
       "   'base',\n",
       "   'approach',\n",
       "   'for',\n",
       "   'widget',\n",
       "   'recognition',\n",
       "   'Object',\n",
       "   'Detection',\n",
       "   'combine',\n",
       "   'with',\n",
       "   'Classification',\n",
       "   'and',\n",
       "   'a',\n",
       "   'one',\n",
       "   'stage',\n",
       "   'Recognition',\n",
       "   'method',\n",
       "   'we',\n",
       "   'train',\n",
       "   'and',\n",
       "   'evaluate',\n",
       "   'the',\n",
       "   'approach',\n",
       "   'on',\n",
       "   'a',\n",
       "   'custom',\n",
       "   '105',\n",
       "   'class',\n",
       "   'widget',\n",
       "   'dataset',\n",
       "   'moreover',\n",
       "   'we',\n",
       "   'present',\n",
       "   'a',\n",
       "   'Computer',\n",
       "   'Vision',\n",
       "   'base',\n",
       "   'method',\n",
       "   'for',\n",
       "   'describe',\n",
       "   'widget',\n",
       "   'use',\n",
       "   'their',\n",
       "   'contextual',\n",
       "   'meaning',\n",
       "   'on',\n",
       "   'Android',\n",
       "   'screen',\n",
       "   'capture',\n",
       "   'keyword',\n",
       "   'Automated',\n",
       "   'Android',\n",
       "   'Testing',\n",
       "   'Test',\n",
       "   'Portability',\n",
       "   'Object',\n",
       "   'Recognition',\n",
       "   'Object',\n",
       "   'Classification',\n",
       "   'Optical',\n",
       "   'Character',\n",
       "   'Recognition',\n",
       "   'Graphical',\n",
       "   'User',\n",
       "   'Interface',\n",
       "   'component',\n",
       "   'Widget',\n",
       "   'Recognition',\n",
       "   '∗',\n",
       "   'Correspondence',\n",
       "   'author',\n",
       "   'current',\n",
       "   'e',\n",
       "   'mail',\n",
       "   'leticiabalbisv@gmail.com',\n",
       "   'in',\n",
       "   'proceeding',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'web',\n",
       "   'WebMe',\n",
       "   'dia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   '2024',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'sbc',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '2753',\n",
       "   '1',\n",
       "   'introduction',\n",
       "   'automate',\n",
       "   'testing',\n",
       "   'of',\n",
       "   'Android',\n",
       "   'application',\n",
       "   'be',\n",
       "   'a',\n",
       "   'critical',\n",
       "   'component',\n",
       "   'in',\n",
       "   'ensure',\n",
       "   'the',\n",
       "   'quality',\n",
       "   'and',\n",
       "   'reliability',\n",
       "   'of',\n",
       "   'software',\n",
       "   'one',\n",
       "   'key',\n",
       "   'aspect',\n",
       "   'of',\n",
       "   'this',\n",
       "   'process',\n",
       "   'be',\n",
       "   'widget',\n",
       "   'recognition',\n",
       "   'which',\n",
       "   'involve',\n",
       "   'identify',\n",
       "   'and',\n",
       "   'interact',\n",
       "   'with',\n",
       "   'various',\n",
       "   'element',\n",
       "   'of',\n",
       "   'a',\n",
       "   'Graphical',\n",
       "   'User',\n",
       "   'Interface',\n",
       "   'GUI',\n",
       "   '36',\n",
       "   'effective',\n",
       "   'widget',\n",
       "   'recognition',\n",
       "   'allow',\n",
       "   'test',\n",
       "   'framework',\n",
       "   'to',\n",
       "   'simulate',\n",
       "   'user',\n",
       "   'action',\n",
       "   'verify',\n",
       "   'the',\n",
       "   'presence',\n",
       "   'and',\n",
       "   'state',\n",
       "   'of',\n",
       "   'User',\n",
       "   'Interface',\n",
       "   'UI',\n",
       "   'component',\n",
       "   'and',\n",
       "   'ensure',\n",
       "   'that',\n",
       "   'application',\n",
       "   'respond',\n",
       "   'correctly',\n",
       "   'to',\n",
       "   'user',\n",
       "   'input',\n",
       "   'this',\n",
       "   'be',\n",
       "   'essential',\n",
       "   'for',\n",
       "   'detect',\n",
       "   'and',\n",
       "   'diagnose',\n",
       "   'issue',\n",
       "   'that',\n",
       "   'might',\n",
       "   'not',\n",
       "   'be',\n",
       "   'apparent',\n",
       "   'through',\n",
       "   'manual',\n",
       "   'testing',\n",
       "   'alone',\n",
       "   'ultimately',\n",
       "   'lead',\n",
       "   'to',\n",
       "   'high',\n",
       "   'quality',\n",
       "   'software',\n",
       "   '34',\n",
       "   'current',\n",
       "   'approach',\n",
       "   'to',\n",
       "   'widget',\n",
       "   'recognition',\n",
       "   'primarily',\n",
       "   'fall',\n",
       "   'into',\n",
       "   'two',\n",
       "   'category',\n",
       "   'GUI',\n",
       "   'hierarchy',\n",
       "   'parsing',\n",
       "   'and',\n",
       "   'Computer',\n",
       "   'Vision',\n",
       "   'CV',\n",
       "   'technique',\n",
       "   'the',\n",
       "   'first',\n",
       "   'one',\n",
       "   'involve',\n",
       "   'extract',\n",
       "   'and',\n",
       "   'analyze',\n",
       "   'the',\n",
       "   'structural',\n",
       "   'information',\n",
       "   'of',\n",
       "   'the',\n",
       "   'application',\n",
       "   '’s',\n",
       "   'UI',\n",
       "   'while',\n",
       "   'CV',\n",
       "   'technique',\n",
       "   'rely',\n",
       "   'on',\n",
       "   'image',\n",
       "   'processing',\n",
       "   'to',\n",
       "   'recognize',\n",
       "   'widget',\n",
       "   'directly',\n",
       "   'from',\n",
       "   'screenshot',\n",
       "   '4',\n",
       "   '17',\n",
       "   '31',\n",
       "   '33',\n",
       "   '40',\n",
       "   'while',\n",
       "   'GUI',\n",
       "   'hierarchy',\n",
       "   'parsing',\n",
       "   'be',\n",
       "   'intuitive',\n",
       "   'and',\n",
       "   'straightforward',\n",
       "   'it',\n",
       "   'often',\n",
       "   'struggle',\n",
       "   'with',\n",
       "   'dynamic',\n",
       "   'content',\n",
       "   'and',\n",
       "   'complex',\n",
       "   'structure',\n",
       "   'lead',\n",
       "   'to',\n",
       "   'incomplete',\n",
       "   'or',\n",
       "   'inaccurate',\n",
       "   'result',\n",
       "   'on',\n",
       "   'the',\n",
       "   'other',\n",
       "   'hand',\n",
       "   'CV',\n",
       "   'base',\n",
       "   'method',\n",
       "   'offer',\n",
       "   'great',\n",
       "   'flexibility',\n",
       "   'and',\n",
       "   'robustness',\n",
       "   'particularly',\n",
       "   'in',\n",
       "   'environment',\n",
       "   'with',\n",
       "   'frequent',\n",
       "   'interface',\n",
       "   'change',\n",
       "   'or',\n",
       "   'non',\n",
       "   'standardized',\n",
       "   'structure',\n",
       "   '37',\n",
       "   'in',\n",
       "   'this',\n",
       "   'paper',\n",
       "   'we',\n",
       "   'explore',\n",
       "   'and',\n",
       "   'compare',\n",
       "   'two',\n",
       "   'primary',\n",
       "   'approach',\n",
       "   'to',\n",
       "   'widget',\n",
       "   'recognition',\n",
       "   'Object',\n",
       "   'Detection',\n",
       "   'combine',\n",
       "   'with',\n",
       "   'Classification',\n",
       "   'and',\n",
       "   'a',\n",
       "   'one',\n",
       "   'stage',\n",
       "   'Recognition',\n",
       "   'method',\n",
       "   'we',\n",
       "   'also',\n",
       "   'address',\n",
       "   'how',\n",
       "   'we',\n",
       "   'generate',\n",
       "   'two',\n",
       "   'dataset',\n",
       "   'tailor',\n",
       "   'for',\n",
       "   'deep',\n",
       "   'learning',\n",
       "   'widget',\n",
       "   'classification',\n",
       "   'and',\n",
       "   'recognition',\n",
       "   'our',\n",
       "   'datum',\n",
       "   'and',\n",
       "   'recognition',\n",
       "   'approach',\n",
       "   'focus',\n",
       "   'on',\n",
       "   'identify',\n",
       "   'a',\n",
       "   'wide',\n",
       "   'variety',\n",
       "   'of',\n",
       "   'widget',\n",
       "   'up',\n",
       "   'to',\n",
       "   '105',\n",
       "   'different',\n",
       "   'class',\n",
       "   'and',\n",
       "   'generate',\n",
       "   'UI',\n",
       "   'context',\n",
       "   'base',\n",
       "   'description',\n",
       "   'for',\n",
       "   'each',\n",
       "   'widget',\n",
       "   'attempt',\n",
       "   'to',\n",
       "   'expand',\n",
       "   'the',\n",
       "   'scope',\n",
       "   'of',\n",
       "   'automate',\n",
       "   'black',\n",
       "   'box',\n",
       "   'Android',\n",
       "   'testing',\n",
       "   '37',\n",
       "   'the',\n",
       "   'main',\n",
       "   'contribution',\n",
       "   'of',\n",
       "   'our',\n",
       "   'work',\n",
       "   'be',\n",
       "   '133',\n",
       "   'WebMedia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'López',\n",
       "   'et',\n",
       "   'al',\n",
       "   'figure',\n",
       "   '1',\n",
       "   'two',\n",
       "   'main',\n",
       "   'approach',\n",
       "   'identify',\n",
       "   'in',\n",
       "   'the',\n",
       "   'literature',\n",
       "   'and',\n",
       "   'industrial',\n",
       "   'tool',\n",
       "   '1',\n",
       "   'we',\n",
       "   'have',\n",
       "   'train',\n",
       "   'two',\n",
       "   'widget',\n",
       "   'recognition',\n",
       "   'approach',\n",
       "   'use',\n",
       "   'a',\n",
       "   'dataset',\n",
       "   'of',\n",
       "   '105',\n",
       "   'class',\n",
       "   'of',\n",
       "   'GUI',\n",
       "   'component',\n",
       "   'and',\n",
       "   'compare',\n",
       "   'their',\n",
       "   'performance',\n",
       "   '2',\n",
       "   'we',\n",
       "   'have',\n",
       "   'train',\n",
       "   'and',\n",
       "   'evaluate',\n",
       "   'a',\n",
       "   'widget',\n",
       "   'identification',\n",
       "   'method',\n",
       "   'that',\n",
       "   'be',\n",
       "   'not',\n",
       "   'limit',\n",
       "   'to',\n",
       "   'class',\n",
       "   'recognition',\n",
       "   'but',\n",
       "   'generate',\n",
       "   'contextbase',\n",
       "   'information',\n",
       "   'for',\n",
       "   'complement',\n",
       "   'the',\n",
       "   'widget',\n",
       "   'descrip',\n",
       "   'tion',\n",
       "   'to',\n",
       "   'the',\n",
       "   'good',\n",
       "   'of',\n",
       "   'our',\n",
       "   'knowledge',\n",
       "   'this',\n",
       "   'be',\n",
       "   'the',\n",
       "   'first',\n",
       "   'widget',\n",
       "   'identification',\n",
       "   'approach',\n",
       "   'recognition',\n",
       "   '+',\n",
       "   'smart',\n",
       "   'description',\n",
       "   'that',\n",
       "   'focus',\n",
       "   'on',\n",
       "   'identify',\n",
       "   'a',\n",
       "   'large',\n",
       "   'variety',\n",
       "   'of',\n",
       "   'widget',\n",
       "   'up',\n",
       "   'to',\n",
       "   '105',\n",
       "   'different',\n",
       "   'class',\n",
       "   'and',\n",
       "   'generate',\n",
       "   'a',\n",
       "   'context',\n",
       "   'base',\n",
       "   'description',\n",
       "   'for',\n",
       "   'each',\n",
       "   'widget',\n",
       "   'identify',\n",
       "   'the',\n",
       "   'rest',\n",
       "   'of',\n",
       "   'the',\n",
       "   'paper',\n",
       "   'be',\n",
       "   'organize',\n",
       "   'as',\n",
       "   'follow',\n",
       "   'section',\n",
       "   '2',\n",
       "   'review',\n",
       "   'the',\n",
       "   'main',\n",
       "   'knowledge',\n",
       "   'and',\n",
       "   'related',\n",
       "   'work',\n",
       "   'with',\n",
       "   'this',\n",
       "   'research',\n",
       "   'section',\n",
       "   '3',\n",
       "   'explain',\n",
       "   'our',\n",
       "   'methodology',\n",
       "   'detail',\n",
       "   'each',\n",
       "   'stage',\n",
       "   'of',\n",
       "   'the',\n",
       "   'widget',\n",
       "   'identification',\n",
       "   'process',\n",
       "   'include',\n",
       "   'recognition',\n",
       "   'smart',\n",
       "   'description',\n",
       "   'and',\n",
       "   'the',\n",
       "   'dataset',\n",
       "   'use',\n",
       "   'section',\n",
       "   '4',\n",
       "   'present',\n",
       "   'the',\n",
       "   'evaluation',\n",
       "   'of',\n",
       "   'our',\n",
       "   'method',\n",
       "   'compare',\n",
       "   'the',\n",
       "   'two',\n",
       "   'recognition',\n",
       "   'technique',\n",
       "   'and',\n",
       "   'assess',\n",
       "   'the',\n",
       "   'performance',\n",
       "   'of',\n",
       "   'our',\n",
       "   'smart',\n",
       "   'descriptor',\n",
       "   'method',\n",
       "   'we',\n",
       "   'also',\n",
       "   'address',\n",
       "   'five',\n",
       "   'research',\n",
       "   'question',\n",
       "   'to',\n",
       "   'demonstrate',\n",
       "   'our',\n",
       "   'contribution',\n",
       "   'finally',\n",
       "   'section',\n",
       "   '5',\n",
       "   'and',\n",
       "   '6',\n",
       "   'provide',\n",
       "   'our',\n",
       "   'final',\n",
       "   'consideration',\n",
       "   'and',\n",
       "   'conclusion',\n",
       "   'respectively',\n",
       "   '2',\n",
       "   'background',\n",
       "   'and',\n",
       "   'relate',\n",
       "   'WORKS',\n",
       "   'widget',\n",
       "   'recognition',\n",
       "   'be',\n",
       "   'crucial',\n",
       "   'for',\n",
       "   'Android',\n",
       "   'testing',\n",
       "   'automation',\n",
       "   'task',\n",
       "   'such',\n",
       "   'as',\n",
       "   'GUI',\n",
       "   'and',\n",
       "   'functional',\n",
       "   'verification',\n",
       "   'it',\n",
       "   'allow',\n",
       "   'the',\n",
       "   'testing',\n",
       "   'framework',\n",
       "   'to',\n",
       "   'interact',\n",
       "   'with',\n",
       "   'and',\n",
       "   'validate',\n",
       "   'the',\n",
       "   'behavior',\n",
       "   'of',\n",
       "   'the',\n",
       "   'application',\n",
       "   '’s',\n",
       "   'UI',\n",
       "   'element',\n",
       "   '53',\n",
       "   'by',\n",
       "   'recognize',\n",
       "   'widget',\n",
       "   'automated',\n",
       "   'test',\n",
       "   'can',\n",
       "   'simulate',\n",
       "   'user',\n",
       "   'action',\n",
       "   'verify',\n",
       "   'the',\n",
       "   'presence',\n",
       "   'and',\n",
       "   'state',\n",
       "   'of',\n",
       "   'UI',\n",
       "   'component',\n",
       "   'and',\n",
       "   'ensure',\n",
       "   'that',\n",
       "   'the',\n",
       "   'app',\n",
       "   'respond',\n",
       "   'correctly',\n",
       "   'to',\n",
       "   'user',\n",
       "   'input',\n",
       "   'this',\n",
       "   'level',\n",
       "   'of',\n",
       "   'interaction',\n",
       "   'be',\n",
       "   'essential',\n",
       "   'for',\n",
       "   'issue',\n",
       "   'detect',\n",
       "   'and',\n",
       "   'diagnose',\n",
       "   'that',\n",
       "   'might',\n",
       "   'not',\n",
       "   'be',\n",
       "   'apparent',\n",
       "   'through',\n",
       "   'manual',\n",
       "   'testing',\n",
       "   'alone',\n",
       "   'lead',\n",
       "   'to',\n",
       "   'high',\n",
       "   'quality',\n",
       "   'and',\n",
       "   'more',\n",
       "   'reliable',\n",
       "   'software',\n",
       "   'moreover',\n",
       "   'widget',\n",
       "   'recognition',\n",
       "   'facilitate',\n",
       "   'the',\n",
       "   'creation',\n",
       "   'of',\n",
       "   'more',\n",
       "   'robust',\n",
       "   'and',\n",
       "   'maintainable',\n",
       "   'test',\n",
       "   'script',\n",
       "   'as',\n",
       "   'the',\n",
       "   'test',\n",
       "   'can',\n",
       "   'adapt',\n",
       "   'to',\n",
       "   'cross',\n",
       "   'device',\n",
       "   'release',\n",
       "   'change',\n",
       "   'in',\n",
       "   'the',\n",
       "   'UI',\n",
       "   'layout',\n",
       "   'or',\n",
       "   'design',\n",
       "   'without',\n",
       "   'require',\n",
       "   'extensive',\n",
       "   'manual',\n",
       "   'update',\n",
       "   '16',\n",
       "   'furthermore',\n",
       "   'maximize',\n",
       "   'the',\n",
       "   'number',\n",
       "   'of',\n",
       "   'widget',\n",
       "   'type',\n",
       "   'that',\n",
       "   'a',\n",
       "   'GUI',\n",
       "   'recognizer',\n",
       "   'can',\n",
       "   'identify',\n",
       "   'be',\n",
       "   'paramount',\n",
       "   'for',\n",
       "   'automate',\n",
       "   'Android',\n",
       "   'testing',\n",
       "   'as',\n",
       "   'it',\n",
       "   'directly',\n",
       "   'influence',\n",
       "   'the',\n",
       "   'comprehensiveness',\n",
       "   'and',\n",
       "   'accuracy',\n",
       "   'of',\n",
       "   'the',\n",
       "   'testing',\n",
       "   'process',\n",
       "   'diverse',\n",
       "   'widget',\n",
       "   'recognition',\n",
       "   'ensure',\n",
       "   'the',\n",
       "   'testing',\n",
       "   'framework',\n",
       "   'can',\n",
       "   'interact',\n",
       "   'with',\n",
       "   'all',\n",
       "   'element',\n",
       "   'of',\n",
       "   'an',\n",
       "   'application',\n",
       "   '’s',\n",
       "   'interface',\n",
       "   'from',\n",
       "   'basic',\n",
       "   'button',\n",
       "   'and',\n",
       "   'text',\n",
       "   'field',\n",
       "   'to',\n",
       "   'complex',\n",
       "   'custom',\n",
       "   'widget',\n",
       "   ...]},\n",
       " {'titulo': 'Fast ISP Mode Decision for the Versatile Video Coding Intra Prediction Using Machine Learning',\n",
       "  'informacoes_url': '',\n",
       "  'idioma': 'english',\n",
       "  'storage_key': '../articles/original/english/985-24755-1-10-20240923.pdf',\n",
       "  'author': 'Larissa Araújo; Adson Duarte; Bruno Zatt; Guilherme Correa; and Daniel Palomino',\n",
       "  'data_publicacao': '11-09-2024',\n",
       "  'resumo': 'The Versatile Video Coding (VVC) standard achieves high compression rates by introducing new encoding tools, such as the Intra Subpartition Prediction (ISP). However, the ISP increases the computational effort necessary to perform the mode decision of the intra prediction step. This paper proposes a fast intra-mode decision solution for the ISP using machine learning. A Decision Tree is employed to predict the most promising ISP modes to be optimal to avoid the costly RDO test of ISP modes that are less likely to be chosen. By reducing the number of modes fully evaluated by the RDO process, the proposed solution achieves an average time-saving of 3.15% with only 0.11% of coding efficiency loss when tested for the common test conditions of VVC. Unlike the related  works, our solution avoids the time overhead of calculating image features by adopting features from the encoding process. Compared with related works, our solution presents competitive time-saving and coding efficiency results. ###',\n",
       "  'keywords': 'VVC, Intra Prediction, ISP, Machine Learning',\n",
       "  'referencias': ['[1] James Bergstra and Yoshua Bengio. 2012. Random search for hyper-parameter\\noptimization. *Journal of machine learning research* 13, 2 (2012), 281–305. https:\\n//www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf',\n",
       "   '[2] Gisle Bjontegaard. 2001. Calculation of average PSNR differences between RDcurves. https://www.itu.int/wftp3/av-arch/video-site/0104_Aus/VCEG-M33.doc\\nVCEG-M33.',\n",
       "   '[3] Frank Bossen, Jill Boyce, Karsten Sühring, Xiang Li, and Vadim Seregin. 2020.\\nVTM common test conditions and software reference configurations for SDR\\nvideo. Retrieved Aug 22, 2024 from https://jvet-experts.org/doc_end_user/\\ncurrent_document.php?id=10545 JVET-T2010-v1.\\n\\n\\n169\\n\\n\\n-----\\n\\nFast ISP Mode Decision for the Versatile Video Coding Intra Prediction Using Machine Learning WebMedia’2024, Juiz de Fora, Brazil',\n",
       "   '[4] Frank Bossen, Karsten Suehring, and Xiang Li. 2018. VTM reference software\\nfor VVC. Retrieved Aug 22, 2024 from https://vcgit.hhi.fraunhofer.de/jvet/\\nVVCSoftware_VTM',\n",
       "   '[5] Benjamin Bross, Ye-Kui Wang, Yan Ye, Shan Liu, Jianle Chen, Gary J. Sullivan,\\nand Jens-Rainer Ohm. 2021. Overview of the Versatile Video Coding (VVC)\\nStandard and its Applications. *IEEE Transactions on Circuits and Systems for Video*\\n*Technology* 31, 10 (2021), 3736–3764. https://doi.org/10.1109/TCSVT.2021.3101953',\n",
       "   '[6] L. Ceci. 2023. Live streaming - Statistics & Facts. Retrieved Jun 20, 2023 from\\nhttps://www.statista.com/topics/8906/live-streaming/#topicOverview',\n",
       "   '[7] Yao-Jen Chang, Hong-Jheng Jhu, Hui-Yu Jiang, Liang Zhao, Xin Zhao, Xiang\\nLi, Shan Liu, Benjamin Bross, Paul Keydel, Heiko Schwarz, Detlev Marpe, and\\nThomas Wiegand. 2019. Multiple Reference Line Coding for Most Probable\\nModes in Intra Prediction. In *2019 Data Compression Conference (DCC)* . IEEE,\\nSnowbird, UT, USA, 559–559. https://doi.org/10.1109/DCC.2019.00071',\n",
       "   '[8] Santiago De-Luxán-Hernández, Valeri George, Jackie Ma, Tung Nguyen, Heiko\\nSchwarz, Detlev Marpe, and Thomas Wiegand. 2019. An Intra Subpartition Coding Mode for VVC. In *2019 IEEE International Conference on Image Processing (ICIP)* .\\nIEEE, Taipei, Taiwan, 1203–1207. https://doi.org/10.1109/ICIP.2019.8803777',\n",
       "   '[9] Adson Duarte, Bruno Zatt, Guilherme Correa, and Daniel Palomino. 2023. Fast\\nIntra Mode Decision Using Machine Learning for the Versatile Video Coding Standard. In *2023 IEEE International Symposium on Circuits and Systems (ISCAS)* . IEEE,\\nMonterey, CA, USA, 1–5. https://doi.org/10.1109/ISCAS46773.2023.10181769',\n",
       "   '[10] International Telecommunication Union (ITU). 2023. Subjective video quality\\nassessment methods for multimedia applications. Retrieved Aug. 22, 2024 from\\nhttps://www.itu.int/rec/T-REC-P.910-202310-I/en',\n",
       "   '[11] Zhi Liu, Mengjun Dong, Xiao Guan, Mengmeng Zhang, and Ruoyu Wang. 2021.\\nFast ISP coding mode optimization algorithm based on CU texture complexity\\nfor VVC. *EURASIP Journal on Image and Video Processing* 2021 (07 2021). https:\\n//doi.org/10.1186/s13640-021-00564-4',\n",
       "   '[12] Alexandre Mercat, Arttu Mäkinen, Joose Sainio, Ari Lemmetti, Marko Viitanen,\\nand Jarno Vanne. 2021. Comparative Rate-Distortion-Complexity Analysis of\\nVVC and HEVC Video Codecs. *IEEE Access* 9 (2021), 67813–67828. https://doi.\\norg/10.1109/ACCESS.2021.3077116',\n",
       "   '[13] Jeeyoon Park, Bumyoon Kim, and Byeungwoo Jeon. 2020. Fast VVC intra prediction mode decision based on block shapes. In *Applications of Digital Image*\\n*Processing XLIII*, Andrew G. Tescher and Touradj Ebrahimi (Eds.), Vol. 11510. International Society for Optics and Photonics, SPIE, Basel, Switzerland, 115102H.\\n\\n\\nhttps://doi.org/10.1117/12.2567919',\n",
       "   '[14] Jeeyoon Park, Bumyoon Kim, Jeehwan Lee, and Byeungwoo Jeon. 2022. Machine\\nLearning-Based Early Skip Decision for Intra Subpartition Prediction in VVC.\\n*IEEE Access* 10 (2022), 111052–111065. https://doi.org/10.1109/ACCESS.2022.\\n3215163',\n",
       "   '[15] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel,\\nBertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron\\nWeiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau,\\nMatthieu Brucher, Matthieu Perrot, and Édouard Duchesnay. 2011. Scikit-learn:\\nMachine Learning in Python. *Journal of Machine Learning Research* 12, 85 (2011),\\n2825–2830. http://jmlr.org/papers/v12/pedregosa11a.html',\n",
       "   '[16] Jonathan Pfaff, Alexey Filippov, Shan Liu, Xin Zhao, Jianle Chen, Santiago DeLuxán-Hernández, Thomas Wiegand, Vasily Rufitskiy, Adarsh Krishnan Ramasubramonian, and Geert Van der Auwera. 2021. Intra Prediction and Mode Coding\\nin VVC. *IEEE Transactions on Circuits and Systems for Video Technology* 31, 10\\n(2021), 3834–3847. https://doi.org/10.1109/TCSVT.2021.3072430',\n",
       "   '[17] Mário Saldanha, Gustavo Sanchez, César Marcon, and Luciano Agostini. 2021.\\nLearning-Based Complexity Reduction Scheme for VVC Intra-Frame Prediction.\\nIn *2021 International Conference on Visual Communications and Image Processing*\\n*(VCIP)* . IEEE, Munich, Germany, 1–5. https://doi.org/10.1109/VCIP53242.2021.\\n9675394',\n",
       "   '[18] Michael Schäfer, Björn Stallenberger, Jonathan Pfaff, Philipp Helle, Heiko\\nSchwarz, Detlev Marpe, and Thomas Wiegand. 2019. An Affine-Linear Intra Prediction With Complexity Constraints. In *2019 IEEE International Con-*\\n*ference on Image Processing (ICIP)* . IEEE, Taipei, Taiwan, 1089–1093. https:\\n//doi.org/10.1109/ICIP.2019.8803724',\n",
       "   '[19] Ícaro Siqueira, Guilherme Correa, and Mateus Grellert. 2020. Rate-distortion and\\ncomplexity comparison of HEVC and VVC video encoders. In *2020 IEEE 11th*\\n*Latin American Symposium on Circuits & Systems (LASCAS)* . IEEE, San Jose, Costa\\nRica, 1–4.',\n",
       "   '[20] G.J. Sullivan and T. Wiegand. 1998. Rate-distortion optimization for video\\ncompression. *IEEE Signal Processing Magazine* 15, 6 (November 1998), 74–90.\\nhttps://doi.org/10.1109/79.733497',\n",
       "   '[21] Liang Zhao, Li Zhang, Siwei Ma, and Debin Zhao. 2011. Fast mode decision\\nalgorithm for intra prediction in HEVC. In *2011 Visual Communications and*\\n*Image Processing (VCIP)* . IEEE, Tainan, Taiwan, 1–4. https://doi.org/10.1109/VCIP.\\n2011.6115979\\n\\n\\n170\\n\\n\\n-----'],\n",
       "  'text': '# **Fast ISP Mode Decision for the Versatile Video Coding Intra** **Prediction Using Machine Learning**\\n\\n## Bruno Zatt\\n#### Video Technology Research Group (ViTech), Graduate Program in Computer Science (PPGC) Federal University of Pelotas (UFPel) Pelotas, Brazil zatt@inf.ufpel.edu.br\\n\\n## Larissa Araújo\\n#### Video Technology Research Group (ViTech), Graduate Program in Computer Science (PPGC) Federal University of Pelotas (UFPel) Pelotas, Brazil ldaaraujo@inf.ufpel.edu.br\\n\\n## Adson Duarte\\n#### Video Technology Research Group (ViTech), Graduate Program in Computer Science (PPGC) Federal University of Pelotas (UFPel) Pelotas, Brazil airduarte@inf.ufpel.edu.br\\n\\n## Guilherme Correa\\n#### Video Technology Research Group (ViTech), Graduate Program in Computer Science (PPGC) Federal University of Pelotas (UFPel) Pelotas, Brazil gcorrea@inf.ufpel.edu.br\\n### **ABSTRACT**\\n\\nThe Versatile Video Coding (VVC) standard achieves high compression rates by introducing new encoding tools, such as the Intra\\nSubpartition Prediction (ISP). However, the ISP increases the computational effort necessary to perform the mode decision of the\\nintra prediction step. This paper proposes a fast intra-mode decision solution for the ISP using machine learning. A Decision Tree\\nis employed to predict the most promising ISP modes to be optimal to avoid the costly RDO test of ISP modes that are less likely\\nto be chosen. By reducing the number of modes fully evaluated\\nby the RDO process, the proposed solution achieves an average\\ntime-saving of 3.15% with only 0.11% of coding efficiency loss when\\ntested for the common test conditions of VVC. Unlike the related\\n\\nworks, our solution avoids the time overhead of calculating image\\nfeatures by adopting features from the encoding process. Compared\\nwith related works, our solution presents competitive time-saving\\nand coding efficiency results.\\n### **KEYWORDS**\\n\\nVVC, Intra Prediction, ISP, Machine Learning\\n### **1 INTRODUCTION**\\n\\nDigital videos have been fundamental in many areas, from entertainment and communication to surveillance applications and\\nlive broadcasts. A study reveals that during the third quarter of\\n2022, live streams featuring gaming-related content accumulated\\napproximately 7.2 billion hours of content watched across leading\\nstreaming platforms [ 6 ]. In this context, video coding standards\\nsuch as the Versatile Video Coding (VVC) [ 5 ] play a crucial role\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n© 2024 SBC – Brazilian Computing Society.\\nISSN 2966-2753\\n\\n## Daniel Palomino\\n#### Video Technology Research Group (ViTech), Graduate Program in Computer Science (PPGC) Federal University of Pelotas (UFPel) Pelotas, Brazil dpalomino@inf.ufpel.edu.br\\n\\nin enabling applications to manipulate high-definition videos for\\nstorage and transmission.\\nThe VVC [ 5 ] is one of the latest and most advanced video coding\\nstandards. It offers superior bit-rate reduction without compromising visual quality when compared to its predecessor, the High\\nEfficiency Video Coding (HEVC) standard [ 19 ]. This is possible due\\nto several new encoding tools introduced in the standard, especially\\nin the intra-prediction step of VVC. While VVC maintains the Planar, DC, and Angular directional modes from HEVC, it extends the\\nnumber of Angular modes from 33 to 65. VVC also introduces the\\nMatrix-weighted Intra Prediction (MIP) [ 18 ] and the Intra Subpartition Prediction (ISP) [ 8 ] to improve prediction accuracy. Combined\\nwith the Planar, DC, and Angular modes, the ISP tool enhances\\nprediction granularity by processing a block through subpartitions\\nin the horizontal or vertical directions. Despite the coding efficiency\\nimprovements introduced by the new encoding tools of VVC, there\\nis a trade-off in encoding time, which makes it 34 times slower\\nthan HEVC [ 12 ]. Therefore, it is important to develop solutions to\\nimprove the encoding time by targeting the new intra-prediction\\ntools in VVC, such as the ISP tool.\\nSome related works propose solutions to save time in the intramode decision in VVC, specifically focusing on the ISP tool. The\\nmain idea of these works is to avoid the costly evaluation performed\\nby the Rate-Distortion Optimization (RDO) process for ISP modes\\nthat are less likely to be optimal. The works usually use heuristics\\nor machine learning solutions to predict the most promising modes.\\nFor instance, in [ 14 ], a machine learning model is trained with a\\nkey feature computed over the image known as the Mean Absolute Sum of Transform coefficients. The model predicts whether\\nthe evaluation for each ISP mode is necessary or can be skipped.\\nAnother approach, presented in [ 17 ], involves training a Decision\\nTree model over another image feature, the block’s variance. The\\nDecision Tree predicts when the evaluation of all ISP candidates\\ncan be skipped. In [ 11 ], the texture complexity of the block is obtained through an image feature called Mean Absolute Deviation.\\n\\n\\n162\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Larissa Araújo, Adson Duarte, Bruno Zatt, Guilherme Correa, and Daniel Palomino\\n\\n\\nThen, this feature is used to decide whether the evaluation of ISP\\ncandidates can be skipped. A heuristic algorithm is proposed in\\n\\n[ 13 ], where the list of ISP candidates is pre-pruned according to the\\nshape of the block and the ISP subpartition direction.\\nAlthough all these works report time-saving results on the ISP\\nmode decision, the solutions proposed by [ 14 ] and [ 11 ] rely on\\ncomputing image features, inevitably introducing a processing time\\noverhead to each one of these mode decision solutions — a fact that\\n\\nis not thoroughly discussed in these works. The solution proposed\\nby [ 13 ] is the only one that does not rely on computing image\\nfeatures. Nevertheless, none of the related works employ solutions\\nusing features available at encoding time, such as the modes of\\nneighbor blocks or rate-distortion costs calculated during the RDO\\nprocess. In this work, we call these features \" **encoding features** \"\\nsince they are available during the encoding process.\\nThis paper proposes a fast ISP mode decision solution using\\nmachine learning for the VVC intra prediction process. Our approach distinguishes from related works by using features available\\nat encoding time, aka encoding features, as input for the machine\\nlearning model. This approach avoids the computations necessary\\nto calculate image features as it is adopted by most of the related\\nworks. We classify the ISP candidates into two distinct classes based\\non their associated intra modes: ***(i) ISP Planar/DC***, comprising ISP\\nsubpartitions associated with Planar/DC modes, and ***(ii) ISP Angu-***\\n***lar***, comprising ISP subpartitions associated with Angular modes.\\nThen, we train a Decision Tree model to predict between these\\ntwo ISP classes using encoding features, such as rate-distortion\\ncosts for various intra-modes and neighbor decisions, which are\\naccessible before the ISP mode decision. Leveraging these features\\nallows our solution to avoid ISP modes that are less likely optimal,\\nsaving encoding time with negligible coding efficiency loss.\\n### **2 VVC INTRA SUBPARTITION PREDICTION**\\n\\nVVC introduced several innovations in the intra prediction. Firstly,\\nit enabled the prediction of video frames through blocks of square\\nand rectangular shapes, incorporating 17 block sizes. Considering\\nthe intra modes, the Planar and DC modes were preserved from the\\nprevious HEVC standard, while VVC expanded the Angular modes\\nfrom 33 to 65, which are indicated by the red arrows in Figure 1.\\nVVC also introduced a novel family of intra modes called MIP [ 18 ].\\nAlongside these, new tools that can be combined with the Planar,\\nDC, and Angular modes were incorporated, such as the Multiple\\nReference Line (MRL) [ 7 ], which extends the number of available\\nreference samples for intra prediction and is showcased in Figure 2,\\nand the ISP [ 8 ], shown in Figure 3, which is the primary focus of\\nthis work.\\n\\nThe ISP tool enables more granular block prediction. For this\\npurpose, the ISP tool horizontally or vertically divides the original\\nimage block into two or four subpartitions, depending on the block\\nsize, as illustrated in Figure 3. For blocks sized 8x4 and 4x8, the\\nISP tool generates only two subpartitions, either in the horizontal\\nor vertical direction, as shown in Figure 3(b) and Figure 3(c), respectively. This restriction ensures that each subpartition contains\\nat least 16 samples. For other block shapes, the ISP tool generates\\nfour subpartitions in either the horizontal or vertical direction, as\\nshown in Figure 3(a). The prediction process for each subpartition\\n\\n\\n34 50 66\\n\\n18\\n\\n\\n\\n2\\n\\n**Figure 1: Intra modes in VVC.**\\n#### MRL 2\\n\\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|\\n|---|---|---|---|---|---|---|\\n||MRL MRL 0 → Block h ← ← w →||||||\\n||||||||\\n||||||||\\n||||||||\\n||||||||\\n||||||||\\n||||||||\\n||||||||\\n||||||||\\n||||||||\\n||||||||\\n||||||||\\n||||||||\\n\\n\\n|Col1|Col2|Col3|Col4|\\n|---|---|---|---|\\n|||||\\n|||||\\n|||||\\n|||||\\n|||||\\n|||||\\n|||||\\n|||||\\n|||||\\n\\n\\n\\n**Figure 2: Multiple Reference Line tool.**\\n\\noccurs sequentially, and samples generated from the prediction of\\none subpartition are used as reference samples for the prediction\\nof the next subpartitions.\\nWhile predictions are conducted individually for each subpartition, they must converge to the same intra mode, such as Planar,\\nDC, or one of the Angular modes. This is done to improve the\\nprediction accuracy of each subpartition and also to save the bits\\nnecessary to signal the intra modes in the bitstream since only one\\nmode will be signaled. The addition of the ISP can improve coding\\nefficiency by approximately 0.57% with a 12% increase in encoding\\ntime [ 8 ]. This increase in encoding time occurs because the ISP tool\\nintroduces an additional step in the intra-mode decision process\\nof VVC. In this process, the encoder evaluates the Planar, DC, and\\nAngular modes twice. First, a list of these modes is evaluated for\\nthe entire block by the RDO, and then, they are evaluated again for\\neach possible ISP subpartition.\\nThe intra-mode decision process determines the best intra mode\\nfor each block by evaluating several possible combinations through\\nthe Rate-Distortion Optimization (RDO) process [ 20 ]. However,\\nthis process is computationally intensive since the encoder must\\nevaluate many intra-mode candidates. For each one of these modes,\\n\\n\\n163\\n\\n\\n-----\\n\\nFast ISP Mode Decision for the Versatile Video Coding Intra Prediction Using Machine Learning WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\n*W*\\n\\n\\n*4*\\n\\n\\n\\n*4*\\n\\n\\n*8*\\n\\n|Col1|Col2|Col3|\\n|---|---|---|\\n||HOR.||\\n\\n\\n\\n**BLOCK**\\n\\n*4*\\n\\n\\n**SPLIT**\\n\\n\\n*8*\\n\\n|Col1|Col2|Col3|Col4|\\n|---|---|---|---|\\n||VER.|||\\n\\n\\n\\n**BLOCK**\\n\\n*4*\\n\\n\\n*4*\\n\\n\\n*8*\\n\\n*2*\\n\\n*4*\\n\\n\\n*4*\\n\\n\\n**SPLIT**\\n\\n\\n*2*\\n\\n\\n*H*\\n\\n\\n\\n\\n*8*\\n\\n\\n*8*\\n\\n\\n*H*\\n\\n\\n*4*\\n\\n*8*\\n\\n**BLOCK**\\n\\n**(b) Two horizontal subpartitions.**\\n\\n**Figure 3: Intra Subpartitions Prediction Tool.**\\n\\n\\n**BLOCK**\\n\\n**(c) Two vertical subpartitions.**\\n\\n\\n**(a) Four subpartitions.**\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n**Figure 4: VTM standard intra mode decision.**\\n\\nthe rate-distortion cost must be computed by the intra-mode decision. This cost is available only after the encoding steps, including\\nprediction, direct and inverse transformation and quantization, and\\nentropy coding. To handle this complexity, the reference software\\nthat implements VVC, called VVC Test Model (VTM) [4], incorporates the Rough Mode Decision (RMD) [ 21 ] and the Most Probable\\nModes (MPM) list [ 16 ]. The main idea behind the RMD and MPM\\nsteps is to generate the RD-List, a subset of the most promising\\nmodes. Only this subset is evaluated by the RDO process, avoiding\\nthe evaluation of modes that are less likely to be optimal.\\nVTM performs the intra-mode decision following the steps of\\nFigure 4. The RMD, MPM, and MRL steps jointly select up to eight\\nintra modes to compose the RD-List. Both the RMD and MRL steps\\ncompute fast rate-distortion costs for the intra modes, selecting the\\nsix best ones. However, while the RMD evaluates the Planar, DC,\\nAngular, and MIP modes with MRL set to zero, as shown in Figure\\n2, the MRL step evaluates the MPM modes twice. One with the MRL\\nset to one and then with MRL set to two. The MPM step generates\\na list of six intra modes likely to be optimal for the current block\\nbased on the best intra modes in neighboring blocks [ 16 ]. The first\\nMPM mode is always the Planar mode, while the remaining ones\\ncan be the DC or one of the Angular modes. If the first two MPM\\nmodes are not already in the RD-List, the MPM step adds them. At\\nthe end of the RD-List, the encoder reserves 16 positions evenly\\ndistributed between the horizontal and vertical subpartitions for\\nISP. VTM includes the same intra modes obtained from the RMD\\n\\n\\nand MPM steps in the horizontal and vertical reserved positions,\\nexcluding the MIP modes. In addition, three Angular modes with\\nthe lowest costs during the RMD step, excluding the ones already\\nin the RD-List, are also selected for the ISP evaluation. Then, the\\nRDO evaluates all non-ISP modes (Planar, DC, Angular, and MIP\\nmodes) and only then starts the evaluation of the ISP modes.\\nWhile the RD-List is a subset of the most promising intra modes,\\nonly one will yield the best result in terms of coding efficiency.\\nEven when we consider only the subset of ISP candidates present\\nin the RD-List, the RDO must evaluate up to 16 modes for a single\\nblock to decide the best ISP mode. In this context, there is a need for\\nsolutions that target reducing the number of modes to be evaluated\\nby the RDO process. The solutions must accurately predict the most\\npromising ISP modes to reduce the computational effort required for\\nthe ISP mode decision with minimal loss of compression efficiency.\\n### **2.1 ISP Occurrence Rate Analysis**\\n\\nWe performed two analyses to evaluate the occurrence rate of the\\nPlanar, DC, and Angular modes during the ISP step. The idea is to\\nbuild our fast ISP mode decision solution based on the occurrence\\n\\nrate of each type of intra mode during the ISP process. We organized\\nthe ISP intra modes in two different classes: ***(i) ISP Planar/DC***\\nand ***(ii) ISP Angular*** . The reason for this classification relies on\\nthe nature of each intra mode. While the Planar and DC modes are\\n\\nbetter for predicting homogeneous textures, the Angular modes\\nare better for predicting directional textures.\\nIn the first analysis, we computed the occurrence rate of each\\nclass organized by block size. The occurrence rate of a given class\\nmeasures how often an intra mode within that class is the best when\\nthe final decision is an ISP. In the second analysis, we computed the\\nfrequency of Angular ISP candidates in the RD-List to understand\\nthe potential to avoid evaluating Angular ISP modes. We selected\\nthe same 15 videos used in the work of [ 9 ] for both analyses in\\nthis study. These videos were specifically chosen by the authors for\\ntheir variety in motion and texture characteristics, as indicated by\\nthe Spatial Information (SI) and Temporal Information (TI) metrics\\n\\n[ 10 ]. Each video was encoded using VTM 18.0 [ 4 ] with the *All Intra*\\nconfiguration and four Quantization Parameters (QP): 22, 27, 32,\\nand 37. Subsequently, for each block, we extracted the final mode\\ndecision (ISP Planar/DC or ISP Angular) and the number of ISP\\ncandidates in the RD-List during encoding. To achieve this, we\\n\\n\\n164\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Larissa Araújo, Adson Duarte, Bruno Zatt, Guilherme Correa, and Daniel Palomino\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|22%|32%|35%|36%|\\n|---|---|---|---|\\n|78%||||\\n||68%|65%|64%|\\n\\n|37%|39%|42%|46%|41%|43%|39%|43%|40%|\\n|---|---|---|---|---|---|---|---|---|\\n|63%|61%|58%|54%|59%|57%|61%|57%|60%|\\n\\n\\n\\n**Figure 5: Occurrence rate of ISP Planar/DC and ISP Angular**\\n**classes by block size.**\\n\\n\\n\\n\\n2.0\\n\\n1.5\\n\\n\\n\\n\\n1.0\\n\\n0.5\\n\\n0.0\\n\\n|1e9|Col2|Col3|Col4|\\n|---|---|---|---|\\n|||||\\n|||||\\n|||||\\n\\n|Col1|Col2|Col3|Col4|\\n|---|---|---|---|\\n|||||\\n\\n\\n0 2 4 6 8 10 12 14\\n# of Angular ISPs\\n\\n\\n**Figure 6: Frequency of angular ISPs in the RD-List.**\\n\\nmodified the VTM code and inserted routines at specific points to\\ncollect the necessary data.\\nFigure 5 presents the occurrence rate in which the ***(i) ISP Pla-***\\n***nar/DC*** and ***(ii) ISP Angular*** classes yield the best rate-distortion\\nresult by block size. We can observe that the ***(i) ISP Planar/DC***\\nclass has a higher occurrence rate when compared to the ***(ii) ISP***\\n***Angular*** class for larger block sizes, particularly for 64x64 and\\n32x32. Furthermore, the ***(i) ISP Planar/DC*** class maintains a higher\\noccurrence rate across all block sizes, obtaining 60% of the occurrence rate on average. In other words, when the ISP is chosen for\\nprediction, the Planar and DC modes are more likely to be chosen.\\nIn the second analysis in Figure 6, we can see the frequency of ISP\\ncandidates associated with Angular modes. We can observe that the\\nnumber of ISP candidates associated with angular modes is always\\neven. That happens because the VTM software evaluates the same\\nintra modes for both horizontal and vertical subpartitions in the ISP.\\nBesides, it is possible to notice that in most cases, there are 8, 10, and\\n12 ISP candidates associated with angular modes. This means that,\\nin most cases, VTM will evaluate 8 to 12 ISP candidates associated\\nwith angular modes for a single block. This finding highlights the\\npotential for reducing the computational effort of the intra mode\\ndecision in VVC by early predicting when the encoder can avoid\\nevaluating the ISP candidates associated with the Angular modes.\\nConsidering the high occurrence rate of the ***(i) ISP Planar/DC***\\nclass and the high frequency of the Angular ISP candidates in the\\nRD-List, it is possible to reduce the computational effort of the ISP\\nmode decision if an accurate machine learning model is employed\\nto predict when the best ISP mode is Planar or DC. When this\\n\\n\\n**Figure 7: Proposed fast ISP mode decision solution.**\\n\\nhappens, the RDO evaluation of many Angular ISP candidates can\\nbe skipped to save encoding time while minimizing the final coding\\nefficiency loss.\\n### **3 FAST ISP MODE DECISION**\\n\\nThis paper proposes a fast ISP mode decision using machine learning for the VVC standard. The main idea is to use the encoding\\nfeatures available at encoding time, thereby avoiding the additional\\noverhead of computing image features. Based on the two analyses previously presented, our solution groups the ISP candidates\\naccording to their associated intra modes in two classes: ***(i) ISP***\\n***Planar/DC*** class, containing the ISP candidates associated with the\\nPlanar and DC modes, and ***(ii) ISP Angular*** class, containing the\\nISP candidates associated with Angular modes. The goal was to ensure that the ISP Planar/DC class contains ISP candidates associated\\nwith Planar and DC modes, which are known to be effective at predicting homogeneous blocks, while the ISP Angular class includes\\nISP candidates associated with Angular modes, which are effective\\nat predicting blocks with directional textures. After defining the\\nclasses, we train a Decision Tree offline using encoding features to\\npredict between these two classes. Once training is complete, we\\nintegrate the final Decision Tree into the VTM. Whenever the Decision Tree predicts the ***(i) ISP Planar/DC*** class, the ISP candidates\\nbelonging to the ***(ii) ISP Angular*** class are not evaluated, saving\\nencoding time. The choice for a Decision Tree model is justified\\nfor two main reasons. First, Decision Trees usually present good\\nprediction accuracy for tabular datasets. Second, Decision Trees\\nhave a fast inference time. As our solution aims to save encoding\\ntime, we can not employ a complex model.\\nFigure 7 presents our solution, where the yellow rectangles represent the new steps introduced by the solution. Since the default\\nintra-mode decision of VTM evaluates all non-ISP modes (Planar,\\nDC, Angular, and MIP modes) and only then starts evaluating the\\n\\n\\n165\\n\\n\\n-----\\n\\nFast ISP Mode Decision for the Versatile Video Coding Intra Prediction Using Machine Learning WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\nISP modes, we added our solution between the RDO for the nonISP modes and the RDO for the ISP modes. This way, the default\\ndecision is the same until the RDO evaluation of the non-ISP modes.\\n\\nThen, the Decision Tree (DT) acquires a series of encoding features from the RMD, the MPM, the RD-List, and the RDO for nonISP modes to predict one of the previously mentioned ISP classes.\\nWhen the Decision Tree outcome is the ***(i) ISP Planar/DC*** class,\\nour solution removes the ISP candidates associated with Angular\\nmodes from the RD-List, and the RDO is performed only for ISP\\ncandidates associated with Planar/DC modes. On the other hand,\\nif the Decision Tree outcome is the ***(ii) ISP Angular*** class, all ISP\\ncandidates are evaluated. We decided never to remove the ISP candi\\ndates associated with Planar/DC modes to bring a balance between\\nthe time reduction and the coding efficiency loss, given the high\\noccurrence rate of the ISP Planar/DC class presented in Figure 5.\\n### **3.1 Feature Extraction and Dataset Generation**\\n\\nTable 1 presents the encoding features extracted from VTM that\\nwere used in our Decision Tree model. The features are presented\\nby name, description, the step where the feature is extracted from\\n(RMD, MPM, RD-List or RDO), the type (Boolean, Decimal or Integer), the *min* and *max* values for the feature, and the number of\\nvalues that the feature provides. RMD computes fast rate-distortion\\ncosts for the Planar, DC, Angular, and MIP modes. These fast costs\\nhint at whether the ISP Planar/DC or ISP Angular classes will produce the best cost. This way, considering the best costs in RMD\\nfor the Planar, DC, Angular, and MIP modes, we extract the Sum\\nof Absolute Differences (SAD), the Sum of Absolute Transformed\\nDifferences (SATD), the estimated number of bits, and the fast ratedistortion costs. Besides that, we also extract the *x* and *y* positions\\nof the block, the best angular and MIP modes, and the best angular\\nand DC MRL [7] numbers.\\nThe MPM provides a list containing six intra modes. The first\\none is always the Planar; the remaining modes will be the DC or\\none of the Angular modes. These six intra modes are likely the best\\nones since they were the best for the left and upper neighboring\\nblocks. Therefore, from the MPM, we extract the number of the\\nfive selected intra modes, excluding the Planar as it is constant, the\\nnumber of the best intra modes in the left and upper neighboring\\nblocks, eight boolean values indicating whether the best intra mode\\nfor the left and upper neighboring blocks is Planar, DC, Angular,\\nor MIP, and a boolean value indicating if the DC is an MPM.\\nThe VTM software distributes the non-ISP modes in the RD\\nList in ascending order according to their fast rate-distortion costs\\nobtained from the RMD step. This way, the modes distribution order\\nin the RD-List can also hint if the ISP Planar/DC or ISP Angular\\nclasses are likely to provide the best rate-distortion cost. Therefore,\\nfrom the RD-List, we extract the position of the first occurrence of\\nPlanar, DC, Angular, and MIP modes and also the mode number of\\nthe first Angular and MIP modes.\\nFinally, since the RDO evaluation for non-ISP modes occurs before the evaluation of the ISP modes, all the complete rate-distortion\\ncosts computed by the RDO for non-ISP modes are available. This\\nway, we extract the best rate-distortion costs obtained by the RDO\\nstep for the Planar, DC, Angular, and MIP modes. It is important\\nto highlight that despite the high number of features used by our\\n\\n\\n**Figure 8: Occurrence rate of ISP classes according to the posi-**\\n**tion of the first Angular mode in the RD-List.**\\n\\nmodel, there is no need for additional computations since they are\\nall available during the encoding process.\\nTo obtain the 48 features presented in Table 1 for training the\\nDecision Tree, we encoded the same 15 videos using the setup\\ndescribed in section 2.1. The dataset contains approximately 800,000\\nsamples balanced by block size, QP, video, and class. Because there\\nis a single dataset for all block sizes, we normalized the features\\nrelated to rate-distortion costs according to Equation (1), where *X* is\\nthe set containing the rate-distortion cost-related features, namely\\nthe SAD, SATD, FracBits, RMD Cost, and RDO Cost in Table 1, *w* is\\nthe width of the block and *h* is the height of the block.\\n\\n*𝑥*\\n*𝑥* _ *𝑛𝑜𝑟𝑚* = *𝑤*    - *ℎ* *[,]* *𝑥* ∈ *𝑋,* *𝑤,ℎ* ∈{4 *,* 8 *,* 16 *,* 32 *,* 64} (1)\\n\\nTo analyze the behavior of some of the features, we computed\\nthe information gain for each one of the encoding features considered in this work. Subsequently, the two features with the highest\\ninformation gains were selected to be analyzed, which are (1) the\\nposition of the first Angular mode in the RD-List and (2) the intra\\nmode that appears in the second position of the MPM list. The analysis of these two encoding features was conducted by computing\\nthe occurrence rate of the ISP Planar/DC and ISP Angular classes\\naccording to the feature’s respective values.\\nFigure 8 illustrates the occurrence rate of the ISP Planar/DC and\\nISP Angular classes according to the position of the first Angular\\nmode in the RD-List. One can notice that the first Angular mode\\noccurs from the first to the eleventh position in the RD-List. When\\nthe first Angular mode occurs in the first position of the RD-List,\\nthe ISP Planar/DC exhibits an occurrence rate of 26%, indicating\\nthat in 74% of the cases, the ISP Angular class results in the best\\nrate-distortion cost. However, as the position of the first Angular\\nmode in the RD-List increases, the ISP Planar/DC class occurrence\\nrate also increases. For instance, when the first Angular mode in\\nthe RD-List occurs from the second to the eleventh position, the ISP\\nPlanar/DC class always achieves a higher occurrence rate, peaking\\nat 97% when the first Angular mode occurs in the eleventh position\\nof the RD-List. In other words, as the position of the first Angular\\nmode in the RD-List increases, it also increases the cases where\\nVTM can avoid the RDO evaluation of the ISP Angular modes.\\nIn Figure 9, the occurrence rate of the ISP Planar/DC and ISP\\nAngular classes is shown according to the intra mode present in\\nthe second position of the MPM List. Since the second position of\\nthe MPM list can contain any intra mode among DC and Angular,\\n\\n\\n\\n\\n166\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Larissa Araújo, Adson Duarte, Bruno Zatt, Guilherme Correa, and Daniel Palomino\\n\\n**Table 1: Encoding features extracted from VTM.**\\n\\n**Name** **Description** **Step** **Type** **Min** **Max** **# of values**\\n\\nBlockPosition The *x* and *y* block positions. RMD Integer 0 4096 2\\nBestAngular Best angular mode. RMD Integer 2 66 1\\nBestMIP Best MIP mode. RMD Integer 0 7 1\\nMRLAngular MRL reference line from the best angular mode . RMD Integer 0 2 1\\nMRLDC MRL reference line from the best DC mode. RMD Integer 0 2 1\\nModesMPM MPM modes excluding Planar. MPM Integer 1 66 5\\nModesPosition Position of the first occurrence of each type of intra mode. RD-List Integer 1 11 4\\nFirstAngular First angular mode in the RD-List. RD-List Integer 2 66 1\\nFirstMIP First MIP mode in the RD-List. RD-List Integer 0 7 1\\nNeighborMode Best intra mode number in the neighboring blocks. MPM Integer 0 66 2\\nNeighborType Best intra mode type in the neighboring blocks. MPM Boolean 0 1 8\\nDCMPM DC mode is an MPM. MPM Boolean 0 1 1\\nSAD Sum of Absolute Differences. RMD Decimal 0.63 1390.38 4\\nSATD Sum of Absolute Transformed Differences. RMD Decimal 0.52 497.81 4\\nFracBits Estimated number of bits. RMD Decimal 1.19 15669.28 4\\n\\nRMD Cost Fast rate-distortion cost. RMD Decimal 0.65 502.01 4\\n\\nRDO Cost Complete rate-distortion cost. RDO Decimal 182.14 1881715.88 4\\n\\n**Total** **48**\\n\\n\\nIn summary, the analysis of these two features reveals the importance of encoding features in predicting when the RDO evaluation\\nof the ISP Angular modes can be avoided by VTM. For instance,\\nFigure 8 shows that when the first Angular mode in the RD-List\\noccurs from the second to the eleventh position, VTM can avoid the\\nRDO evaluation of the ISP Angular modes most of the time, given\\nthe higher occurrence rate of the ISP Planar/DC class. Specifically,\\nwhen the first Angular mode occurs in the ninth, tenth, or eleventh\\nposition, VTM can avoid the RDO evaluation of the ISP Angular\\nmodes in 89%, 84%, and 97% of the cases, respectively. Similarly,\\nFigure 9 demonstrates that when the DC mode is in the second\\nposition of the MPM list, VTM can avoid the RDO evaluation of\\nthe ISP Angular modes in 62% of the cases.\\nAlthough only the two features with the highest information\\ngains were analyzed, similar behavior is expected from the remaining features, such as the positions of the first occurrence of the\\nPlanar, DC, and MIP modes, the rate-distortion costs associated\\nwith the best Planar, DC, Angular, and MIP modes, and the intra\\nmode that occurs at each position of the MPM list. Since all these\\nencoding features are already computed by VTM and available at\\nencoding time, there is no additional overhead from computing\\nfeatures from the image, as is common in most related works.\\n### **3.2 Decision Tree Training**\\n\\n\\n\\n\\n\\n\\n**Figure 9: Occurrence rate of ISP classes according to the intra**\\n**mode in second position on the MPM list.**\\n\\nwe grouped the values of this feature into two categories: DC, with\\ncases where the second position of the MPM list has the DC mode,\\nand Angular, with cases where the second position of the MPM list\\nhas one of the Angular modes (modes 2 to 66 in Figure 1).\\nFrom Figure 9, one can see that when the second position of\\nthe MPM list has the DC mode, the ISP Planar/DC class achieves a\\n68% occurrence rate. This means that in only 32% of the cases, the\\nISP Angular class achieves the best rate-distortion cost. In contrast,\\nwhen the second position of the MPM list has one of the Angular\\nmodes, the opposite occurs, and the ISP Angular class achieves a\\nhigher occurrence rate of 68%. Therefore, the analysis of this feature\\nreveals that when the DC mode is the second in the MPM list, the\\nISP Planar/DC class has a higher chance of containing the best\\nrate-distortion cost. As a result, when the DC mode is the second\\nin the MPM list, VTM can avoid the RDO evaluation of the ISP\\nAngular modes in 68% of the cases.\\n\\n\\nUsing the Scikit-learn library [ 15 ], the dataset was split into 80%\\nfor training and validation, reserving the remaining 20% for testing.\\nThis division ensured that our model never saw 20% of the data\\n\\nthroughout the training and validation stages. We performed the\\ntraining and validation with two steps: a Random Search and a\\nGrid Search. The Random Search [ 1 ] step involved the evaluation\\nof 1,000 random combinations across a wide search space over the\\nhyperparameters *criterion*, *min samples split*, *min samples leaf*, *max*\\n*depth*, *max leaf nodes*, and *max features* . Each combination was\\n\\n\\n167\\n\\n\\n-----\\n\\nFast ISP Mode Decision for the Versatile Video Coding Intra Prediction Using Machine Learning WebMedia’2024, Juiz de Fora, Brazil\\n\\n#### ISP Angular ISP Planar/DC\\n\\n#### ISP ISP Angular Planar/DC Predicted label\\n\\n#### 0.8 0.6 0.4 0.2\\n\\n\\n**Figure 10: Confusion matrix for the final model in the test**\\n\\n**set.**\\n\\nevaluated using a 5-fold cross-validation approach over 80% of the\\ndata separated for the training and validation stages. Subsequently,\\nwe employed the Random Search results to compute the Pearson\\nCorrelation between each hyperparameter and the F1-score.\\nThe *max leaf nodes* and the *max features* were the two hyperparameters with the highest correlation with an increased F1-score.\\nThen, these two hyperparameters went for a refining phase in the\\nGrid Search step, while the remaining stayed at their default values\\nor the best values found in the Random Search. The Grid Search\\n\\nalso considered the evaluation of combinations through a 5-fold\\ncross-validation over the same 80% of the data separated for training and validation stages. The final model is obtained from the\\nhyperparameter combination that generated the best F1-score in\\nthe Grid Search.\\nFigure 10 presents the confusion matrix obtained by the final\\nDecision Tree model when evaluated under the 20% of the data not\\n\\nused during the Random Search and Grid Search steps, reserved for\\ntesting purposes. In the main diagonal, we have the accurate predictions made by the model, while down and above the main diagonal,\\nwe have the wrong predictions made by the model. The final model\\nobtained accuracies of 77% and 81% for the ISP Planar/DC and ISP\\nAngular classes, respectively.\\nSince we apply the Decision Tree model in the context of a\\nsolution to reduce the encoding time in video coding, we can classify\\nthe wrong predictions made by the model into two categories: **time**\\n**errors** and **coding efficiency errors** . A **time error** happens when\\nthe model misclassifies an example of the ISP Planar/DC class in\\nthe ISP Angular class. There is no coding efficiency loss when that\\nhappens because our solution will not remove the ISP candidates\\nassociated with the Planar/DC modes from the RD-List. However,\\n\\na **time error** occurs since the RDO evaluation for the ISP modes\\n\\ncould be performed exclusively for the ISP candidates associated\\nwith the Planar/DC modes. On the other hand, a **coding efficiency**\\n**error** happens when the model misclassifies an example belonging\\nto the ISP Angular class in the ISP Planar/DC class. Then, our\\nsolution removes the ISP candidates associated with the Angular\\nmodes, reducing the encoding time but providing a loss of coding\\nefficiency. By looking at Figure 10, **time errors** occur 23% of the\\ntime, while **coding efficiency errors** occur 19% of the time. Given\\nthe negligible difference between the two types of errors, these\\n\\n\\nresults highlight the model’s effectiveness in balancing the tradeoff between the time reduction and the loss of coding efficiency.\\n### **4 EXPERIMENTAL RESULTS**\\n\\nTo evaluate the performance of our solution, we followed the Common Test Conditions (CTC) [ 3 ] of VVC, where we encoded 22 video\\nsequences with the *All Intra* configuration and the QP values 22,\\n27, 32, and 37, both in the anchor VTM 18.0 and in the modified\\nVTM 18.0 with our solution. The modified VTM 18.0 includes the\\nfinal Decision Tree model, which is integrated to generate predictions for all processed blocks during encoding. We encoded the\\nvideos sequentially on a dedicated server with an Intel® Core™\\ni7-8700K processor and 16GB of RAM. **None** of the videos from the\\nCTC of VVC were used to train the Decision Tree. Therefore, we\\nevaluated our solution using videos that our model never saw. To\\nobtain the performance of our solution, we calculate two metrics:\\nthe time-saving (TS), obtained by comparing the encoding time of\\nthe anchor with our solution, and the coding efficiency, measured in\\nterms of BDBR [ 2 ], which calculates the bit-rate variation between\\ntwo encoders considering the same visual quality.\\nTable 2 presents the results regarding Class, Video, TS, and BDBR.\\nThe classes are defined according to the Common Test Conditions\\n(CTC) of VVC [ 3 ] and indicate the resolution of the videos. Videos\\nin classes A1 and A2 have 4K resolution, class B videos have 1080p\\nresolution, class C videos have 480p resolution, class D videos have\\n240p resolution, and class E videos have 720p resolution.\\nOur solution obtained a time saving of 3.15% with only 0.11% of\\ncoding efficiency loss on average. The best result is observed for the\\n*Campfire* video sequence, with a time saving of 5.18% and a coding\\nefficiency loss of 0.01%. This video achieved the best result because\\nit has a simple texture containing many homogeneous areas, which\\nis appropriate for the ISP Planar/DC class. As a result, our model\\npredicts the ISP Planar/DC class more often, avoiding evaluating\\nthe ISP Angular class. Conversely, the *FourPeople* video sequence\\npresented the worst result, with a time saving of 1.79% and a coding\\nefficiency loss of 0.11%. This video has a more complex texture,\\npresenting many edges that are more suitable for the ISP Angular\\nclass. Consequently, our model predicts this class more frequently,\\nreducing the time-saving potential.\\nFor high-definition videos, such as those in classes A1, A2, and B,\\nour solution achieves the highest time saving results, as shown by\\nthe averages for these specific classes in Table 2. For instance, in the\\nA1 class, which includes 4K resolution videos, our solution obtains\\nthe highest average time saving of 4.24% with only a 0.03% loss in\\ncoding efficiency. Similarly, in classes A2 and B, which contain 4K\\nand 1080p video resolutions, our solution achieves the second and\\nthird highest average time saving of 3.48% and 3.55%, respectively,\\nwith only a 0.06% and 0.11% loss in coding efficiency. These results\\nare extremely important because high-definition videos require\\nthe most significant encoding times. In other words, our solution\\neffectively saves time in the encoding of videos where it is most\\ncritical while introducing only minor losses in coding efficiency.\\nTo summarize, our solution demonstrates the capability to reduce\\nencoding time with minimal loss in coding efficiency across all\\nevaluated video sequences. This is possible through a Decision\\nTree model, which leverages features directly extracted from the\\n\\n\\n168\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Larissa Araújo, Adson Duarte, Bruno Zatt, Guilherme Correa, and Daniel Palomino\\n\\n\\n**Table 2: Time-saving and coding efficiency results.**\\n\\n**Class** **Video** **TS** **BDBR**\\n\\nTango2 4.07% 0.05%\\nA1 FoodMarket4 3.48% 0.02%\\n\\nCampFire 5.18% 0.01%\\n\\nCatRobot 2.79% 0.09%\\n\\nA2 DaylightRoad2 3.30% 0.06%\\nParkRunning3 4.36% 0.04%\\n\\n\\nB\\n\\nC\\n\\nD\\n\\n\\nMarketPlace 5.06% 0.05%\\n\\nRitualDance 3.80% 0.13%\\n\\nCactus 4.25% 0.13%\\n\\nBasketballDrive 2.24% 0.14%\\n\\nBQTerrace 2.40% 0.09%\\n\\nRaceHorsesC 4.69% 0.14%\\n\\nBQMall 1.92% 0.21%\\nPartyScene 3.06% 0.14%\\nBasketballDrill 2.08% 0.14%\\n\\nRaceHorses 3.13% 0.05%\\n\\nBQSquare 2.21% 0.14%\\nBlowingBubbles 3.82% 0.14%\\nBasketballPass 1.79% 0.16%\\n\\n\\nFourPeople 1.49% 0.18%\\nE Johnny 2.10% 0.19%\\nKristenAndSara 2.19% 0.13%\\n\\n**Average (A1)** **4.24%** **0.03%**\\n**Average (A2)** **3.48%** **0.06%**\\n**Average (B)** **3.55%** **0.11%**\\n**Average (Overall)** **3.15%** **0.11%**\\n\\nencoding process, thereby circumventing the need for additional\\ncomputations to generate input features.\\nTable 3 compares our solution with related works, considering\\nthe software version, time saving (TS), BDBR, and the TS/BDBR\\nratio, representing the trade-off between time saving and coding efficiency loss. The solutions proposed by both [ 17 ] and [ 13 ] obtained\\nbetter results in terms of time saving. However, they also obtained\\na higher coding efficiency loss than our solution. Specifically, [ 17 ]\\ndemonstrated a smaller TS/BDBR trade-off than our solution, while\\n\\n[ 13 ] showed a trade-off similar to ours. On the other hand, the\\nsolutions presented by [ 14 ] and [ 11 ] achieve a similar BDBR when\\ncompared to our solution while providing greater time saving and\\nTS/BDBR trade-off.\\nIt is essential to observe that the impact of the time spent computing the input features used by the solutions in [ 14 ]-[ 11 ] is not\\ndiscussed. These include calculating image features such as block\\nvariance, mean absolute deviation (which entails summing the differences between the value of each luminance sample and the mean\\nof all samples), and the mean absolute sum of transform coefficients.\\nThe latter involves the summation of all transformed coefficients\\nwithin the block.\\n\\n\\n**Table 3: Comparison with related works.**\\n\\n**Solution** **Software** **TS** **BDBR** **TS/BDBR**\\n\\n**Our** **VTM 18.0** **3.15%** **0.11%** **28.64**\\n\\nPark [14] VTM 11.0 7.20% 0.08% 90.00\\nSaldanha [17] VTM 10.0 8.32% 0.31% 26.84\\nLiu [11] VTM 08.0 7.00% 0.09% 77.78\\nPark [13] VTM 09.0 12.11% 0.43% 28.16\\n\\nConsidering the complexity of calculating such features, the timesaving results obtained by these works might vary significantly on\\ndifferent computing systems. For instance, many of these image\\nfeatures could be calculated in parallel with the encoding process\\non systems with an embedded GPU. However, if a GPU is unavailable, the image features should be calculated sequentially before the\\nproposed ISP mode decisions, adding significant computations and\\nreducing the potential for time-saving. Unlike these related works,\\nour solution circumvents time overhead by exclusively utilizing\\nfeatures derived from the encoding process, achieving competitive\\nresults even without the power of the image features. To the best\\nof our knowledge, our work is the first in its approach to incorporating encoding features to minimize the number of evaluated ISP\\ncandidates in the intra mode decision of VVC.\\n### **5 CONCLUSION**\\n\\nThis paper presented a fast ISP mode decision solution using machine learning for the VVC standard. An analysis of ISP mode\\noccurrence revealed the prevalence of Planar and DC modes when\\nthe ISP tool is used. In contrast, we found a high frequency of\\nmany Angular modes in the candidate modes list for ISP. From\\nthese findings, we decided to group the ISP modes into two classes\\naccording to their associated intra mode: ISP Planar/DC and ISP\\nAngular. Then, we employed a Decision Tree trained with encoding\\nfeatures to predict between these classes. Whenever the Decision\\nTree predicts the ISP Planar/DC class, our solution avoids evaluating the modes in the ISP Angular class, reducing the encoding\\ntime. The experimental results demonstrate the effectiveness of\\nthe proposed solution in reducing encoding time while preserving\\ncoding efficiency. Compared to related works, our solution presents\\ncompetitive results and avoids additional computations to generate\\nthe input features for the proposed fast mode decision solution.\\n### **ACKNOWLEDGMENTS**\\n\\nThe authors thank FAPERGS, CNPq and CAPES (Finance Code 001)\\nBrazilian research support agencies that financed this investigation.\\n### **REFERENCES**\\n\\n[1] James Bergstra and Yoshua Bengio. 2012. Random search for hyper-parameter\\noptimization. *Journal of machine learning research* 13, 2 (2012), 281–305. https:\\n//www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf\\n\\n[2] Gisle Bjontegaard. 2001. Calculation of average PSNR differences between RDcurves. https://www.itu.int/wftp3/av-arch/video-site/0104_Aus/VCEG-M33.doc\\nVCEG-M33.\\n\\n[3] Frank Bossen, Jill Boyce, Karsten Sühring, Xiang Li, and Vadim Seregin. 2020.\\nVTM common test conditions and software reference configurations for SDR\\nvideo. Retrieved Aug 22, 2024 from https://jvet-experts.org/doc_end_user/\\ncurrent_document.php?id=10545 JVET-T2010-v1.\\n\\n\\n169\\n\\n\\n-----\\n\\nFast ISP Mode Decision for the Versatile Video Coding Intra Prediction Using Machine Learning WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\n\\n[4] Frank Bossen, Karsten Suehring, and Xiang Li. 2018. VTM reference software\\nfor VVC. Retrieved Aug 22, 2024 from https://vcgit.hhi.fraunhofer.de/jvet/\\nVVCSoftware_VTM\\n\\n[5] Benjamin Bross, Ye-Kui Wang, Yan Ye, Shan Liu, Jianle Chen, Gary J. Sullivan,\\nand Jens-Rainer Ohm. 2021. Overview of the Versatile Video Coding (VVC)\\nStandard and its Applications. *IEEE Transactions on Circuits and Systems for Video*\\n*Technology* 31, 10 (2021), 3736–3764. https://doi.org/10.1109/TCSVT.2021.3101953\\n\\n[6] L. Ceci. 2023. Live streaming - Statistics & Facts. Retrieved Jun 20, 2023 from\\nhttps://www.statista.com/topics/8906/live-streaming/#topicOverview\\n\\n[7] Yao-Jen Chang, Hong-Jheng Jhu, Hui-Yu Jiang, Liang Zhao, Xin Zhao, Xiang\\nLi, Shan Liu, Benjamin Bross, Paul Keydel, Heiko Schwarz, Detlev Marpe, and\\nThomas Wiegand. 2019. Multiple Reference Line Coding for Most Probable\\nModes in Intra Prediction. In *2019 Data Compression Conference (DCC)* . IEEE,\\nSnowbird, UT, USA, 559–559. https://doi.org/10.1109/DCC.2019.00071\\n\\n[8] Santiago De-Luxán-Hernández, Valeri George, Jackie Ma, Tung Nguyen, Heiko\\nSchwarz, Detlev Marpe, and Thomas Wiegand. 2019. An Intra Subpartition Coding Mode for VVC. In *2019 IEEE International Conference on Image Processing (ICIP)* .\\nIEEE, Taipei, Taiwan, 1203–1207. https://doi.org/10.1109/ICIP.2019.8803777\\n\\n[9] Adson Duarte, Bruno Zatt, Guilherme Correa, and Daniel Palomino. 2023. Fast\\nIntra Mode Decision Using Machine Learning for the Versatile Video Coding Standard. In *2023 IEEE International Symposium on Circuits and Systems (ISCAS)* . IEEE,\\nMonterey, CA, USA, 1–5. https://doi.org/10.1109/ISCAS46773.2023.10181769\\n\\n[10] International Telecommunication Union (ITU). 2023. Subjective video quality\\nassessment methods for multimedia applications. Retrieved Aug. 22, 2024 from\\nhttps://www.itu.int/rec/T-REC-P.910-202310-I/en\\n\\n[11] Zhi Liu, Mengjun Dong, Xiao Guan, Mengmeng Zhang, and Ruoyu Wang. 2021.\\nFast ISP coding mode optimization algorithm based on CU texture complexity\\nfor VVC. *EURASIP Journal on Image and Video Processing* 2021 (07 2021). https:\\n//doi.org/10.1186/s13640-021-00564-4\\n\\n[12] Alexandre Mercat, Arttu Mäkinen, Joose Sainio, Ari Lemmetti, Marko Viitanen,\\nand Jarno Vanne. 2021. Comparative Rate-Distortion-Complexity Analysis of\\nVVC and HEVC Video Codecs. *IEEE Access* 9 (2021), 67813–67828. https://doi.\\norg/10.1109/ACCESS.2021.3077116\\n\\n[13] Jeeyoon Park, Bumyoon Kim, and Byeungwoo Jeon. 2020. Fast VVC intra prediction mode decision based on block shapes. In *Applications of Digital Image*\\n*Processing XLIII*, Andrew G. Tescher and Touradj Ebrahimi (Eds.), Vol. 11510. International Society for Optics and Photonics, SPIE, Basel, Switzerland, 115102H.\\n\\n\\nhttps://doi.org/10.1117/12.2567919\\n\\n[14] Jeeyoon Park, Bumyoon Kim, Jeehwan Lee, and Byeungwoo Jeon. 2022. Machine\\nLearning-Based Early Skip Decision for Intra Subpartition Prediction in VVC.\\n*IEEE Access* 10 (2022), 111052–111065. https://doi.org/10.1109/ACCESS.2022.\\n3215163\\n\\n[15] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel,\\nBertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron\\nWeiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau,\\nMatthieu Brucher, Matthieu Perrot, and Édouard Duchesnay. 2011. Scikit-learn:\\nMachine Learning in Python. *Journal of Machine Learning Research* 12, 85 (2011),\\n2825–2830. http://jmlr.org/papers/v12/pedregosa11a.html\\n\\n[16] Jonathan Pfaff, Alexey Filippov, Shan Liu, Xin Zhao, Jianle Chen, Santiago DeLuxán-Hernández, Thomas Wiegand, Vasily Rufitskiy, Adarsh Krishnan Ramasubramonian, and Geert Van der Auwera. 2021. Intra Prediction and Mode Coding\\nin VVC. *IEEE Transactions on Circuits and Systems for Video Technology* 31, 10\\n(2021), 3834–3847. https://doi.org/10.1109/TCSVT.2021.3072430\\n\\n[17] Mário Saldanha, Gustavo Sanchez, César Marcon, and Luciano Agostini. 2021.\\nLearning-Based Complexity Reduction Scheme for VVC Intra-Frame Prediction.\\nIn *2021 International Conference on Visual Communications and Image Processing*\\n*(VCIP)* . IEEE, Munich, Germany, 1–5. https://doi.org/10.1109/VCIP53242.2021.\\n9675394\\n\\n[18] Michael Schäfer, Björn Stallenberger, Jonathan Pfaff, Philipp Helle, Heiko\\nSchwarz, Detlev Marpe, and Thomas Wiegand. 2019. An Affine-Linear Intra Prediction With Complexity Constraints. In *2019 IEEE International Con-*\\n*ference on Image Processing (ICIP)* . IEEE, Taipei, Taiwan, 1089–1093. https:\\n//doi.org/10.1109/ICIP.2019.8803724\\n\\n[19] Ícaro Siqueira, Guilherme Correa, and Mateus Grellert. 2020. Rate-distortion and\\ncomplexity comparison of HEVC and VVC video encoders. In *2020 IEEE 11th*\\n*Latin American Symposium on Circuits & Systems (LASCAS)* . IEEE, San Jose, Costa\\nRica, 1–4.\\n\\n[20] G.J. Sullivan and T. Wiegand. 1998. Rate-distortion optimization for video\\ncompression. *IEEE Signal Processing Magazine* 15, 6 (November 1998), 74–90.\\nhttps://doi.org/10.1109/79.733497\\n\\n[21] Liang Zhao, Li Zhang, Siwei Ma, and Debin Zhao. 2011. Fast mode decision\\nalgorithm for intra prediction in HEVC. In *2011 Visual Communications and*\\n*Image Processing (VCIP)* . IEEE, Tainan, Taiwan, 1–4. https://doi.org/10.1109/VCIP.\\n2011.6115979\\n\\n\\n170\\n\\n\\n-----\\n\\n',\n",
       "  'artigo_tokenizado': ['#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Fast',\n",
       "   'ISP',\n",
       "   'Mode',\n",
       "   'Decision',\n",
       "   'for',\n",
       "   'the',\n",
       "   'Versatile',\n",
       "   'Video',\n",
       "   'Coding',\n",
       "   'Intra',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Prediction',\n",
       "   'Using',\n",
       "   'Machine',\n",
       "   'Learning',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Bruno',\n",
       "   'Zatt',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Video',\n",
       "   'Technology',\n",
       "   'Research',\n",
       "   'Group',\n",
       "   '(',\n",
       "   'ViTech',\n",
       "   ')',\n",
       "   ',',\n",
       "   'Graduate',\n",
       "   'Program',\n",
       "   'in',\n",
       "   'Computer',\n",
       "   'Science',\n",
       "   '(',\n",
       "   'PPGC',\n",
       "   ')',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Pelotas',\n",
       "   '(',\n",
       "   'UFPel',\n",
       "   ')',\n",
       "   'Pelotas',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   'zatt@inf.ufpel.edu.br',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Larissa',\n",
       "   'Araújo',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Video',\n",
       "   'Technology',\n",
       "   'Research',\n",
       "   'Group',\n",
       "   '(',\n",
       "   'ViTech',\n",
       "   ')',\n",
       "   ',',\n",
       "   'Graduate',\n",
       "   'Program',\n",
       "   'in',\n",
       "   'Computer',\n",
       "   'Science',\n",
       "   '(',\n",
       "   'PPGC',\n",
       "   ')',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Pelotas',\n",
       "   '(',\n",
       "   'UFPel',\n",
       "   ')',\n",
       "   'Pelotas',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   'ldaaraujo@inf.ufpel.edu.br',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Adson',\n",
       "   'Duarte',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Video',\n",
       "   'Technology',\n",
       "   'Research',\n",
       "   'Group',\n",
       "   '(',\n",
       "   'ViTech',\n",
       "   ')',\n",
       "   ',',\n",
       "   'Graduate',\n",
       "   'Program',\n",
       "   'in',\n",
       "   'Computer',\n",
       "   'Science',\n",
       "   '(',\n",
       "   'PPGC',\n",
       "   ')',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Pelotas',\n",
       "   '(',\n",
       "   'UFPel',\n",
       "   ')',\n",
       "   'Pelotas',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   'airduarte@inf.ufpel.edu.br',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Guilherme',\n",
       "   'Correa',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Video',\n",
       "   'Technology',\n",
       "   'Research',\n",
       "   'Group',\n",
       "   '(',\n",
       "   'ViTech',\n",
       "   ')',\n",
       "   ',',\n",
       "   'Graduate',\n",
       "   'Program',\n",
       "   'in',\n",
       "   'Computer',\n",
       "   'Science',\n",
       "   '(',\n",
       "   'PPGC',\n",
       "   ')',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Pelotas',\n",
       "   '(',\n",
       "   'UFPel',\n",
       "   ')',\n",
       "   'Pelotas',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   'gcorrea@inf.ufpel.edu.br',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'ABSTRACT',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'The',\n",
       "   'Versatile',\n",
       "   'Video',\n",
       "   'Coding',\n",
       "   '(',\n",
       "   'VVC',\n",
       "   ')',\n",
       "   'standard',\n",
       "   'achieves',\n",
       "   'high',\n",
       "   'compression',\n",
       "   'rates',\n",
       "   'by',\n",
       "   'introducing',\n",
       "   'new',\n",
       "   'encoding',\n",
       "   'tools',\n",
       "   ',',\n",
       "   'such',\n",
       "   'as',\n",
       "   'the',\n",
       "   'Intra',\n",
       "   '\\n',\n",
       "   'Subpartition',\n",
       "   'Prediction',\n",
       "   '(',\n",
       "   'ISP',\n",
       "   ')',\n",
       "   '.',\n",
       "   'However',\n",
       "   ',',\n",
       "   'the',\n",
       "   'ISP',\n",
       "   'increases',\n",
       "   'the',\n",
       "   'computational',\n",
       "   'effort',\n",
       "   'necessary',\n",
       "   'to',\n",
       "   'perform',\n",
       "   'the',\n",
       "   'mode',\n",
       "   'decision',\n",
       "   'of',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'intra',\n",
       "   'prediction',\n",
       "   'step',\n",
       "   '.',\n",
       "   'This',\n",
       "   'paper',\n",
       "   'proposes',\n",
       "   'a',\n",
       "   'fast',\n",
       "   'intra',\n",
       "   '-',\n",
       "   'mode',\n",
       "   'decision',\n",
       "   'solution',\n",
       "   'for',\n",
       "   'the',\n",
       "   'ISP',\n",
       "   'using',\n",
       "   'machine',\n",
       "   'learning',\n",
       "   '.',\n",
       "   'A',\n",
       "   'Decision',\n",
       "   'Tree',\n",
       "   '\\n',\n",
       "   'is',\n",
       "   'employed',\n",
       "   'to',\n",
       "   'predict',\n",
       "   'the',\n",
       "   'most',\n",
       "   'promising',\n",
       "   'ISP',\n",
       "   'modes',\n",
       "   'to',\n",
       "   'be',\n",
       "   'optimal',\n",
       "   'to',\n",
       "   'avoid',\n",
       "   'the',\n",
       "   'costly',\n",
       "   'RDO',\n",
       "   'test',\n",
       "   'of',\n",
       "   'ISP',\n",
       "   'modes',\n",
       "   'that',\n",
       "   'are',\n",
       "   'less',\n",
       "   'likely',\n",
       "   '\\n',\n",
       "   'to',\n",
       "   'be',\n",
       "   'chosen',\n",
       "   '.',\n",
       "   'By',\n",
       "   'reducing',\n",
       "   'the',\n",
       "   'number',\n",
       "   'of',\n",
       "   'modes',\n",
       "   'fully',\n",
       "   'evaluated',\n",
       "   '\\n',\n",
       "   'by',\n",
       "   'the',\n",
       "   'RDO',\n",
       "   'process',\n",
       "   ',',\n",
       "   'the',\n",
       "   'proposed',\n",
       "   'solution',\n",
       "   'achieves',\n",
       "   'an',\n",
       "   'average',\n",
       "   '\\n',\n",
       "   'time',\n",
       "   '-',\n",
       "   'saving',\n",
       "   'of',\n",
       "   '3.15',\n",
       "   '%',\n",
       "   'with',\n",
       "   'only',\n",
       "   '0.11',\n",
       "   '%',\n",
       "   'of',\n",
       "   'coding',\n",
       "   'efficiency',\n",
       "   'loss',\n",
       "   'when',\n",
       "   '\\n',\n",
       "   'tested',\n",
       "   'for',\n",
       "   'the',\n",
       "   'common',\n",
       "   'test',\n",
       "   'conditions',\n",
       "   'of',\n",
       "   'VVC',\n",
       "   '.',\n",
       "   'Unlike',\n",
       "   'the',\n",
       "   'related',\n",
       "   '\\n\\n',\n",
       "   'works',\n",
       "   ',',\n",
       "   'our',\n",
       "   'solution',\n",
       "   'avoids',\n",
       "   'the',\n",
       "   'time',\n",
       "   'overhead',\n",
       "   'of',\n",
       "   'calculating',\n",
       "   'image',\n",
       "   '\\n',\n",
       "   'features',\n",
       "   'by',\n",
       "   'adopting',\n",
       "   'features',\n",
       "   'from',\n",
       "   'the',\n",
       "   'encoding',\n",
       "   'process',\n",
       "   '.',\n",
       "   'Compared',\n",
       "   '\\n',\n",
       "   'with',\n",
       "   'related',\n",
       "   'works',\n",
       "   ',',\n",
       "   'our',\n",
       "   'solution',\n",
       "   'presents',\n",
       "   'competitive',\n",
       "   'time',\n",
       "   '-',\n",
       "   'saving',\n",
       "   '\\n',\n",
       "   'and',\n",
       "   'coding',\n",
       "   'efficiency',\n",
       "   'results',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'KEYWORDS',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'VVC',\n",
       "   ',',\n",
       "   'Intra',\n",
       "   'Prediction',\n",
       "   ',',\n",
       "   'ISP',\n",
       "   ',',\n",
       "   'Machine',\n",
       "   'Learning',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   '1',\n",
       "   'INTRODUCTION',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Digital',\n",
       "   'videos',\n",
       "   'have',\n",
       "   'been',\n",
       "   'fundamental',\n",
       "   'in',\n",
       "   'many',\n",
       "   'areas',\n",
       "   ',',\n",
       "   'from',\n",
       "   'entertainment',\n",
       "   'and',\n",
       "   'communication',\n",
       "   'to',\n",
       "   'surveillance',\n",
       "   'applications',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'live',\n",
       "   'broadcasts',\n",
       "   '.',\n",
       "   'A',\n",
       "   'study',\n",
       "   'reveals',\n",
       "   'that',\n",
       "   'during',\n",
       "   'the',\n",
       "   'third',\n",
       "   'quarter',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   '2022',\n",
       "   ',',\n",
       "   'live',\n",
       "   'streams',\n",
       "   'featuring',\n",
       "   'gaming',\n",
       "   '-',\n",
       "   'related',\n",
       "   'content',\n",
       "   'accumulated',\n",
       "   '\\n',\n",
       "   'approximately',\n",
       "   '7.2',\n",
       "   'billion',\n",
       "   'hours',\n",
       "   'of',\n",
       "   'content',\n",
       "   'watched',\n",
       "   'across',\n",
       "   'leading',\n",
       "   '\\n',\n",
       "   'streaming',\n",
       "   'platforms',\n",
       "   '[',\n",
       "   '6',\n",
       "   ']',\n",
       "   '.',\n",
       "   'In',\n",
       "   'this',\n",
       "   'context',\n",
       "   ',',\n",
       "   'video',\n",
       "   'coding',\n",
       "   'standards',\n",
       "   '\\n',\n",
       "   'such',\n",
       "   'as',\n",
       "   'the',\n",
       "   'Versatile',\n",
       "   'Video',\n",
       "   'Coding',\n",
       "   '(',\n",
       "   'VVC',\n",
       "   ')',\n",
       "   '[',\n",
       "   '5',\n",
       "   ']',\n",
       "   'play',\n",
       "   'a',\n",
       "   'crucial',\n",
       "   'role',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   ':',\n",
       "   'Proceedings',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Web',\n",
       "   '(',\n",
       "   'WebMe',\n",
       "   '\\n',\n",
       "   'dia’2024',\n",
       "   ')',\n",
       "   '.',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '.',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   ':',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   ',',\n",
       "   '2024',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'SBC',\n",
       "   '–',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '-',\n",
       "   '2753',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Daniel',\n",
       "   'Palomino',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Video',\n",
       "   'Technology',\n",
       "   'Research',\n",
       "   'Group',\n",
       "   '(',\n",
       "   'ViTech',\n",
       "   ')',\n",
       "   ',',\n",
       "   'Graduate',\n",
       "   'Program',\n",
       "   'in',\n",
       "   'Computer',\n",
       "   'Science',\n",
       "   '(',\n",
       "   'PPGC',\n",
       "   ')',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Pelotas',\n",
       "   '(',\n",
       "   'UFPel',\n",
       "   ')',\n",
       "   'Pelotas',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   'dpalomino@inf.ufpel.edu.br',\n",
       "   '\\n\\n',\n",
       "   'in',\n",
       "   'enabling',\n",
       "   'applications',\n",
       "   'to',\n",
       "   'manipulate',\n",
       "   'high',\n",
       "   '-',\n",
       "   'definition',\n",
       "   'videos',\n",
       "   'for',\n",
       "   '\\n',\n",
       "   'storage',\n",
       "   'and',\n",
       "   'transmission',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'The',\n",
       "   'VVC',\n",
       "   '[',\n",
       "   '5',\n",
       "   ']',\n",
       "   'is',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'latest',\n",
       "   'and',\n",
       "   'most',\n",
       "   'advanced',\n",
       "   'video',\n",
       "   'coding',\n",
       "   '\\n',\n",
       "   'standards',\n",
       "   '.',\n",
       "   'It',\n",
       "   'offers',\n",
       "   'superior',\n",
       "   'bit',\n",
       "   '-',\n",
       "   'rate',\n",
       "   'reduction',\n",
       "   'without',\n",
       "   'compromising',\n",
       "   'visual',\n",
       "   'quality',\n",
       "   'when',\n",
       "   'compared',\n",
       "   'to',\n",
       "   'its',\n",
       "   'predecessor',\n",
       "   ',',\n",
       "   'the',\n",
       "   'High',\n",
       "   '\\n',\n",
       "   'Efficiency',\n",
       "   'Video',\n",
       "   'Coding',\n",
       "   '(',\n",
       "   'HEVC',\n",
       "   ')',\n",
       "   'standard',\n",
       "   '[',\n",
       "   '19',\n",
       "   ']',\n",
       "   '.',\n",
       "   'This',\n",
       "   'is',\n",
       "   'possible',\n",
       "   'due',\n",
       "   '\\n',\n",
       "   'to',\n",
       "   'several',\n",
       "   'new',\n",
       "   'encoding',\n",
       "   'tools',\n",
       "   'introduced',\n",
       "   'in',\n",
       "   'the',\n",
       "   'standard',\n",
       "   ',',\n",
       "   'especially',\n",
       "   '\\n',\n",
       "   'in',\n",
       "   'the',\n",
       "   'intra',\n",
       "   '-',\n",
       "   'prediction',\n",
       "   'step',\n",
       "   'of',\n",
       "   'VVC',\n",
       "   '.',\n",
       "   'While',\n",
       "   'VVC',\n",
       "   'maintains',\n",
       "   'the',\n",
       "   'Planar',\n",
       "   ',',\n",
       "   'DC',\n",
       "   ',',\n",
       "   'and',\n",
       "   'Angular',\n",
       "   'directional',\n",
       "   'modes',\n",
       "   'from',\n",
       "   'HEVC',\n",
       "   ',',\n",
       "   'it',\n",
       "   'extends',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'number',\n",
       "   'of',\n",
       "   'Angular',\n",
       "   'modes',\n",
       "   'from',\n",
       "   '33',\n",
       "   'to',\n",
       "   '65',\n",
       "   '.',\n",
       "   'VVC',\n",
       "   'also',\n",
       "   'introduces',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'Matrix',\n",
       "   '-',\n",
       "   'weighted',\n",
       "   'Intra',\n",
       "   'Prediction',\n",
       "   '(',\n",
       "   'MIP',\n",
       "   ')',\n",
       "   '[',\n",
       "   '18',\n",
       "   ']',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Intra',\n",
       "   'Subpartition',\n",
       "   'Prediction',\n",
       "   '(',\n",
       "   'ISP',\n",
       "   ')',\n",
       "   '[',\n",
       "   '8',\n",
       "   ']',\n",
       "   'to',\n",
       "   'improve',\n",
       "   'prediction',\n",
       "   'accuracy',\n",
       "   '.',\n",
       "   'Combined',\n",
       "   '\\n',\n",
       "   'with',\n",
       "   'the',\n",
       "   'Planar',\n",
       "   ',',\n",
       "   'DC',\n",
       "   ',',\n",
       "   'and',\n",
       "   'Angular',\n",
       "   'modes',\n",
       "   ',',\n",
       "   'the',\n",
       "   'ISP',\n",
       "   'tool',\n",
       "   'enhances',\n",
       "   '\\n',\n",
       "   'prediction',\n",
       "   'granularity',\n",
       "   'by',\n",
       "   'processing',\n",
       "   'a',\n",
       "   'block',\n",
       "   'through',\n",
       "   'subpartitions',\n",
       "   '\\n',\n",
       "   'in',\n",
       "   'the',\n",
       "   'horizontal',\n",
       "   'or',\n",
       "   'vertical',\n",
       "   'directions',\n",
       "   '.',\n",
       "   'Despite',\n",
       "   'the',\n",
       "   'coding',\n",
       "   'efficiency',\n",
       "   '\\n',\n",
       "   'improvements',\n",
       "   'introduced',\n",
       "   'by',\n",
       "   'the',\n",
       "   'new',\n",
       "   'encoding',\n",
       "   'tools',\n",
       "   'of',\n",
       "   'VVC',\n",
       "   ',',\n",
       "   'there',\n",
       "   '\\n',\n",
       "   'is',\n",
       "   'a',\n",
       "   'trade',\n",
       "   '-',\n",
       "   'off',\n",
       "   'in',\n",
       "   'encoding',\n",
       "   'time',\n",
       "   ',',\n",
       "   'which',\n",
       "   'makes',\n",
       "   'it',\n",
       "   '34',\n",
       "   'times',\n",
       "   'slower',\n",
       "   '\\n',\n",
       "   'than',\n",
       "   'HEVC',\n",
       "   '[',\n",
       "   '12',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Therefore',\n",
       "   ',',\n",
       "   'it',\n",
       "   'is',\n",
       "   'important',\n",
       "   'to',\n",
       "   'develop',\n",
       "   'solutions',\n",
       "   'to',\n",
       "   '\\n',\n",
       "   'improve',\n",
       "   'the',\n",
       "   'encoding',\n",
       "   'time',\n",
       "   'by',\n",
       "   'targeting',\n",
       "   'the',\n",
       "   'new',\n",
       "   'intra',\n",
       "   '-',\n",
       "   'prediction',\n",
       "   '\\n',\n",
       "   'tools',\n",
       "   'in',\n",
       "   'VVC',\n",
       "   ',',\n",
       "   'such',\n",
       "   'as',\n",
       "   'the',\n",
       "   'ISP',\n",
       "   'tool',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Some',\n",
       "   'related',\n",
       "   'works',\n",
       "   'propose',\n",
       "   'solutions',\n",
       "   'to',\n",
       "   'save',\n",
       "   'time',\n",
       "   'in',\n",
       "   'the',\n",
       "   'intramode',\n",
       "   'decision',\n",
       "   'in',\n",
       "   'VVC',\n",
       "   ',',\n",
       "   'specifically',\n",
       "   'focusing',\n",
       "   'on',\n",
       "   'the',\n",
       "   'ISP',\n",
       "   'tool',\n",
       "   '.',\n",
       "   'The',\n",
       "   '\\n',\n",
       "   'main',\n",
       "   'idea',\n",
       "   'of',\n",
       "   'these',\n",
       "   'works',\n",
       "   'is',\n",
       "   'to',\n",
       "   'avoid',\n",
       "   'the',\n",
       "   'costly',\n",
       "   'evaluation',\n",
       "   'performed',\n",
       "   '\\n',\n",
       "   'by',\n",
       "   'the',\n",
       "   'Rate',\n",
       "   '-',\n",
       "   'Distortion',\n",
       "   'Optimization',\n",
       "   '(',\n",
       "   'RDO',\n",
       "   ')',\n",
       "   'process',\n",
       "   'for',\n",
       "   'ISP',\n",
       "   'modes',\n",
       "   '\\n',\n",
       "   'that',\n",
       "   'are',\n",
       "   'less',\n",
       "   'likely',\n",
       "   'to',\n",
       "   'be',\n",
       "   'optimal',\n",
       "   '.',\n",
       "   'The',\n",
       "   'works',\n",
       "   'usually',\n",
       "   'use',\n",
       "   'heuristics',\n",
       "   '\\n',\n",
       "   'or',\n",
       "   'machine',\n",
       "   'learning',\n",
       "   'solutions',\n",
       "   'to',\n",
       "   'predict',\n",
       "   'the',\n",
       "   'most',\n",
       "   'promising',\n",
       "   'modes',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'For',\n",
       "   'instance',\n",
       "   ',',\n",
       "   'in',\n",
       "   '[',\n",
       "   '14',\n",
       "   ']',\n",
       "   ',',\n",
       "   'a',\n",
       "   'machine',\n",
       "   'learning',\n",
       "   'model',\n",
       "   'is',\n",
       "   'trained',\n",
       "   'with',\n",
       "   'a',\n",
       "   '\\n',\n",
       "   'key',\n",
       "   'feature',\n",
       "   'computed',\n",
       "   'over',\n",
       "   'the',\n",
       "   'image',\n",
       "   'known',\n",
       "   'as',\n",
       "   'the',\n",
       "   'Mean',\n",
       "   'Absolute',\n",
       "   'Sum',\n",
       "   'of',\n",
       "   'Transform',\n",
       "   'coefficients',\n",
       "   '.',\n",
       "   'The',\n",
       "   'model',\n",
       "   'predicts',\n",
       "   'whether',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'evaluation',\n",
       "   'for',\n",
       "   'each',\n",
       "   'ISP',\n",
       "   'mode',\n",
       "   'is',\n",
       "   'necessary',\n",
       "   'or',\n",
       "   'can',\n",
       "   'be',\n",
       "   'skipped',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Another',\n",
       "   'approach',\n",
       "   ',',\n",
       "   'presented',\n",
       "   'in',\n",
       "   '[',\n",
       "   '17',\n",
       "   ']',\n",
       "   ',',\n",
       "   'involves',\n",
       "   'training',\n",
       "   'a',\n",
       "   'Decision',\n",
       "   '\\n',\n",
       "   'Tree',\n",
       "   'model',\n",
       "   'over',\n",
       "   'another',\n",
       "   'image',\n",
       "   'feature',\n",
       "   ',',\n",
       "   'the',\n",
       "   'block',\n",
       "   '’s',\n",
       "   'variance',\n",
       "   '.',\n",
       "   'The',\n",
       "   '\\n',\n",
       "   'Decision',\n",
       "   'Tree',\n",
       "   'predicts',\n",
       "   'when',\n",
       "   'the',\n",
       "   'evaluation',\n",
       "   'of',\n",
       "   'all',\n",
       "   'ISP',\n",
       "   'candidates',\n",
       "   '\\n',\n",
       "   'can',\n",
       "   'be',\n",
       "   'skipped',\n",
       "   '.',\n",
       "   'In',\n",
       "   '[',\n",
       "   ...],\n",
       "  'pos_tagger': '',\n",
       "  'lema': ['fast',\n",
       "   'isp',\n",
       "   'Mode',\n",
       "   'Decision',\n",
       "   'for',\n",
       "   'the',\n",
       "   'Versatile',\n",
       "   'Video',\n",
       "   'Coding',\n",
       "   'Intra',\n",
       "   'prediction',\n",
       "   'use',\n",
       "   'Machine',\n",
       "   'Learning',\n",
       "   'Bruno',\n",
       "   'Zatt',\n",
       "   'Video',\n",
       "   'Technology',\n",
       "   'Research',\n",
       "   'Group',\n",
       "   'ViTech',\n",
       "   'Graduate',\n",
       "   'Program',\n",
       "   'in',\n",
       "   'Computer',\n",
       "   'Science',\n",
       "   'PPGC',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Pelotas',\n",
       "   'UFPel',\n",
       "   'Pelotas',\n",
       "   'Brazil',\n",
       "   'zatt@inf.ufpel.edu.br',\n",
       "   'Larissa',\n",
       "   'Araújo',\n",
       "   'Video',\n",
       "   'Technology',\n",
       "   'Research',\n",
       "   'Group',\n",
       "   'ViTech',\n",
       "   'Graduate',\n",
       "   'Program',\n",
       "   'in',\n",
       "   'Computer',\n",
       "   'Science',\n",
       "   'PPGC',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Pelotas',\n",
       "   'UFPel',\n",
       "   'Pelotas',\n",
       "   'Brazil',\n",
       "   'ldaaraujo@inf.ufpel.edu.br',\n",
       "   'Adson',\n",
       "   'Duarte',\n",
       "   'Video',\n",
       "   'Technology',\n",
       "   'Research',\n",
       "   'Group',\n",
       "   'ViTech',\n",
       "   'Graduate',\n",
       "   'Program',\n",
       "   'in',\n",
       "   'Computer',\n",
       "   'Science',\n",
       "   'PPGC',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Pelotas',\n",
       "   'UFPel',\n",
       "   'Pelotas',\n",
       "   'Brazil',\n",
       "   'airduarte@inf.ufpel.edu.br',\n",
       "   'Guilherme',\n",
       "   'Correa',\n",
       "   'Video',\n",
       "   'Technology',\n",
       "   'Research',\n",
       "   'Group',\n",
       "   'ViTech',\n",
       "   'Graduate',\n",
       "   'Program',\n",
       "   'in',\n",
       "   'Computer',\n",
       "   'Science',\n",
       "   'PPGC',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Pelotas',\n",
       "   'UFPel',\n",
       "   'Pelotas',\n",
       "   'Brazil',\n",
       "   'gcorrea@inf.ufpel.edu.br',\n",
       "   'ABSTRACT',\n",
       "   'the',\n",
       "   'Versatile',\n",
       "   'Video',\n",
       "   'Coding',\n",
       "   'VVC',\n",
       "   'standard',\n",
       "   'achieve',\n",
       "   'high',\n",
       "   'compression',\n",
       "   'rate',\n",
       "   'by',\n",
       "   'introduce',\n",
       "   'new',\n",
       "   'encoding',\n",
       "   'tool',\n",
       "   'such',\n",
       "   'as',\n",
       "   'the',\n",
       "   'Intra',\n",
       "   'Subpartition',\n",
       "   'Prediction',\n",
       "   'ISP',\n",
       "   'however',\n",
       "   'the',\n",
       "   'isp',\n",
       "   'increase',\n",
       "   'the',\n",
       "   'computational',\n",
       "   'effort',\n",
       "   'necessary',\n",
       "   'to',\n",
       "   'perform',\n",
       "   'the',\n",
       "   'mode',\n",
       "   'decision',\n",
       "   'of',\n",
       "   'the',\n",
       "   'intra',\n",
       "   'prediction',\n",
       "   'step',\n",
       "   'this',\n",
       "   'paper',\n",
       "   'propose',\n",
       "   'a',\n",
       "   'fast',\n",
       "   'intra',\n",
       "   'mode',\n",
       "   'decision',\n",
       "   'solution',\n",
       "   'for',\n",
       "   'the',\n",
       "   'isp',\n",
       "   'use',\n",
       "   'machine',\n",
       "   'learning',\n",
       "   'a',\n",
       "   'Decision',\n",
       "   'Tree',\n",
       "   'be',\n",
       "   'employ',\n",
       "   'to',\n",
       "   'predict',\n",
       "   'the',\n",
       "   'most',\n",
       "   'promising',\n",
       "   'isp',\n",
       "   'mode',\n",
       "   'to',\n",
       "   'be',\n",
       "   'optimal',\n",
       "   'to',\n",
       "   'avoid',\n",
       "   'the',\n",
       "   'costly',\n",
       "   'RDO',\n",
       "   'test',\n",
       "   'of',\n",
       "   'isp',\n",
       "   'mode',\n",
       "   'that',\n",
       "   'be',\n",
       "   'less',\n",
       "   'likely',\n",
       "   'to',\n",
       "   'be',\n",
       "   'choose',\n",
       "   'by',\n",
       "   'reduce',\n",
       "   'the',\n",
       "   'number',\n",
       "   'of',\n",
       "   'mode',\n",
       "   'fully',\n",
       "   'evaluate',\n",
       "   'by',\n",
       "   'the',\n",
       "   'RDO',\n",
       "   'process',\n",
       "   'the',\n",
       "   'propose',\n",
       "   'solution',\n",
       "   'achieve',\n",
       "   'an',\n",
       "   'average',\n",
       "   'time',\n",
       "   'saving',\n",
       "   'of',\n",
       "   '3.15',\n",
       "   'with',\n",
       "   'only',\n",
       "   '0.11',\n",
       "   'of',\n",
       "   'code',\n",
       "   'efficiency',\n",
       "   'loss',\n",
       "   'when',\n",
       "   'test',\n",
       "   'for',\n",
       "   'the',\n",
       "   'common',\n",
       "   'test',\n",
       "   'condition',\n",
       "   'of',\n",
       "   'VVC',\n",
       "   'unlike',\n",
       "   'the',\n",
       "   'related',\n",
       "   'work',\n",
       "   'our',\n",
       "   'solution',\n",
       "   'avoid',\n",
       "   'the',\n",
       "   'time',\n",
       "   'overhead',\n",
       "   'of',\n",
       "   'calculate',\n",
       "   'image',\n",
       "   'feature',\n",
       "   'by',\n",
       "   'adopt',\n",
       "   'feature',\n",
       "   'from',\n",
       "   'the',\n",
       "   'encoding',\n",
       "   'process',\n",
       "   'compare',\n",
       "   'with',\n",
       "   'related',\n",
       "   'work',\n",
       "   'our',\n",
       "   'solution',\n",
       "   'present',\n",
       "   'competitive',\n",
       "   'time',\n",
       "   'saving',\n",
       "   'and',\n",
       "   'code',\n",
       "   'efficiency',\n",
       "   'result',\n",
       "   'keyword',\n",
       "   'VVC',\n",
       "   'Intra',\n",
       "   'Prediction',\n",
       "   'ISP',\n",
       "   'Machine',\n",
       "   'Learning',\n",
       "   '1',\n",
       "   'introduction',\n",
       "   'Digital',\n",
       "   'video',\n",
       "   'have',\n",
       "   'be',\n",
       "   'fundamental',\n",
       "   'in',\n",
       "   'many',\n",
       "   'area',\n",
       "   'from',\n",
       "   'entertainment',\n",
       "   'and',\n",
       "   'communication',\n",
       "   'to',\n",
       "   'surveillance',\n",
       "   'application',\n",
       "   'and',\n",
       "   'live',\n",
       "   'broadcast',\n",
       "   'a',\n",
       "   'study',\n",
       "   'reveal',\n",
       "   'that',\n",
       "   'during',\n",
       "   'the',\n",
       "   'third',\n",
       "   'quarter',\n",
       "   'of',\n",
       "   '2022',\n",
       "   'live',\n",
       "   'stream',\n",
       "   'feature',\n",
       "   'gaming',\n",
       "   'relate',\n",
       "   'content',\n",
       "   'accumulate',\n",
       "   'approximately',\n",
       "   '7.2',\n",
       "   'billion',\n",
       "   'hour',\n",
       "   'of',\n",
       "   'content',\n",
       "   'watch',\n",
       "   'across',\n",
       "   'lead',\n",
       "   'streaming',\n",
       "   'platform',\n",
       "   '6',\n",
       "   'in',\n",
       "   'this',\n",
       "   'context',\n",
       "   'video',\n",
       "   'code',\n",
       "   'standard',\n",
       "   'such',\n",
       "   'as',\n",
       "   'the',\n",
       "   'Versatile',\n",
       "   'Video',\n",
       "   'Coding',\n",
       "   'VVC',\n",
       "   '5',\n",
       "   'play',\n",
       "   'a',\n",
       "   'crucial',\n",
       "   'role',\n",
       "   'in',\n",
       "   'proceeding',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'web',\n",
       "   'WebMe',\n",
       "   'dia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   '2024',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'sbc',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '2753',\n",
       "   'Daniel',\n",
       "   'Palomino',\n",
       "   'Video',\n",
       "   'Technology',\n",
       "   'Research',\n",
       "   'Group',\n",
       "   'ViTech',\n",
       "   'Graduate',\n",
       "   'Program',\n",
       "   'in',\n",
       "   'Computer',\n",
       "   'Science',\n",
       "   'PPGC',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Pelotas',\n",
       "   'UFPel',\n",
       "   'Pelotas',\n",
       "   'Brazil',\n",
       "   'dpalomino@inf.ufpel.edu.br',\n",
       "   'in',\n",
       "   'enable',\n",
       "   'application',\n",
       "   'to',\n",
       "   'manipulate',\n",
       "   'high',\n",
       "   'definition',\n",
       "   'video',\n",
       "   'for',\n",
       "   'storage',\n",
       "   'and',\n",
       "   'transmission',\n",
       "   'the',\n",
       "   'VVC',\n",
       "   '5',\n",
       "   'be',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'late',\n",
       "   'and',\n",
       "   'most',\n",
       "   'advanced',\n",
       "   'video',\n",
       "   'coding',\n",
       "   'standard',\n",
       "   'it',\n",
       "   'offer',\n",
       "   'superior',\n",
       "   'bit',\n",
       "   'rate',\n",
       "   'reduction',\n",
       "   'without',\n",
       "   'compromise',\n",
       "   'visual',\n",
       "   'quality',\n",
       "   'when',\n",
       "   'compare',\n",
       "   'to',\n",
       "   'its',\n",
       "   'predecessor',\n",
       "   'the',\n",
       "   'High',\n",
       "   'Efficiency',\n",
       "   'Video',\n",
       "   'Coding',\n",
       "   'hevc',\n",
       "   'standard',\n",
       "   '19',\n",
       "   'this',\n",
       "   'be',\n",
       "   'possible',\n",
       "   'due',\n",
       "   'to',\n",
       "   'several',\n",
       "   'new',\n",
       "   'encoding',\n",
       "   'tool',\n",
       "   'introduce',\n",
       "   'in',\n",
       "   'the',\n",
       "   'standard',\n",
       "   'especially',\n",
       "   'in',\n",
       "   'the',\n",
       "   'intra',\n",
       "   'prediction',\n",
       "   'step',\n",
       "   'of',\n",
       "   'VVC',\n",
       "   'while',\n",
       "   'VVC',\n",
       "   'maintain',\n",
       "   'the',\n",
       "   'Planar',\n",
       "   'DC',\n",
       "   'and',\n",
       "   'angular',\n",
       "   'directional',\n",
       "   'mode',\n",
       "   'from',\n",
       "   'HEVC',\n",
       "   'it',\n",
       "   'extend',\n",
       "   'the',\n",
       "   'number',\n",
       "   'of',\n",
       "   'Angular',\n",
       "   'mode',\n",
       "   'from',\n",
       "   '33',\n",
       "   'to',\n",
       "   '65',\n",
       "   'VVC',\n",
       "   'also',\n",
       "   'introduce',\n",
       "   'the',\n",
       "   'Matrix',\n",
       "   'weight',\n",
       "   'Intra',\n",
       "   'Prediction',\n",
       "   'MIP',\n",
       "   '18',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Intra',\n",
       "   'Subpartition',\n",
       "   'Prediction',\n",
       "   'ISP',\n",
       "   '8',\n",
       "   'to',\n",
       "   'improve',\n",
       "   'prediction',\n",
       "   'accuracy',\n",
       "   'combine',\n",
       "   'with',\n",
       "   'the',\n",
       "   'Planar',\n",
       "   'DC',\n",
       "   'and',\n",
       "   'Angular',\n",
       "   'mode',\n",
       "   'the',\n",
       "   'isp',\n",
       "   'tool',\n",
       "   'enhance',\n",
       "   'prediction',\n",
       "   'granularity',\n",
       "   'by',\n",
       "   'process',\n",
       "   'a',\n",
       "   'block',\n",
       "   'through',\n",
       "   'subpartition',\n",
       "   'in',\n",
       "   'the',\n",
       "   'horizontal',\n",
       "   'or',\n",
       "   'vertical',\n",
       "   'direction',\n",
       "   'despite',\n",
       "   'the',\n",
       "   'code',\n",
       "   'efficiency',\n",
       "   'improvement',\n",
       "   'introduce',\n",
       "   'by',\n",
       "   'the',\n",
       "   'new',\n",
       "   'encoding',\n",
       "   'tool',\n",
       "   'of',\n",
       "   'VVC',\n",
       "   'there',\n",
       "   'be',\n",
       "   'a',\n",
       "   'trade',\n",
       "   'off',\n",
       "   'in',\n",
       "   'encode',\n",
       "   'time',\n",
       "   'which',\n",
       "   'make',\n",
       "   'it',\n",
       "   '34',\n",
       "   'time',\n",
       "   'slow',\n",
       "   'than',\n",
       "   'hevc',\n",
       "   '12',\n",
       "   'therefore',\n",
       "   'it',\n",
       "   'be',\n",
       "   'important',\n",
       "   'to',\n",
       "   'develop',\n",
       "   'solution',\n",
       "   'to',\n",
       "   'improve',\n",
       "   'the',\n",
       "   'encoding',\n",
       "   'time',\n",
       "   'by',\n",
       "   'target',\n",
       "   'the',\n",
       "   'new',\n",
       "   'intra',\n",
       "   'prediction',\n",
       "   'tool',\n",
       "   'in',\n",
       "   'VVC',\n",
       "   'such',\n",
       "   'as',\n",
       "   'the',\n",
       "   'isp',\n",
       "   'tool',\n",
       "   'some',\n",
       "   'relate',\n",
       "   'work',\n",
       "   'propose',\n",
       "   'solution',\n",
       "   'to',\n",
       "   'save',\n",
       "   'time',\n",
       "   'in',\n",
       "   'the',\n",
       "   'intramode',\n",
       "   'decision',\n",
       "   'in',\n",
       "   'VVC',\n",
       "   'specifically',\n",
       "   'focus',\n",
       "   'on',\n",
       "   'the',\n",
       "   'isp',\n",
       "   'tool',\n",
       "   'the',\n",
       "   'main',\n",
       "   'idea',\n",
       "   'of',\n",
       "   'these',\n",
       "   'work',\n",
       "   'be',\n",
       "   'to',\n",
       "   'avoid',\n",
       "   'the',\n",
       "   'costly',\n",
       "   'evaluation',\n",
       "   'perform',\n",
       "   'by',\n",
       "   'the',\n",
       "   'rate',\n",
       "   'Distortion',\n",
       "   'Optimization',\n",
       "   'RDO',\n",
       "   'process',\n",
       "   'for',\n",
       "   'isp',\n",
       "   'mode',\n",
       "   'that',\n",
       "   'be',\n",
       "   'less',\n",
       "   'likely',\n",
       "   'to',\n",
       "   'be',\n",
       "   'optimal',\n",
       "   'the',\n",
       "   'work',\n",
       "   'usually',\n",
       "   'use',\n",
       "   'heuristic',\n",
       "   'or',\n",
       "   'machine',\n",
       "   'learn',\n",
       "   'solution',\n",
       "   'to',\n",
       "   'predict',\n",
       "   'the',\n",
       "   'most',\n",
       "   'promising',\n",
       "   'mode',\n",
       "   'for',\n",
       "   'instance',\n",
       "   'in',\n",
       "   '14',\n",
       "   'a',\n",
       "   'machine',\n",
       "   'learning',\n",
       "   'model',\n",
       "   'be',\n",
       "   'train',\n",
       "   'with',\n",
       "   'a',\n",
       "   'key',\n",
       "   'feature',\n",
       "   'compute',\n",
       "   'over',\n",
       "   'the',\n",
       "   'image',\n",
       "   'know',\n",
       "   'as',\n",
       "   'the',\n",
       "   'Mean',\n",
       "   'Absolute',\n",
       "   'Sum',\n",
       "   'of',\n",
       "   'Transform',\n",
       "   'coefficient',\n",
       "   'the',\n",
       "   'model',\n",
       "   'predict',\n",
       "   'whether',\n",
       "   'the',\n",
       "   'evaluation',\n",
       "   'for',\n",
       "   'each',\n",
       "   'isp',\n",
       "   'mode',\n",
       "   'be',\n",
       "   'necessary',\n",
       "   'or',\n",
       "   'can',\n",
       "   'be',\n",
       "   'skip',\n",
       "   'another',\n",
       "   'approach',\n",
       "   'present',\n",
       "   'in',\n",
       "   '17',\n",
       "   'involve',\n",
       "   'train',\n",
       "   'a',\n",
       "   'decision',\n",
       "   'Tree',\n",
       "   'model',\n",
       "   'over',\n",
       "   'another',\n",
       "   'image',\n",
       "   'feature',\n",
       "   'the',\n",
       "   'block',\n",
       "   '’s',\n",
       "   'variance',\n",
       "   'the',\n",
       "   'Decision',\n",
       "   'Tree',\n",
       "   'predict',\n",
       "   'when',\n",
       "   'the',\n",
       "   'evaluation',\n",
       "   'of',\n",
       "   'all',\n",
       "   'isp',\n",
       "   'candidate',\n",
       "   'can',\n",
       "   'be',\n",
       "   'skip',\n",
       "   'in',\n",
       "   '11',\n",
       "   'the',\n",
       "   'texture',\n",
       "   'complexity',\n",
       "   'of',\n",
       "   'the',\n",
       "   'block',\n",
       "   'be',\n",
       "   'obtain',\n",
       "   'through',\n",
       "   'an',\n",
       "   'image',\n",
       "   'feature',\n",
       "   'call',\n",
       "   'Mean',\n",
       "   'Absolute',\n",
       "   'Deviation',\n",
       "   '162',\n",
       "   'WebMedia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Larissa',\n",
       "   'Araújo',\n",
       "   'Adson',\n",
       "   'Duarte',\n",
       "   'Bruno',\n",
       "   'Zatt',\n",
       "   'Guilherme',\n",
       "   'Correa',\n",
       "   'and',\n",
       "   'Daniel',\n",
       "   'Palomino',\n",
       "   'then',\n",
       "   'this',\n",
       "   'feature',\n",
       "   'be',\n",
       "   'use',\n",
       "   'to',\n",
       "   'decide',\n",
       "   'whether',\n",
       "   'the',\n",
       "   'evaluation',\n",
       "   'of',\n",
       "   'isp',\n",
       "   'candidate',\n",
       "   'can',\n",
       "   'be',\n",
       "   'skip',\n",
       "   'a',\n",
       "   'heuristic',\n",
       "   'algorithm',\n",
       "   'be',\n",
       "   'propose',\n",
       "   'in',\n",
       "   '13',\n",
       "   'where',\n",
       "   'the',\n",
       "   'list',\n",
       "   'of',\n",
       "   'isp',\n",
       "   'candidate',\n",
       "   'be',\n",
       "   'pre',\n",
       "   'prune',\n",
       "   'accord',\n",
       "   'to',\n",
       "   'the',\n",
       "   'shape',\n",
       "   'of',\n",
       "   'the',\n",
       "   'block',\n",
       "   'and',\n",
       "   'the',\n",
       "   'ISP',\n",
       "   'subpartition',\n",
       "   'direction',\n",
       "   'although',\n",
       "   'all',\n",
       "   'these',\n",
       "   'work',\n",
       "   'report',\n",
       "   'time',\n",
       "   'save',\n",
       "   'result',\n",
       "   'on',\n",
       "   'the',\n",
       "   'isp',\n",
       "   'mode',\n",
       "   'decision',\n",
       "   'the',\n",
       "   'solution',\n",
       "   'propose',\n",
       "   'by',\n",
       "   '14',\n",
       "   'and',\n",
       "   '11',\n",
       "   'rely',\n",
       "   'on',\n",
       "   'compute',\n",
       "   'image',\n",
       "   'feature',\n",
       "   'inevitably',\n",
       "   'introduce',\n",
       "   'a',\n",
       "   'processing',\n",
       "   'time',\n",
       "   'overhead',\n",
       "   'to',\n",
       "   'each',\n",
       "   'one',\n",
       "   'of',\n",
       "   'these',\n",
       "   'mode',\n",
       "   'decision',\n",
       "   'solution',\n",
       "   'a',\n",
       "   'fact',\n",
       "   'that',\n",
       "   'be',\n",
       "   'not',\n",
       "   'thoroughly',\n",
       "   'discuss',\n",
       "   'in',\n",
       "   'these',\n",
       "   'work',\n",
       "   'the',\n",
       "   'solution',\n",
       "   'propose',\n",
       "   'by',\n",
       "   '13',\n",
       "   'be',\n",
       "   'the',\n",
       "   'only',\n",
       "   'one',\n",
       "   'that',\n",
       "   'do',\n",
       "   'not',\n",
       "   'rely',\n",
       "   'on',\n",
       "   'compute',\n",
       "   'image',\n",
       "   'feature',\n",
       "   'nevertheless',\n",
       "   'none',\n",
       "   'of',\n",
       "   'the',\n",
       "   'relate',\n",
       "   'work',\n",
       "   'employ',\n",
       "   'solution',\n",
       "   'use',\n",
       "   'feature',\n",
       "   'available',\n",
       "   'at',\n",
       "   'encode',\n",
       "   'time',\n",
       "   'such',\n",
       "   'as',\n",
       "   'the',\n",
       "   'mode',\n",
       "   'of',\n",
       "   'neighbor',\n",
       "   'block',\n",
       "   'or',\n",
       "   'rate',\n",
       "   'distortion',\n",
       "   'cost',\n",
       "   'calculate',\n",
       "   'during',\n",
       "   'the',\n",
       "   'RDO',\n",
       "   'process',\n",
       "   'in',\n",
       "   'this',\n",
       "   'work',\n",
       "   'we',\n",
       "   'call',\n",
       "   'these',\n",
       "   'feature',\n",
       "   'encode',\n",
       "   'feature',\n",
       "   'since',\n",
       "   'they',\n",
       "   'be',\n",
       "   'available',\n",
       "   'during',\n",
       "   'the',\n",
       "   'encoding',\n",
       "   'process',\n",
       "   'this',\n",
       "   'paper',\n",
       "   'propose',\n",
       "   'a',\n",
       "   'fast',\n",
       "   'isp',\n",
       "   'mode',\n",
       "   'decision',\n",
       "   'solution',\n",
       "   'use',\n",
       "   'machine',\n",
       "   'learning',\n",
       "   'for',\n",
       "   'the',\n",
       "   'VVC',\n",
       "   'intra',\n",
       "   'prediction',\n",
       "   'process',\n",
       "   'our',\n",
       "   'approach',\n",
       "   'distinguish',\n",
       "   'from',\n",
       "   'related',\n",
       "   'work',\n",
       "   'by',\n",
       "   'use',\n",
       "   'feature',\n",
       "   'available',\n",
       "   'at',\n",
       "   'encode',\n",
       "   'time',\n",
       "   'aka',\n",
       "   'encode',\n",
       "   'feature',\n",
       "   'as',\n",
       "   'input',\n",
       "   'for',\n",
       "   'the',\n",
       "   'machine',\n",
       "   'learning',\n",
       "   'model',\n",
       "   'this',\n",
       "   'approach',\n",
       "   'avoid',\n",
       "   'the',\n",
       "   'computation',\n",
       "   'necessary',\n",
       "   'to',\n",
       "   'calculate',\n",
       "   'image',\n",
       "   'feature',\n",
       "   'as',\n",
       "   'it',\n",
       "   'be',\n",
       "   'adopt',\n",
       "   'by',\n",
       "   'most',\n",
       "   'of',\n",
       "   'the',\n",
       "   'related',\n",
       "   'work',\n",
       "   'we',\n",
       "   'classify',\n",
       "   'the',\n",
       "   'isp',\n",
       "   'candidate',\n",
       "   'into',\n",
       "   'two',\n",
       "   'distinct',\n",
       "   'class',\n",
       "   'base',\n",
       "   'on',\n",
       "   'their',\n",
       "   'associate',\n",
       "   'intra',\n",
       "   'mode',\n",
       "   'i',\n",
       "   'ISP',\n",
       "   'Planar',\n",
       "   'DC',\n",
       "   'comprise',\n",
       "   'isp',\n",
       "   'subpartition',\n",
       "   'associate',\n",
       "   'with',\n",
       "   'Planar',\n",
       "   ...]},\n",
       " {'titulo': 'Acceptance and Usability of Complex Medical Systems',\n",
       "  'informacoes_url': '',\n",
       "  'idioma': 'english',\n",
       "  'storage_key': '../articles/original/english/985-24740-1-10-20240923.pdf',\n",
       "  'author': 'Fábio Ap. Cândido da Silva; André Pimenta Freire; and Marluce Rodrigues Pereira',\n",
       "  'data_publicacao': '11-09-2024',\n",
       "  'resumo': 'The increasing demand for imaging tests has made radiology information systems crucial in medical practice, especially those based on web technology. These systems include Picture Archiving and Communication System (PACS), Radiology Information Systems (RIS), and Hospital Information System (HIS), generate and manipulate images through specialized software. To operate this complex software, require attention to detail and image manipulation techniques for accurate diagnoses. Usability issues in medical image manipulation software, given the process of adapting to new software and complex tasks, can result in inaccurate diagnoses with clinical impact. This is a qualitative study, which is based on the work routines of radiology professionals, focusing on issues of cognitive learning, interaction, and usability with radiology software. Moderate usability tests with radiology technicians were conducted to identify the difficulties and challenges they encounter while using medical image manipulation software. The analysis identified 64 problems grouped into 20 categories and organized under Visual Presentation, Content, Information Architecture, and Interactivity. The paper emphasizes violated heuristics and describes how these problem categories impact users in their medical activities and their influence on the clinical process. The obtained results provide insights to enhance usability practices and recommendations, aiming to support the development systems used in radiology practice. ###',\n",
       "  'keywords': 'usability issues, radiology systems, qualitative analysis',\n",
       "  'referencias': ['[1] Reza Abbasi, Monireh Sadeqi Jabali, Reza Khajouei, and Hamidreza Tadayon.\\n2020. Investigating the satisfaction level of physicians in regards to implementing\\nmedical Picture Archiving and Communication System (PACS). *BMC medical*\\n*informatics and decision making* 20, 1 (2020), 1–8.',\n",
       "   '[2] M. Alhajeri and S.G.S. Shah. 2019. Limitations in and Solutions for Improving the\\nFunctionality of Picture Archiving and Communication System: an Exploratory\\nStudy of PACS Professionals’ Perspectives. *Journal of Digital Imaging* 32 (2019),\\n54–67.',\n",
       "   '[3] Priya Darshini B, Deepan Chakkaravarthy N, Gokul B, and Sabari Maharaja B.\\n2023. A Web-based Dicom Image and Plane Viewer. In *2023 Fifth International*\\n*Conference on Electrical, Computer and Communication Technologies (ICECCT)* .\\nIEEE, 1–7.',\n",
       "   '[4] Rade R Babić, Zoran Milošević, and Gordana Stanković-Babić. 2012. Web technology in health information system. *Acta Facultatis Medicae Naissensis* 29, 2\\n(2012), 81–87.',\n",
       "   '[5] Mirza Mansoor Baig, Hamid GholamHosseini, Aasia A Moqeem, Farhaan Mirza,\\nand Maria Lindén. 2017. A systematic review of wearable patient monitoring\\nsystems–current challenges and opportunities for clinical adoption. *Journal of*\\n*medical systems* 41, 7 (2017), 1–9.',\n",
       "   '[6] Virginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology.\\n*Qualitative research in psychology* 3, 2 (2006), 77–101.',\n",
       "   '[7] S. Cronin, B. Kane, and G. Doherty. 2021. A Qualitative Analysis of the Needs\\nand Experiences of Hospital-based Clinicians when Accessing Medical Imaging.\\n*Journal of digital imaging* (2021), 1––12. https://doi.org/10.1007/s10278-02100446-1',\n",
       "   '[8] Fábio A. C. da Silva, André P. Freire, and Marluce R. Pereira. 2022. Understanding Interaction and Organizational Issues in Radiology Information Systems: a\\nQualitative Study with Health Professionals. In *XVIII Brazilian Symposium on*\\n*Information Systems* . –.',\n",
       "   '[9] Camila Rodrigues Dias, Marluce Rodrigues Pereira, and Andre Pimenta Freire.\\n2017. Qualitative review of usability problems in health information systems for\\nradiology. *Journal of biomedical informatics* 76 (2017), 19–33.',\n",
       "   '[10] K Anders Ericsson and Herbert A Simon. 1984. *Protocol analysis: Verbal reports*\\n*as data.* MIT Press, Cambridge, MA, USA.',\n",
       "   '[11] Misagh Zahiri Esfahani, Reza Khajouei, and Mohammad Reza Baneshi. 2018. Augmentation of the think aloud method with users’ perspectives for the selection of\\na picture archiving and communication system. *Journal of Biomedical Informatics*\\n80 (2018), 43–51.',\n",
       "   '[12] Ina Geldermann, Christoph Grouls, Christiane Kuhl, Thomas M Deserno, and\\nCord Spreckelsen. 2013. Black box integration of computer-aided diagnosis into\\nPACS deserves a second chance: results of a usability study concerning bone age\\nassessment. *Journal of Digital Imaging* 6 (2013), 698—-708.',\n",
       "   '[13] Hai K Huang. 2003. Enterprise PACS and image distribution. *Computerized*\\n*Medical Imaging and Graphics* 27, 2-3 (2003), 241–253.',\n",
       "   '[14] Fateme Rangraz Jeddi, Ehsan Nabovati, Reyhane Bigham, and Razieh Farrahi.\\n2020. Usability evaluation of a comprehensive national health information system:\\nA heuristic evaluation. *Informatics in Medicine Unlocked* 19 (2020), 100332.',\n",
       "   '[15] Wiard Jorritsma, Fokie Cnossen, Rudi A Dierckx, Matthijs Oudkerk, and Peter MA\\nvan Ooijen. 2016. Pattern mining of user interaction logs for a post-deployment\\nusability evaluation of a radiology PACS client. *International Journal of Medical*\\n*Informatics* 85, 1 (2016), 36–42.',\n",
       "   '[16] Wiard Jorritsma, Fokie Cnossen, Rudi A Dierckx, Matthijs Oudkerk, and Peter MA Van Ooijen. 2016. Post-deployment usability evaluation of a radiology\\nworkstation. *International Journal of Medical Informatics* 85, 1 (2016), 28–35.',\n",
       "   '[17] Wiard Jorritsma, Fokie Cnossen, and Peter MA van Ooijen. 2014. Merits of\\nusability testing for PACS selection. *International Journal of Medical Informatics*\\n83, 1 (2014), 27–36.',\n",
       "   '[18] Wiard Jorritsma, Fokie Cnossen, and Peter MA van Ooijen. 2015. Adaptive\\nsupport for user interface customization: a study in radiology. *International*\\n*Journal of Human-Computer Studies* 77 (2015), 1–9.',\n",
       "   '[19] Brandan Kennedy, Ellen Kerns, Y Raymond Chan, Barbara S Chaparro, and\\nSarah D Fouquet. 2019. Safeuristics! Do Heuristic Evaluation Violation Severity\\nRatings Correlate with Patient Safety Severity Ratings for a Native Electronic\\nHealth Record Mobile Application? *Applied clinical informatics* 10, 02 (2019),\\n210–218.',\n",
       "   '[20] Dasueran Kim, Peter Kang, Jungmin Yun, Sung-Hye Park, Jeong-Wook Seo, and\\nPeom Park. 2014. Study on User Interface of Pathology Picture Archiving and\\nCommunication System. *Healthcare Informatics Research* 20, 1 (2014), 45–51.',\n",
       "   '[21] Andre W Kushniruk and Vimla L Patel. 2004. Cognitive and usability engineering\\nmethods for the evaluation of clinical information systems. *Journal of biomedical*\\n*informatics* 37, 1 (2004), 56–76.',\n",
       "   '[22] J. Lazar, J. H. Feng, and H. Hochheiser. 2017. *Research Methods in Human-Computer*\\n*Interaction* . Morgan Kaufmann is an imprint of Elsevier, Cambridge, MA, USA.',\n",
       "   '[23] James R Lewis. 1995. IBM computer usability satisfaction questionnaires: psychometric evaluation and instructions for use. *International Journal of Human-*\\n*Computer Interaction* 7, 1 (1995), 57–78.',\n",
       "   '[24] Svetlana Z Lowry, Patricia Abbott, Michael C Gibbons, Svetlana Z Lowry, Robert\\nNorth, Emily S Patterson, Matthew T Quinn, Mala Ramaiah, Robert M Schumacher, and Jiajie Zhang. 2012. *Technical Evaluation, Testing, and Validatiaon of*\\n*the Usability of Electronic Health Records* . US Department of Commerce, National\\nInstitute of Standards and Technology.',\n",
       "   '[25] Dimitrios Markonis, Frederic Baroz, Rafael Luis Ruiz De Castaneda, Celia Boyer,\\nand Henning Müller. 2013. User Tests for Assessing a Medical Image Retrieval\\nSystem: A Pilot Study. In *MEDINFO 2013* . Studies in Health Technology and\\nInformatics, Vol. 192. IOS Press, Amsterdam, Netherlands, 224–8.',\n",
       "   '[26] Dimitrios Markonis, Markus Holzer, Frederic Baroz, Rafael Luis Ruiz De Castaneda, Célia Boyer, Georg Langs, and Henning Müller. 2015. User-oriented\\nevaluation of a medical image retrieval system for radiologists. *International*\\n*Journal of Medical Informatics* 84, 10 (2015), 774–783.',\n",
       "   '[27] Luan Martins, Adriana Bueno, Alexandre Defelicibus, Rodrigo Drummond, Renan\\nValieris, Yu-Tao Zhu, Israel Da Silva, and Liang Zhao. 2023. WSI2ML – An\\nOpen-Source Whole Slide Image Annotation Software for Machine Learning\\nApplications. In *Anais do XXIX Simpósio Brasileiro de Sistemas Multimídia e Web*\\n(Ribeirão Preto/SP). SBC, Porto Alegre, RS, Brasil, 104–109. https://sol.sbc.org.\\nbr/index.php/webmedia/article/view/25871',\n",
       "   '[28] Mary L McHugh. 2012. Interrater reliability: the kappa statistic. *Biochemia*\\n*medica* 22, 3 (2012), 276–282.',\n",
       "   '[29] J. Nielsen. 2020. 10 Usability Heuristics for User Interface Design. *Nielsen Norman*\\n*Group* (2020). https://www.nngroup.com/articles/ten-usability-heuristics',\n",
       "   '[30] Jakob Nielsen and Rolf Molich. 1990. Heuristic evaluation of user interfaces.\\nIn *Proceedings of the SIGCHI conference on Human factors in computing systems* .\\n249–256.',\n",
       "   '[31] Kenneth Olbrish, Paul Shanken, Donna Rabe, Lorraine Steven, and Nicholas\\nIrizarry. 2011. Four-year enterprise PACS support trend analysis. *Journal of*\\n*Digital Imaging* 24 (2011), 284—-294.',\n",
       "   '[32] Helen Petrie and Christopher Power. 2012. What do users really care about? A\\ncomparison of usability problems found by users and experts on highly interactive\\nwebsites. In *Proceedings of the SIGCHI Conference on Human Factors in Computing*\\n*Systems* . 2107–2116.',\n",
       "   '[33] Lizawati Salahuddin, Zuraini Ismail, Ummi Rabaah Hashim, Nor Haslinda Ismail,\\nRaja Rina Raja Ikram, Fiza Abdul Rahim, and Noor Hafizah Hassan. 2020. Healthcare practitioner behaviours that influence unsafe use of hospital information\\nsystems. *Health informatics journal* 26, 1 (2020), 420–434.',\n",
       "   '[34] Sandro Luis F. F. C. Silva, Marcelo Fornazin, and Rodrigo Pereira dos Santos. 2020.\\nAnalysis and Modeling of Emergent Systems in the Health Information System\\nDomain. In *XVI Brazilian Symposium on Information Systems* . 1–8.',\n",
       "   '[35] Marcílio Ferreira Souza-Júnior, Lívio Antonio Monteiro Amorim, Lucas Emanoel\\nPaulino Silva Santos, Jorge Silva Correia-Neto, and Hugo Araujo Souza. 2019.\\nAprimoramento de Interfaces de Usuários de Sistemas de Informação em Saúde no\\nAtendimento Pré-hospitalar na Ótica da Consciência Situacional. *iSys-Brazilian*\\n*Journal of Information Systems* 12, 4 (2019), 98–116.',\n",
       "   '[36] US National Electrical Manufacturers Association. 2021. Digital Imaging and Communications in Medicine (DICOM) stanrdard.\\nhttp://dicom.nema.org/medical/dicom/current/output/html/part01.html.\\naccessed on 19/05/2021.',\n",
       "   '[37] Cleverson Vieira, Marcus Oliveira, Marcelo Guimarães, Leonardo Rocha, and\\nDiego Dias. 2023. Applied Explainable Artificial Intelligence (XAI) in the classification of retinal images for support in the diagnosis of Glaucoma. In *Anais*\\n*do XXIX Simpósio Brasileiro de Sistemas Multimídia e Web* (Ribeirão Preto/SP).\\nSBC, Porto Alegre, RS, Brasil, 82–90. https://sol.sbc.org.br/index.php/webmedia/\\narticle/view/25869',\n",
       "   '[38] Misagh Zahiri Esfahani, Jamileh Farokhzadian, Kambiz Bahaadinbeigy, and Reza\\nKhajouei. 2019. Factors influencing the selection of a picture archiving and\\ncommunication system: A qualitative study. *The International journal of health*\\n*planning and management* 34, 2 (2019), 780–793.\\n\\n\\n36\\n\\n\\n-----'],\n",
       "  'text': '# **Acceptance and Usability of Complex Medical Systems**\\n## A Study with Radiology Professionals\\n\\n## Fábio Ap. Cândido da Silva\\n#### fabio.acs@gmail.com UFLA - Federal University of Lavras Lavras, Minas Gerais, Brasil\\n### **ABSTRACT**\\n\\n## André Pimenta Freire\\n#### apfreire@ufla.br UFLA - Federal University of Lavras Lavras, Minas Gerais, Brasil\\n\\n## Marluce Rodrigues Pereira\\n#### marluce@ufla.br UFLA - Federal University of Lavras Lavras, Minas Gerais, Brasil\\n\\n\\nThe increasing demand for imaging tests has made radiology information systems crucial in medical practice, especially those based\\non web technology. These systems include Picture Archiving and\\nCommunication System (PACS), Radiology Information Systems\\n(RIS), and Hospital Information System (HIS), generate and manipulate images through specialized software. To operate this complex\\nsoftware, require attention to detail and image manipulation techniques for accurate diagnoses. Usability issues in medical image\\nmanipulation software, given the process of adapting to new software and complex tasks, can result in inaccurate diagnoses with\\nclinical impact. This is a qualitative study, which is based on the\\nwork routines of radiology professionals, focusing on issues of cognitive learning, interaction, and usability with radiology software.\\nModerate usability tests with radiology technicians were conducted\\nto identify the difficulties and challenges they encounter while using medical image manipulation software. The analysis identified\\n64 problems grouped into 20 categories and organized under Visual\\nPresentation, Content, Information Architecture, and Interactivity.\\nThe paper emphasizes violated heuristics and describes how these\\nproblem categories impact users in their medical activities and their\\ninfluence on the clinical process. The obtained results provide insights to enhance usability practices and recommendations, aiming\\nto support the development systems used in radiology practice.\\n### **KEYWORDS**\\n\\nusability issues, radiology systems, qualitative analysis\\n### **1 INTRODUCTION**\\n\\nHealth information systems are extremely important in medical\\npractice and clinical interventions, and are essential for the activities carried out by health professionals in hospitals and clinics\\n\\n[ 5, 8, 14, 34, 35 ]. There are works in the literature that apply computing to medical images to assist healthcare professionals. The\\nwork of Vieira et al . [37] used Applied Explainable Artificial Intelligence (XAI) in the classification of retinal images to support\\nGlaucoma diagnoses. Martins et al . [27] presented a web-based\\nplatform that provides a intuitive interface suitable for Machine\\nLearning computational pathology research to be easily carried out.\\nIn this study, the focus is on radiology systems, which are essential for carrying out clinical analyses. The tasks involving these\\nanalyses require attention to detail through image manipulation\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n© 2024 SBC – Brazilian Computing Society.\\nISSN 2966-2753\\n\\n\\ntechniques to make a medical diagnosis. Are systems that have\\nled to optimizing the quality and productivity of healthcare professionals [ 1 ]. Figure 1 shows a simplified workflow of radiology\\nprofessionals and communication between systems. These professionals operate radiology equipment to generate medical images,\\nwhich are manipulated in specific software and archived in the\\nImage Archiving and Communication System (PACS). The images\\ncan be viewed on workstations, being accessed through the Radiology Information Systems (RIS) and Hospital Information System\\n(HIS). The area painted in yellow between the arrows indicated\\nwith numbers 1 and 2 in the figure indicates that the focus of this\\nstudy covers the processes that integrate the tasks in medical image\\nmanipulation software [8, 9].\\n\\n**Figure 1: Simplified layout of professional workflow and**\\n**inter-system communication**\\n\\nSource: Adapted from Silva [8]\\n\\nIt is important to highlight that many healthcare systems were\\ndeveloped based on web technology, especially PACS and DICOM\\n(Digital Imaging and Communications in Medicine) imaging systems [3, 4].\\nMedical systems used in radiology are multimedia systems with\\nthe ability to integrate and present medical images alongside text\\nsuch as medical reports. The interactivity in viewing images, with\\nzoom, rotation and annotation functionalities, in addition to the\\nability to present several photos in an integrated and simultaneous\\nway, and the transmission of medical data for remote and collaborative access, demonstrate the multimedia essence of the system.\\n\\n\\n28\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Silva et al.\\n\\n\\nPrevious studies have carried out several evaluations on PACS\\n\\nand RIS systems and found several usability problems that impact the performance of complex image capture and analysis tasks\\n\\n[ 9, 11, 15, 16, 38 ]. However, there is limited knowledge about image manipulation systems, whose aspects influence the procedural\\naspects and results of clinical examinations in hospitals and clinics,\\nmainly in the Brazilian scenario. Another important aspect that\\nwas also revealed in a study with radiology professionals [ 8 ], points\\nout that the work routine, the characteristics of the systems and\\nthe interaction of radiology professionals with these systems, are\\nconditioned by the hospital or clinic process, where the system is\\nprocess-centric and not user-centric.\\nThis paper aims to study and qualitatively analyze the postprocessing tasks of medical images performed by radiology professionals, observing the aspects of cognitive learning, the software\\nusability problems encountered in these tasks, how these problems impact the process of hospitals and clinics, and how radiology\\nprofessionals accept these systems.\\nThe study involves usability testing, given the CAAE research\\nprotocol nº 49170921.6.0000.5148, to understand in practice the\\ntasks of radiology professionals and identify the difficulties and\\nusability problems that these professionals present when using a\\nmedical image manipulation system model.\\nThis study seeks to answer which usability factors influence\\nthe use of radiology systems in the Brazilian context and what\\nthe impact of these problems would be on the radiology service\\nprovided.\\nThe paper is organized as follows. Section 2 presents the main\\nconcepts of radiology information systems, DICOM standard and\\nrelated works. Section 3 presents the main methodological aspects.\\nSection 4 presents the results, Section 5 the discussion and, finally,\\nSection 6 the conclusions and future work.\\n### **2 BACKGROUND**\\n\\nThis section describes concepts about information systems in radiology and related works.\\n### **2.1 Radiology Information Systems and DICOM** **standard**\\n\\nHospital radiology departments have been using digital systems\\non a large scale, which has generated an increasing volume of data.\\nThis reinforces that the solutions to manage these data and digital\\nimages are adopting a PACS and RIS system [38].\\nAccording to Dias et al . [9] and Huang [13], PACS is a short- and\\nlong-term image management system which consists of archiving\\nmedical images. These images are generated by medical equipment,\\nsuch as digital radiography, computed radiography, computed tomography (CT), magnetic resonance imaging (MRI) and ultrasound.\\nPACS plays a vital role in health information systems, helping\\nreduce costs, facilitate access to medical images and improve workflow in the radiology department. PACS improves the processing,\\nstorage and transmission of medical images for radiologists [ 38 ].\\nRIS and HIS are management systems that distribute data about\\ndiagnoses, procedures and patient exams over the network. The difference between them is that RIS is a radiology information system\\nwhile HIS is a hospital information system. The complexity this\\n\\n\\nsystems can lead to different problems, such as procedural errors,\\ndelay in diagnosis, possible errors in the diagnosis of results and\\neven discomfort or stress on the part of health professionals who\\nuse these systems for a long time. The usability of these systems is\\nessential to avoid errors in collection and diagnosis.\\nImages generated by radiology systems have a specific file format\\ncalled DICOM (Digital Imaging and Communications in Medicine),\\na standard model for storing medical image information [ 36 ]. This\\nstandard provides a framework that allows the exchange of multiple\\nmedical images and related information stored in a single format\\nby the PACS system.\\nFor post-processing tasks of DICOM images, medical image viewing software is used with measurement, zoom, contrast, and other\\ntools to review patient exam images and work with other medical\\ndata. This software may be from the same company as the radiology\\nequipment or third-party software. In addition to being essential\\nsoftware during an exam to obtain the medical report, there are\\nweb versions for patient access to consult the exams.\\n### **2.2 Related Works**\\n\\nSeveral studies in the literature have reported evaluations and analyzes of usability in radiology systems. In 2017, a systematic literature review conducted by Dias et al . [9] examined and compiled\\nusability issues identified in ten primary studies involving radiology\\nprofessionals, resulting in 90 problem cases. The qualitative analysis\\nrevealed the causes and effects of the identified usability problems,\\nclassifying them according to the usability heuristics established\\nby Nielsen and Molich [30], which were later improved and popularized by Nielsen [29] . The study provided implications related to\\nthe most common problems, with the top five heuristics with the\\nhighest number of reported usability problems being “Flexibility\\nand efficiency of use”, “Consistency and standards”, “Match between system and the real world”, “Recognition rather than recall”,\\nand “Help and documentation” [ 12, 15 – 18, 20, 25, 26, 31 ]. These\\nimplications include: attention to sequential steps in accordance\\nwith clinical analysis practices; direct access to crucial information for clinical decision-making; facilitate integration with other\\nsystems for producing clinical reports; improve efficient access to\\nimages that require simultaneous analysis; assisting in efficient\\nbasic image manipulations within the system; ensure consistency\\nin patient identification to avoid misinterpretations; align information architecture with clinical terminology; maintain consistency\\nin important features across tasks; system rules that accommodate\\nreal-world clinical procedures; accurate recognition capabilities;\\nand easily recognizable feature activation.\\nA study by Esfahani et al . [11] emphasized the significance of\\nuser interaction in selecting PACS. The study employed the ThinkAloud protocol in conjunction with a post-usability questionnaire to\\ncompare user interaction issues across various PACS user interfaces.\\nThe assessment focused on efficiency, encompassing aspects of usability such as efficiency, learning, error, and satisfaction, revealing\\nuser interaction challenges within three tested PACS systems.\\nA study conducted by Salahuddin et al . [33] delved into the behavior of healthcare professionals regarding the adoption of a Health\\nInformation System (HIS) and its impact on patient health. Through\\n\\n\\n29\\n\\n\\n-----\\n\\nAcceptance and Usability of Complex Medical Systems WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\na qualitative approach, the study conducted semi-structured interviews with 31 physicians across three hospitals that had implemented the system. Thematic qualitative analysis of the interview\\nresults revealed four key themes: (1) carelessness, (2) alternative\\nsolutions, (3) non-compliance with the procedure and (4) copying\\nand pasting habits. The study provides practical examples of how\\nthese professional behaviors may lead to unintended consequences\\nin the utilization of the HIS.\\n\\nIn a more recent study by da Silva et al . [8], semi-structured\\ninterviews were conducted with 10 radiologic technologists to gain\\ninsight into the daily work routines of professionals in the radiology field, encompassing processes, workload, responsibilities,\\nand challenges associated with the use of radiology systems. The\\ndata from this study underwent thematic analysis, facilitating the\\norganization and description of a dataset that identified issues, comprehended processes, responsibilities, and factors influencing the\\nroutines of radiology professionals. Several important findings were\\npresented, revealing that the work routines, system characteristics,\\nand interactions of radiology professionals with the systems are\\ncontingent upon the processes of the hospital or clinic, demonstrating how participants had difficulties reporting when asked about\\nsystem usability. The narratives underscore the need for professionals to adapt to system features, even if it requires memorizing\\nactions and utilizing functionalities in languages they may not fully\\ncomprehend.\\nDespite progress in studies of radiology systems, there is a lack\\nof information about the Brazilian scenario. Previous research has\\n\\nfocused mainly on aspects of usability and impact, often failing\\nto explore in depth the implications of these systems in the work\\nprocesses of healthcare professionals, as well as in hospital and\\nclinical contexts. This study seeks to fill this gap by carrying out\\nusability tests with radiology professionals, providing a comprehensive perspective on the use and challenges faced by these users in\\nmedical imaging software, enabling future work to deepen studies\\non the various aspects presented in this paper.\\n### **3 METHODS**\\n\\nThis study analyzes DICOM image post-processing tasks, evaluating the usability aspects of radiology systems in use in Brazil.\\nThe study included usability tests, which led to significant results\\nregarding users’ behaviour and difficulties in using the software.\\nThe method counted on content analysis and thematic analysis\\nto divide problems into categories according to how they affect the\\ninteraction. Data analysis was qualitative, where the unstructured\\ndata found were transformed into texts and other artifacts in a\\n\\ndetailed description of the situation or problem, considering the\\nessential aspects [22].\\n### **3.1 Participants**\\n\\nFour male and two female radiology technicians were recruited\\n(Table 1). The recruitment of participants for usability tests was\\ndone through contacts on social networks, e-mails, and by indication from the participants themselves. The research protocol\\nwas approved and registered by the university’s Research Ethics\\nCommittee with protocol CAAE 49170921.6.0000.5148 in August\\n\\n2021.\\n\\n\\n**Table 1: Participant details**\\n\\n**#** **Academic level** **G*** **State** **W*** **Workplace**\\n\\n1 PhD M Pará 4 UPA*\\n\\n2 BSc de g ree F Paraná 6 Hos p ital\\n3 S p ecialization M São Paulo 17 Hos p ital\\n4 S p ecialization M São Paulo 9 UPA*\\n5 BSc de g ree F São Paulo 10 Clinic\\n6 Specialization M São Paulo 5 Hospital\\n\\n** G - Gender | W - Work in years | UPA - Emergency care unit*\\n\\nThe participants had different characteristics, such as regionally,\\nplace of work and professional experience. Regarding academic\\ntraining, three of the participants had specializations, two had a\\nbachelor’s degree and one had a doctorate. Regarding the place of\\nwork, three worked in the hospital, two in the UPA Unidade de\\nPronto Atendimento, which translated means “Emergency Care\\nUnit”), an emergency care unit, and the other worked in a clinic. The\\naverage professional experience of these participants is eight years,\\nwith the most experienced participant having seventeen years of\\nexperience and the least experienced having four years.\\nRegarding the level of knowledge and experience with using\\nthe computer, the majority considered themselves to have an acceptable level, on a scale between low, acceptable and advanced\\nlevels. Likewise, most claim to have a good level of experience with\\nradiology systems.\\n### **3.2 Usability testing**\\n\\nThis study applied a usability assessment in radiology systems with\\nusers as a data collection instrument. These tests were carried out\\n\\nremotely from November 2021 to March 2022, using Google Meet,\\nwith video recording, participant audio, and a computer screen\\nlater used to document all important and valuable information.\\nDuring the evaluations, the Think-Aloud [ 10 ] protocol was used as\\na specific technique, which suggests the user describe aloud what\\nhe is thinking and doing while performing tasks. This technique\\nfocuses on user cognition when interacting with the system [11].\\nUsability testing was moderated and performed remotely and\\nindividually. The participant remotely accessed the primary researcher’s computer, which contained the software and images\\nused during the test, through the Google Remote Desktop service.\\nThen the participants were given the tasks to perform while moderated by the researcher.\\nThe software used in this study for the tests is the JiveX DICOM\\nViewer, free software for non-commercial use for viewing images\\nin DICOM format. JiveX DICOM Viewer is software that runs installed on the computer, unlike some software that runs in web\\nbrowsers. In this software, it is possible to manipulate the image\\nusing several tools that are common in software used by radiology\\nprofessionals. The JiveX DICOM Viewer software was chosen after\\nresearch and testing on some software with the help of a professor\\nand professional in the field of radiology. As the tests were planned\\nto be run remotely, it was decided to choose free software, without\\nthe need to create accounts, that did not consume much memory\\nduring image processing and that had a sufficient variety of tools\\n\\n\\n30\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Silva et al.\\n\\n\\nto carry out the tasks. Some sets of example medical images, and\\nthe user manual for this software are available for download on the\\nVisus website [1] .\\n\\nThe main author provided participants with images publicly\\navailable by the TCIA service, which de-identifies and hosts an\\nextensive archive of medical cancer images accessible for public\\ndownload [2] . The download of medical images is done through specific software called NBIA Data Retriever. Installation guidelines\\ncan be accessed from the site [3] . The queries for images can be made\\nat https://public.cancerimagingarchive.net/nbia-search.\\nThe tasks were shared with the participants by Google Meet\\ne which task was read. The tasks performed were: Select layout;\\nAdjust the position (rotation); Adjust the size (zoom); execute windowing (darken, lighten, highlighting); Describe the side or name of\\nthe incident; Open four images in 2x2 layout; Export file; and Print.\\nThe aspects studied and evaluated are cognitive learning (perception and attention, comprehension, memory, and active learning),\\ninteraction and usability problems with information systems and\\nother elements related the radiology systems [21].\\nIt is worth mentioning that none of the participants knew the\\nsoftware used in the test. Each test took about 60 minutes. After\\n\\ncompleting the tasks, two satisfaction and usability questionnaires\\nwere applied to assess the participants’ post-task impressions. One\\nof the questionnaires is a demographic survey developed by the\\nauthor himself, which includes, for example, the participant’s education, age, position held, time of experience with the use of the\\ncomputer, and the system. The other questionnaire is the PSSUQ\\n\\n- Post-Study System Usability Questionnaire [ 23 ], an instrument\\nbased on a script of post-test questions with 19 items that assess\\nuser satisfaction with the usability of a system. In this case, the\\nauthor used an adapted version of this research in European Portuguese. These questionnaires are available in Portuguese in link\\nhttps://bit.ly/questionnaires-pssuq. The PSSUQ questionnaire is\\nimportant to understand participants’ acceptance of new software,\\neven if the tools presented are similar to the software they are used\\nto on a daily basis at work.\\n### **3.3 Test data analysis**\\n\\nThis section outlines the analysis methods employed in conducting\\nthe usability tests for this study. The approach is based in content\\nand thematic analysis techniques, categorizing identified issues\\nbased on their impact on user interaction. The qualitative data analysis involves transforming unstructured data into detailed textual\\ndescriptions and other artifacts that provide an in-depth understanding of the situation or problem, considering essential aspects\\n\\n[ 22 ]. The usability issues identified in the content analysis were\\nsystematically organized using thematic analysis principles [6].\\nThe problems detected during the recorded tasks were transcribed by principal author, recording the usability problems identified in the recordings. Transcriptions and coding were carried\\nout in the Microsoft Word word processor, then transported and\\norganized in Google Sheets.\\n\\n1 https://www.visus.com/en/downloads/jivex-dicom-viewer.html\\n2 https://www.cancerimagingarchive.net/about-the-cancer-imaging-archive-tcia\\n3 https://wiki.cancerimagingarchive.net/display/NBIA/Downloading+TCIA+Images\\n\\n\\nOpen coding was performed by authors after identifying the\\nproblems and generating the initial codes to categorize the problems found. After each round of open coding, the version of the\\ncategorization instrument was used to assess inter-coder reliability,\\nbeing evaluated based on the Cohen coefficient [ 28 ]. After reaching\\nan acceptable level of reliability, differences were discussed and\\nresolved between the authors. The next step in thematic analysis\\ninvolved identifying themes for the categories identified in the categorization round. For categorization and organization into themes,\\nthe four major categories (Physical Presentation, Content, Information Architecture, and Interactivity) described in the work of\\nPetrie and Power [32] were adapted. Finally, at the end of the stage,\\nthe themes, categories and usability problems identified during the\\nentire process were presented.\\nThis analysis consolidated recommendations for the designs and\\nacceptance of radiology systems, incorporating the usability aspects\\nidentified and related to the Brazilian context.\\n### **4 RESULTS**\\n\\nThis section presents the results obtained in analysing usability\\ntests carried out with six radiology professionals for this research.\\nThe software used in the usability test had no integration with a\\nPACS and RIS system. So, we only consider the process of editing\\nthe radiology image done by the radiology technician.\\n### **4.1 Problem categories**\\n\\nThe researchers found 64 problem situations, with an average of\\n10 problems per user. Problems were separated into 20 problem\\ncategories organized into the following themes: Visual Presentation,\\nContent, Information Architecture, and Interactivity. The Table 2\\nis organized by problem categories, presenting just a few problem\\nsituations as examples in each category. In this link (https://bit.\\nly/tab-categorization) you will have access to another table with\\nall problem situations, that is organized by users/problems found.\\nFor better understand the tables it is important to note that two\\ntypes of codes appear in both tables. In the case of the \"U4-P5\"\\ncode, for example, this identifies that \"U4\" is user 4 and \"P5\" is this\\nuser’s problem 5. While in the case of the \"visu1\" code, for example,\\nit identifies the theme \"Visual Presentation\", with the number 1\\nrepresenting a problem category of this theme.\\n**Visual Presentation** details the visual presentation of the software to the user. Findings from four categories indicate that users\\nencountered challenges in navigating the layout due to a lack of\\nclarity and organization in its functionalities. The presentation of\\ncertain icons also led to user confusion. Consequently, several times,\\nsome users took time to find the interactive elements to perform\\nthe tasks.\\n\\nThe **Content** theme has four categories, outlining issues related\\nto the layout. These problems include sections displaying excessive\\ncontent, often with unclear contexts, occasional duplication, and\\nterms lacking precise definitions, making it challenging for users\\nto discern their functionality.\\n**Information Architecture** theme reveals structural issues within\\n\\nthe system that impact the user’s tasks. This encompasses two\\ncategories, elucidating scenarios where task execution could be\\nstreamlined for users, such as actions easily performed with the\\n\\n\\n31\\n\\n\\n-----\\n\\nAcceptance and Usability of Complex Medical Systems WebMedia’2024, Juiz de Fora, Brazil\\n\\n**Table 2: Problem categories organized into themes**\\n\\n**Code** **Category** **Occurrences** **Example**\\n\\n\\n\\nToolbar for windowing task has different options that accomplish the same\\nvisu1 Unclear or confusing layout 6\\ng oal ( U4-P5 )\\n\\n\\nvisu2\\n\\n\\nText/interactive element\\nis not clear/distinguished\\nenough to identify its functionalit y\\n\\n\\nThe user did not realize that there is another interactive element to describe\\n\\n10 the side and the radiological incidence that inserts only the text without the\\nindicative arrow (U2-P2)\\n\\n\\nInteractive elements with difThe icon of the \"reset\" element is confused with the icon of the \"rotate image\"\\nvisu3 ferent functionality have sim- 3\\nelement (U4-P2)\\nilar icons\\n\\nIt takes time to find the de- It took a long time for the user to find the text tool that allows describing the\\nvisu4 7\\nsired interactive element side and radiolo g ical incidence ( U3-P4 )\\n\\nLayout with too much concont1 2 Mouse action drop-down menu presents many unnecessary options (U6-P4)\\ntent confuses the user\\n\\nRadiology image viewing software content is not available in other languages\\ncont2 Content is not clear enough 7\\n( U2-P7 )\\n\\nDuplicate or contradictory The \"reset\" interactive element exists in two places, but performs different\\ncont3 1\\ncontent actions ( U5-P9 )\\n\\nThe interactive element represented by the floppy disk icon does not make it\\ncont4 Undefined terms 1\\nclear what the user is savin g ( U5-P10 )\\n\\nThere is not enough structure The actions performed with the mouse can be changed, but require several\\narch1 3\\nfor the content ste p s that hinder the user ( U5-P11 )\\n\\nPurpose of the structure is un- There are two options to display more than one radiology exam image on the\\narch2 4\\nclear screen with different ob j ectives that confuse the user ( U5-P8 )\\n\\nLack of information on how\\nIn the radiology image viewer, the user does not know how he activated and\\ninte1 to proceed and why things 2\\nhow to close the full-screen mode (U3-P8)\\nare ha pp enin g\\n\\nExcessive effort required by If the user makes a mistake in a procedure, it is necessary to reset and redo\\ninte2 2\\nthe user ever y thin g a g ain ( U4-P9 )\\n\\nSystem does not allow user If there is any misfit in the radiology image during the editing process, the user\\ninte3 to revert wrongly performed 5\\ncannot revert to the previous action (U5-P2)\\naction\\n\\nSoftware does not generate The software does not provide visual feedback when switching between one\\ninte4 3\\nfeedback on user actions radiolo gy ima g e and another ( U4-P1 )\\n\\nOpening the same radiology image in the layout with more than one image\\ninte5 Illogical interaction sequence 7\\np review ( U3-P10 )\\n\\nResult of the action perThe \"ESC\" key, by default, coincides with the action to exit full-screen mode,\\ninte6 formed does not meet the 11\\nwhich does not occur in this radiology image viewer (U6-P8)\\nuser’s ex p ectation\\n\\nExpected interactive func- The option to undo the last action is missing in this radiology image viewer\\ninte7 3\\ntionalit y is absent ( U1-P3 )\\n\\nSecurity issues not high- Software allows opening images of different patients in the same work window\\ninte8 1\\nli g hted and does not have a division to identif y these ima g es ( U4-P11 )\\n\\nMissing error/warning mes- When resetting the radiology image to the initial state, the software does not\\ninte9 4\\nsa g es ask if the user wants to p roceed with the action ( U1-P3 )\\n\\nThe user took a long time to perform a task because he could not use the tool\\ninte10 Delay to perform a task 4\\nproperly (U5-P5)\\n\\n32\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Silva et al.\\n\\n\\nmouse. The other category highlights instances where the user may\\nfind it less intuitive to utilize multi-layout options for displaying\\nimages.\\n**Interactivity** theme is the one that most presents problem situations, encompassing ten categories to delineate each scenario.\\nNotably, four categories deserve emphasis: \"Lack of information\\non how to proceed and why things are happening\", \"System does\\nnot allow the user to revert wrongly performed action\", \"Illogical\\ninteraction sequence\", and \"Result of the action performed does not\\nmeet the user’s expectation\". During the tests, it became evident\\nthat a majority of users encountered challenges. For instance, upon\\nentering full screen, users found themselves disoriented, lacking\\nguidance on what had transpired and how to revert. Additionally, users faced difficulty undoing incorrectly performed actions\\nand experienced instances where the outcomes did not align with\\ntheir expectations. These challenges necessitated users to navigate\\nthrough unnecessary steps to resolve or complete tasks.\\n### **4.2 Usability Problems Encountered**\\n\\nThis section describes some categories obtained by analyzing the\\nusability test data, to exemplify how the problem affected the user.\\n\\n*4.2.1* *Text or interactive element is not clear enough to identify*\\n*its functionality.* The design must speak the user’s language. It is\\nnecessary to ensure that the user understands the meaning without\\nlooking for a definition or remembering what it means [29].\\nDuring the tests, several users used a text tool to write the incidence on the exam image. There are two tools with the same\\npurpose, but one of these tools includes an arrow, and some users\\ntried to remove the arrow without success. Others took a while to\\n\\nrealize that the other tool did not include the arrow. Figure 2 shows\\nthe detail of each tool.\\n\\n**Figure 2: Interactive element is not distinguished enough to**\\n**identify its functionality**\\n\\nSource: Screenshot of JiveX DICOM Viewer software\\n\\nWe had ten hits for this category that violated the following\\nNielsen heuristics: “Aesthetic and minimalist design”, “Match between system and real world”, “Recognition rather than recall”, and\\n“Consistency and standards”.\\n\\n\\n*4.2.2* *Content is not clear enough.* We had seven problem situations\\nin this category, and we understand that five violated the heuristic\\n“Match between system and the real world”. The leading cause is\\nusers’ difficulty with the English language.\\n\\n**Figure 3: Content is not clear enough**\\nSource: Screenshot of JiveX DICOM Viewer software\\n\\nFor the example, let us use the problem situation that violated the\\n“Consistency and standards” heuristic. Figure 3 shows four options\\nfor exporting exam images, and the task was to export four exams.\\nThe user chose the option he understood to be exporting all exams\\nbut ended up exporting only one.\\n\\n*4.2.3* *Purpose of the structure is unclear.* Improving the ability to\\nlearn by keeping the types of consistency (internal and external)\\nhelps the user to understand and perform tasks. In this category,\\nwe had four problem occurrences that affect this “Consistency and\\npatterns” heuristic and another two: “Match between system and\\nthe real world” and “Visibility of system status”.\\nIn Figure 4 is the options for exporting exams, and the result did\\nnot match what the user expected.\\n\\n*4.2.4* *Result of the action performed does not meet the user’s ex-*\\n*pectation.* This category describes when the result of the action\\nperformed by the user is unexpected. Eleven problem situations\\nwere found in the tests with the radiology software that did not\\ncorrespond to what the user expected. We had six violations of\\nthe “Consistency and standards” heuristic, the others violated the\\n“Flexibility and efficiency of use”, “Match between the system and\\nthe real world” and “User control and freedom” heuristics.\\n\\nSome examples mentioned above also fall into this category. For\\nexample, the user tried to zoom in on the exam with the mouse and\\nthe result ended up changing the contrast in the image. In another\\ncase, they exported four exams and the result only exports one. By\\nmistake, the user enters in full screen and tries to exit by pressing\\nthe “ESC” key, which does not work.\\n### **4.3 Violated Nielsen’s heuristics**\\n\\nIn addition to categorization, each problem was analyzed by assigning the violated heuristic on the Nielsen scale [ 29 ]. For more\\n\\n\\n33\\n\\n\\n-----\\n\\nAcceptance and Usability of Complex Medical Systems WebMedia’2024, Juiz de Fora, Brazil\\n### **4.4 Post study questionnaire**\\n\\nAfter the usability test, participants received a link to the PSSUQ [4]\\n\\npost-study questionnaire to assess their satisfaction with the software’s usability. The questionnaire brought some interesting insights, as shown the number of responses for each question show\\nin Figure 5. Overall, all participants say they are satisfied with the\\nsystem they used in the test. Most (83%) understand that the system\\nhas a pleasant interface, easy to understand and learn. However,\\ntwo participants understood that they could complete the tasks efficiently. Only one participant said that the information the system\\npresented was clear. Moreover, none of the participants agreed that\\nthe system indicated an error and helped resolve it.\\nAnalyzing the aspect of user acceptance of the tested system,\\nit is possible to see that users will adhere to the system, as they\\nunderstand that over time they will be able to get used to the system,\\nminimizing the problems faced during testing.\\n\\n**Figure 5: PSSUQ questionnaire result**\\n### **4.5 Research limitations**\\n\\n\\n**Figure 4: Purpose of the structure is unclear**\\n\\nSource: Screenshot of JiveX DICOM Viewer software\\n\\ndetails, see the complete table (https://bit.ly/tab-categorization),\\nwhich contains the problem situations characterized with the respective violated heuristics.\\nThe results of the analysis that are based on Nielsen’s [ 29 ] heuristics, reference usability issues. Of Nielsen’s ten heuristics, the study\\npresented a violation in 7 of these heuristics, and they are: **Visibility**\\n**of System Status** (3 occurrences); **Match Between System and**\\n**the Real World** (13 occurrences); **User Control and Freedom**\\n(8 occurrences); **Consistency and Standards** (14 occurrences);\\n**Recognition Rather than Recall** (4 occurrences); **Flexibility**\\n**and Efficiency of Use** (8 occurrences); and **Aesthetic and Mini-**\\n**malist Design** (14 occurrences).\\n\\n\\nThe ideal scenario for usability testing would be \"in loco\", however, there were still many restrictions imposed by the COVID-19\\npandemic at the time. Therefore, the way found to carry out the usability tests was to provide commercial software for public and free\\nuse, with medical images used from a public and free image bank.\\nThe chosen software works with the process of refining the exam\\nimages before being sent to the PACS, which allows the doctor to\\nanalyze and write the report. However, this is another limitation,\\nwe did not have a PACS system or an HIS or RIS system to integrate. With this, we mitigate usability testing tasks, specifically for\\nradiology technicians. As most of the participants had experience\\nonly with X-ray exams, we chose to work only with this scenario,\\nas MRI and CT exams are more complex. Thus, it was impossible\\nto mitigate tasks such as collecting patient data from the RIS and\\nsending the exam to PACS, because the software also does not have\\na patient registration module. On the other hand, carrying out the\\ntests remotely allowed us to have a greater diversity of participants,\\ngiven their professional characteristics, location and experience.\\n### **5 DISCUSSION**\\n\\nThis section discusses the problems observed in analyzing usability\\ntests with radiology systems, mapping the different issues that can\\naffect the tasks of radiology professionals, analysis processes and\\n\\n4 https://bit.ly/questionnaires-pssuq\\n\\n\\n34\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Silva et al.\\n\\n\\npatient exam results. This study brought four essential aspects:\\nthe visual presentation of the system, the content, the information\\narchitecture and the interactivity with the system.\\nThe visual presentation of the radiology system is overloaded,\\nconfusing users due to elements with unclear functionality and\\nsimilar icons with different purposes. Users faced difficulty in locating hidden elements or identifying them visually. Content-related\\nissues, such as unnecessary information in drop-down menus and\\nunclear options for exporting exams, also impeded task completion.\\nDuplicate or contradictory content, poorly defined terms, and incongruent representation of actions, like saving a custom layout\\nwith a floppy disk icon, are some examples. The lack of a coherent information architecture resulted in inconsistencies, requiring\\nusers to switch between mouse actions and disrupting workflow.\\nSome system structures lacked clarity, with redundant options that\\nseemingly served the same purpose, difficult complicating usability.\\nIn the systematic mapping of the literature carried out in 2017 by\\nDias et al . [9], the heuristics “Match between system and the real\\nworld” and “Consistency and standards” also appear with many\\noccurrences in the current study. A good example is the category\\n“(inte6) Result of the action performed does not meet the user’s\\nexpectation” which has ten occurrences if we look at the table 2.\\nThe other two studies in the literature [ 2, 11 ] are more related to\\nPACS systems, but the current study may indicate aspects regarding\\nintegration with PACS and RIS systems. Although there was no\\nsuch integration in the tests, it is still possible to discuss some\\naspects. The example of the category “(inte8) Security issues not\\nhighlighted\" reveals that the software allows opening images of\\ndifferent patients in the same work area without separating. We\\ncan only imagine how serious this situation would be.\\nIn the qualitative study [ 7 ] performed out with an analysis of\\nthe needs and experience of doctors who use PACS, it presented\\nresults that describe factors like tasks and resources; workflow;\\nperformance issues; and training. In addition, the author addresses\\nthe situations where usability problems may occur but does not go\\ninto detail. In contrast, the usability tests of the current study delve\\ndeeper into the topic of tasks and features. While the study related\\npresents reports from the participants, the present study reports\\nthe situations as they emerged in an actual simulated test, with the\\nproblems highlighted and contextualized.\\nAnother aspect is related to the study [ 8 ] on the routine of health\\nprofessionals, as a clinical and hospital environment defines an\\nintense work routine and well-defined processes in which users\\nwho interact with health information systems are conditioned on\\nthis process and the characteristics of the systems.\\nTo answer which usability factors influence the use of radiology\\nsystems in the Brazilian context, this study reveals the difficulties\\nusers have in completing some of the tasks, due to some facts: Many\\nradiology equipment and software are not available in Portuguese,\\nespecially when deals with tomography and magnetic resonance\\nsystems. This is an important aspect, as many visual presentation\\nand content problems appeared in the tests and compromised users\\nwith little knowledge of the English language. The medical image\\nmanipulation software used in the tests is different from what the\\nparticipants were used to. This explain the difficulties in carrying\\nout the tasks that they normally carry out on a daily basis at work.\\n\\n\\nResponding to another question in this study, what would be\\nthe impact of these problems on the radiology service provided, we\\nnote that the usability problems reported in this study can directly\\nimpact several aspects, such as, for example, changes to patient\\nexams, delays in carrying out of a simple task, errors in handling the\\nexam image, not being able to correct a basic error, often becoming\\ndependent on support. Many of these problems may be related to\\nadaptation to a specific process, need for training, difficulties with\\na foreign language, lack of standardization of layout and icons of\\nradiology systems and optimization of the resources presented.\\n### **6 CONCLUSION**\\n\\nThe paper studied a radiology system to find instances of problems\\naffecting users in their medical tasks and how these problems can\\nbecome severe enough to affect the clinical process. A moderated\\nusability test was conducted remotely with six radiology technical\\nprofessionals using radiology image visualization software that\\nallowed editing and refining of the image before sending it for\\nanalysis and medical report. Analysis of the study resulted in 64\\nusability issues, organized into 20 categories that provide essential\\ninsights to evolve current usability practices and recommendations\\nto support the design of complex medical systems used in radiology\\npractice.\\nThe paper argued that usability factors in radiology systems need\\nattention, even if the tests were done in non-commercial software,\\nbut present the essential tools for the tasks performed by radiology\\ntechnicians. The results were significant to denote which usability\\naspects violate Nielsen’s usability heuristics and how these problems can interfere with the clinical process. Adjustments to the\\nproblems found can improve and speed up the execution of tasks,\\nprevent the user from making errors, ensure that the system is\\norganized and secure with patient data and exams, and ensure that\\nphysicians receive this data consistently to carry out the analysis\\nand report. As a contribution to the area of Multimedia, Hypermedia and Web, although the software used in this study is not\\na web version, this study shows opportunities for new research\\non complex medical systems and greater reflection on interaction\\nneeds and understanding the specificities of the Brazilian context,\\nunderstanding that several of these systems are based on web technology, such as RPACS (https://rpacs.com.br) and Vue PACS (used\\nseveral by hospitals and clinics), as well as DICOM image viewers\\n(https://dicomviewer.net and https://medevel.com/dwv/).\\nFor future work, we want to conduct studies with a more significant number of professionals, include more systems for testing\\nand validate of severity of usability, as the safety severity scale by\\nKennedy et al . [19] and Lowry et al . [24], and patient safety based\\non the \"rating\" made by professionals in the medical field. Additionally, a severity scale should encourage user-centric development\\nprocesses with a focus on security, facilitating the design of interfaces with good usability, with more secure, and providing methods\\nto measure and validate user performance before deployment.\\n### **ACKNOWLEDGMENTS**\\n\\nTo Elvio, for his help in choosing and recommending medical image\\nmanipulation software. The authors also thank FAPEMIG, CAPES\\nand CNPq for funding parts of this study.\\n\\n\\n35\\n\\n\\n-----\\n\\nAcceptance and Usability of Complex Medical Systems WebMedia’2024, Juiz de Fora, Brazil\\n\\n### **REFERENCES**\\n\\n[1] Reza Abbasi, Monireh Sadeqi Jabali, Reza Khajouei, and Hamidreza Tadayon.\\n2020. Investigating the satisfaction level of physicians in regards to implementing\\nmedical Picture Archiving and Communication System (PACS). *BMC medical*\\n*informatics and decision making* 20, 1 (2020), 1–8.\\n\\n[2] M. Alhajeri and S.G.S. Shah. 2019. Limitations in and Solutions for Improving the\\nFunctionality of Picture Archiving and Communication System: an Exploratory\\nStudy of PACS Professionals’ Perspectives. *Journal of Digital Imaging* 32 (2019),\\n54–67.\\n\\n[3] Priya Darshini B, Deepan Chakkaravarthy N, Gokul B, and Sabari Maharaja B.\\n2023. A Web-based Dicom Image and Plane Viewer. In *2023 Fifth International*\\n*Conference on Electrical, Computer and Communication Technologies (ICECCT)* .\\nIEEE, 1–7.\\n\\n[4] Rade R Babić, Zoran Milošević, and Gordana Stanković-Babić. 2012. Web technology in health information system. *Acta Facultatis Medicae Naissensis* 29, 2\\n(2012), 81–87.\\n\\n[5] Mirza Mansoor Baig, Hamid GholamHosseini, Aasia A Moqeem, Farhaan Mirza,\\nand Maria Lindén. 2017. A systematic review of wearable patient monitoring\\nsystems–current challenges and opportunities for clinical adoption. *Journal of*\\n*medical systems* 41, 7 (2017), 1–9.\\n\\n[6] Virginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology.\\n*Qualitative research in psychology* 3, 2 (2006), 77–101.\\n\\n[7] S. Cronin, B. Kane, and G. Doherty. 2021. A Qualitative Analysis of the Needs\\nand Experiences of Hospital-based Clinicians when Accessing Medical Imaging.\\n*Journal of digital imaging* (2021), 1––12. https://doi.org/10.1007/s10278-02100446-1\\n\\n[8] Fábio A. C. da Silva, André P. Freire, and Marluce R. Pereira. 2022. Understanding Interaction and Organizational Issues in Radiology Information Systems: a\\nQualitative Study with Health Professionals. In *XVIII Brazilian Symposium on*\\n*Information Systems* . –.\\n\\n[9] Camila Rodrigues Dias, Marluce Rodrigues Pereira, and Andre Pimenta Freire.\\n2017. Qualitative review of usability problems in health information systems for\\nradiology. *Journal of biomedical informatics* 76 (2017), 19–33.\\n\\n[10] K Anders Ericsson and Herbert A Simon. 1984. *Protocol analysis: Verbal reports*\\n*as data.* MIT Press, Cambridge, MA, USA.\\n\\n[11] Misagh Zahiri Esfahani, Reza Khajouei, and Mohammad Reza Baneshi. 2018. Augmentation of the think aloud method with users’ perspectives for the selection of\\na picture archiving and communication system. *Journal of Biomedical Informatics*\\n80 (2018), 43–51.\\n\\n[12] Ina Geldermann, Christoph Grouls, Christiane Kuhl, Thomas M Deserno, and\\nCord Spreckelsen. 2013. Black box integration of computer-aided diagnosis into\\nPACS deserves a second chance: results of a usability study concerning bone age\\nassessment. *Journal of Digital Imaging* 6 (2013), 698—-708.\\n\\n[13] Hai K Huang. 2003. Enterprise PACS and image distribution. *Computerized*\\n*Medical Imaging and Graphics* 27, 2-3 (2003), 241–253.\\n\\n[14] Fateme Rangraz Jeddi, Ehsan Nabovati, Reyhane Bigham, and Razieh Farrahi.\\n2020. Usability evaluation of a comprehensive national health information system:\\nA heuristic evaluation. *Informatics in Medicine Unlocked* 19 (2020), 100332.\\n\\n[15] Wiard Jorritsma, Fokie Cnossen, Rudi A Dierckx, Matthijs Oudkerk, and Peter MA\\nvan Ooijen. 2016. Pattern mining of user interaction logs for a post-deployment\\nusability evaluation of a radiology PACS client. *International Journal of Medical*\\n*Informatics* 85, 1 (2016), 36–42.\\n\\n[16] Wiard Jorritsma, Fokie Cnossen, Rudi A Dierckx, Matthijs Oudkerk, and Peter MA Van Ooijen. 2016. Post-deployment usability evaluation of a radiology\\nworkstation. *International Journal of Medical Informatics* 85, 1 (2016), 28–35.\\n\\n[17] Wiard Jorritsma, Fokie Cnossen, and Peter MA van Ooijen. 2014. Merits of\\nusability testing for PACS selection. *International Journal of Medical Informatics*\\n83, 1 (2014), 27–36.\\n\\n[18] Wiard Jorritsma, Fokie Cnossen, and Peter MA van Ooijen. 2015. Adaptive\\nsupport for user interface customization: a study in radiology. *International*\\n*Journal of Human-Computer Studies* 77 (2015), 1–9.\\n\\n[19] Brandan Kennedy, Ellen Kerns, Y Raymond Chan, Barbara S Chaparro, and\\nSarah D Fouquet. 2019. Safeuristics! Do Heuristic Evaluation Violation Severity\\nRatings Correlate with Patient Safety Severity Ratings for a Native Electronic\\nHealth Record Mobile Application? *Applied clinical informatics* 10, 02 (2019),\\n210–218.\\n\\n[20] Dasueran Kim, Peter Kang, Jungmin Yun, Sung-Hye Park, Jeong-Wook Seo, and\\nPeom Park. 2014. Study on User Interface of Pathology Picture Archiving and\\nCommunication System. *Healthcare Informatics Research* 20, 1 (2014), 45–51.\\n\\n[21] Andre W Kushniruk and Vimla L Patel. 2004. Cognitive and usability engineering\\nmethods for the evaluation of clinical information systems. *Journal of biomedical*\\n*informatics* 37, 1 (2004), 56–76.\\n\\n[22] J. Lazar, J. H. Feng, and H. Hochheiser. 2017. *Research Methods in Human-Computer*\\n*Interaction* . Morgan Kaufmann is an imprint of Elsevier, Cambridge, MA, USA.\\n\\n[23] James R Lewis. 1995. IBM computer usability satisfaction questionnaires: psychometric evaluation and instructions for use. *International Journal of Human-*\\n*Computer Interaction* 7, 1 (1995), 57–78.\\n\\n\\n\\n[24] Svetlana Z Lowry, Patricia Abbott, Michael C Gibbons, Svetlana Z Lowry, Robert\\nNorth, Emily S Patterson, Matthew T Quinn, Mala Ramaiah, Robert M Schumacher, and Jiajie Zhang. 2012. *Technical Evaluation, Testing, and Validatiaon of*\\n*the Usability of Electronic Health Records* . US Department of Commerce, National\\nInstitute of Standards and Technology.\\n\\n[25] Dimitrios Markonis, Frederic Baroz, Rafael Luis Ruiz De Castaneda, Celia Boyer,\\nand Henning Müller. 2013. User Tests for Assessing a Medical Image Retrieval\\nSystem: A Pilot Study. In *MEDINFO 2013* . Studies in Health Technology and\\nInformatics, Vol. 192. IOS Press, Amsterdam, Netherlands, 224–8.\\n\\n[26] Dimitrios Markonis, Markus Holzer, Frederic Baroz, Rafael Luis Ruiz De Castaneda, Célia Boyer, Georg Langs, and Henning Müller. 2015. User-oriented\\nevaluation of a medical image retrieval system for radiologists. *International*\\n*Journal of Medical Informatics* 84, 10 (2015), 774–783.\\n\\n[27] Luan Martins, Adriana Bueno, Alexandre Defelicibus, Rodrigo Drummond, Renan\\nValieris, Yu-Tao Zhu, Israel Da Silva, and Liang Zhao. 2023. WSI2ML – An\\nOpen-Source Whole Slide Image Annotation Software for Machine Learning\\nApplications. In *Anais do XXIX Simpósio Brasileiro de Sistemas Multimídia e Web*\\n(Ribeirão Preto/SP). SBC, Porto Alegre, RS, Brasil, 104–109. https://sol.sbc.org.\\nbr/index.php/webmedia/article/view/25871\\n\\n[28] Mary L McHugh. 2012. Interrater reliability: the kappa statistic. *Biochemia*\\n*medica* 22, 3 (2012), 276–282.\\n\\n[29] J. Nielsen. 2020. 10 Usability Heuristics for User Interface Design. *Nielsen Norman*\\n*Group* (2020). https://www.nngroup.com/articles/ten-usability-heuristics\\n\\n[30] Jakob Nielsen and Rolf Molich. 1990. Heuristic evaluation of user interfaces.\\nIn *Proceedings of the SIGCHI conference on Human factors in computing systems* .\\n249–256.\\n\\n[31] Kenneth Olbrish, Paul Shanken, Donna Rabe, Lorraine Steven, and Nicholas\\nIrizarry. 2011. Four-year enterprise PACS support trend analysis. *Journal of*\\n*Digital Imaging* 24 (2011), 284—-294.\\n\\n[32] Helen Petrie and Christopher Power. 2012. What do users really care about? A\\ncomparison of usability problems found by users and experts on highly interactive\\nwebsites. In *Proceedings of the SIGCHI Conference on Human Factors in Computing*\\n*Systems* . 2107–2116.\\n\\n[33] Lizawati Salahuddin, Zuraini Ismail, Ummi Rabaah Hashim, Nor Haslinda Ismail,\\nRaja Rina Raja Ikram, Fiza Abdul Rahim, and Noor Hafizah Hassan. 2020. Healthcare practitioner behaviours that influence unsafe use of hospital information\\nsystems. *Health informatics journal* 26, 1 (2020), 420–434.\\n\\n[34] Sandro Luis F. F. C. Silva, Marcelo Fornazin, and Rodrigo Pereira dos Santos. 2020.\\nAnalysis and Modeling of Emergent Systems in the Health Information System\\nDomain. In *XVI Brazilian Symposium on Information Systems* . 1–8.\\n\\n[35] Marcílio Ferreira Souza-Júnior, Lívio Antonio Monteiro Amorim, Lucas Emanoel\\nPaulino Silva Santos, Jorge Silva Correia-Neto, and Hugo Araujo Souza. 2019.\\nAprimoramento de Interfaces de Usuários de Sistemas de Informação em Saúde no\\nAtendimento Pré-hospitalar na Ótica da Consciência Situacional. *iSys-Brazilian*\\n*Journal of Information Systems* 12, 4 (2019), 98–116.\\n\\n[36] US National Electrical Manufacturers Association. 2021. Digital Imaging and Communications in Medicine (DICOM) stanrdard.\\nhttp://dicom.nema.org/medical/dicom/current/output/html/part01.html.\\naccessed on 19/05/2021.\\n\\n[37] Cleverson Vieira, Marcus Oliveira, Marcelo Guimarães, Leonardo Rocha, and\\nDiego Dias. 2023. Applied Explainable Artificial Intelligence (XAI) in the classification of retinal images for support in the diagnosis of Glaucoma. In *Anais*\\n*do XXIX Simpósio Brasileiro de Sistemas Multimídia e Web* (Ribeirão Preto/SP).\\nSBC, Porto Alegre, RS, Brasil, 82–90. https://sol.sbc.org.br/index.php/webmedia/\\narticle/view/25869\\n\\n[38] Misagh Zahiri Esfahani, Jamileh Farokhzadian, Kambiz Bahaadinbeigy, and Reza\\nKhajouei. 2019. Factors influencing the selection of a picture archiving and\\ncommunication system: A qualitative study. *The International journal of health*\\n*planning and management* 34, 2 (2019), 780–793.\\n\\n\\n36\\n\\n\\n-----\\n\\n',\n",
       "  'artigo_tokenizado': ['#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Acceptance',\n",
       "   'and',\n",
       "   'Usability',\n",
       "   'of',\n",
       "   'Complex',\n",
       "   'Medical',\n",
       "   'Systems',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'A',\n",
       "   'Study',\n",
       "   'with',\n",
       "   'Radiology',\n",
       "   'Professionals',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Fábio',\n",
       "   'Ap',\n",
       "   '.',\n",
       "   'Cândido',\n",
       "   'da',\n",
       "   'Silva',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'fabio.acs@gmail.com',\n",
       "   'UFLA',\n",
       "   '-',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Lavras',\n",
       "   'Lavras',\n",
       "   ',',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   ',',\n",
       "   'Brasil',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'ABSTRACT',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'André',\n",
       "   'Pimenta',\n",
       "   'Freire',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'apfreire@ufla.br',\n",
       "   'UFLA',\n",
       "   '-',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Lavras',\n",
       "   'Lavras',\n",
       "   ',',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   ',',\n",
       "   'Brasil',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Marluce',\n",
       "   'Rodrigues',\n",
       "   'Pereira',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'marluce@ufla.br',\n",
       "   'UFLA',\n",
       "   '-',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Lavras',\n",
       "   'Lavras',\n",
       "   ',',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   ',',\n",
       "   'Brasil',\n",
       "   '\\n\\n\\n',\n",
       "   'The',\n",
       "   'increasing',\n",
       "   'demand',\n",
       "   'for',\n",
       "   'imaging',\n",
       "   'tests',\n",
       "   'has',\n",
       "   'made',\n",
       "   'radiology',\n",
       "   'information',\n",
       "   'systems',\n",
       "   'crucial',\n",
       "   'in',\n",
       "   'medical',\n",
       "   'practice',\n",
       "   ',',\n",
       "   'especially',\n",
       "   'those',\n",
       "   'based',\n",
       "   '\\n',\n",
       "   'on',\n",
       "   'web',\n",
       "   'technology',\n",
       "   '.',\n",
       "   'These',\n",
       "   'systems',\n",
       "   'include',\n",
       "   'Picture',\n",
       "   'Archiving',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'Communication',\n",
       "   'System',\n",
       "   '(',\n",
       "   'PACS',\n",
       "   ')',\n",
       "   ',',\n",
       "   'Radiology',\n",
       "   'Information',\n",
       "   'Systems',\n",
       "   '\\n',\n",
       "   '(',\n",
       "   'RIS',\n",
       "   ')',\n",
       "   ',',\n",
       "   'and',\n",
       "   'Hospital',\n",
       "   'Information',\n",
       "   'System',\n",
       "   '(',\n",
       "   'HIS',\n",
       "   ')',\n",
       "   ',',\n",
       "   'generate',\n",
       "   'and',\n",
       "   'manipulate',\n",
       "   'images',\n",
       "   'through',\n",
       "   'specialized',\n",
       "   'software',\n",
       "   '.',\n",
       "   'To',\n",
       "   'operate',\n",
       "   'this',\n",
       "   'complex',\n",
       "   '\\n',\n",
       "   'software',\n",
       "   ',',\n",
       "   'require',\n",
       "   'attention',\n",
       "   'to',\n",
       "   'detail',\n",
       "   'and',\n",
       "   'image',\n",
       "   'manipulation',\n",
       "   'techniques',\n",
       "   'for',\n",
       "   'accurate',\n",
       "   'diagnoses',\n",
       "   '.',\n",
       "   'Usability',\n",
       "   'issues',\n",
       "   'in',\n",
       "   'medical',\n",
       "   'image',\n",
       "   '\\n',\n",
       "   'manipulation',\n",
       "   'software',\n",
       "   ',',\n",
       "   'given',\n",
       "   'the',\n",
       "   'process',\n",
       "   'of',\n",
       "   'adapting',\n",
       "   'to',\n",
       "   'new',\n",
       "   'software',\n",
       "   'and',\n",
       "   'complex',\n",
       "   'tasks',\n",
       "   ',',\n",
       "   'can',\n",
       "   'result',\n",
       "   'in',\n",
       "   'inaccurate',\n",
       "   'diagnoses',\n",
       "   'with',\n",
       "   '\\n',\n",
       "   'clinical',\n",
       "   'impact',\n",
       "   '.',\n",
       "   'This',\n",
       "   'is',\n",
       "   'a',\n",
       "   'qualitative',\n",
       "   'study',\n",
       "   ',',\n",
       "   'which',\n",
       "   'is',\n",
       "   'based',\n",
       "   'on',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'work',\n",
       "   'routines',\n",
       "   'of',\n",
       "   'radiology',\n",
       "   'professionals',\n",
       "   ',',\n",
       "   'focusing',\n",
       "   'on',\n",
       "   'issues',\n",
       "   'of',\n",
       "   'cognitive',\n",
       "   'learning',\n",
       "   ',',\n",
       "   'interaction',\n",
       "   ',',\n",
       "   'and',\n",
       "   'usability',\n",
       "   'with',\n",
       "   'radiology',\n",
       "   'software',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Moderate',\n",
       "   'usability',\n",
       "   'tests',\n",
       "   'with',\n",
       "   'radiology',\n",
       "   'technicians',\n",
       "   'were',\n",
       "   'conducted',\n",
       "   '\\n',\n",
       "   'to',\n",
       "   'identify',\n",
       "   'the',\n",
       "   'difficulties',\n",
       "   'and',\n",
       "   'challenges',\n",
       "   'they',\n",
       "   'encounter',\n",
       "   'while',\n",
       "   'using',\n",
       "   'medical',\n",
       "   'image',\n",
       "   'manipulation',\n",
       "   'software',\n",
       "   '.',\n",
       "   'The',\n",
       "   'analysis',\n",
       "   'identified',\n",
       "   '\\n',\n",
       "   '64',\n",
       "   'problems',\n",
       "   'grouped',\n",
       "   'into',\n",
       "   '20',\n",
       "   'categories',\n",
       "   'and',\n",
       "   'organized',\n",
       "   'under',\n",
       "   'Visual',\n",
       "   '\\n',\n",
       "   'Presentation',\n",
       "   ',',\n",
       "   'Content',\n",
       "   ',',\n",
       "   'Information',\n",
       "   'Architecture',\n",
       "   ',',\n",
       "   'and',\n",
       "   'Interactivity',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'The',\n",
       "   'paper',\n",
       "   'emphasizes',\n",
       "   'violated',\n",
       "   'heuristics',\n",
       "   'and',\n",
       "   'describes',\n",
       "   'how',\n",
       "   'these',\n",
       "   '\\n',\n",
       "   'problem',\n",
       "   'categories',\n",
       "   'impact',\n",
       "   'users',\n",
       "   'in',\n",
       "   'their',\n",
       "   'medical',\n",
       "   'activities',\n",
       "   'and',\n",
       "   'their',\n",
       "   '\\n',\n",
       "   'influence',\n",
       "   'on',\n",
       "   'the',\n",
       "   'clinical',\n",
       "   'process',\n",
       "   '.',\n",
       "   'The',\n",
       "   'obtained',\n",
       "   'results',\n",
       "   'provide',\n",
       "   'insights',\n",
       "   'to',\n",
       "   'enhance',\n",
       "   'usability',\n",
       "   'practices',\n",
       "   'and',\n",
       "   'recommendations',\n",
       "   ',',\n",
       "   'aiming',\n",
       "   '\\n',\n",
       "   'to',\n",
       "   'support',\n",
       "   'the',\n",
       "   'development',\n",
       "   'systems',\n",
       "   'used',\n",
       "   'in',\n",
       "   'radiology',\n",
       "   'practice',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'KEYWORDS',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'usability',\n",
       "   'issues',\n",
       "   ',',\n",
       "   'radiology',\n",
       "   'systems',\n",
       "   ',',\n",
       "   'qualitative',\n",
       "   'analysis',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   '1',\n",
       "   'INTRODUCTION',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Health',\n",
       "   'information',\n",
       "   'systems',\n",
       "   'are',\n",
       "   'extremely',\n",
       "   'important',\n",
       "   'in',\n",
       "   'medical',\n",
       "   '\\n',\n",
       "   'practice',\n",
       "   'and',\n",
       "   'clinical',\n",
       "   'interventions',\n",
       "   ',',\n",
       "   'and',\n",
       "   'are',\n",
       "   'essential',\n",
       "   'for',\n",
       "   'the',\n",
       "   'activities',\n",
       "   'carried',\n",
       "   'out',\n",
       "   'by',\n",
       "   'health',\n",
       "   'professionals',\n",
       "   'in',\n",
       "   'hospitals',\n",
       "   'and',\n",
       "   'clinics',\n",
       "   '\\n\\n',\n",
       "   '[',\n",
       "   '5',\n",
       "   ',',\n",
       "   '8',\n",
       "   ',',\n",
       "   '14',\n",
       "   ',',\n",
       "   '34',\n",
       "   ',',\n",
       "   '35',\n",
       "   ']',\n",
       "   '.',\n",
       "   'There',\n",
       "   'are',\n",
       "   'works',\n",
       "   'in',\n",
       "   'the',\n",
       "   'literature',\n",
       "   'that',\n",
       "   'apply',\n",
       "   'computing',\n",
       "   'to',\n",
       "   'medical',\n",
       "   'images',\n",
       "   'to',\n",
       "   'assist',\n",
       "   'healthcare',\n",
       "   'professionals',\n",
       "   '.',\n",
       "   'The',\n",
       "   '\\n',\n",
       "   'work',\n",
       "   'of',\n",
       "   'Vieira',\n",
       "   'et',\n",
       "   'al',\n",
       "   '.',\n",
       "   '[',\n",
       "   '37',\n",
       "   ']',\n",
       "   'used',\n",
       "   'Applied',\n",
       "   'Explainable',\n",
       "   'Artificial',\n",
       "   'Intelligence',\n",
       "   '(',\n",
       "   'XAI',\n",
       "   ')',\n",
       "   'in',\n",
       "   'the',\n",
       "   'classification',\n",
       "   'of',\n",
       "   'retinal',\n",
       "   'images',\n",
       "   'to',\n",
       "   'support',\n",
       "   '\\n',\n",
       "   'Glaucoma',\n",
       "   'diagnoses',\n",
       "   '.',\n",
       "   'Martins',\n",
       "   'et',\n",
       "   'al',\n",
       "   '.',\n",
       "   '[',\n",
       "   '27',\n",
       "   ']',\n",
       "   'presented',\n",
       "   'a',\n",
       "   'web',\n",
       "   '-',\n",
       "   'based',\n",
       "   '\\n',\n",
       "   'platform',\n",
       "   'that',\n",
       "   'provides',\n",
       "   'a',\n",
       "   'intuitive',\n",
       "   'interface',\n",
       "   'suitable',\n",
       "   'for',\n",
       "   'Machine',\n",
       "   '\\n',\n",
       "   'Learning',\n",
       "   'computational',\n",
       "   'pathology',\n",
       "   'research',\n",
       "   'to',\n",
       "   'be',\n",
       "   'easily',\n",
       "   'carried',\n",
       "   'out',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'In',\n",
       "   'this',\n",
       "   'study',\n",
       "   ',',\n",
       "   'the',\n",
       "   'focus',\n",
       "   'is',\n",
       "   'on',\n",
       "   'radiology',\n",
       "   'systems',\n",
       "   ',',\n",
       "   'which',\n",
       "   'are',\n",
       "   'essential',\n",
       "   'for',\n",
       "   'carrying',\n",
       "   'out',\n",
       "   'clinical',\n",
       "   'analyses',\n",
       "   '.',\n",
       "   'The',\n",
       "   'tasks',\n",
       "   'involving',\n",
       "   'these',\n",
       "   '\\n',\n",
       "   'analyses',\n",
       "   'require',\n",
       "   'attention',\n",
       "   'to',\n",
       "   'detail',\n",
       "   'through',\n",
       "   'image',\n",
       "   'manipulation',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   ':',\n",
       "   'Proceedings',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Web',\n",
       "   '(',\n",
       "   'WebMe',\n",
       "   '\\n',\n",
       "   'dia’2024',\n",
       "   ')',\n",
       "   '.',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '.',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   ':',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   ',',\n",
       "   '2024',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'SBC',\n",
       "   '–',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '-',\n",
       "   '2753',\n",
       "   '\\n\\n\\n',\n",
       "   'techniques',\n",
       "   'to',\n",
       "   'make',\n",
       "   'a',\n",
       "   'medical',\n",
       "   'diagnosis',\n",
       "   '.',\n",
       "   'Are',\n",
       "   'systems',\n",
       "   'that',\n",
       "   'have',\n",
       "   '\\n',\n",
       "   'led',\n",
       "   'to',\n",
       "   'optimizing',\n",
       "   'the',\n",
       "   'quality',\n",
       "   'and',\n",
       "   'productivity',\n",
       "   'of',\n",
       "   'healthcare',\n",
       "   'professionals',\n",
       "   '[',\n",
       "   '1',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Figure',\n",
       "   '1',\n",
       "   'shows',\n",
       "   'a',\n",
       "   'simplified',\n",
       "   'workflow',\n",
       "   'of',\n",
       "   'radiology',\n",
       "   '\\n',\n",
       "   'professionals',\n",
       "   'and',\n",
       "   'communication',\n",
       "   'between',\n",
       "   'systems',\n",
       "   '.',\n",
       "   'These',\n",
       "   'professionals',\n",
       "   'operate',\n",
       "   'radiology',\n",
       "   'equipment',\n",
       "   'to',\n",
       "   'generate',\n",
       "   'medical',\n",
       "   'images',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'which',\n",
       "   'are',\n",
       "   'manipulated',\n",
       "   'in',\n",
       "   'specific',\n",
       "   'software',\n",
       "   'and',\n",
       "   'archived',\n",
       "   'in',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'Image',\n",
       "   'Archiving',\n",
       "   'and',\n",
       "   'Communication',\n",
       "   'System',\n",
       "   '(',\n",
       "   'PACS',\n",
       "   ')',\n",
       "   '.',\n",
       "   'The',\n",
       "   'images',\n",
       "   '\\n',\n",
       "   'can',\n",
       "   'be',\n",
       "   'viewed',\n",
       "   'on',\n",
       "   'workstations',\n",
       "   ',',\n",
       "   'being',\n",
       "   'accessed',\n",
       "   'through',\n",
       "   'the',\n",
       "   'Radiology',\n",
       "   'Information',\n",
       "   'Systems',\n",
       "   '(',\n",
       "   'RIS',\n",
       "   ')',\n",
       "   'and',\n",
       "   'Hospital',\n",
       "   'Information',\n",
       "   'System',\n",
       "   '\\n',\n",
       "   '(',\n",
       "   'HIS',\n",
       "   ')',\n",
       "   '.',\n",
       "   'The',\n",
       "   'area',\n",
       "   'painted',\n",
       "   'in',\n",
       "   'yellow',\n",
       "   'between',\n",
       "   'the',\n",
       "   'arrows',\n",
       "   'indicated',\n",
       "   '\\n',\n",
       "   'with',\n",
       "   'numbers',\n",
       "   '1',\n",
       "   'and',\n",
       "   '2',\n",
       "   'in',\n",
       "   'the',\n",
       "   'figure',\n",
       "   'indicates',\n",
       "   'that',\n",
       "   'the',\n",
       "   'focus',\n",
       "   'of',\n",
       "   'this',\n",
       "   '\\n',\n",
       "   'study',\n",
       "   'covers',\n",
       "   'the',\n",
       "   'processes',\n",
       "   'that',\n",
       "   'integrate',\n",
       "   'the',\n",
       "   'tasks',\n",
       "   'in',\n",
       "   'medical',\n",
       "   'image',\n",
       "   '\\n',\n",
       "   'manipulation',\n",
       "   'software',\n",
       "   '[',\n",
       "   '8',\n",
       "   ',',\n",
       "   '9',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n\\n',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Figure',\n",
       "   '1',\n",
       "   ':',\n",
       "   'Simplified',\n",
       "   'layout',\n",
       "   'of',\n",
       "   'professional',\n",
       "   'workflow',\n",
       "   'and',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n',\n",
       "   '*',\n",
       "   '*',\n",
       "   'inter',\n",
       "   '-',\n",
       "   'system',\n",
       "   'communication',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Source',\n",
       "   ':',\n",
       "   'Adapted',\n",
       "   'from',\n",
       "   'Silva',\n",
       "   '[',\n",
       "   '8',\n",
       "   ']',\n",
       "   '\\n\\n',\n",
       "   'It',\n",
       "   'is',\n",
       "   'important',\n",
       "   'to',\n",
       "   'highlight',\n",
       "   'that',\n",
       "   'many',\n",
       "   'healthcare',\n",
       "   'systems',\n",
       "   'were',\n",
       "   '\\n',\n",
       "   'developed',\n",
       "   'based',\n",
       "   'on',\n",
       "   'web',\n",
       "   'technology',\n",
       "   ',',\n",
       "   'especially',\n",
       "   'PACS',\n",
       "   'and',\n",
       "   'DICOM',\n",
       "   '\\n',\n",
       "   '(',\n",
       "   'Digital',\n",
       "   'Imaging',\n",
       "   'and',\n",
       "   'Communications',\n",
       "   'in',\n",
       "   'Medicine',\n",
       "   ')',\n",
       "   'imaging',\n",
       "   'systems',\n",
       "   '[',\n",
       "   '3',\n",
       "   ',',\n",
       "   '4',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Medical',\n",
       "   'systems',\n",
       "   'used',\n",
       "   'in',\n",
       "   'radiology',\n",
       "   'are',\n",
       "   'multimedia',\n",
       "   'systems',\n",
       "   'with',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'ability',\n",
       "   'to',\n",
       "   'integrate',\n",
       "   'and',\n",
       "   'present',\n",
       "   'medical',\n",
       "   'images',\n",
       "   'alongside',\n",
       "   'text',\n",
       "   '\\n',\n",
       "   'such',\n",
       "   'as',\n",
       "   'medical',\n",
       "   'reports',\n",
       "   '.',\n",
       "   'The',\n",
       "   'interactivity',\n",
       "   'in',\n",
       "   'viewing',\n",
       "   'images',\n",
       "   ',',\n",
       "   'with',\n",
       "   '\\n',\n",
       "   'zoom',\n",
       "   ',',\n",
       "   'rotation',\n",
       "   'and',\n",
       "   'annotation',\n",
       "   'functionalities',\n",
       "   ',',\n",
       "   'in',\n",
       "   'addition',\n",
       "   'to',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'ability',\n",
       "   'to',\n",
       "   'present',\n",
       "   'several',\n",
       "   'photos',\n",
       "   'in',\n",
       "   'an',\n",
       "   'integrated',\n",
       "   'and',\n",
       "   'simultaneous',\n",
       "   '\\n',\n",
       "   'way',\n",
       "   ',',\n",
       "   'and',\n",
       "   'the',\n",
       "   'transmission',\n",
       "   'of',\n",
       "   'medical',\n",
       "   'data',\n",
       "   'for',\n",
       "   'remote',\n",
       "   'and',\n",
       "   'collaborative',\n",
       "   'access',\n",
       "   ',',\n",
       "   'demonstrate',\n",
       "   'the',\n",
       "   'multimedia',\n",
       "   'essence',\n",
       "   'of',\n",
       "   'the',\n",
       "   'system',\n",
       "   '.',\n",
       "   '\\n\\n\\n',\n",
       "   '28',\n",
       "   '\\n\\n\\n',\n",
       "   '-----',\n",
       "   '\\n\\n',\n",
       "   'WebMedia’2024',\n",
       "   ',',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   'Silva',\n",
       "   'et',\n",
       "   'al',\n",
       "   '.',\n",
       "   '\\n\\n\\n',\n",
       "   'Previous',\n",
       "   'studies',\n",
       "   'have',\n",
       "   'carried',\n",
       "   'out',\n",
       "   'several',\n",
       "   'evaluations',\n",
       "   'on',\n",
       "   'PACS',\n",
       "   '\\n\\n',\n",
       "   'and',\n",
       "   'RIS',\n",
       "   'systems',\n",
       "   'and',\n",
       "   'found',\n",
       "   'several',\n",
       "   'usability',\n",
       "   'problems',\n",
       "   'that',\n",
       "   'impact',\n",
       "   'the',\n",
       "   'performance',\n",
       "   'of',\n",
       "   'complex',\n",
       "   'image',\n",
       "   'capture',\n",
       "   'and',\n",
       "   'analysis',\n",
       "   'tasks',\n",
       "   '\\n\\n',\n",
       "   '[',\n",
       "   '9',\n",
       "   ',',\n",
       "   '11',\n",
       "   ',',\n",
       "   '15',\n",
       "   ',',\n",
       "   '16',\n",
       "   ',',\n",
       "   '38',\n",
       "   ']',\n",
       "   '.',\n",
       "   'However',\n",
       "   ',',\n",
       "   'there',\n",
       "   'is',\n",
       "   'limited',\n",
       "   'knowledge',\n",
       "   'about',\n",
       "   'image',\n",
       "   'manipulation',\n",
       "   'systems',\n",
       "   ',',\n",
       "   'whose',\n",
       "   'aspects',\n",
       "   'influence',\n",
       "   'the',\n",
       "   'procedural',\n",
       "   '\\n',\n",
       "   'aspects',\n",
       "   'and',\n",
       "   'results',\n",
       "   'of',\n",
       "   'clinical',\n",
       "   'examinations',\n",
       "   'in',\n",
       "   'hospitals',\n",
       "   'and',\n",
       "   'clinics',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'mainly',\n",
       "   'in',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'scenario',\n",
       "   '.',\n",
       "   'Another',\n",
       "   'important',\n",
       "   'aspect',\n",
       "   'that',\n",
       "   '\\n',\n",
       "   'was',\n",
       "   'also',\n",
       "   'revealed',\n",
       "   'in',\n",
       "   'a',\n",
       "   'study',\n",
       "   'with',\n",
       "   'radiology',\n",
       "   'professionals',\n",
       "   '[',\n",
       "   '8',\n",
       "   ']',\n",
       "   ',',\n",
       "   'points',\n",
       "   '\\n',\n",
       "   'out',\n",
       "   'that',\n",
       "   'the',\n",
       "   'work',\n",
       "   ...],\n",
       "  'pos_tagger': '',\n",
       "  'lema': ['Acceptance',\n",
       "   'and',\n",
       "   'Usability',\n",
       "   'of',\n",
       "   'Complex',\n",
       "   'Medical',\n",
       "   'Systems',\n",
       "   'a',\n",
       "   'study',\n",
       "   'with',\n",
       "   'Radiology',\n",
       "   'professional',\n",
       "   'Fábio',\n",
       "   'Ap',\n",
       "   'Cândido',\n",
       "   'da',\n",
       "   'Silva',\n",
       "   'fabio.acs@gmail.com',\n",
       "   'UFLA',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Lavras',\n",
       "   'Lavras',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'Brasil',\n",
       "   'ABSTRACT',\n",
       "   'André',\n",
       "   'Pimenta',\n",
       "   'Freire',\n",
       "   'apfreire@ufla.br',\n",
       "   'UFLA',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Lavras',\n",
       "   'Lavras',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'Brasil',\n",
       "   'Marluce',\n",
       "   'Rodrigues',\n",
       "   'Pereira',\n",
       "   'marluce@ufla.br',\n",
       "   'UFLA',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Lavras',\n",
       "   'Lavras',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'Brasil',\n",
       "   'the',\n",
       "   'increase',\n",
       "   'demand',\n",
       "   'for',\n",
       "   'imaging',\n",
       "   'test',\n",
       "   'have',\n",
       "   'make',\n",
       "   'radiology',\n",
       "   'information',\n",
       "   'system',\n",
       "   'crucial',\n",
       "   'in',\n",
       "   'medical',\n",
       "   'practice',\n",
       "   'especially',\n",
       "   'those',\n",
       "   'base',\n",
       "   'on',\n",
       "   'web',\n",
       "   'technology',\n",
       "   'these',\n",
       "   'system',\n",
       "   'include',\n",
       "   'Picture',\n",
       "   'archiving',\n",
       "   'and',\n",
       "   'Communication',\n",
       "   'System',\n",
       "   'PACS',\n",
       "   'Radiology',\n",
       "   'Information',\n",
       "   'Systems',\n",
       "   'RIS',\n",
       "   'and',\n",
       "   'Hospital',\n",
       "   'Information',\n",
       "   'System',\n",
       "   'HIS',\n",
       "   'generate',\n",
       "   'and',\n",
       "   'manipulate',\n",
       "   'image',\n",
       "   'through',\n",
       "   'specialized',\n",
       "   'software',\n",
       "   'to',\n",
       "   'operate',\n",
       "   'this',\n",
       "   'complex',\n",
       "   'software',\n",
       "   'require',\n",
       "   'attention',\n",
       "   'to',\n",
       "   'detail',\n",
       "   'and',\n",
       "   'image',\n",
       "   'manipulation',\n",
       "   'technique',\n",
       "   'for',\n",
       "   'accurate',\n",
       "   'diagnosis',\n",
       "   'usability',\n",
       "   'issue',\n",
       "   'in',\n",
       "   'medical',\n",
       "   'image',\n",
       "   'manipulation',\n",
       "   'software',\n",
       "   'give',\n",
       "   'the',\n",
       "   'process',\n",
       "   'of',\n",
       "   'adapt',\n",
       "   'to',\n",
       "   'new',\n",
       "   'software',\n",
       "   'and',\n",
       "   'complex',\n",
       "   'task',\n",
       "   'can',\n",
       "   'result',\n",
       "   'in',\n",
       "   'inaccurate',\n",
       "   'diagnosis',\n",
       "   'with',\n",
       "   'clinical',\n",
       "   'impact',\n",
       "   'this',\n",
       "   'be',\n",
       "   'a',\n",
       "   'qualitative',\n",
       "   'study',\n",
       "   'which',\n",
       "   'be',\n",
       "   'base',\n",
       "   'on',\n",
       "   'the',\n",
       "   'work',\n",
       "   'routine',\n",
       "   'of',\n",
       "   'radiology',\n",
       "   'professional',\n",
       "   'focus',\n",
       "   'on',\n",
       "   'issue',\n",
       "   'of',\n",
       "   'cognitive',\n",
       "   'learning',\n",
       "   'interaction',\n",
       "   'and',\n",
       "   'usability',\n",
       "   'with',\n",
       "   'radiology',\n",
       "   'software',\n",
       "   'moderate',\n",
       "   'usability',\n",
       "   'test',\n",
       "   'with',\n",
       "   'radiology',\n",
       "   'technician',\n",
       "   'be',\n",
       "   'conduct',\n",
       "   'to',\n",
       "   'identify',\n",
       "   'the',\n",
       "   'difficulty',\n",
       "   'and',\n",
       "   'challenge',\n",
       "   'they',\n",
       "   'encounter',\n",
       "   'while',\n",
       "   'use',\n",
       "   'medical',\n",
       "   'image',\n",
       "   'manipulation',\n",
       "   'software',\n",
       "   'the',\n",
       "   'analysis',\n",
       "   'identify',\n",
       "   '64',\n",
       "   'problem',\n",
       "   'group',\n",
       "   'into',\n",
       "   '20',\n",
       "   'category',\n",
       "   'and',\n",
       "   'organize',\n",
       "   'under',\n",
       "   'Visual',\n",
       "   'Presentation',\n",
       "   'Content',\n",
       "   'Information',\n",
       "   'Architecture',\n",
       "   'and',\n",
       "   'Interactivity',\n",
       "   'the',\n",
       "   'paper',\n",
       "   'emphasize',\n",
       "   'violate',\n",
       "   'heuristic',\n",
       "   'and',\n",
       "   'describe',\n",
       "   'how',\n",
       "   'these',\n",
       "   'problem',\n",
       "   'category',\n",
       "   'impact',\n",
       "   'user',\n",
       "   'in',\n",
       "   'their',\n",
       "   'medical',\n",
       "   'activity',\n",
       "   'and',\n",
       "   'their',\n",
       "   'influence',\n",
       "   'on',\n",
       "   'the',\n",
       "   'clinical',\n",
       "   'process',\n",
       "   'the',\n",
       "   'obtain',\n",
       "   'result',\n",
       "   'provide',\n",
       "   'insight',\n",
       "   'to',\n",
       "   'enhance',\n",
       "   'usability',\n",
       "   'practice',\n",
       "   'and',\n",
       "   'recommendation',\n",
       "   'aim',\n",
       "   'to',\n",
       "   'support',\n",
       "   'the',\n",
       "   'development',\n",
       "   'system',\n",
       "   'use',\n",
       "   'in',\n",
       "   'radiology',\n",
       "   'practice',\n",
       "   'keyword',\n",
       "   'usability',\n",
       "   'issue',\n",
       "   'radiology',\n",
       "   'system',\n",
       "   'qualitative',\n",
       "   'analysis',\n",
       "   '1',\n",
       "   'introduction',\n",
       "   'Health',\n",
       "   'information',\n",
       "   'system',\n",
       "   'be',\n",
       "   'extremely',\n",
       "   'important',\n",
       "   'in',\n",
       "   'medical',\n",
       "   'practice',\n",
       "   'and',\n",
       "   'clinical',\n",
       "   'intervention',\n",
       "   'and',\n",
       "   'be',\n",
       "   'essential',\n",
       "   'for',\n",
       "   'the',\n",
       "   'activity',\n",
       "   'carry',\n",
       "   'out',\n",
       "   'by',\n",
       "   'health',\n",
       "   'professional',\n",
       "   'in',\n",
       "   'hospital',\n",
       "   'and',\n",
       "   'clinic',\n",
       "   '5',\n",
       "   '8',\n",
       "   '14',\n",
       "   '34',\n",
       "   '35',\n",
       "   'there',\n",
       "   'be',\n",
       "   'work',\n",
       "   'in',\n",
       "   'the',\n",
       "   'literature',\n",
       "   'that',\n",
       "   'apply',\n",
       "   'computing',\n",
       "   'to',\n",
       "   'medical',\n",
       "   'image',\n",
       "   'to',\n",
       "   'assist',\n",
       "   'healthcare',\n",
       "   'professional',\n",
       "   'the',\n",
       "   'work',\n",
       "   'of',\n",
       "   'Vieira',\n",
       "   'et',\n",
       "   'al',\n",
       "   '37',\n",
       "   'use',\n",
       "   'Applied',\n",
       "   'Explainable',\n",
       "   'Artificial',\n",
       "   'Intelligence',\n",
       "   'XAI',\n",
       "   'in',\n",
       "   'the',\n",
       "   'classification',\n",
       "   'of',\n",
       "   'retinal',\n",
       "   'image',\n",
       "   'to',\n",
       "   'support',\n",
       "   'Glaucoma',\n",
       "   'diagnosis',\n",
       "   'martin',\n",
       "   'et',\n",
       "   'al',\n",
       "   '27',\n",
       "   'present',\n",
       "   'a',\n",
       "   'web',\n",
       "   'base',\n",
       "   'platform',\n",
       "   'that',\n",
       "   'provide',\n",
       "   'a',\n",
       "   'intuitive',\n",
       "   'interface',\n",
       "   'suitable',\n",
       "   'for',\n",
       "   'machine',\n",
       "   'learn',\n",
       "   'computational',\n",
       "   'pathology',\n",
       "   'research',\n",
       "   'to',\n",
       "   'be',\n",
       "   'easily',\n",
       "   'carry',\n",
       "   'out',\n",
       "   'in',\n",
       "   'this',\n",
       "   'study',\n",
       "   'the',\n",
       "   'focus',\n",
       "   'be',\n",
       "   'on',\n",
       "   'radiology',\n",
       "   'system',\n",
       "   'which',\n",
       "   'be',\n",
       "   'essential',\n",
       "   'for',\n",
       "   'carry',\n",
       "   'out',\n",
       "   'clinical',\n",
       "   'analysis',\n",
       "   'the',\n",
       "   'task',\n",
       "   'involve',\n",
       "   'these',\n",
       "   'analysis',\n",
       "   'require',\n",
       "   'attention',\n",
       "   'to',\n",
       "   'detail',\n",
       "   'through',\n",
       "   'image',\n",
       "   'manipulation',\n",
       "   'in',\n",
       "   'proceeding',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'web',\n",
       "   'WebMe',\n",
       "   'dia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   '2024',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'sbc',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '2753',\n",
       "   'technique',\n",
       "   'to',\n",
       "   'make',\n",
       "   'a',\n",
       "   'medical',\n",
       "   'diagnosis',\n",
       "   'be',\n",
       "   'system',\n",
       "   'that',\n",
       "   'have',\n",
       "   'lead',\n",
       "   'to',\n",
       "   'optimize',\n",
       "   'the',\n",
       "   'quality',\n",
       "   'and',\n",
       "   'productivity',\n",
       "   'of',\n",
       "   'healthcare',\n",
       "   'professional',\n",
       "   '1',\n",
       "   'figure',\n",
       "   '1',\n",
       "   'show',\n",
       "   'a',\n",
       "   'simplified',\n",
       "   'workflow',\n",
       "   'of',\n",
       "   'radiology',\n",
       "   'professional',\n",
       "   'and',\n",
       "   'communication',\n",
       "   'between',\n",
       "   'system',\n",
       "   'these',\n",
       "   'professional',\n",
       "   'operate',\n",
       "   'radiology',\n",
       "   'equipment',\n",
       "   'to',\n",
       "   'generate',\n",
       "   'medical',\n",
       "   'image',\n",
       "   'which',\n",
       "   'be',\n",
       "   'manipulate',\n",
       "   'in',\n",
       "   'specific',\n",
       "   'software',\n",
       "   'and',\n",
       "   'archive',\n",
       "   'in',\n",
       "   'the',\n",
       "   'Image',\n",
       "   'Archiving',\n",
       "   'and',\n",
       "   'Communication',\n",
       "   'System',\n",
       "   'PACS',\n",
       "   'the',\n",
       "   'image',\n",
       "   'can',\n",
       "   'be',\n",
       "   'view',\n",
       "   'on',\n",
       "   'workstation',\n",
       "   'be',\n",
       "   'access',\n",
       "   'through',\n",
       "   'the',\n",
       "   'Radiology',\n",
       "   'Information',\n",
       "   'Systems',\n",
       "   'RIS',\n",
       "   'and',\n",
       "   'Hospital',\n",
       "   'Information',\n",
       "   'System',\n",
       "   'HIS',\n",
       "   'the',\n",
       "   'area',\n",
       "   'paint',\n",
       "   'in',\n",
       "   'yellow',\n",
       "   'between',\n",
       "   'the',\n",
       "   'arrow',\n",
       "   'indicate',\n",
       "   'with',\n",
       "   'number',\n",
       "   '1',\n",
       "   'and',\n",
       "   '2',\n",
       "   'in',\n",
       "   'the',\n",
       "   'figure',\n",
       "   'indicate',\n",
       "   'that',\n",
       "   'the',\n",
       "   'focus',\n",
       "   'of',\n",
       "   'this',\n",
       "   'study',\n",
       "   'cover',\n",
       "   'the',\n",
       "   'process',\n",
       "   'that',\n",
       "   'integrate',\n",
       "   'the',\n",
       "   'task',\n",
       "   'in',\n",
       "   'medical',\n",
       "   'image',\n",
       "   'manipulation',\n",
       "   'software',\n",
       "   '8',\n",
       "   '9',\n",
       "   'figure',\n",
       "   '1',\n",
       "   'simplify',\n",
       "   'layout',\n",
       "   'of',\n",
       "   'professional',\n",
       "   'workflow',\n",
       "   'and',\n",
       "   'inter',\n",
       "   'system',\n",
       "   'communication',\n",
       "   'source',\n",
       "   'adapt',\n",
       "   'from',\n",
       "   'Silva',\n",
       "   '8',\n",
       "   'it',\n",
       "   'be',\n",
       "   'important',\n",
       "   'to',\n",
       "   'highlight',\n",
       "   'that',\n",
       "   'many',\n",
       "   'healthcare',\n",
       "   'system',\n",
       "   'be',\n",
       "   'develop',\n",
       "   'base',\n",
       "   'on',\n",
       "   'web',\n",
       "   'technology',\n",
       "   'especially',\n",
       "   'PACS',\n",
       "   'and',\n",
       "   'DICOM',\n",
       "   'Digital',\n",
       "   'Imaging',\n",
       "   'and',\n",
       "   'Communications',\n",
       "   'in',\n",
       "   'Medicine',\n",
       "   'imaging',\n",
       "   'system',\n",
       "   '3',\n",
       "   '4',\n",
       "   'medical',\n",
       "   'system',\n",
       "   'use',\n",
       "   'in',\n",
       "   'radiology',\n",
       "   'be',\n",
       "   'multimedia',\n",
       "   'system',\n",
       "   'with',\n",
       "   'the',\n",
       "   'ability',\n",
       "   'to',\n",
       "   'integrate',\n",
       "   'and',\n",
       "   'present',\n",
       "   'medical',\n",
       "   'image',\n",
       "   'alongside',\n",
       "   'text',\n",
       "   'such',\n",
       "   'as',\n",
       "   'medical',\n",
       "   'report',\n",
       "   'the',\n",
       "   'interactivity',\n",
       "   'in',\n",
       "   'view',\n",
       "   'image',\n",
       "   'with',\n",
       "   'zoom',\n",
       "   'rotation',\n",
       "   'and',\n",
       "   'annotation',\n",
       "   'functionality',\n",
       "   'in',\n",
       "   'addition',\n",
       "   'to',\n",
       "   'the',\n",
       "   'ability',\n",
       "   'to',\n",
       "   'present',\n",
       "   'several',\n",
       "   'photo',\n",
       "   'in',\n",
       "   'an',\n",
       "   'integrated',\n",
       "   'and',\n",
       "   'simultaneous',\n",
       "   'way',\n",
       "   'and',\n",
       "   'the',\n",
       "   'transmission',\n",
       "   'of',\n",
       "   'medical',\n",
       "   'datum',\n",
       "   'for',\n",
       "   'remote',\n",
       "   'and',\n",
       "   'collaborative',\n",
       "   'access',\n",
       "   'demonstrate',\n",
       "   'the',\n",
       "   'multimedia',\n",
       "   'essence',\n",
       "   'of',\n",
       "   'the',\n",
       "   'system',\n",
       "   '28',\n",
       "   'WebMedia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Silva',\n",
       "   'et',\n",
       "   'al',\n",
       "   'previous',\n",
       "   'study',\n",
       "   'have',\n",
       "   'carry',\n",
       "   'out',\n",
       "   'several',\n",
       "   'evaluation',\n",
       "   'on',\n",
       "   'PACS',\n",
       "   'and',\n",
       "   'RIS',\n",
       "   'system',\n",
       "   'and',\n",
       "   'find',\n",
       "   'several',\n",
       "   'usability',\n",
       "   'problem',\n",
       "   'that',\n",
       "   'impact',\n",
       "   'the',\n",
       "   'performance',\n",
       "   'of',\n",
       "   'complex',\n",
       "   'image',\n",
       "   'capture',\n",
       "   'and',\n",
       "   'analysis',\n",
       "   'task',\n",
       "   '9',\n",
       "   '11',\n",
       "   '15',\n",
       "   '16',\n",
       "   '38',\n",
       "   'however',\n",
       "   'there',\n",
       "   'be',\n",
       "   'limited',\n",
       "   'knowledge',\n",
       "   'about',\n",
       "   'image',\n",
       "   'manipulation',\n",
       "   'system',\n",
       "   'whose',\n",
       "   'aspect',\n",
       "   'influence',\n",
       "   'the',\n",
       "   'procedural',\n",
       "   'aspect',\n",
       "   'and',\n",
       "   'result',\n",
       "   'of',\n",
       "   'clinical',\n",
       "   'examination',\n",
       "   'in',\n",
       "   'hospital',\n",
       "   'and',\n",
       "   'clinic',\n",
       "   'mainly',\n",
       "   'in',\n",
       "   'the',\n",
       "   'brazilian',\n",
       "   'scenario',\n",
       "   'another',\n",
       "   'important',\n",
       "   'aspect',\n",
       "   'that',\n",
       "   'be',\n",
       "   'also',\n",
       "   'reveal',\n",
       "   'in',\n",
       "   'a',\n",
       "   'study',\n",
       "   'with',\n",
       "   'radiology',\n",
       "   'professional',\n",
       "   '8',\n",
       "   'point',\n",
       "   'out',\n",
       "   'that',\n",
       "   'the',\n",
       "   'work',\n",
       "   'routine',\n",
       "   'the',\n",
       "   'characteristic',\n",
       "   'of',\n",
       "   'the',\n",
       "   'system',\n",
       "   'and',\n",
       "   'the',\n",
       "   'interaction',\n",
       "   'of',\n",
       "   'radiology',\n",
       "   'professional',\n",
       "   'with',\n",
       "   'these',\n",
       "   'system',\n",
       "   'be',\n",
       "   'condition',\n",
       "   'by',\n",
       "   'the',\n",
       "   'hospital',\n",
       "   'or',\n",
       "   'clinic',\n",
       "   'process',\n",
       "   'where',\n",
       "   'the',\n",
       "   'system',\n",
       "   'be',\n",
       "   'process',\n",
       "   'centric',\n",
       "   'and',\n",
       "   'not',\n",
       "   'user',\n",
       "   'centric',\n",
       "   'this',\n",
       "   'paper',\n",
       "   'aim',\n",
       "   'to',\n",
       "   'study',\n",
       "   'and',\n",
       "   'qualitatively',\n",
       "   'analyze',\n",
       "   'the',\n",
       "   'postprocessing',\n",
       "   'task',\n",
       "   'of',\n",
       "   'medical',\n",
       "   'image',\n",
       "   'perform',\n",
       "   'by',\n",
       "   'radiology',\n",
       "   'professional',\n",
       "   'observe',\n",
       "   'the',\n",
       "   'aspect',\n",
       "   'of',\n",
       "   'cognitive',\n",
       "   'learning',\n",
       "   'the',\n",
       "   'software',\n",
       "   'usability',\n",
       "   'problem',\n",
       "   'encounter',\n",
       "   'in',\n",
       "   'these',\n",
       "   'task',\n",
       "   'how',\n",
       "   'these',\n",
       "   'problem',\n",
       "   'impact',\n",
       "   'the',\n",
       "   'process',\n",
       "   'of',\n",
       "   'hospital',\n",
       "   'and',\n",
       "   'clinic',\n",
       "   'and',\n",
       "   'how',\n",
       "   'radiology',\n",
       "   'professional',\n",
       "   'accept',\n",
       "   'these',\n",
       "   'system',\n",
       "   'the',\n",
       "   'study',\n",
       "   'involve',\n",
       "   'usability',\n",
       "   'testing',\n",
       "   'give',\n",
       "   'the',\n",
       "   'CAAE',\n",
       "   'research',\n",
       "   'protocol',\n",
       "   'nº',\n",
       "   '49170921.6.0000.5148',\n",
       "   'to',\n",
       "   'understand',\n",
       "   'in',\n",
       "   'practice',\n",
       "   'the',\n",
       "   'task',\n",
       "   'of',\n",
       "   'radiology',\n",
       "   'professional',\n",
       "   'and',\n",
       "   'identify',\n",
       "   'the',\n",
       "   'difficulty',\n",
       "   'and',\n",
       "   'usability',\n",
       "   'problem',\n",
       "   'that',\n",
       "   'these',\n",
       "   'professional',\n",
       "   'present',\n",
       "   'when',\n",
       "   'use',\n",
       "   'a',\n",
       "   'medical',\n",
       "   'image',\n",
       "   'manipulation',\n",
       "   'system',\n",
       "   'model',\n",
       "   'this',\n",
       "   'study',\n",
       "   'seek',\n",
       "   'to',\n",
       "   'answer',\n",
       "   'which',\n",
       "   'usability',\n",
       "   'factor',\n",
       "   'influence',\n",
       "   'the',\n",
       "   'use',\n",
       "   'of',\n",
       "   'radiology',\n",
       "   'system',\n",
       "   'in',\n",
       "   'the',\n",
       "   'brazilian',\n",
       "   'context',\n",
       "   'and',\n",
       "   'what',\n",
       "   'the',\n",
       "   'impact',\n",
       "   'of',\n",
       "   'these',\n",
       "   'problem',\n",
       "   'would',\n",
       "   'be',\n",
       "   'on',\n",
       "   'the',\n",
       "   'radiology',\n",
       "   'service',\n",
       "   'provide',\n",
       "   'the',\n",
       "   'paper',\n",
       "   'be',\n",
       "   'organize',\n",
       "   'as',\n",
       "   'follow',\n",
       "   'section',\n",
       "   '2',\n",
       "   'present',\n",
       "   'the',\n",
       "   'main',\n",
       "   'concept',\n",
       "   'of',\n",
       "   'radiology',\n",
       "   'information',\n",
       "   'system',\n",
       "   'DICOM',\n",
       "   'standard',\n",
       "   'and',\n",
       "   'relate',\n",
       "   'work',\n",
       "   'section',\n",
       "   '3',\n",
       "   'present',\n",
       "   'the',\n",
       "   'main',\n",
       "   'methodological',\n",
       "   'aspect',\n",
       "   'section',\n",
       "   '4',\n",
       "   'present',\n",
       "   'the',\n",
       "   'result',\n",
       "   'section',\n",
       "   '5',\n",
       "   'the',\n",
       "   'discussion',\n",
       "   'and',\n",
       "   'finally',\n",
       "   'section',\n",
       "   '6',\n",
       "   'the',\n",
       "   'conclusion',\n",
       "   'and',\n",
       "   'future',\n",
       "   'work',\n",
       "   '2',\n",
       "   'BACKGROUND',\n",
       "   'this',\n",
       "   'section',\n",
       "   'describe',\n",
       "   'concept',\n",
       "   'about',\n",
       "   'information',\n",
       "   'system',\n",
       "   'in',\n",
       "   'radiology',\n",
       "   'and',\n",
       "   'related',\n",
       "   'work',\n",
       "   '2.1',\n",
       "   'Radiology',\n",
       "   'Information',\n",
       "   'Systems',\n",
       "   'and',\n",
       "   'DICOM',\n",
       "   'standard',\n",
       "   'hospital',\n",
       "   'radiology',\n",
       "   'department',\n",
       "   'have',\n",
       "   'be',\n",
       "   'use',\n",
       "   'digital',\n",
       "   'system',\n",
       "   'on',\n",
       "   'a',\n",
       "   'large',\n",
       "   'scale',\n",
       "   'which',\n",
       "   'have',\n",
       "   'generate',\n",
       "   'an',\n",
       "   'increase',\n",
       "   'volume',\n",
       "   'of',\n",
       "   'datum',\n",
       "   'this',\n",
       "   'reinforce',\n",
       "   'that',\n",
       "   'the',\n",
       "   'solution',\n",
       "   'to',\n",
       "   'manage',\n",
       "   'these',\n",
       "   'datum',\n",
       "   'and',\n",
       "   'digital',\n",
       "   'image',\n",
       "   'be',\n",
       "   'adopt',\n",
       "   'a',\n",
       "   'PACS',\n",
       "   'and',\n",
       "   'RIS',\n",
       "   ...]},\n",
       " {'titulo': 'An Ensemble Approach to Facial Deepfake Detection Using Self-Supervised Features',\n",
       "  'informacoes_url': '',\n",
       "  'idioma': 'english',\n",
       "  'storage_key': '../articles/original/english/985-24741-1-10-20240923.pdf',\n",
       "  'author': 'Yan Martins B. Gurevitz Cunha; José Matheus C. Boaro; Daniel de Sousa Moraes; Pedro Cutrim dos Santos; Polyana Bezerra da Costa; Antonio José Grandson Busson; Julio Cesar Duarte; and Sérgio Colcher',\n",
       "  'data_publicacao': '11-09-2024',\n",
       "  'resumo': 'Substantial efforts have been dedicated to developing methods for detecting deepfake content, especially with the creation of large and diverse datasets with both higher image quality and demographic features. In this scenario, CNN-based approaches showed good initial success, later improved by their combination with Vision Transformers. More recently, Foundation Models (FMs) have emerged, improving performance across many visual tasks, including deepfake detection, and combining self-supervised features generated by FMs with CNN-based classifiers has resulted in significant performance gains. However, taking advantage of multiple maps of self-supervised features is not as straightforward as just adding more channels to the classifier. Therefore, this work explores ensemble techniques to effectively utilize these diverse selfsupervised feature maps for realistic facial deepfake detection. Our experiments indicate that combining the output results of different classifiers, each one utilizing a single map of self-supervised features, leads to significant performance improvements, and several committee approaches consistently outperform individual classifiers, demonstrating the potential of these methods in enhancing deepfake detection accuracy. ###',\n",
       "  'keywords': 'deep fake detection, self-supervised, vision transformers, deep learning, foundation models',\n",
       "  'referencias': ['[1] Redha Ali, Russell C. Hardie, Barath Narayanan Narayanan, and Supun De Silva.\\n2019. Deep Learning Ensemble Methods for Skin Lesion Analysis towards\\nMelanoma Detection. In *2019 IEEE National Aerospace and Electronics Confer-*\\n*ence (NAECON)* . IEEE, Dayton, OH, USA, 311–316. https://doi.org/10.1109/\\nNAECON46414.2019.9058245',\n",
       "   '[2] Roberto Amoroso, Davide Morelli, Marcella Cornia, Lorenzo Baraldi, Alberto Del\\nBimbo, and Rita Cucchiara. 2024. Parents and Children: Distinguishing Multimodal DeepFakes from Natural Images. arXiv:2304.00500 [cs.CV] https:\\n//arxiv.org/abs/2304.00500',\n",
       "   '[3] Ben Beaumont-Thomas. 2024. Taylor Swift deepfake pornography sparks renewed calls for US legislation. https://www.theguardian.com/music/2024/jan/26/\\ntaylor-swift-deepfake-pornography-sparks-renewed-calls-for-us-legislation.',\n",
       "   '[4] Nicolò Bonettini, Edoardo Daniele Cannas, Sara Mandelli, Luca Bondi, Paolo\\nBestagini, and Stefano Tubaro. 2021. Video Face Manipulation Detection Through\\nEnsemble of CNNs. In *2020 25th International Conference on Pattern Recognition*\\n*(ICPR)* . IEEE, Milan, Italy, 5012–5019. https://doi.org/10.1109/ICPR48806.2021.\\n9412711',\n",
       "   '[5] Preeti Chaudhary, Aditya Verma, Vinay Kukreja, and Rishabh Sharma. 2024. Integrating Deep Learning and Ensemble Methods for Robust Tomato Disease Detection: A Hybrid CNN-RF Model Analysis. In *2024 11th International Conference on*\\n*Reliability, Infocom Technologies and Optimization (Trends and Future Directions)*\\n*(ICRITO)* . IEEE, Noida, India, 1–4. https://doi.org/10.1109/ICRITO61523.2024.\\n10522213',\n",
       "   '[6] François Chollet. 2017. Xception: Deep learning with depthwise separable convolutions. In *Proceedings of the IEEE conference on computer vision and pattern*\\n*recognition* . IEEE, Honolulu, HI, USA, 1251–1258. https://doi.org/10.1109/CVPR.\\n2017.195',\n",
       "   '[7] Thipwimon Chompookham and OJIEL Surinta. 2021. Ensemble methods with\\ndeep convolutional neural networks for plant leaf recognition. *ICIC Express*\\n*Letters* 15, 6 (2021), 553–565.',\n",
       "   '[8] Davide Alessandro Coccomini, Nicola Messina, Claudio Gennaro, and Fabrizio\\nFalchi. 2022. Combining EfficientNet and Vision Transformers for Video Deepfake\\nDetection. In *Image Analysis and Processing – ICIAP 2022*, Stan Sclaroff, Cosimo\\nDistante, Marco Leo, Giovanni M. Farinella, and Federico Tombari (Eds.). Springer\\nInternational Publishing, Cham, 219–229. https://doi.org/10.1007/978-3-03106433-3_19',\n",
       "   '[9] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua\\nBengio. 2016. Binarized Neural Networks: Training Deep Neural Networks with\\nWeights and Activations Constrained to +1 or -1. arXiv:1602.02830 [cs.LG]\\nhttps://arxiv.org/abs/1602.02830',\n",
       "   '[10] Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin\\nWang, and Cristian Canton Ferrer. 2020. The DeepFake Detection Challenge\\nDataset. arXiv:2006.07397 [cs.CV]',\n",
       "   '[11] Nikolaos Giatsoglou, Symeon Papadopoulos, and Ioannis Kompatsiaris. 2023. Investigation of ensemble methods for the detection of deepfake face manipulations.\\narXiv:2304.07395 [cs.CV] https://arxiv.org/abs/2304.07395',\n",
       "   '[12] Bruno Rocha Gomes, Antonio J. G. Busson, José Boaro, and Sérgio Colcher.\\n2023. Realistic Facial Deep Fakes Detection Through Self-Supervised Features\\nGenerated by a Self-Distilled Vision Transformer. In *Proceedings of the 29th*\\n*Brazilian Symposium on Multimedia and the Web (WebMedia ’23)* . Association for\\nComputing Machinery, New York, NY, USA, 177–183. https://doi.org/10.1145/\\n3617023.3617047',\n",
       "   '[13] Young-Jin Heo, Young-Ju Choi, Young-Woon Lee, and Byung-Gyu Kim. 2021.\\nDeepfake detection scheme based on vision transformer and distillation. *arXiv*\\n*preprint arXiv:2104.01353* abs/2104.01353 (2021), 7 pages. https://doi.org/10.48550/\\narXiv.2104.01353',\n",
       "   '[14] Brittaney Kiefer. 2023. This Brand’s Social Experiment Uses AI to Expose the Dark\\nSide of ’Sharenting’. https://www.adweek.com/brand-marketing/this-brandssocial-experiment-uses-ai-to-expose-the-dark-side-of-sharenting/.',\n",
       "   '[15] Romeo Lanzino, Federico Fontana, Anxhelo Diko, Marco Raoul Marini, and Luigi\\nCinque. 2024. Faster Than Lies: Real-time Deepfake Detection using Binary\\nNeural Networks. In *Proceedings of the IEEE/CVF Conference on Computer Vision*\\n*and Pattern Recognition (CVPR) Workshops* . IEEE, Seattle, WA, USA, 3771–3780.',\n",
       "   '[16] Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, and Siwei Lyu. 2020. Celeb-df:\\nA large-scale challenging dataset for deepfake forensics. In *Proceedings of the*\\n*IEEE/CVF Conference on Computer Vision and Pattern Recognition* . IEEE, Seattle,\\nWA, USA, 3207–3216.',\n",
       "   '[17] Sachin Mehta, Ezgi Mercan, Jamen Bartlett, Donald Weaver, Joann G. Elmore, and\\nLinda Shapiro. 2018. Y-Net: Joint Segmentation and Classification for Diagnosis\\nof Breast Biopsy Images. In *Medical Image Computing and Computer Assisted*\\n*Intervention – MICCAI 2018: 21st International Conference, Granada, Spain, Sep-*\\n*tember 16-20, 2018, Proceedings, Part II* (Granada, Spain). Springer-Verlag, Berlin,\\nHeidelberg, 893–901.',\n",
       "   '[18] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec,\\nVasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin ElNouby, et al . 2024. Dinov2: Learning robust visual features without supervision.\\n*Transactions on Machine Learning Research Journal* 1 (2024), 1–31. https://doi.\\norg/10.48550/arxiv.2304.07193',\n",
       "   '[19] Artem A Pokroy and Alexey D Egorov. 2021. EfficientNets for deepfake detection:\\nComparison of pretrained models. In *2021 IEEE Conference of Russian Young Re-*\\n*searchers in Electrical and Electronic Engineering (ElConRus)* . IEEE, St. Petersburg,\\nMoscow, Russia, 598–600. https://doi.org/10.1109/ElConRus51938.2021.9396092',\n",
       "   '[20] Tal Reiss, Bar Cavia, and Yedid Hoshen. 2023. Detecting Deepfakes Without\\nSeeing Any. *ArXiv* abs/2311.01458 (2023), 16 pages. https://api.semanticscholar.\\norg/CorpusID:264935112\\n\\n\\n43\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Cunha et al.',\n",
       "   '[21] Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies,\\nand Matthias Nießner. 2019. Faceforensics++: Learning to detect manipulated\\nfacial images. In *Proceedings of the IEEE/CVF International Conference on Computer*\\n*Vision* . IEEE, Seoul, Korea (South), 1–11. https://doi.org/10.1109/ICCV.2019.00009',\n",
       "   '[22] Rhianna Schmunk. 2024. Explicit fake images of Taylor Swift prove laws haven’t\\nkept pace with tech, experts say. https://www.cbc.ca/news/canada/taylor-swiftai-images-highlight-need-for-better-legislation-1.7096094.',\n",
       "   '[23] Laura Stroebel, Mark Llewellyn, Tricia Hartley, Tsui Shan Ip, and Mohiuddin Ahmed. 2023. A systematic literature review on the effectiveness of deepfake detection techniques. *Journal of Cyber Security Tech-*\\n*nology* 7, 2 (2023), 83–113. https://doi.org/10.1080/23742917.2023.2192888\\narXiv:https://doi.org/10.1080/23742917.2023.2192888',\n",
       "   '[24] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi.\\n2017. Inception-v4, inception-resnet and the impact of residual connections on\\nlearning. In *Thirty-first AAAI conference on artificial intelligence* . AAAI Press, San\\nFrancisco, California, USA, 4278–4284. https://doi.org/10.48550/arXiv.1602.07261',\n",
       "   '[25] Mingxing Tan and Quoc Le. 2019. EfficientNet: Rethinking Model Scaling for\\nConvolutional Neural Networks. In *Proceedings of the 36th International Confer-*\\n*ence on Machine Learning (Proceedings of Machine Learning Research, Vol. 97)*,\\nKamalika Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR, Long Beach, CA,\\nUSA, 6105–6114. https://proceedings.mlr.press/v97/tan19a.html',\n",
       "   '[26] Eric Tjon, Melody Moh, and Teng-Sheng Moh. 2021. Eff-YNet: A Dual Task\\nNetwork for DeepFake Detection and Segmentation. In *2021 15th International*\\n*Conference on Ubiquitous Information Management and Communication (IMCOM)* .\\nIEEE, Seoul, Korea (South), 1–8. https://doi.org/10.1109/IMCOM51814.2021.\\n9377373',\n",
       "   '[27] Loc Trinh and Yan Liu. 2021. An Examination of Fairness of AI Models for\\nDeepfake Detection. arXiv:2105.00558 [cs.CV]',\n",
       "   '[28] Junke Wang, Zuxuan Wu, Wenhao Ouyang, Xintong Han, Jingjing Chen, YuGang Jiang, and Ser-Nam Li. 2022. M2TR: Multi-modal Multi-scale Transformers\\nfor Deepfake Detection. In *Proceedings of the 2022 International Conference on Mul-*\\n*timedia Retrieval* (Newark, NJ, USA) *(ICMR ’22)* . Association for Computing Machinery, New York, NY, USA, 615–623. https://doi.org/10.1145/3512527.3531415',\n",
       "   '[29] Ying Xu, Philipp Terhörst, Kiran Raja, and Marius Pedersen. 2023. A Comprehensive Analysis of AI Biases in DeepFake Detection With Massively Annotated\\nDatabases. arXiv:2208.05845 [cs.CV]',\n",
       "   '[30] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao. 2016. Joint face\\ndetection and alignment using multitask cascaded convolutional networks. *IEEE*\\n*signal processing letters* 23, 10 (2016), 1499–1503. https://doi.org/10.1109/LSP.\\n2016.2603342',\n",
       "   '[31] Hanqing Zhao, Wenbo Zhou, Dongdong Chen, Weiming Zhang, and\\nNenghai Yu. 2022. Self-supervised Transformer for Deepfake Detection.\\narXiv:2203.01265 [cs.CV] https://arxiv.org/abs/2203.01265\\n\\n\\n44\\n\\n\\n-----'],\n",
       "  'text': '# **An Ensemble Approach to Facial Deepfake Detection Using** **Self-Supervised Features**\\n\\n## Yan Martins B. Gurevitz Cunha\\n#### yangurevitz@telemidia.puc-rio.br Telemidia Lab. – Pontifical Catholic University of Rio de Janeiro\\n## Pedro Cutrim dos Santos\\n#### thiagocutrim98@gmail.com Telemidia Lab. – Pontifical Catholic University of Rio de Janeiro\\n\\n## José Matheus C. Boaro\\n#### boaro@telemidia.puc-rio.br Telemidia Lab. – Pontifical Catholic University of Rio de Janeiro\\n## Polyana Bezerra da Costa\\n#### polyanabcosta@gmail.com Telemidia Lab. – Pontifical Catholic University of Rio de Janeiro\\n\\n## Daniel de Sousa Moraes\\n#### danielmoraes@telemidia.puc-rio.br Telemidia Lab. – Pontifical Catholic University of Rio de Janeiro\\n## Antonio José Grandson Busson\\n#### antonio.busson@btgpactual.com BTG Pactual\\n\\n## Julio Cesar Duarte\\n#### duarte@ime.eb.br Computing Engineering Dept. – Military Institute of Engineering\\n### **ABSTRACT**\\n\\nSubstantial efforts have been dedicated to developing methods for\\ndetecting deepfake content, especially with the creation of large\\nand diverse datasets with both higher image quality and demographic features. In this scenario, CNN-based approaches showed\\ngood initial success, later improved by their combination with Vision Transformers. More recently, Foundation Models (FMs) have\\nemerged, improving performance across many visual tasks, including deepfake detection, and combining self-supervised features\\ngenerated by FMs with CNN-based classifiers has resulted in significant performance gains. However, taking advantage of multiple\\nmaps of self-supervised features is not as straightforward as just\\nadding more channels to the classifier. Therefore, this work explores ensemble techniques to effectively utilize these diverse selfsupervised feature maps for realistic facial deepfake detection. Our\\nexperiments indicate that combining the output results of different\\nclassifiers, each one utilizing a single map of self-supervised features, leads to significant performance improvements, and several\\ncommittee approaches consistently outperform individual classifiers, demonstrating the potential of these methods in enhancing\\ndeepfake detection accuracy.\\n### **KEYWORDS**\\n\\ndeep fake detection, self-supervised, vision transformers, deep\\nlearning, foundation models\\n### **1 INTRODUCTION**\\n\\nIn recent years, there has been an increased focus on the effects\\nof deepfake multimedia content on public discourse and personal\\nlives. This has manifested in forms such as explicit fake images\\nof celebrities [ 22 ] or politically sensitive material, facilitating the\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n© 2024 SBC – Brazilian Computing Society.\\nISSN 2966-2753\\n\\n## Sérgio Colcher\\n#### colcher@inf.puc-rio.br Telemidia Lab. – Pontifical Catholic University of Rio de Janeiro\\n\\nspread of misinformation and identity theft and possibly leading to\\nthreats of violence [ 14 ]. This scenario has renewed societal discussions on the nature of online content and led to legislative debates\\nin countries such as the United States on reducing the creation and\\ndistribution of this kind of media [3].\\nConcurrent with the advancement of deepfake generators, significant progress has also been made in efforts to improve the detection\\nand containment of this kind of content. This starts with the cre\\nation of large-scale datasets for deepfake detection in video and\\nimage formats [ 10, 21 ], enabling the development of methods to\\nbetter differentiate between real and fake content. In this context,\\nCNN-based models showed initial success, and their combination\\nwith Vision Transformers (ViT) managed to achieve state-of-art\\nperformance in recent years [13].\\nAlso, the recent emergence of Foundation Models (FMs) opened\\nthe door for new approaches to deepfake detection. In this light,\\nstudies have demonstrated that using self-supervised features generated by pre-trained FMs in combination with CNN-based models\\ncan result in significant performance improvements [ 12 ] . In this\\napproach, the authors incorporated feature maps generated by FMs\\nas extra channels in CNN input, showing that, when used individually, each map would improve the performance of its base model.\\nHowever, it has also been shown by Gomes et al . [ 12 ] that simultaneously applying multiple attention maps to the same model by\\nsimply adding more channels to the input does not necessarily\\nimprove performance, leaving open the following question: *Is there*\\n*a way to better combine multiple attention maps to achieve superior*\\n*deepfake detection performance?*\\nTo pursue this question, we conducted experiments with DINOv2 FM [ 18 ] and the CNN-based XceptionNet [ 6 ], chosen for\\ntheir established use and strong performance in this task. Initially,\\nwe evaluated each self-supervised feature map generated by DINOv2 individually, resulting in three distinct models. Next, we tried\\nmultiple committee approaches to combine these models’ predictions. For our experiments, we used both the Deepfake Detection\\nChallenge (DFDC) [ 10 ] and Face Forensics [ 21 ] datasets. In both\\n\\n\\n37\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Cunha et al.\\n\\n\\ndatasets, our proposed committees performed better than using a\\nsingle attention map, while the performance gain varied depending\\non the specific committee approach employed.\\nThe remainder of this paper is organized as follows. Section section 2 covers related works on deepfake generation and detection,\\nas well as research on committee approaches for deep learning\\nmodels. Section 3 describes the employed architectures and details\\nthe different committee approaches proposed for this task. Section 4 discusses how we conducted our experimental methodology,\\nincluding specifics on data, setup, and results analysis. Finally, Section 5 presents our final thoughts and outlines possible directions\\nfor further research in this field.\\n### **2 RELATED WORK**\\n\\nIn this section, we go over other important research that relates\\nto the topics covered in this work. Subsection 2.1 discusses recent\\nadvances in deepfake detection, focusing on methodologies aimed\\nat countering the generation of synthetic media. Subsection 2.2\\nexplores diverse works related to the combination of ensemble approaches and CNN models for image processing, enhancing model\\nrobustness, accuracy, and generalization across diverse datasets\\nand tasks.\\n### **2.1 Methods for Facial Deepfake Detection**\\n\\nRecent years have seen a large increase in efforts dedicated to\\ndetecting realistic deepfakes and differentiating them from genuine\\nfacial media, with video and images being the dominant targets of\\nthese works [ 23 ]. This progress has been enabled by the production\\nof a multitude of recent datasets for the task, such as the Deepfake\\nDetection Challenge (DFDC) [ 10 ], Celeb-DF [ 16 ], FaceForensics,\\nand FaceForensics++ [ 21 ] datasets. These datasets contain large\\nvolumes of visual data with a variety of deepfake techniques and\\nan increasing, though still insufficient, concern for demographic\\nfairness [ 27 ], allowing their use for techniques that deal with both\\nimage and video.\\nMany recent works have been based on CNN classifiers, typically\\nemploying either EfficientNet [ 25 ] or XceptionNet [ 6 ], often combining other types of architectures. Tjon et al . [ 26 ] used EfficientNet\\nB4 as an encoder in combination with Y-Net [ 17 ], achieving very\\ngood performance on the DFDC dataset for both image and video.\\nDue to its prominence, EfficientNet has been considered one of\\nthe best classifiers for this task, with entire studies dedicated to\\nfurther analyzing its overall performance, such as the one conducted\\nby Pokroy and Egorov [ 19 ]. In their work, the authors experimented\\nwith different versions of the architecture, which vary according\\nto the dimensions of the input data and the number of trainable\\nparameters. They trained each variant, from EfficientNet B0 to B7,\\nfor twenty epochs on the DFDC dataset, concluding that larger\\nnetworks do not necessarily achieve superior performance.\\nMeanwhile, XceptionNet has gained notoriety in deepfake detection due to its state-of-the-art performance on the FaceForensics\\ndataset [ 21 ] and its high performance on the DFDC dataset when\\ncombined with self-supervised features generated by FMs [ 12 ] .\\nIn this scenario, it achieved performance superior to that of the\\nEfficientNet.\\n\\n\\nWith the recent emergence of transformers on computer vision\\ntasks, many researchers have also attempted to apply this technique\\nto deepfake detection. Heo et al . [ 13 ] combined an EfficientNet B7\\npre-trained on the DFDC dataset with a vision transformer by\\nmerging the embeddings extracted by these models and passing\\nthem on to the transformer encoder. This approach achieved stateof-the-art performance on the DFDC dataset for video format.\\nSimilarly, both Coccomini et al . [ 8 ] and Wang et al . [ 28 ] proposed\\narchitectures that combine vision transformers and CNN-based\\nclassifiers. The former established an ensemble of two branches\\nof Efficient ViT, one to deal with smaller features and the other to\\nhandle larger ones. Meanwhile, the latter proposed a multiscale\\narchitecture to identify regions synthesized by generative models.\\nBoth works achieved high performance on multiple datasets, further\\nestablishing vision transformers as a dominant approach for the\\ntask.\\n\\nAt the same time, FMs have started gaining ground recently,\\nleading to the possibility that they could potentially overtake vision transformers in deepfake detection. In this light, Zhao et al .\\n\\n[ 31 ] proposed a self-supervised approach employing Contrastive\\nLearning to detect deep fake videos through features obtained\\nfrom lip movements, with two encoders for audio and video. This\\nwork reached close to state-of-the-art performance on the FaceForensics++ dataset, showcasing the potential of self-supervised approaches. Meanwhile, considering image analysis, self-supervised\\nfeatures have demonstrated significant improvements in the performance of CNN-based classifiers [12] .\\nReiss et al . [ 20 ] have shown that combining textual information\\nand other contextual sources with audio-visual data input can result in performance improvements, especially for certain types of\\nattacks where data about the target is publicly available.\\nLastly, Lanzino et al . [ 15 ] demonstrated that using Binary Neural\\nNetworks [ 9 ] can achieve performance close to the state-of-the-art\\non the recent COCOFake dataset [ 2 ], while keeping efficient in\\nterms of computational cost.\\n### **2.2 Committee Approaches for Image** **Classification**\\n\\nAli et al . [ 1 ] proposed a simple ensemble approach combining\\nVGG19-UNet and DeeplabV3+ architectures for melanoma detection. Their experiments on the ISIC 2018 dataset, consisting of 2,594\\ndermoscopy images, demonstrated promising results with an overall accuracy of 93.6%, an average Jaccard Index of 0.815, and a dice\\ncoefficient of 0.887 on the test dataset. They highlighted the efficacy\\nof ensemble techniques over individual architectures, especially\\nin challenging cases like low contrast, ink, and dark corner artifacts, emphasizing its robustness and potential for broader imaging\\napplications.\\nIn their research on plant leaf recognition and disease detection, Chompookham and Surinta [ 7 ] addressed the complexities\\nof computer vision challenges by proposing an ensemble CNN\\napproach combining MobileNetV1, MobileNetV2, NASNetMobile,\\nDenseNet121, and Xception models to enhance recognition accuracy. Ensemble techniques such as weighted averages were applied to combine predictions from multiple CNN models, showing\\nsuperior performance over individual models across all datasets.\\n\\n\\n38\\n\\n\\n-----\\n\\nAn Ensemble Approach to Facial Deepfake Detection Using Self-Supervised Features WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\nConsequently, their approach achieved accuracies of 99.93% and\\n99.47% on the tomato and corn leaf disease datasets, respectively,\\ndemonstrating that ensemble methods enhance the performance of\\nCNN architectures.\\n\\nBonettini et al . [ 4 ] addressed the need for robust detection of\\nmanipulated faces in video sequences by exploring ensemble approaches using CNNs. Their study employed EfficientNetB4 as the\\nbase model, enhancing it with attention layers and siamese training\\ntechniques to improve detection accuracy on video datasets. The\\nevaluation of the FaceForensics++ and DFDC datasets, yielding\\nAUC values of 0.9444 and 0.8800, respectively, indicated superior\\nperformance when combining different CNN models compared to\\nbaseline methods. This highlights the efficacy of attention-based\\nmodifications in enhancing detection accuracy while providing\\ninsights into useful discriminative features for effective detection.\\nAdditionally, it addresses practical constraints by ensuring computational efficiency while achieving processing speeds suitable for\\nreal-world applications in limited hardware scenarios.\\nAlso in the realm of deepfake detection, Giatsoglou et al . [ 11 ]\\ninvestigated several ensemble architectures designed to enhance\\nrobustness and generalization across different types of facial manipulations, including deepfakes generated by technologies like\\nFaceSwap and NeuralTextures. Using the FaceForensics++ dataset\\nfor training and evaluation, the research employs EfficientNet-B0\\nas the base classifier due to its balance of performance and resource\\nefficiency, while using simple ensembles like binary detection, multiclass attribution, one-manipulation-vs-real, and one-manipulationvs-rest. The results indicated that while ensembles can outperform\\nindividual models under certain conditions, their generalization\\nacross newer and more diverse datasets remains a challenge. This\\nhighlighted the need for future work to improve the ability of ensembles to detect increasingly sophisticated and varied manipulations\\nin digital media.\\nFinally, Chaudhary et al . [ 5 ] proposed the integration of CNNs\\nand Random Forest (RF) for the efficient classification of tomato\\ndiseases. By employing CNNs for feature extraction and the RF ensemble for accurate classification across four categories, the model\\nachieved an overall accuracy of 97.03%. The performance of the\\nhybrid CNN-RF approach surpassed traditional CNN-based models,\\nwhich typically achieve accuracies between 88% to 92%, emphasizing the synergistic benefits of combining deep feature extraction\\ncapabilities with the interpretability and generalization power of\\n\\nRF.\\nWhile prior studies have explored different deepfake detection\\nmethods and ensemble approaches involving CNN models, this\\nwork integrates multiple sets of self-supervised features from FMs\\nto enhance detection accuracy. Unlike traditional methods that\\noften rely on single-model architectures or simple ensemble techniques, our approach considers the diverse feature representations\\nof advanced FMs, resulting in a more robust and generalized detection system.\\n### **3 METHOD**\\n\\nFigure 1 illustrates our proposed integration of committee approaches for combining different self-supervised features, as well\\nas the overall data flow of the classification process. It starts with\\n\\n\\nextracting a frame from the original video and applying a face\\ndetector that results in an RGB patch *x* ∈ R [(H][ ×][ W][ ×][ 3)] of a face.\\nNext, the self-supervised facial model generates three feature maps\\n*x* *𝑎𝑚* 0 *, x* *𝑎𝑚* 1 *, x* *𝑎𝑚* 2 ∈ R [(H][ ×][ W][ ×][ 1)] from the given patch. Subsequently,\\nthe original RGB patch *x* is individually concatenated with *x* *𝑎𝑚* 0,\\n*x* *𝑎𝑚* 1 and *x* *𝑎𝑚* 2, resulting in three tensors *x* *𝑡* 0 *, x* *𝑡* 1 *, x* *𝑡* 2 ∈ R [(H][ ×][ W][ ×][ 4)],\\neach of which is fed into a different classifier model trained to use\\nthat specific set of self-supervised features. The probabilistic output\\nresults of each classifier are finally sent to a committee, which uses\\nthem to determine whether an image should be classified as real or\\nfake.\\nIn the remainder of this section, we briefly discuss our chosen\\narchitectures, introducing the self-supervised facial feature extractor and the CNN-based classifier in Subsection 3.1. Furthermore, in\\nSubsection 3.2, we elaborate on the different committee approaches\\nwe tested, describing their distinct methodologies for combining\\nthe results from the classifiers.\\n### **3.1 Models**\\n\\n*3.1.1* *Self-Supervised Facial Feature Extractor.* We used a model\\nfrom the DINO family to extract the self-attention activation maps\\nfrom images, specifically opting for the newer DINOv2 [ 18 ]. This\\nupdated model significantly outperforms its predecessor through\\nthree main improvements: significantly larger and more diverse\\ntraining dataset known as LVD-142M, with 142 million images, enhanced training algorithms and implementation techniques using\\nPyTorch2 [1] and xFormers [2] for better stability and efficiency, and advanced knowledge distillation process for compressing large models\\ninto smaller ones without substantial accuracy loss. These enhancements contribute to DINOv2’s superior understanding, segmentation capabilities, and performance across several tasks, maintaining\\nhigh efficiency even with reduced model sizes.\\nThe authors provided on their GitHub Repository [3] the weights\\nof the pre-trained models both in the ViT-Base architecture, with\\n86M parameters and in ViT-Small (ViT-S), with 21M parameters.\\nFor our study, we selected the pre-trained ViT-S/14 model based on\\nits good performance and efficiency in both time and computational\\nresources to extract the self-attention maps from our dataset.\\nSimilarly to previous works [ 12 ], we employed transfer learning from a pre-trained model to generate three different attention\\nheads for self-supervised facial features from each facial image in\\nthe dataset. By applying a multi-crop strategy, we generated different views of the input image, which are subsequently processed by\\nthe networks that comprise the DINO model, generating probability\\ndistributions by normalizing the networks’ output with a softmax\\nfunction. These probabilities are then mapped onto the image, producing the attention maps. Figure 2 illustrates the self-attention\\nactivation maps extracted by DINOv2, showcasing examples of both\\ncorrectly and incorrectly classified instances by our best committee\\nclassifier.\\n\\n*3.1.2* *CNN-based Classifier.* In deepfake detection, considering selfsupervised features has been shown to improve the performance\\n\\n1 https://pytorch.org/get-started/pytorch-2.0/\\n2 https://github.com/facebookresearch/xformers\\n3 https://github.com/facebookresearch/dinov2\\n\\n\\n39\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Cunha et al.\\n\\n\\n\\n\\n\\n|Video Face Detector Frame Extractor Cropped Face Image + Self-Supervised Classifier 0 Features 0 1 1 Committee 2 2 Self-Supervised Facial Model Real | Fake|Col2|Col3|\\n|---|---|---|\\n||Committee||\\n||||\\n|Self-Supervised Facial Model|||\\n||Real | Fake||\\n\\n\\n**Figure 1: Proposed method for deepfake classification.**\\n\\n\\nof multiple CNN-based classifiers [ 12 ], resulting in various architectures as viable candidates for such tasks, including InceptionResnet [ 24 ], EfficientNet B4 [ 25 ] and XceptionNet [ 6 ]. We chose\\nto focus, for this work, on the latter due to its robust performance\\nwhen combined with self-supervised features [ 12 ] and its prevalence among other works in deepfake detection [ 4, 21 ], allowing\\nfor more direct comparisons with existing research findings.\\n### **3.2 Ensemble Techniques**\\n\\nTo combine the results of the three classifiers, we tested five different committee approaches denoted as *c0*, *c1*, *c2*, *c3* and *c4* . Each\\napproach prioritizes different aspects in terms of performance from\\nour classifier models.\\n*c0* employs a simple majority vote, disregarding the probabilistic result outputs of each model and considering only their final\\nclassifications. The goal of this committee is to deal with situations\\nwhere individual models may make mistakes, which can happen\\neven with our highest-performing classifier. Furthermore, it also\\nserves as a baseline for our approach.\\n*c1* is a weighted vote approach where we aggregate the probabilistic scores assigned by each classifier to each class, selecting the\\nclass with the highest total score. This approach differentiates itself\\nfrom *c0* in cases where a minority classifier is considerably more\\nconfident when compared to the majority classifiers. It works under\\nthe principle that a confident model is more likely to be correct.\\nThis assumption is validated by our finding, where aggregating\\n\\n\\nindividual results from our three classifiers, classifications with\\nconfidence over 0.85 were approximately 30% more accurate.\\nFollowing the same logic, *c2* is a confidence-based committee\\nwhere if a single classifier has confidence above a given threshold, its classification is considered final. If no classifier meets this\\nthreshold or multiple classifiers do, the committee defaults to the\\nweighted voting approach similar to *c1* . For this committee, we\\nfound our best results using a threshold of 0.85. This approach\\nwas expected to perform closely to *c1*, differentiating itself only in\\nscenarios where a single high-confident classifier is not enough to\\nsurpass the combined confidence of the other classifiers.\\n*c3* is another variation based on the confidence premise. It works\\nsimilarly to *c1*, but it enhances the weight of votes from classifiers\\nthat exhibit confidence above a specified threshold to improve their\\nnumerical advantage. We again achieved our best results with a\\nthreshold of 0.85, multiplying the votes from confident classifiers\\nby a factor of 1.5. This approach amplifies the influence of highconfident classifiers while still considering the collective outputs\\nof all classifiers through a weighted voting scheme.\\nLastly, *c4* is an MLP-based stacked committee that aims to learn\\nmore advanced patterns from the individual classifications. The\\nmodel receives the probabilities of an image being fake, according\\nto each classifier individually, and outputs its own probability estimate for the image being fake. In our experiments, the optimal\\narchitecture consisted of three dense layers with respective sizes\\nof 16, 8, and 1, with the first two layers using the ReLU activation\\nand the last one using a sigmoid to output the final probability.\\n\\n\\n40\\n\\n\\n-----\\n\\nAn Ensemble Approach to Facial Deepfake Detection Using Self-Supervised Features WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\n\\n\\n**Figure 2: Examples of the attention heads extracted using DINOv2. A “positive” classification means that our best committee**\\n**classified the image as fake, while “negative” means that it was labeled as real.**\\n\\n\\nThis approach employs the MLP’s ability to learn intricate relationships among its inputs (our individual classifier outputs), potentially\\nimproving overall classification accuracy.\\n### **4 EXPERIMENTS**\\n\\nIn this section, we present our experiments designed to measure the\\nimpact of different committee approaches described in Section 3, in\\nterms of performance, aiming to learn if significant improvements\\ncan be achieved by committees over using a single attention map.\\nThe first subsection covers the datasets used in our experiments, for\\nboth training and testing. The second subsection describes the setup\\nused to conduct the experimental evaluation. Lastly, the third subsection elaborates on the results, comparing them to our baselines\\nand previous works in the field of Deepfake Detection.\\n### **4.1 Datasets**\\n\\nWe mainly conducted experiments on the Deepfake Detection Challenge (DFDC) dataset [ 10 ], using a large subset as our training data\\nand a smaller one for testing purposes. As pointed out by other\\nworks [ 12 ], this dataset provides a diverse set of lighting conditions,\\nresolutions, image qualities, and demographic attributes, with the\\n\\n\\nlatter being particularly important for developing fairer models for\\nthis kind of task [29].\\nFor the sake of a more direct comparison to previous works, we\\nfollowed a similar process of preparing an image dataset from each\\nvideo in the DFDC dataset as described in Gomes et al . [ 12 ] . We\\nextracted approximately 10 frames on average from each video and\\nused the Multitask Cascaded Convolutional Network (MTCNN) face\\ndetector [ 30 ], with identical parameters, to identify and crop faces\\nfrom each frame. This process was applied across more than 124,000\\nvideos in the dataset, resulting in 1,086,737 images for training and\\n144,316 for validation.\\nIn addition to using the DFDC dataset, we also employed the\\nLow-Quality (LQ) version of the Face Forensics [ 21 ] dataset to further validate our approach. This choice came from the past use\\nof the XceptionNet [ 6 ] on this dataset, leading to direct comparisons among its standard version, the ones using self-supervised\\nattention [ 12 ], and our proposed committee approaches combining\\nthese attention maps. To this end, we followed the same methods\\nof frame extraction and cropping provided by the dataset authors.\\n\\n\\n41\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Cunha et al.\\n\\n### **4.2 Setup**\\n\\nIn our experiments, following the approach of Gomes et al . [ 12 ], we\\ntrained four models of our chosen architecture: a baseline model\\n\\ntrained only on 3-channel RGB facial image inputs, and three additional models, each using one of the three attention maps presented\\nin Section 3.1.1 as a fourth channel. The goal was to determine a\\nperformance baseline and evaluate how each attention map individually impacted performance, serving as a basis for comparison\\nwith our different committee approaches. The training was conducted exclusively on images extracted from the DFDC dataset [ 10 ],\\nfollowing the process described in Subsection 4.1, while the Face\\nForensics dataset [21] was reserved for testing purposes.\\nFor training and evaluation, we kept the default input sizes of\\nour chosen architecture. We also used an Adam optimizer with a\\nlearning rate of 1e-4 and the categorical cross-entropy loss function.\\nOur computational setup included a system with 48 GB RAM, 850\\nGB of storage capacity, and an NVIDIA RTX 2080 with 11 GB\\nVRAM.\\n### **4.3 Results**\\n\\nWe evaluated our proposed committees using both the validation\\nsubset of the DFDC dataset and the Face Forensics dataset, as described in Subsection 4.1. For validation, we used AUC and F1-Score\\nas our primary metrics due to their ability to provide deeper insights\\ninto model performance, especially important in deepfake detection [ 29 ]. Conversely, for the FaceForensics segment, we judged\\nthat simple accuracy was sufficient as it would lead to a more direct\\nand clear comparison with the results reported by Rossler et al .\\n\\n[ 21 ]. Comparing our results to those obtained without the use of\\nself-attention, as well as those using each attention map individually, each of our proposed approaches showed improvements in\\nperformance to varying degrees.\\nTable 1 shows the results from the DFDC dataset, highlighting\\nthe performance impact of the different committee approaches. **c0**\\nrepresented our simplest committee, with performance matching\\nits simplicity when compared to more complex approaches. Despite its straightforward methodology approach, it still achieved\\nslightly superior performance over the best individual classifier,\\nwith an AUC of 92.16%, a 0.25% improvement over the baseline,\\nand a marginal 0.06% increase over Map 1. However, this modest\\ngain suggests a need to explore other approaches to achieve more\\nsubstantial performance improvements.\\nThe *c1* approach showed significant improvements over *c0*, achieving an AUC of 92.47%, which is 0.56% higher than the baseline\\nand 0.31% higher than *c0* . Additionally, it presented an F1-Score\\nof 85.04%, representing a gain of 0.68% over the baseline, 0.36%\\nover Map 1, and 0.33% over *c0* . This superior performance over *c0*\\nindicates the presence of instances where two models with low\\nconfidence might misclassify an image through a simple majority approach, highlighting the better results of confidence-based\\nstrategies.\\nIn the same rationale, the *c2* and *c3* approaches exhibited very\\nsimilar performance, with AUCs of 92.52% and 92.56%, and F1Scores of 85.07% and 85.12%, respectively. Both results confirm our\\nassumptions about prioritizing classifications with high confidence,\\n\\n\\n**Table 1: Performance impact of different approaches on the**\\n**validation subset of the DFDC dataset, showing how commit-**\\n**tees improve performance over the use of individual atten-**\\n**tion maps.**\\n\\n|Approach|AUC (%) Diff. (%)|F1-Score (%) Diff. (%)|\\n|---|---|---|\\n|Baseline|91.91 - - -|84.36 - - -|\\n|Map 0 Map 1 Map 2|92.03 +0.12 92.10 +0.19 91.99 +0.08|84.39 +0.03 84.68 +0.32 84.37 +0.01|\\n|c0 c1 c2 c3 c4|92.16 +0.25 92.47 +0.56 92.52 +0.61 92.56 +0.65 93.22 +1.31|84.71 +0.35 85.04 +0.68 85.07 +0.71 85.12 +0.76 85.63 +1.27|\\n\\n\\n\\nas both committees showed improvements over our previous approaches. Furthermore, the superiority of *c3* over *c2* indicates that\\nrelying on high confidence alone is not enough to decide a final\\nclassification, especially in cases when other models are equally\\nconfident but collectively more accurate.\\nFinally, the *c4* committee stood out as the best-performing approach by a significant margin. It achieved an AUC of 93.22%, surpassing the baseline by 1.31%, Map 1 by 1.12%, and *c3* by 0.66%.\\nAdditionally, it achieved an F1 Score of 85.63%, with incremental gains of 1.27%, 0.95%, and 0.51% over the baseline, Map 1 and\\n*c3*, respectively. These considerable gains highlight the presence\\nof intricate patterns in the relationships among the three models,\\nmeaning that simple voting mechanisms might overlook scenarios\\nthat require more complex decisions for accurate classification.\\nConversely, Table 2 shows the results from our experiments on\\nthe LQ version of the Face Forensics dataset, comparing them to\\nthe baseline performance of the XceptionNet reported in a previous\\nstudy [ 21 ]. While the best individual classifier (Map 1) showed\\na significant improvement of 1.62% over the considered baseline,\\nachieving an accuracy of 82.62%, it was again surpassed by all\\ncommittee approaches. The hierarchy of performance among the\\ncommittees remained the same, with *c0* being significantly behind\\nat an accuracy of 82.71%, *c1*, *c2*, and *c3* showing similar results of\\n83.24%, 83.31%, and 83.42%, respectively. Once more, *c4* presented\\nthe most significant gain margin, achieving an accuracy of 84.73%,\\nwhich is 3.73% higher than the baseline model. This last value\\ncorresponded to an AUC of 82.54% and an F1-Score of 80.27%.\\n### **5 CONCLUSION**\\n\\nThe growing prevalence of deepfake creation steers research into\\nadvancing detection techniques, a critical challenge in digital media.\\nThrough exploring strategies integrating sophisticated models and\\nensemble learning, improvements in detection accuracy can be\\nachieved, enhancing our ability to differentiate between genuine\\nand manipulated content. These efforts reflect a continuous step\\nto mitigate the risks associated with deceptive media in online\\n\\nenvironments.\\n\\n\\n42\\n\\n\\n-----\\n\\nAn Ensemble Approach to Facial Deepfake Detection Using Self-Supervised Features WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\n**Table 2: Performance impact of the different approaches on**\\n**the LQ version of the Face Forensics dataset.**\\n\\n|Approach|Accuracy (%) Diff. (%)|\\n|---|---|\\n|Baseline|81.00 - - -|\\n|Map 0 Map 1 Map 2|82.21 +1.21 82.62 +1.62 81.87 +0.87|\\n|c0 c1 c2 c3 c4|82.71 +1.71 83.24 +2.24 83.31 +2.31 83.42 +2.42 84.73 +3.73|\\n\\n\\n\\nIn this work, we showed how combining multiple maps of selfsupervised features generated by an FM, through the use of different committee approaches, has a positive performance impact\\non realistic facial deepfake detection by experimenting with three\\nclassifiers, trained on different feature maps and combined through\\nvarious committee techniques, on two prominent datasets for the\\ntask. These findings significantly contribute by improving results\\nfound with approaches that relied on a single feature map [ 12 ],\\ndemonstrating that integrating multiple maps can enhance performance in deepfake detection. This especially solves situations\\nwhere the optimal feature map to be used is uncertain.\\nThe results show that each committee approach outperformed using a single attention map, indicating significant disparities among\\ndifferent approaches. Techniques based on classifier confidence\\nshowed improved predictions compared to simple majority voting\\napproaches. Finally, an MLP-based approach, trained to identify\\nmore complex patterns in prediction relationships, achieved the\\nhighest performance.\\nThe success of these approaches opens a door for larger committees that can combine more classifiers and utilize other selfsupervised feature maps. This indicates the potential to extract\\nricher information from each frame, enhancing detection performance. Furthermore, our findings suggest that similar techniques\\ncould be adapted for video formats, which would not require frame\\nextraction. This is enabled by advancements in FMs such as DINOv2 [ 18 ], which have already shown significant performance\\ngains for video content in recent years.\\n### **ACKNOWLEDGMENTS**\\n\\nThis material is based upon work supported by the Air Force Office\\nof Scientific Research under award number FA9550-22-1-0475, and\\nalso financed by the Coordenação de Aperfeiçoamento de Pessoal\\nde Nível Superior – Brasil (CAPES) and by CNPQ.\\n### **REFERENCES**\\n\\n[1] Redha Ali, Russell C. Hardie, Barath Narayanan Narayanan, and Supun De Silva.\\n2019. Deep Learning Ensemble Methods for Skin Lesion Analysis towards\\nMelanoma Detection. In *2019 IEEE National Aerospace and Electronics Confer-*\\n*ence (NAECON)* . IEEE, Dayton, OH, USA, 311–316. https://doi.org/10.1109/\\nNAECON46414.2019.9058245\\n\\n[2] Roberto Amoroso, Davide Morelli, Marcella Cornia, Lorenzo Baraldi, Alberto Del\\nBimbo, and Rita Cucchiara. 2024. Parents and Children: Distinguishing Multimodal DeepFakes from Natural Images. arXiv:2304.00500 [cs.CV] https:\\n//arxiv.org/abs/2304.00500\\n\\n\\n\\n[3] Ben Beaumont-Thomas. 2024. Taylor Swift deepfake pornography sparks renewed calls for US legislation. https://www.theguardian.com/music/2024/jan/26/\\ntaylor-swift-deepfake-pornography-sparks-renewed-calls-for-us-legislation.\\n\\n[4] Nicolò Bonettini, Edoardo Daniele Cannas, Sara Mandelli, Luca Bondi, Paolo\\nBestagini, and Stefano Tubaro. 2021. Video Face Manipulation Detection Through\\nEnsemble of CNNs. In *2020 25th International Conference on Pattern Recognition*\\n*(ICPR)* . IEEE, Milan, Italy, 5012–5019. https://doi.org/10.1109/ICPR48806.2021.\\n9412711\\n\\n[5] Preeti Chaudhary, Aditya Verma, Vinay Kukreja, and Rishabh Sharma. 2024. Integrating Deep Learning and Ensemble Methods for Robust Tomato Disease Detection: A Hybrid CNN-RF Model Analysis. In *2024 11th International Conference on*\\n*Reliability, Infocom Technologies and Optimization (Trends and Future Directions)*\\n*(ICRITO)* . IEEE, Noida, India, 1–4. https://doi.org/10.1109/ICRITO61523.2024.\\n10522213\\n\\n[6] François Chollet. 2017. Xception: Deep learning with depthwise separable convolutions. In *Proceedings of the IEEE conference on computer vision and pattern*\\n*recognition* . IEEE, Honolulu, HI, USA, 1251–1258. https://doi.org/10.1109/CVPR.\\n2017.195\\n\\n[7] Thipwimon Chompookham and OJIEL Surinta. 2021. Ensemble methods with\\ndeep convolutional neural networks for plant leaf recognition. *ICIC Express*\\n*Letters* 15, 6 (2021), 553–565.\\n\\n[8] Davide Alessandro Coccomini, Nicola Messina, Claudio Gennaro, and Fabrizio\\nFalchi. 2022. Combining EfficientNet and Vision Transformers for Video Deepfake\\nDetection. In *Image Analysis and Processing – ICIAP 2022*, Stan Sclaroff, Cosimo\\nDistante, Marco Leo, Giovanni M. Farinella, and Federico Tombari (Eds.). Springer\\nInternational Publishing, Cham, 219–229. https://doi.org/10.1007/978-3-03106433-3_19\\n\\n[9] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua\\nBengio. 2016. Binarized Neural Networks: Training Deep Neural Networks with\\nWeights and Activations Constrained to +1 or -1. arXiv:1602.02830 [cs.LG]\\nhttps://arxiv.org/abs/1602.02830\\n\\n[10] Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin\\nWang, and Cristian Canton Ferrer. 2020. The DeepFake Detection Challenge\\nDataset. arXiv:2006.07397 [cs.CV]\\n\\n[11] Nikolaos Giatsoglou, Symeon Papadopoulos, and Ioannis Kompatsiaris. 2023. Investigation of ensemble methods for the detection of deepfake face manipulations.\\narXiv:2304.07395 [cs.CV] https://arxiv.org/abs/2304.07395\\n\\n[12] Bruno Rocha Gomes, Antonio J. G. Busson, José Boaro, and Sérgio Colcher.\\n2023. Realistic Facial Deep Fakes Detection Through Self-Supervised Features\\nGenerated by a Self-Distilled Vision Transformer. In *Proceedings of the 29th*\\n*Brazilian Symposium on Multimedia and the Web (WebMedia ’23)* . Association for\\nComputing Machinery, New York, NY, USA, 177–183. https://doi.org/10.1145/\\n3617023.3617047\\n\\n[13] Young-Jin Heo, Young-Ju Choi, Young-Woon Lee, and Byung-Gyu Kim. 2021.\\nDeepfake detection scheme based on vision transformer and distillation. *arXiv*\\n*preprint arXiv:2104.01353* abs/2104.01353 (2021), 7 pages. https://doi.org/10.48550/\\narXiv.2104.01353\\n\\n[14] Brittaney Kiefer. 2023. This Brand’s Social Experiment Uses AI to Expose the Dark\\nSide of ’Sharenting’. https://www.adweek.com/brand-marketing/this-brandssocial-experiment-uses-ai-to-expose-the-dark-side-of-sharenting/.\\n\\n[15] Romeo Lanzino, Federico Fontana, Anxhelo Diko, Marco Raoul Marini, and Luigi\\nCinque. 2024. Faster Than Lies: Real-time Deepfake Detection using Binary\\nNeural Networks. In *Proceedings of the IEEE/CVF Conference on Computer Vision*\\n*and Pattern Recognition (CVPR) Workshops* . IEEE, Seattle, WA, USA, 3771–3780.\\n\\n[16] Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, and Siwei Lyu. 2020. Celeb-df:\\nA large-scale challenging dataset for deepfake forensics. In *Proceedings of the*\\n*IEEE/CVF Conference on Computer Vision and Pattern Recognition* . IEEE, Seattle,\\nWA, USA, 3207–3216.\\n\\n[17] Sachin Mehta, Ezgi Mercan, Jamen Bartlett, Donald Weaver, Joann G. Elmore, and\\nLinda Shapiro. 2018. Y-Net: Joint Segmentation and Classification for Diagnosis\\nof Breast Biopsy Images. In *Medical Image Computing and Computer Assisted*\\n*Intervention – MICCAI 2018: 21st International Conference, Granada, Spain, Sep-*\\n*tember 16-20, 2018, Proceedings, Part II* (Granada, Spain). Springer-Verlag, Berlin,\\nHeidelberg, 893–901.\\n\\n[18] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec,\\nVasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin ElNouby, et al . 2024. Dinov2: Learning robust visual features without supervision.\\n*Transactions on Machine Learning Research Journal* 1 (2024), 1–31. https://doi.\\norg/10.48550/arxiv.2304.07193\\n\\n[19] Artem A Pokroy and Alexey D Egorov. 2021. EfficientNets for deepfake detection:\\nComparison of pretrained models. In *2021 IEEE Conference of Russian Young Re-*\\n*searchers in Electrical and Electronic Engineering (ElConRus)* . IEEE, St. Petersburg,\\nMoscow, Russia, 598–600. https://doi.org/10.1109/ElConRus51938.2021.9396092\\n\\n[20] Tal Reiss, Bar Cavia, and Yedid Hoshen. 2023. Detecting Deepfakes Without\\nSeeing Any. *ArXiv* abs/2311.01458 (2023), 16 pages. https://api.semanticscholar.\\norg/CorpusID:264935112\\n\\n\\n43\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Cunha et al.\\n\\n\\n\\n[21] Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies,\\nand Matthias Nießner. 2019. Faceforensics++: Learning to detect manipulated\\nfacial images. In *Proceedings of the IEEE/CVF International Conference on Computer*\\n*Vision* . IEEE, Seoul, Korea (South), 1–11. https://doi.org/10.1109/ICCV.2019.00009\\n\\n[22] Rhianna Schmunk. 2024. Explicit fake images of Taylor Swift prove laws haven’t\\nkept pace with tech, experts say. https://www.cbc.ca/news/canada/taylor-swiftai-images-highlight-need-for-better-legislation-1.7096094.\\n\\n[23] Laura Stroebel, Mark Llewellyn, Tricia Hartley, Tsui Shan Ip, and Mohiuddin Ahmed. 2023. A systematic literature review on the effectiveness of deepfake detection techniques. *Journal of Cyber Security Tech-*\\n*nology* 7, 2 (2023), 83–113. https://doi.org/10.1080/23742917.2023.2192888\\narXiv:https://doi.org/10.1080/23742917.2023.2192888\\n\\n[24] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi.\\n2017. Inception-v4, inception-resnet and the impact of residual connections on\\nlearning. In *Thirty-first AAAI conference on artificial intelligence* . AAAI Press, San\\nFrancisco, California, USA, 4278–4284. https://doi.org/10.48550/arXiv.1602.07261\\n\\n[25] Mingxing Tan and Quoc Le. 2019. EfficientNet: Rethinking Model Scaling for\\nConvolutional Neural Networks. In *Proceedings of the 36th International Confer-*\\n*ence on Machine Learning (Proceedings of Machine Learning Research, Vol. 97)*,\\nKamalika Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR, Long Beach, CA,\\nUSA, 6105–6114. https://proceedings.mlr.press/v97/tan19a.html\\n\\n\\n\\n[26] Eric Tjon, Melody Moh, and Teng-Sheng Moh. 2021. Eff-YNet: A Dual Task\\nNetwork for DeepFake Detection and Segmentation. In *2021 15th International*\\n*Conference on Ubiquitous Information Management and Communication (IMCOM)* .\\nIEEE, Seoul, Korea (South), 1–8. https://doi.org/10.1109/IMCOM51814.2021.\\n9377373\\n\\n[27] Loc Trinh and Yan Liu. 2021. An Examination of Fairness of AI Models for\\nDeepfake Detection. arXiv:2105.00558 [cs.CV]\\n\\n[28] Junke Wang, Zuxuan Wu, Wenhao Ouyang, Xintong Han, Jingjing Chen, YuGang Jiang, and Ser-Nam Li. 2022. M2TR: Multi-modal Multi-scale Transformers\\nfor Deepfake Detection. In *Proceedings of the 2022 International Conference on Mul-*\\n*timedia Retrieval* (Newark, NJ, USA) *(ICMR ’22)* . Association for Computing Machinery, New York, NY, USA, 615–623. https://doi.org/10.1145/3512527.3531415\\n\\n[29] Ying Xu, Philipp Terhörst, Kiran Raja, and Marius Pedersen. 2023. A Comprehensive Analysis of AI Biases in DeepFake Detection With Massively Annotated\\nDatabases. arXiv:2208.05845 [cs.CV]\\n\\n[30] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao. 2016. Joint face\\ndetection and alignment using multitask cascaded convolutional networks. *IEEE*\\n*signal processing letters* 23, 10 (2016), 1499–1503. https://doi.org/10.1109/LSP.\\n2016.2603342\\n\\n[31] Hanqing Zhao, Wenbo Zhou, Dongdong Chen, Weiming Zhang, and\\nNenghai Yu. 2022. Self-supervised Transformer for Deepfake Detection.\\narXiv:2203.01265 [cs.CV] https://arxiv.org/abs/2203.01265\\n\\n\\n44\\n\\n\\n-----\\n\\n',\n",
       "  'artigo_tokenizado': ['#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'An',\n",
       "   'Ensemble',\n",
       "   'Approach',\n",
       "   'to',\n",
       "   'Facial',\n",
       "   'Deepfake',\n",
       "   'Detection',\n",
       "   'Using',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Self',\n",
       "   '-',\n",
       "   'Supervised',\n",
       "   'Features',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Yan',\n",
       "   'Martins',\n",
       "   'B.',\n",
       "   'Gurevitz',\n",
       "   'Cunha',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'yangurevitz@telemidia.puc-rio.br',\n",
       "   'Telemidia',\n",
       "   'Lab',\n",
       "   '.',\n",
       "   '–',\n",
       "   'Pontifical',\n",
       "   'Catholic',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Rio',\n",
       "   'de',\n",
       "   'Janeiro',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Pedro',\n",
       "   'Cutrim',\n",
       "   'dos',\n",
       "   'Santos',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'thiagocutrim98@gmail.com',\n",
       "   'Telemidia',\n",
       "   'Lab',\n",
       "   '.',\n",
       "   '–',\n",
       "   'Pontifical',\n",
       "   'Catholic',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Rio',\n",
       "   'de',\n",
       "   'Janeiro',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'José',\n",
       "   'Matheus',\n",
       "   'C.',\n",
       "   'Boaro',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'boaro@telemidia.puc-rio.br',\n",
       "   'Telemidia',\n",
       "   'Lab',\n",
       "   '.',\n",
       "   '–',\n",
       "   'Pontifical',\n",
       "   'Catholic',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Rio',\n",
       "   'de',\n",
       "   'Janeiro',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Polyana',\n",
       "   'Bezerra',\n",
       "   'da',\n",
       "   'Costa',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'polyanabcosta@gmail.com',\n",
       "   'Telemidia',\n",
       "   'Lab',\n",
       "   '.',\n",
       "   '–',\n",
       "   'Pontifical',\n",
       "   'Catholic',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Rio',\n",
       "   'de',\n",
       "   'Janeiro',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Daniel',\n",
       "   'de',\n",
       "   'Sousa',\n",
       "   'Moraes',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'danielmoraes@telemidia.puc-rio.br',\n",
       "   'Telemidia',\n",
       "   'Lab',\n",
       "   '.',\n",
       "   '–',\n",
       "   'Pontifical',\n",
       "   'Catholic',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Rio',\n",
       "   'de',\n",
       "   'Janeiro',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Antonio',\n",
       "   'José',\n",
       "   'Grandson',\n",
       "   'Busson',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'antonio.busson@btgpactual.com',\n",
       "   'BTG',\n",
       "   'Pactual',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Julio',\n",
       "   'Cesar',\n",
       "   'Duarte',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'duarte@ime.eb.br',\n",
       "   'Computing',\n",
       "   'Engineering',\n",
       "   'Dept',\n",
       "   '.',\n",
       "   '–',\n",
       "   'Military',\n",
       "   'Institute',\n",
       "   'of',\n",
       "   'Engineering',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'ABSTRACT',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Substantial',\n",
       "   'efforts',\n",
       "   'have',\n",
       "   'been',\n",
       "   'dedicated',\n",
       "   'to',\n",
       "   'developing',\n",
       "   'methods',\n",
       "   'for',\n",
       "   '\\n',\n",
       "   'detecting',\n",
       "   'deepfake',\n",
       "   'content',\n",
       "   ',',\n",
       "   'especially',\n",
       "   'with',\n",
       "   'the',\n",
       "   'creation',\n",
       "   'of',\n",
       "   'large',\n",
       "   '\\n',\n",
       "   'and',\n",
       "   'diverse',\n",
       "   'datasets',\n",
       "   'with',\n",
       "   'both',\n",
       "   'higher',\n",
       "   'image',\n",
       "   'quality',\n",
       "   'and',\n",
       "   'demographic',\n",
       "   'features',\n",
       "   '.',\n",
       "   'In',\n",
       "   'this',\n",
       "   'scenario',\n",
       "   ',',\n",
       "   'CNN',\n",
       "   '-',\n",
       "   'based',\n",
       "   'approaches',\n",
       "   'showed',\n",
       "   '\\n',\n",
       "   'good',\n",
       "   'initial',\n",
       "   'success',\n",
       "   ',',\n",
       "   'later',\n",
       "   'improved',\n",
       "   'by',\n",
       "   'their',\n",
       "   'combination',\n",
       "   'with',\n",
       "   'Vision',\n",
       "   'Transformers',\n",
       "   '.',\n",
       "   'More',\n",
       "   'recently',\n",
       "   ',',\n",
       "   'Foundation',\n",
       "   'Models',\n",
       "   '(',\n",
       "   'FMs',\n",
       "   ')',\n",
       "   'have',\n",
       "   '\\n',\n",
       "   'emerged',\n",
       "   ',',\n",
       "   'improving',\n",
       "   'performance',\n",
       "   'across',\n",
       "   'many',\n",
       "   'visual',\n",
       "   'tasks',\n",
       "   ',',\n",
       "   'including',\n",
       "   'deepfake',\n",
       "   'detection',\n",
       "   ',',\n",
       "   'and',\n",
       "   'combining',\n",
       "   'self',\n",
       "   '-',\n",
       "   'supervised',\n",
       "   'features',\n",
       "   '\\n',\n",
       "   'generated',\n",
       "   'by',\n",
       "   'FMs',\n",
       "   'with',\n",
       "   'CNN',\n",
       "   '-',\n",
       "   'based',\n",
       "   'classifiers',\n",
       "   'has',\n",
       "   'resulted',\n",
       "   'in',\n",
       "   'significant',\n",
       "   'performance',\n",
       "   'gains',\n",
       "   '.',\n",
       "   'However',\n",
       "   ',',\n",
       "   'taking',\n",
       "   'advantage',\n",
       "   'of',\n",
       "   'multiple',\n",
       "   '\\n',\n",
       "   'maps',\n",
       "   'of',\n",
       "   'self',\n",
       "   '-',\n",
       "   'supervised',\n",
       "   'features',\n",
       "   'is',\n",
       "   'not',\n",
       "   'as',\n",
       "   'straightforward',\n",
       "   'as',\n",
       "   'just',\n",
       "   '\\n',\n",
       "   'adding',\n",
       "   'more',\n",
       "   'channels',\n",
       "   'to',\n",
       "   'the',\n",
       "   'classifier',\n",
       "   '.',\n",
       "   'Therefore',\n",
       "   ',',\n",
       "   'this',\n",
       "   'work',\n",
       "   'explores',\n",
       "   'ensemble',\n",
       "   'techniques',\n",
       "   'to',\n",
       "   'effectively',\n",
       "   'utilize',\n",
       "   'these',\n",
       "   'diverse',\n",
       "   'selfsupervised',\n",
       "   'feature',\n",
       "   'maps',\n",
       "   'for',\n",
       "   'realistic',\n",
       "   'facial',\n",
       "   'deepfake',\n",
       "   'detection',\n",
       "   '.',\n",
       "   'Our',\n",
       "   '\\n',\n",
       "   'experiments',\n",
       "   'indicate',\n",
       "   'that',\n",
       "   'combining',\n",
       "   'the',\n",
       "   'output',\n",
       "   'results',\n",
       "   'of',\n",
       "   'different',\n",
       "   '\\n',\n",
       "   'classifiers',\n",
       "   ',',\n",
       "   'each',\n",
       "   'one',\n",
       "   'utilizing',\n",
       "   'a',\n",
       "   'single',\n",
       "   'map',\n",
       "   'of',\n",
       "   'self',\n",
       "   '-',\n",
       "   'supervised',\n",
       "   'features',\n",
       "   ',',\n",
       "   'leads',\n",
       "   'to',\n",
       "   'significant',\n",
       "   'performance',\n",
       "   'improvements',\n",
       "   ',',\n",
       "   'and',\n",
       "   'several',\n",
       "   '\\n',\n",
       "   'committee',\n",
       "   'approaches',\n",
       "   'consistently',\n",
       "   'outperform',\n",
       "   'individual',\n",
       "   'classifiers',\n",
       "   ',',\n",
       "   'demonstrating',\n",
       "   'the',\n",
       "   'potential',\n",
       "   'of',\n",
       "   'these',\n",
       "   'methods',\n",
       "   'in',\n",
       "   'enhancing',\n",
       "   '\\n',\n",
       "   'deepfake',\n",
       "   'detection',\n",
       "   'accuracy',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'KEYWORDS',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'deep',\n",
       "   'fake',\n",
       "   'detection',\n",
       "   ',',\n",
       "   'self',\n",
       "   '-',\n",
       "   'supervised',\n",
       "   ',',\n",
       "   'vision',\n",
       "   'transformers',\n",
       "   ',',\n",
       "   'deep',\n",
       "   '\\n',\n",
       "   'learning',\n",
       "   ',',\n",
       "   'foundation',\n",
       "   'models',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   '1',\n",
       "   'INTRODUCTION',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   'recent',\n",
       "   'years',\n",
       "   ',',\n",
       "   'there',\n",
       "   'has',\n",
       "   'been',\n",
       "   'an',\n",
       "   'increased',\n",
       "   'focus',\n",
       "   'on',\n",
       "   'the',\n",
       "   'effects',\n",
       "   '\\n',\n",
       "   'of',\n",
       "   'deepfake',\n",
       "   'multimedia',\n",
       "   'content',\n",
       "   'on',\n",
       "   'public',\n",
       "   'discourse',\n",
       "   'and',\n",
       "   'personal',\n",
       "   '\\n',\n",
       "   'lives',\n",
       "   '.',\n",
       "   'This',\n",
       "   'has',\n",
       "   'manifested',\n",
       "   'in',\n",
       "   'forms',\n",
       "   'such',\n",
       "   'as',\n",
       "   'explicit',\n",
       "   'fake',\n",
       "   'images',\n",
       "   '\\n',\n",
       "   'of',\n",
       "   'celebrities',\n",
       "   '[',\n",
       "   '22',\n",
       "   ']',\n",
       "   'or',\n",
       "   'politically',\n",
       "   'sensitive',\n",
       "   'material',\n",
       "   ',',\n",
       "   'facilitating',\n",
       "   'the',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   ':',\n",
       "   'Proceedings',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Web',\n",
       "   '(',\n",
       "   'WebMe',\n",
       "   '\\n',\n",
       "   'dia’2024',\n",
       "   ')',\n",
       "   '.',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '.',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   ':',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   ',',\n",
       "   '2024',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'SBC',\n",
       "   '–',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '-',\n",
       "   '2753',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Sérgio',\n",
       "   'Colcher',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'colcher@inf.puc-rio.br',\n",
       "   'Telemidia',\n",
       "   'Lab',\n",
       "   '.',\n",
       "   '–',\n",
       "   'Pontifical',\n",
       "   'Catholic',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Rio',\n",
       "   'de',\n",
       "   'Janeiro',\n",
       "   '\\n\\n',\n",
       "   'spread',\n",
       "   'of',\n",
       "   'misinformation',\n",
       "   'and',\n",
       "   'identity',\n",
       "   'theft',\n",
       "   'and',\n",
       "   'possibly',\n",
       "   'leading',\n",
       "   'to',\n",
       "   '\\n',\n",
       "   'threats',\n",
       "   'of',\n",
       "   'violence',\n",
       "   '[',\n",
       "   '14',\n",
       "   ']',\n",
       "   '.',\n",
       "   'This',\n",
       "   'scenario',\n",
       "   'has',\n",
       "   'renewed',\n",
       "   'societal',\n",
       "   'discussions',\n",
       "   'on',\n",
       "   'the',\n",
       "   'nature',\n",
       "   'of',\n",
       "   'online',\n",
       "   'content',\n",
       "   'and',\n",
       "   'led',\n",
       "   'to',\n",
       "   'legislative',\n",
       "   'debates',\n",
       "   '\\n',\n",
       "   'in',\n",
       "   'countries',\n",
       "   'such',\n",
       "   'as',\n",
       "   'the',\n",
       "   'United',\n",
       "   'States',\n",
       "   'on',\n",
       "   'reducing',\n",
       "   'the',\n",
       "   'creation',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'distribution',\n",
       "   'of',\n",
       "   'this',\n",
       "   'kind',\n",
       "   'of',\n",
       "   'media',\n",
       "   '[',\n",
       "   '3',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Concurrent',\n",
       "   'with',\n",
       "   'the',\n",
       "   'advancement',\n",
       "   'of',\n",
       "   'deepfake',\n",
       "   'generators',\n",
       "   ',',\n",
       "   'significant',\n",
       "   'progress',\n",
       "   'has',\n",
       "   'also',\n",
       "   'been',\n",
       "   'made',\n",
       "   'in',\n",
       "   'efforts',\n",
       "   'to',\n",
       "   'improve',\n",
       "   'the',\n",
       "   'detection',\n",
       "   '\\n',\n",
       "   'and',\n",
       "   'containment',\n",
       "   'of',\n",
       "   'this',\n",
       "   'kind',\n",
       "   'of',\n",
       "   'content',\n",
       "   '.',\n",
       "   'This',\n",
       "   'starts',\n",
       "   'with',\n",
       "   'the',\n",
       "   'cre',\n",
       "   '\\n',\n",
       "   'ation',\n",
       "   'of',\n",
       "   'large',\n",
       "   '-',\n",
       "   'scale',\n",
       "   'datasets',\n",
       "   'for',\n",
       "   'deepfake',\n",
       "   'detection',\n",
       "   'in',\n",
       "   'video',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'image',\n",
       "   'formats',\n",
       "   '[',\n",
       "   '10',\n",
       "   ',',\n",
       "   '21',\n",
       "   ']',\n",
       "   ',',\n",
       "   'enabling',\n",
       "   'the',\n",
       "   'development',\n",
       "   'of',\n",
       "   'methods',\n",
       "   'to',\n",
       "   '\\n',\n",
       "   'better',\n",
       "   'differentiate',\n",
       "   'between',\n",
       "   'real',\n",
       "   'and',\n",
       "   'fake',\n",
       "   'content',\n",
       "   '.',\n",
       "   'In',\n",
       "   'this',\n",
       "   'context',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'CNN',\n",
       "   '-',\n",
       "   'based',\n",
       "   'models',\n",
       "   'showed',\n",
       "   'initial',\n",
       "   'success',\n",
       "   ',',\n",
       "   'and',\n",
       "   'their',\n",
       "   'combination',\n",
       "   '\\n',\n",
       "   'with',\n",
       "   'Vision',\n",
       "   'Transformers',\n",
       "   '(',\n",
       "   'ViT',\n",
       "   ')',\n",
       "   'managed',\n",
       "   'to',\n",
       "   'achieve',\n",
       "   'state',\n",
       "   '-',\n",
       "   'of',\n",
       "   '-',\n",
       "   'art',\n",
       "   '\\n',\n",
       "   'performance',\n",
       "   'in',\n",
       "   'recent',\n",
       "   'years',\n",
       "   '[',\n",
       "   '13',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Also',\n",
       "   ',',\n",
       "   'the',\n",
       "   'recent',\n",
       "   'emergence',\n",
       "   'of',\n",
       "   'Foundation',\n",
       "   'Models',\n",
       "   '(',\n",
       "   'FMs',\n",
       "   ')',\n",
       "   'opened',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'door',\n",
       "   'for',\n",
       "   'new',\n",
       "   'approaches',\n",
       "   'to',\n",
       "   'deepfake',\n",
       "   'detection',\n",
       "   '.',\n",
       "   'In',\n",
       "   'this',\n",
       "   'light',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'studies',\n",
       "   'have',\n",
       "   'demonstrated',\n",
       "   'that',\n",
       "   'using',\n",
       "   'self',\n",
       "   '-',\n",
       "   'supervised',\n",
       "   'features',\n",
       "   'generated',\n",
       "   'by',\n",
       "   'pre',\n",
       "   '-',\n",
       "   'trained',\n",
       "   'FMs',\n",
       "   'in',\n",
       "   'combination',\n",
       "   'with',\n",
       "   'CNN',\n",
       "   '-',\n",
       "   'based',\n",
       "   'models',\n",
       "   '\\n',\n",
       "   'can',\n",
       "   'result',\n",
       "   'in',\n",
       "   'significant',\n",
       "   'performance',\n",
       "   'improvements',\n",
       "   '[',\n",
       "   '12',\n",
       "   ']',\n",
       "   '.',\n",
       "   'In',\n",
       "   'this',\n",
       "   '\\n',\n",
       "   'approach',\n",
       "   ',',\n",
       "   'the',\n",
       "   'authors',\n",
       "   'incorporated',\n",
       "   'feature',\n",
       "   'maps',\n",
       "   'generated',\n",
       "   'by',\n",
       "   'FMs',\n",
       "   '\\n',\n",
       "   'as',\n",
       "   'extra',\n",
       "   'channels',\n",
       "   'in',\n",
       "   'CNN',\n",
       "   'input',\n",
       "   ',',\n",
       "   'showing',\n",
       "   'that',\n",
       "   ',',\n",
       "   'when',\n",
       "   'used',\n",
       "   'individually',\n",
       "   ',',\n",
       "   'each',\n",
       "   'map',\n",
       "   'would',\n",
       "   'improve',\n",
       "   'the',\n",
       "   'performance',\n",
       "   'of',\n",
       "   'its',\n",
       "   'base',\n",
       "   'model',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'However',\n",
       "   ',',\n",
       "   'it',\n",
       "   'has',\n",
       "   'also',\n",
       "   'been',\n",
       "   'shown',\n",
       "   'by',\n",
       "   'Gomes',\n",
       "   'et',\n",
       "   'al',\n",
       "   '.',\n",
       "   '[',\n",
       "   '12',\n",
       "   ']',\n",
       "   'that',\n",
       "   'simultaneously',\n",
       "   'applying',\n",
       "   'multiple',\n",
       "   'attention',\n",
       "   'maps',\n",
       "   'to',\n",
       "   'the',\n",
       "   'same',\n",
       "   'model',\n",
       "   'by',\n",
       "   '\\n',\n",
       "   'simply',\n",
       "   'adding',\n",
       "   'more',\n",
       "   'channels',\n",
       "   'to',\n",
       "   'the',\n",
       "   'input',\n",
       "   'does',\n",
       "   'not',\n",
       "   'necessarily',\n",
       "   '\\n',\n",
       "   'improve',\n",
       "   'performance',\n",
       "   ',',\n",
       "   'leaving',\n",
       "   'open',\n",
       "   'the',\n",
       "   'following',\n",
       "   'question',\n",
       "   ':',\n",
       "   '*',\n",
       "   'Is',\n",
       "   'there',\n",
       "   '*',\n",
       "   '\\n',\n",
       "   '*',\n",
       "   'a',\n",
       "   'way',\n",
       "   'to',\n",
       "   'better',\n",
       "   'combine',\n",
       "   'multiple',\n",
       "   'attention',\n",
       "   'maps',\n",
       "   'to',\n",
       "   'achieve',\n",
       "   'superior',\n",
       "   '*',\n",
       "   '\\n',\n",
       "   '*',\n",
       "   'deepfake',\n",
       "   'detection',\n",
       "   'performance',\n",
       "   '?',\n",
       "   '*',\n",
       "   '\\n',\n",
       "   'To',\n",
       "   'pursue',\n",
       "   'this',\n",
       "   'question',\n",
       "   ',',\n",
       "   'we',\n",
       "   'conducted',\n",
       "   'experiments',\n",
       "   'with',\n",
       "   'DINOv2',\n",
       "   'FM',\n",
       "   '[',\n",
       "   '18',\n",
       "   ']',\n",
       "   'and',\n",
       "   'the',\n",
       "   'CNN',\n",
       "   '-',\n",
       "   'based',\n",
       "   'XceptionNet',\n",
       "   '[',\n",
       "   '6',\n",
       "   ']',\n",
       "   ',',\n",
       "   'chosen',\n",
       "   'for',\n",
       "   '\\n',\n",
       "   'their',\n",
       "   'established',\n",
       "   'use',\n",
       "   'and',\n",
       "   'strong',\n",
       "   'performance',\n",
       "   'in',\n",
       "   'this',\n",
       "   'task',\n",
       "   '.',\n",
       "   'Initially',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'we',\n",
       "   'evaluated',\n",
       "   'each',\n",
       "   'self',\n",
       "   '-',\n",
       "   'supervised',\n",
       "   'feature',\n",
       "   'map',\n",
       "   'generated',\n",
       "   'by',\n",
       "   'DINOv2',\n",
       "   'individually',\n",
       "   ',',\n",
       "   'resulting',\n",
       "   'in',\n",
       "   'three',\n",
       "   'distinct',\n",
       "   'models',\n",
       "   '.',\n",
       "   'Next',\n",
       "   ',',\n",
       "   'we',\n",
       "   'tried',\n",
       "   '\\n',\n",
       "   'multiple',\n",
       "   'committee',\n",
       "   'approaches',\n",
       "   'to',\n",
       "   'combine',\n",
       "   'these',\n",
       "   'models',\n",
       "   '’',\n",
       "   'predictions',\n",
       "   '.',\n",
       "   'For',\n",
       "   'our',\n",
       "   'experiments',\n",
       "   ',',\n",
       "   'we',\n",
       "   'used',\n",
       "   'both',\n",
       "   'the',\n",
       "   'Deepfake',\n",
       "   'Detection',\n",
       "   '\\n',\n",
       "   'Challenge',\n",
       "   '(',\n",
       "   'DFDC',\n",
       "   ')',\n",
       "   '[',\n",
       "   '10',\n",
       "   ']',\n",
       "   'and',\n",
       "   'Face',\n",
       "   'Forensics',\n",
       "   '[',\n",
       "   '21',\n",
       "   ']',\n",
       "   'datasets',\n",
       "   '.',\n",
       "   'In',\n",
       "   'both',\n",
       "   '\\n\\n\\n',\n",
       "   '37',\n",
       "   '\\n\\n\\n',\n",
       "   ...],\n",
       "  'pos_tagger': '',\n",
       "  'lema': ['an',\n",
       "   'Ensemble',\n",
       "   'Approach',\n",
       "   'to',\n",
       "   'Facial',\n",
       "   'Deepfake',\n",
       "   'Detection',\n",
       "   'use',\n",
       "   'self',\n",
       "   'supervise',\n",
       "   'feature',\n",
       "   'Yan',\n",
       "   'martin',\n",
       "   'B.',\n",
       "   'Gurevitz',\n",
       "   'Cunha',\n",
       "   'yangurevitz@telemidia.puc-rio.br',\n",
       "   'Telemidia',\n",
       "   'Lab',\n",
       "   'Pontifical',\n",
       "   'Catholic',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Rio',\n",
       "   'de',\n",
       "   'Janeiro',\n",
       "   'Pedro',\n",
       "   'Cutrim',\n",
       "   'do',\n",
       "   'santos',\n",
       "   'thiagocutrim98@gmail.com',\n",
       "   'Telemidia',\n",
       "   'Lab',\n",
       "   'Pontifical',\n",
       "   'Catholic',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Rio',\n",
       "   'de',\n",
       "   'Janeiro',\n",
       "   'José',\n",
       "   'Matheus',\n",
       "   'C.',\n",
       "   'Boaro',\n",
       "   'boaro@telemidia.puc-rio.br',\n",
       "   'Telemidia',\n",
       "   'Lab',\n",
       "   'Pontifical',\n",
       "   'Catholic',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Rio',\n",
       "   'de',\n",
       "   'Janeiro',\n",
       "   'Polyana',\n",
       "   'Bezerra',\n",
       "   'da',\n",
       "   'Costa',\n",
       "   'polyanabcosta@gmail.com',\n",
       "   'Telemidia',\n",
       "   'Lab',\n",
       "   'Pontifical',\n",
       "   'Catholic',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Rio',\n",
       "   'de',\n",
       "   'Janeiro',\n",
       "   'Daniel',\n",
       "   'de',\n",
       "   'Sousa',\n",
       "   'Moraes',\n",
       "   'danielmoraes@telemidia.puc-rio.br',\n",
       "   'Telemidia',\n",
       "   'Lab',\n",
       "   'Pontifical',\n",
       "   'Catholic',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Rio',\n",
       "   'de',\n",
       "   'Janeiro',\n",
       "   'Antonio',\n",
       "   'José',\n",
       "   'Grandson',\n",
       "   'Busson',\n",
       "   'antonio.busson@btgpactual.com',\n",
       "   'BTG',\n",
       "   'Pactual',\n",
       "   'Julio',\n",
       "   'Cesar',\n",
       "   'Duarte',\n",
       "   'duarte@ime.eb.br',\n",
       "   'Computing',\n",
       "   'Engineering',\n",
       "   'Dept',\n",
       "   'Military',\n",
       "   'Institute',\n",
       "   'of',\n",
       "   'Engineering',\n",
       "   'ABSTRACT',\n",
       "   'substantial',\n",
       "   'effort',\n",
       "   'have',\n",
       "   'be',\n",
       "   'dedicate',\n",
       "   'to',\n",
       "   'develop',\n",
       "   'method',\n",
       "   'for',\n",
       "   'detect',\n",
       "   'deepfake',\n",
       "   'content',\n",
       "   'especially',\n",
       "   'with',\n",
       "   'the',\n",
       "   'creation',\n",
       "   'of',\n",
       "   'large',\n",
       "   'and',\n",
       "   'diverse',\n",
       "   'dataset',\n",
       "   'with',\n",
       "   'both',\n",
       "   'high',\n",
       "   'image',\n",
       "   'quality',\n",
       "   'and',\n",
       "   'demographic',\n",
       "   'feature',\n",
       "   'in',\n",
       "   'this',\n",
       "   'scenario',\n",
       "   'CNN',\n",
       "   'base',\n",
       "   'approach',\n",
       "   'show',\n",
       "   'good',\n",
       "   'initial',\n",
       "   'success',\n",
       "   'later',\n",
       "   'improve',\n",
       "   'by',\n",
       "   'their',\n",
       "   'combination',\n",
       "   'with',\n",
       "   'Vision',\n",
       "   'Transformers',\n",
       "   'more',\n",
       "   'recently',\n",
       "   'Foundation',\n",
       "   'Models',\n",
       "   'FMs',\n",
       "   'have',\n",
       "   'emerge',\n",
       "   'improve',\n",
       "   'performance',\n",
       "   'across',\n",
       "   'many',\n",
       "   'visual',\n",
       "   'task',\n",
       "   'include',\n",
       "   'deepfake',\n",
       "   'detection',\n",
       "   'and',\n",
       "   'combine',\n",
       "   'self',\n",
       "   'supervise',\n",
       "   'feature',\n",
       "   'generate',\n",
       "   'by',\n",
       "   'fm',\n",
       "   'with',\n",
       "   'CNN',\n",
       "   'base',\n",
       "   'classifier',\n",
       "   'have',\n",
       "   'result',\n",
       "   'in',\n",
       "   'significant',\n",
       "   'performance',\n",
       "   'gain',\n",
       "   'however',\n",
       "   'take',\n",
       "   'advantage',\n",
       "   'of',\n",
       "   'multiple',\n",
       "   'map',\n",
       "   'of',\n",
       "   'self',\n",
       "   'supervise',\n",
       "   'feature',\n",
       "   'be',\n",
       "   'not',\n",
       "   'as',\n",
       "   'straightforward',\n",
       "   'as',\n",
       "   'just',\n",
       "   'add',\n",
       "   'more',\n",
       "   'channel',\n",
       "   'to',\n",
       "   'the',\n",
       "   'classifier',\n",
       "   'therefore',\n",
       "   'this',\n",
       "   'work',\n",
       "   'explore',\n",
       "   'ensemble',\n",
       "   'technique',\n",
       "   'to',\n",
       "   'effectively',\n",
       "   'utilize',\n",
       "   'these',\n",
       "   'diverse',\n",
       "   'selfsupervise',\n",
       "   'feature',\n",
       "   'map',\n",
       "   'for',\n",
       "   'realistic',\n",
       "   'facial',\n",
       "   'deepfake',\n",
       "   'detection',\n",
       "   'our',\n",
       "   'experiment',\n",
       "   'indicate',\n",
       "   'that',\n",
       "   'combine',\n",
       "   'the',\n",
       "   'output',\n",
       "   'result',\n",
       "   'of',\n",
       "   'different',\n",
       "   'classifier',\n",
       "   'each',\n",
       "   'one',\n",
       "   'utilize',\n",
       "   'a',\n",
       "   'single',\n",
       "   'map',\n",
       "   'of',\n",
       "   'self',\n",
       "   'supervise',\n",
       "   'feature',\n",
       "   'lead',\n",
       "   'to',\n",
       "   'significant',\n",
       "   'performance',\n",
       "   'improvement',\n",
       "   'and',\n",
       "   'several',\n",
       "   'committee',\n",
       "   'approach',\n",
       "   'consistently',\n",
       "   'outperform',\n",
       "   'individual',\n",
       "   'classifier',\n",
       "   'demonstrate',\n",
       "   'the',\n",
       "   'potential',\n",
       "   'of',\n",
       "   'these',\n",
       "   'method',\n",
       "   'in',\n",
       "   'enhance',\n",
       "   'deepfake',\n",
       "   'detection',\n",
       "   'accuracy',\n",
       "   'keyword',\n",
       "   'deep',\n",
       "   'fake',\n",
       "   'detection',\n",
       "   'self',\n",
       "   'supervise',\n",
       "   'vision',\n",
       "   'transformer',\n",
       "   'deep',\n",
       "   'learning',\n",
       "   'foundation',\n",
       "   'model',\n",
       "   '1',\n",
       "   'introduction',\n",
       "   'in',\n",
       "   'recent',\n",
       "   'year',\n",
       "   'there',\n",
       "   'have',\n",
       "   'be',\n",
       "   'an',\n",
       "   'increase',\n",
       "   'focus',\n",
       "   'on',\n",
       "   'the',\n",
       "   'effect',\n",
       "   'of',\n",
       "   'deepfake',\n",
       "   'multimedia',\n",
       "   'content',\n",
       "   'on',\n",
       "   'public',\n",
       "   'discourse',\n",
       "   'and',\n",
       "   'personal',\n",
       "   'life',\n",
       "   'this',\n",
       "   'have',\n",
       "   'manifest',\n",
       "   'in',\n",
       "   'form',\n",
       "   'such',\n",
       "   'as',\n",
       "   'explicit',\n",
       "   'fake',\n",
       "   'image',\n",
       "   'of',\n",
       "   'celebrity',\n",
       "   '22',\n",
       "   'or',\n",
       "   'politically',\n",
       "   'sensitive',\n",
       "   'material',\n",
       "   'facilitate',\n",
       "   'the',\n",
       "   'in',\n",
       "   'proceeding',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'web',\n",
       "   'WebMe',\n",
       "   'dia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   '2024',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'sbc',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '2753',\n",
       "   'Sérgio',\n",
       "   'Colcher',\n",
       "   'colcher@inf.puc-rio.br',\n",
       "   'Telemidia',\n",
       "   'Lab',\n",
       "   'Pontifical',\n",
       "   'Catholic',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Rio',\n",
       "   'de',\n",
       "   'Janeiro',\n",
       "   'spread',\n",
       "   'of',\n",
       "   'misinformation',\n",
       "   'and',\n",
       "   'identity',\n",
       "   'theft',\n",
       "   'and',\n",
       "   'possibly',\n",
       "   'lead',\n",
       "   'to',\n",
       "   'threat',\n",
       "   'of',\n",
       "   'violence',\n",
       "   '14',\n",
       "   'this',\n",
       "   'scenario',\n",
       "   'have',\n",
       "   'renew',\n",
       "   'societal',\n",
       "   'discussion',\n",
       "   'on',\n",
       "   'the',\n",
       "   'nature',\n",
       "   'of',\n",
       "   'online',\n",
       "   'content',\n",
       "   'and',\n",
       "   'lead',\n",
       "   'to',\n",
       "   'legislative',\n",
       "   'debate',\n",
       "   'in',\n",
       "   'country',\n",
       "   'such',\n",
       "   'as',\n",
       "   'the',\n",
       "   'United',\n",
       "   'States',\n",
       "   'on',\n",
       "   'reduce',\n",
       "   'the',\n",
       "   'creation',\n",
       "   'and',\n",
       "   'distribution',\n",
       "   'of',\n",
       "   'this',\n",
       "   'kind',\n",
       "   'of',\n",
       "   'medium',\n",
       "   '3',\n",
       "   'Concurrent',\n",
       "   'with',\n",
       "   'the',\n",
       "   'advancement',\n",
       "   'of',\n",
       "   'deepfake',\n",
       "   'generator',\n",
       "   'significant',\n",
       "   'progress',\n",
       "   'have',\n",
       "   'also',\n",
       "   'be',\n",
       "   'make',\n",
       "   'in',\n",
       "   'effort',\n",
       "   'to',\n",
       "   'improve',\n",
       "   'the',\n",
       "   'detection',\n",
       "   'and',\n",
       "   'containment',\n",
       "   'of',\n",
       "   'this',\n",
       "   'kind',\n",
       "   'of',\n",
       "   'content',\n",
       "   'this',\n",
       "   'start',\n",
       "   'with',\n",
       "   'the',\n",
       "   'cre',\n",
       "   'ation',\n",
       "   'of',\n",
       "   'large',\n",
       "   'scale',\n",
       "   'dataset',\n",
       "   'for',\n",
       "   'deepfake',\n",
       "   'detection',\n",
       "   'in',\n",
       "   'video',\n",
       "   'and',\n",
       "   'image',\n",
       "   'format',\n",
       "   '10',\n",
       "   '21',\n",
       "   'enable',\n",
       "   'the',\n",
       "   'development',\n",
       "   'of',\n",
       "   'method',\n",
       "   'to',\n",
       "   'well',\n",
       "   'differentiate',\n",
       "   'between',\n",
       "   'real',\n",
       "   'and',\n",
       "   'fake',\n",
       "   'content',\n",
       "   'in',\n",
       "   'this',\n",
       "   'context',\n",
       "   'CNN',\n",
       "   'base',\n",
       "   'model',\n",
       "   'show',\n",
       "   'initial',\n",
       "   'success',\n",
       "   'and',\n",
       "   'their',\n",
       "   'combination',\n",
       "   'with',\n",
       "   'Vision',\n",
       "   'Transformers',\n",
       "   'ViT',\n",
       "   'manage',\n",
       "   'to',\n",
       "   'achieve',\n",
       "   'state',\n",
       "   'of',\n",
       "   'art',\n",
       "   'performance',\n",
       "   'in',\n",
       "   'recent',\n",
       "   'year',\n",
       "   '13',\n",
       "   'also',\n",
       "   'the',\n",
       "   'recent',\n",
       "   'emergence',\n",
       "   'of',\n",
       "   'Foundation',\n",
       "   'Models',\n",
       "   'FMs',\n",
       "   'open',\n",
       "   'the',\n",
       "   'door',\n",
       "   'for',\n",
       "   'new',\n",
       "   'approach',\n",
       "   'to',\n",
       "   'deepfake',\n",
       "   'detection',\n",
       "   'in',\n",
       "   'this',\n",
       "   'light',\n",
       "   'study',\n",
       "   'have',\n",
       "   'demonstrate',\n",
       "   'that',\n",
       "   'use',\n",
       "   'self',\n",
       "   'supervise',\n",
       "   'feature',\n",
       "   'generate',\n",
       "   'by',\n",
       "   'pre',\n",
       "   'train',\n",
       "   'fm',\n",
       "   'in',\n",
       "   'combination',\n",
       "   'with',\n",
       "   'CNN',\n",
       "   'base',\n",
       "   'model',\n",
       "   'can',\n",
       "   'result',\n",
       "   'in',\n",
       "   'significant',\n",
       "   'performance',\n",
       "   'improvement',\n",
       "   '12',\n",
       "   'in',\n",
       "   'this',\n",
       "   'approach',\n",
       "   'the',\n",
       "   'author',\n",
       "   'incorporate',\n",
       "   'feature',\n",
       "   'map',\n",
       "   'generate',\n",
       "   'by',\n",
       "   'fm',\n",
       "   'as',\n",
       "   'extra',\n",
       "   'channel',\n",
       "   'in',\n",
       "   'CNN',\n",
       "   'input',\n",
       "   'show',\n",
       "   'that',\n",
       "   'when',\n",
       "   'use',\n",
       "   'individually',\n",
       "   'each',\n",
       "   'map',\n",
       "   'would',\n",
       "   'improve',\n",
       "   'the',\n",
       "   'performance',\n",
       "   'of',\n",
       "   'its',\n",
       "   'base',\n",
       "   'model',\n",
       "   'however',\n",
       "   'it',\n",
       "   'have',\n",
       "   'also',\n",
       "   'be',\n",
       "   'show',\n",
       "   'by',\n",
       "   'Gomes',\n",
       "   'et',\n",
       "   'al',\n",
       "   '12',\n",
       "   'that',\n",
       "   'simultaneously',\n",
       "   'apply',\n",
       "   'multiple',\n",
       "   'attention',\n",
       "   'map',\n",
       "   'to',\n",
       "   'the',\n",
       "   'same',\n",
       "   'model',\n",
       "   'by',\n",
       "   'simply',\n",
       "   'add',\n",
       "   'more',\n",
       "   'channel',\n",
       "   'to',\n",
       "   'the',\n",
       "   'input',\n",
       "   'do',\n",
       "   'not',\n",
       "   'necessarily',\n",
       "   'improve',\n",
       "   'performance',\n",
       "   'leave',\n",
       "   'open',\n",
       "   'the',\n",
       "   'following',\n",
       "   'question',\n",
       "   'be',\n",
       "   'there',\n",
       "   'a',\n",
       "   'way',\n",
       "   'to',\n",
       "   'well',\n",
       "   'combine',\n",
       "   'multiple',\n",
       "   'attention',\n",
       "   'map',\n",
       "   'to',\n",
       "   'achieve',\n",
       "   'superior',\n",
       "   'deepfake',\n",
       "   'detection',\n",
       "   'performance',\n",
       "   'to',\n",
       "   'pursue',\n",
       "   'this',\n",
       "   'question',\n",
       "   'we',\n",
       "   'conduct',\n",
       "   'experiment',\n",
       "   'with',\n",
       "   'DINOv2',\n",
       "   'FM',\n",
       "   '18',\n",
       "   'and',\n",
       "   'the',\n",
       "   'CNN',\n",
       "   'base',\n",
       "   'XceptionNet',\n",
       "   '6',\n",
       "   'choose',\n",
       "   'for',\n",
       "   'their',\n",
       "   'establish',\n",
       "   'use',\n",
       "   'and',\n",
       "   'strong',\n",
       "   'performance',\n",
       "   'in',\n",
       "   'this',\n",
       "   'task',\n",
       "   'initially',\n",
       "   'we',\n",
       "   'evaluate',\n",
       "   'each',\n",
       "   'self',\n",
       "   'supervise',\n",
       "   'feature',\n",
       "   'map',\n",
       "   'generate',\n",
       "   'by',\n",
       "   'DINOv2',\n",
       "   'individually',\n",
       "   'result',\n",
       "   'in',\n",
       "   'three',\n",
       "   'distinct',\n",
       "   'model',\n",
       "   'next',\n",
       "   'we',\n",
       "   'try',\n",
       "   'multiple',\n",
       "   'committee',\n",
       "   'approach',\n",
       "   'to',\n",
       "   'combine',\n",
       "   'these',\n",
       "   'model',\n",
       "   'prediction',\n",
       "   'for',\n",
       "   'our',\n",
       "   'experiment',\n",
       "   'we',\n",
       "   'use',\n",
       "   'both',\n",
       "   'the',\n",
       "   'Deepfake',\n",
       "   'Detection',\n",
       "   'Challenge',\n",
       "   'DFDC',\n",
       "   '10',\n",
       "   'and',\n",
       "   'Face',\n",
       "   'Forensics',\n",
       "   '21',\n",
       "   'dataset',\n",
       "   'in',\n",
       "   'both',\n",
       "   '37',\n",
       "   'WebMedia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Cunha',\n",
       "   'et',\n",
       "   'al',\n",
       "   'dataset',\n",
       "   'our',\n",
       "   'propose',\n",
       "   'committee',\n",
       "   'perform',\n",
       "   'well',\n",
       "   'than',\n",
       "   'use',\n",
       "   'a',\n",
       "   'single',\n",
       "   'attention',\n",
       "   'map',\n",
       "   'while',\n",
       "   'the',\n",
       "   'performance',\n",
       "   'gain',\n",
       "   'varied',\n",
       "   'depend',\n",
       "   'on',\n",
       "   'the',\n",
       "   'specific',\n",
       "   'committee',\n",
       "   'approach',\n",
       "   'employ',\n",
       "   'the',\n",
       "   'remainder',\n",
       "   'of',\n",
       "   'this',\n",
       "   'paper',\n",
       "   'be',\n",
       "   'organize',\n",
       "   'as',\n",
       "   'follow',\n",
       "   'section',\n",
       "   'section',\n",
       "   '2',\n",
       "   'cover',\n",
       "   'relate',\n",
       "   'work',\n",
       "   'on',\n",
       "   'deepfake',\n",
       "   'generation',\n",
       "   'and',\n",
       "   'detection',\n",
       "   'as',\n",
       "   'well',\n",
       "   'as',\n",
       "   'research',\n",
       "   'on',\n",
       "   'committee',\n",
       "   'approach',\n",
       "   'for',\n",
       "   'deep',\n",
       "   'learning',\n",
       "   'model',\n",
       "   'section',\n",
       "   '3',\n",
       "   'describe',\n",
       "   'the',\n",
       "   'employ',\n",
       "   'architecture',\n",
       "   'and',\n",
       "   'detail',\n",
       "   'the',\n",
       "   'different',\n",
       "   'committee',\n",
       "   'approach',\n",
       "   'propose',\n",
       "   'for',\n",
       "   'this',\n",
       "   'task',\n",
       "   'section',\n",
       "   '4',\n",
       "   'discuss',\n",
       "   'how',\n",
       "   'we',\n",
       "   'conduct',\n",
       "   'our',\n",
       "   'experimental',\n",
       "   'methodology',\n",
       "   'include',\n",
       "   'specific',\n",
       "   'on',\n",
       "   'datum',\n",
       "   'setup',\n",
       "   'and',\n",
       "   'result',\n",
       "   'analysis',\n",
       "   'finally',\n",
       "   'Section',\n",
       "   '5',\n",
       "   'present',\n",
       "   'our',\n",
       "   'final',\n",
       "   'thought',\n",
       "   'and',\n",
       "   'outline',\n",
       "   'possible',\n",
       "   'direction',\n",
       "   'for',\n",
       "   'further',\n",
       "   'research',\n",
       "   'in',\n",
       "   'this',\n",
       "   'field',\n",
       "   '2',\n",
       "   'relate',\n",
       "   'work',\n",
       "   'in',\n",
       "   'this',\n",
       "   'section',\n",
       "   'we',\n",
       "   'go',\n",
       "   'over',\n",
       "   'other',\n",
       "   'important',\n",
       "   'research',\n",
       "   'that',\n",
       "   'relate',\n",
       "   'to',\n",
       "   'the',\n",
       "   'topic',\n",
       "   'cover',\n",
       "   'in',\n",
       "   'this',\n",
       "   'work',\n",
       "   'subsection',\n",
       "   '2.1',\n",
       "   'discuss',\n",
       "   'recent',\n",
       "   'advance',\n",
       "   'in',\n",
       "   'deepfake',\n",
       "   'detection',\n",
       "   'focus',\n",
       "   'on',\n",
       "   'methodology',\n",
       "   'aim',\n",
       "   'at',\n",
       "   'counter',\n",
       "   'the',\n",
       "   'generation',\n",
       "   'of',\n",
       "   'synthetic',\n",
       "   'medium',\n",
       "   'subsection',\n",
       "   '2.2',\n",
       "   'explore',\n",
       "   'diverse',\n",
       "   'work',\n",
       "   'relate',\n",
       "   'to',\n",
       "   'the',\n",
       "   'combination',\n",
       "   'of',\n",
       "   'ensemble',\n",
       "   'approach',\n",
       "   'and',\n",
       "   'CNN',\n",
       "   'model',\n",
       "   'for',\n",
       "   'image',\n",
       "   'processing',\n",
       "   'enhance',\n",
       "   'model',\n",
       "   'robustness',\n",
       "   'accuracy',\n",
       "   'and',\n",
       "   'generalization',\n",
       "   'across',\n",
       "   'diverse',\n",
       "   'dataset',\n",
       "   'and',\n",
       "   'task',\n",
       "   '2.1',\n",
       "   'Methods',\n",
       "   'for',\n",
       "   'Facial',\n",
       "   'Deepfake',\n",
       "   'Detection',\n",
       "   'recent',\n",
       "   'year',\n",
       "   'have',\n",
       "   'see',\n",
       "   'a',\n",
       "   'large',\n",
       "   'increase',\n",
       "   'in',\n",
       "   'effort',\n",
       "   'dedicate',\n",
       "   'to',\n",
       "   'detect',\n",
       "   'realistic',\n",
       "   'deepfake',\n",
       "   'and',\n",
       "   'differentiate',\n",
       "   'they',\n",
       "   'from',\n",
       "   'genuine',\n",
       "   'facial',\n",
       "   'medium',\n",
       "   'with',\n",
       "   'video',\n",
       "   'and',\n",
       "   'image',\n",
       "   'be',\n",
       "   'the',\n",
       "   'dominant',\n",
       "   'target',\n",
       "   'of',\n",
       "   'these',\n",
       "   'work',\n",
       "   '23',\n",
       "   'this',\n",
       "   'progress',\n",
       "   'have',\n",
       "   'be',\n",
       "   'enable',\n",
       "   'by',\n",
       "   'the',\n",
       "   'production',\n",
       "   'of',\n",
       "   'a',\n",
       "   'multitude',\n",
       "   'of',\n",
       "   'recent',\n",
       "   'dataset',\n",
       "   'for',\n",
       "   'the',\n",
       "   'task',\n",
       "   'such',\n",
       "   'as',\n",
       "   'the',\n",
       "   'Deepfake',\n",
       "   'Detection',\n",
       "   'Challenge',\n",
       "   'DFDC',\n",
       "   '10',\n",
       "   'Celeb',\n",
       "   'DF',\n",
       "   '16',\n",
       "   'FaceForensics',\n",
       "   'and',\n",
       "   'FaceForensics++',\n",
       "   '21',\n",
       "   'dataset',\n",
       "   'these',\n",
       "   'dataset',\n",
       "   'contain',\n",
       "   'large',\n",
       "   'volume',\n",
       "   'of',\n",
       "   'visual',\n",
       "   'datum',\n",
       "   'with',\n",
       "   'a',\n",
       "   'variety',\n",
       "   'of',\n",
       "   'deepfake',\n",
       "   'technique',\n",
       "   'and',\n",
       "   'an',\n",
       "   'increasing',\n",
       "   'though',\n",
       "   'still',\n",
       "   'insufficient',\n",
       "   'concern',\n",
       "   'for',\n",
       "   'demographic',\n",
       "   'fairness',\n",
       "   '27',\n",
       "   'allow',\n",
       "   'their',\n",
       "   'use',\n",
       "   'for',\n",
       "   'technique',\n",
       "   'that',\n",
       "   'deal',\n",
       "   'with',\n",
       "   'both',\n",
       "   ...]},\n",
       " {'titulo': 'A Machine-Learning-Driven Fast Video-based Point Cloud Compression (V-PCC)',\n",
       "  'informacoes_url': '',\n",
       "  'idioma': 'english',\n",
       "  'storage_key': '../articles/original/english/985-24739-1-10-20240923.pdf',\n",
       "  'author': 'Gustavo Rehbein; Eduardo Costa; Guilherme Corrêa; Cristiano Santos; and Marcelo Porto',\n",
       "  'data_publicacao': '11-09-2024',\n",
       "  'resumo': 'In recent years, 3D point cloud content has gained attention due to its application possibilities, such as multimedia systems, virtual, augmented, and mixed reality, through the mapping and visualization of environments and/or 3D objects, real-time immersive communications, and autonomous driving systems. However, raw point clouds demand a large amount of data for their representation, and compression is mandatory to allow efficient transmission and storage. The MPEG group proposed the Video-based Point Cloud Compression (V-PCC) standard, which is a dynamic point cloud encoder based on the use of video encoders through projections into 2D space. However, V-PCC demands a high computational cost, demanding fast implementations for real-time processing and, especially, for mobile device applications. In this paper, a machinelearning-based fast implementation of V-PCC is proposed, where the main approach is the use of trained decision trees to speed up the block partitioning process during the point cloud compression. The results show that the proposed fast V-PCC solution is able to achieve an encoding time reduction of 42.73% for the geometry video sub-stream and 55.3% for the attribute video sub-stream, with a minimal impact on bitrate and objective quality. ###',\n",
       "  'keywords': 'point clouds, machine learning, V-PCC, complexity reduction',\n",
       "  'referencias': ['[1] Gisle Bjontegaard. 2001. Calculation of average PSNR differences between RDcurves. *ITU SG16 Doc. VCEG-M33* (2001).',\n",
       "   '[2] Benjamin Bross, Ye-Kui Wang, Yan Ye, Shan Liu, Jianle Chen, Gary J Sullivan,\\nand Jens-Rainer Ohm. 2021. Overview of the versatile video coding (VVC)\\nstandard and its applications. *IEEE Transactions on Circuits and Systems for Video*\\n*Technology* 31, 10 (2021), 3736–3764.',\n",
       "   '[3] Guilherme Correa, Pedro Assuncao, Luis A. da Silva Cruz, and Luciano Agostini.\\n2014. Classification-based early termination for coding tree structure decision\\nin HEVC. In *2014 21st IEEE International Conference on Electronics, Circuits and*\\n*Systems (ICECS)* . 239–242.',\n",
       "   '[4] Guilherme Correa, Pedro A. Assuncao, Luciano Volcan Agostini, and Luis A.\\nda Silva Cruz. 2015. Fast HEVC Encoding Decisions Using Data Mining. *IEEE*\\n*Transactions on Circuits and Systems for Video Technology* 25, 4 (2015), 660–673.',\n",
       "   '[5] Tianyu Dong, Kyutae Kim, and Euee S. Jang. 2021. Performance Evaluation of\\nthe Codec Agnostic Approach in MPEG-I Video-Based Point Cloud Compression.\\n*IEEE Access* 9 (2021), 167990–168003.',\n",
       "   '[6] Jay Fenlason. 2024. Gprof. https://ftp.gnu.org/old-gnu/Manuals/gprof-2.9.1/\\nhtml_mono/gprof.html. Accessed: 2024-04-15.',\n",
       "   '[7] Wei Gao, Hang Yuan, Ge Li, Zhu Li, and Hui Yuan. 2023. Low Complexity Coding\\nUnit Decision for Video-Based Point Cloud Compression. *IEEE Transactions on*\\n\\n\\n*Image Processing* 33 (2023), 149–162.',\n",
       "   '[8] Danillo Graziosi, Ohji Nakagami, Satoru Kuma, Alexandre Zaghetto, Teruhiko\\nSuzuki, and Ali Tabatabai. 2020. An overview of ongoing point cloud compression\\nstandardization activities: Video-based (V-PCC) and geometry-based (G-PCC).\\n*APSIPA Transactions on Signal and Information Processing* 9 (2020), e13.',\n",
       "   '[9] Dan Grois, Detlev Marpe, Amit Mulayoff, Benaya Itzhaky, and Ofer Hadar. 2013.\\nPerformance comparison of H.265/MPEG-HEVC, VP9, and H.264/MPEG-AVC\\nencoders. In *2013 Picture Coding Symposium (PCS)* . 394–397.',\n",
       "   '[10] Il-Koo Kim, Junghye Min, Tammy Lee, Woo-Jin Han, and JeongHoon Park. 2012.\\nBlock partitioning structure in the HEVC standard. *IEEE transactions on circuits*\\n*and systems for video technology* 22, 12 (2012), 1697–1706.',\n",
       "   '[11] Tianyi Li, Mai Xu, and Xin Deng. 2017. A deep convolutional neural network approach for complexity reduction on intra-mode HEVC. In *2017 IEEE International*\\n*Conference on Multimedia and Expo (ICME)* . IEEE, 1255–1260.',\n",
       "   '[12] Yue Li, Jun Huang, Chaofeng Wang, and Hongyue Huang. 2024. Unsupervised\\nlearning-based fast CU size decision for geometry videos in V-PCC. *Journal of*\\n*Real-Time Image Processing* 21, 1 (2024), 11.',\n",
       "   '[13] MPEG. 2020. Common Test Conditions for V3C and V-PCC. *ISO/IEC JTC 1/SC*\\n*29/WG 11* (2020).',\n",
       "   '[14] MPEG. 2024. Video Point Cloud Compression - VPCC - mpeg-pcc-tmc2 test\\nmodel candidate software. https://github.com/MPEGGroup/mpeg-pcc-tmc2.',\n",
       "   '[15] Sang-hyo Park and Je-Won Kang. 2020. Fast multi-type tree partitioning for\\nversatile video coding using a lightweight neural network. *IEEE Transactions on*\\n*Multimedia* 23 (2020), 4388–4399.',\n",
       "   '[16] Marius Preda. 2020. V-PCC codec description. *ISO/IEC JTC 1/SC 29/WG 7, Virtual*\\n(2020).',\n",
       "   '[17] R Schaefer. 2017. Call for proposals for point cloud compression V2. In *ISO/IEC*\\n*JTC1 SC29/WG11 MPEG, 117th Meeting. Hobart, TAS* .',\n",
       "   '[18] Yun Song, Biao Zeng, Miaohui Wang, and Zelin Deng. 2022. An efficient lowcomplexity block partition scheme for VVC intra coding. *Journal of Real-Time*\\n*Image Processing* (2022), 1–12.',\n",
       "   '[19] Gary J Sullivan, Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand. 2012.\\nOverview of the high efficiency video coding (HEVC) standard. *IEEE Transactions*\\n*on circuits and systems for video technology* 22, 12 (2012), 1649–1668.',\n",
       "   '[20] Vivienne Sze, Madhukar Budagavi, and Gary J Sullivan. 2014. High efficiency\\nvideo coding (HEVC). In *Integrated circuit and systems, algorithms and architec-*\\n*tures* . Vol. 39. Springer, 40.',\n",
       "   '[21] Alexandre Tissier, Wassim Hamidouche, Jarno Vanne, F Galpin, and Daniel\\nMenard. 2020. CNN oriented complexity reduction of VVC intra encoder. In *2020*\\n*IEEE International Conference on Image Processing (ICIP)* . IEEE, 3139–3143.',\n",
       "   '[22] Yihan Wang, Yongfang Wang, Tengyao Cui, and Zhijun Fang. 2024. Fast VideoBased Point Cloud Compression Based on Early Termination and Transformer\\nModel. *IEEE Transactions on Emerging Topics in Computational Intelligence* (2024).',\n",
       "   '[23] Natasha Westland, André Seixas Dias, and Marta Mrak. 2019. Decision trees for\\ncomplexity reduction in video compression. In *2019 IEEE International Conference*\\n*on Image Processing (ICIP)* . IEEE, 2666–2670.',\n",
       "   '[24] Yun Zhang, Sam Kwong, and Shiqi Wang. 2020. Machine learning based video\\ncoding optimizations: A survey. *Information Sciences* 506 (2020), 395–423.\\n\\n\\n27\\n\\n\\n-----'],\n",
       "  'text': '# **A Machine-Learning-Driven Fast Video-based Point Cloud** **Compression (V-PCC)**\\n\\n## Gustavo Rehbein\\n#### ghrehbein@inf.ufpel.edu.br Universidade Federal de Pelotas Video Technology Research Group – Vitech Graduate Program in Computing Pelotas, Brazil\\n\\n## Eduardo Costa\\n#### edfcosta@inf.ufpel.edu.br Universidade Federal de Pelotas Video Technology Research Group – Vitech Pelotas, Brazil\\n\\n## Guilherme Corrêa\\n#### gcorrea@inf.ufpel.edu.br Universidade Federal de Pelotas Video Technology Research Group – Vitech Graduate Program in Computing Pelotas, Brazil\\n\\n## Cristiano Santos\\n#### cfdsantos@inf.ufpel.edu.br Universidade Federal de Pelotas Video Technology Research Group – Vitech Graduate Program in Computing Pelotas, Brazil\\n### **ABSTRACT**\\n\\nIn recent years, 3D point cloud content has gained attention due\\nto its application possibilities, such as multimedia systems, virtual,\\naugmented, and mixed reality, through the mapping and visualization of environments and/or 3D objects, real-time immersive\\ncommunications, and autonomous driving systems. However, raw\\npoint clouds demand a large amount of data for their representation,\\nand compression is mandatory to allow efficient transmission and\\nstorage. The MPEG group proposed the Video-based Point Cloud\\nCompression (V-PCC) standard, which is a dynamic point cloud\\nencoder based on the use of video encoders through projections\\ninto 2D space. However, V-PCC demands a high computational\\ncost, demanding fast implementations for real-time processing and,\\nespecially, for mobile device applications. In this paper, a machinelearning-based fast implementation of V-PCC is proposed, where\\nthe main approach is the use of trained decision trees to speed up\\nthe block partitioning process during the point cloud compression.\\nThe results show that the proposed fast V-PCC solution is able to\\nachieve an encoding time reduction of 42.73% for the geometry\\nvideo sub-stream and 55.3% for the attribute video sub-stream, with\\na minimal impact on bitrate and objective quality.\\n### **KEYWORDS**\\n\\npoint clouds, machine learning, V-PCC, complexity reduction\\n### **1 INTRODUCTION**\\n\\nPoint clouds can be used in many applications, such as 3D environment mapping, representation of historical objects or monuments,\\nand virtual, augmented, and mixed realities, providing detailed 3D\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n© 2024 SBC – Brazilian Computing Society.\\nISSN 2966-2753\\n\\n## Marcelo Porto\\n#### porto@inf.ufpel.edu.br Universidade Federal de Pelotas Video Technology Research Group – Vitech Graduate Program in Computing Pelotas, Brazil\\n\\nrepresentation and multiple points of view of objects and scenes. Regarding the application context, point clouds can be categorized into\\nthree types: (Category 1) static, which represents the capture of a\\nscene or object at a single time instant; (Category 2) dynamic, when\\na sequence of point clouds is captured over time; and (Category 3)\\ndynamically captured through multiple partially overlapping scans,\\ntypically during dynamic mapping of a large-scale environment.\\nNevertheless, as non-compressed point clouds comprise a large\\namount of data, storage and transmission become challenging tasks\\ndepending on the point cloud resolution and the device on which\\nit is processed. Thus, compression becomes mandatory to allow for\\nthe efficient transmission and storage of this type of 3D content.\\nThe Motion Picture Experts Group (MPEG) defined two Point Cloud\\nCompression (PCC) standard specifications: the Video-based PCC\\n(V-PCC) [ 16 ], related to dynamic point clouds, and the Geometrybased PCC (G-PCC), designed to compress static and dynamically\\nacquired point clouds [ 8 ]. While the V-PCC coding approach is\\nbased on 2D projections of the point clouds to be further compressed with a regular 2D video encoder, G-PCC encodes the content directly in the 3D space [8].\\nV-PCC is an interesting alternative to point clouds compression,\\nsince the already existing video encoders can be used. Although\\nV-PCC provides an efficient solution for compressing the data generated by point clouds, this encoder inherits the complexity of video\\nencoding. This occurs because V-PCC, after the point cloud 2D\\nprojection step, uses an HEVC [ 19 ] video encoder. In this process,\\nthe HEVC video encoder needs to handle three video sub-streams\\n\\ngenerated in the 2D projection step by V-PCC [ 16 ]. One video substream is generated to indicate the occupancy map, which shows\\nwhere points actually exist due to the sparse and irregular nature\\nof a point cloud. Additionally, a second sub-stream is necessary to\\nindicate the geometric position of each point, i.e., the position on\\nthe Z-axis or depth. A third sub-stream, called Attribute sub-stream,\\nindicates the color or appearance assigned to each point [16].\\n\\n\\n20\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Rehbein et al.\\n\\n\\nThere are many works in literature proposing the use of machine\\nlearning to reduce video compression complexity, as [ 21 ], [ 15 ],\\n\\n[ 24 ]. One of the most efficient approach in to predict the blocks\\npartitioning during the encoding process [ 3 ], [ 18 ], [ 23 ], [ 4 ]. This\\ntype of prediction prevents the encoder to evaluate smaller block\\nsizes, thereby reducing complexity and achieving time savings.\\nThis work proposes a machine-learning-based fast V-PCC implementation that employs an already trained model originally\\nproposed for 2D video compression [ 3 ], to reduce the complexity\\nof point cloud compression in the V-PCC standard. To this end,\\nthe use and adaptation of a pre-trained model to accelerate video\\ncompression is proposed as an alternative to speeding up the point\\ncloud compression process in the Test Model Category 2 (TMC2)\\n\\n[14], the reference software of the V-PCC standard.\\nThe machine learning model used achieved 84% accuracy on the\\ntesting dataset [ 3 ]. This enabled a 42.73% time reduction in the video\\nencoding step for the geometry sub-stream and a 55.3% reduction for\\nthe attributes sub-stream. The loss in coding efficiency, measured\\nwith the BD-Rate metric [ 1 ], is of 4.2% for geometry (using D2\\nmetric) and 3.2% for color attributes, respectively. The results show\\nthat while there is an impact on encoding efficiency, this can be\\noutweighed by the significant gains in complexity reduction.\\nThis paper is organized as follows: Section 2 presents related\\nwork, Section 3 introduces the V-PCC standard and its operation,\\nSection 4 outlines the proposed method, Section 5 presents the\\nresults of the experiments conducted, and Section 6 provides the\\nconclusions.\\n### **2 VIDEO-BASED POINT CLOUD** **COMPRESSION (V-PCC)**\\n\\nThe Video-based Point Cloud Compression (V-PCC) is a standard\\nfor point cloud compression proposed by MPEG to encode dynamic\\npoint clouds [ 17 ]. In other words, this encoder aims to compress\\npoint cloud content analogous to 2D videos, meaning that it has\\ntemporally adjacent frames that provide a sense of motion to the\\n\\ncontent.\\n\\nAs the name suggests, V-PCC is a video-based encoder, which\\nmeans it uses a video encoder in its encoding stages, by default the\\nHigh-Efficiency Video Coding (HEVC) [ 19 ][ 5 ]. However, to enable\\nencoding via HEVC, a 2D projection process of the point clouds is\\nperformed, which is illustrated in Figure 1. As can be seen in Figure\\n1, the 2D projection process is analogous to having virtual cameras\\nregistering parts of the point cloud (Figure 1 (a)), and combining\\nthose camera images into a mosaic, i.e. an image that contains the\\ncollection of projected 2D patches (Figure 1 (b)) [8].\\nThe encoder flow proposed by the V-PCC standard is illustrated\\nin Figure 2. It is possible to see that the 2D projection stages initially occur through the patch generation, which is responsible for\\nslicing the cloud into several parts. These patches then undergo\\nthe padding and packing process to assemble the 2D image. In this\\nprocess, three video sub-streams are generated: one for geometry\\ninformation, which maps the changes of the depth of each patch,\\ninformation which otherwise would be lost in the 2D projection\\nprocess (generated in Geometry image generation); a second for the\\noccupancy map information, used to inform where points exist in\\nthe 2D projection of the point cloud, which is necessary due to the\\n\\n\\n**Figure 1: 3D to 2D projection of point cloud in V-PCC. (a)**\\n**Point cloud projection into planes, (b) collection of patches**\\n\\nsparse nature of point clouds; and a third for the color information\\nof each point in the point cloud (Attribute image generation). The\\nlatter also goes through a process of smoothing the edges present\\nin the patches (Smoothing). After the 2D projection process is completed, these three video sub-streams (Figure 3) are sent to the\\nHEVC video encoder, which needs to be independently activated\\nto handle each of these sub-streams.\\nThe V-PCC standard offers two main configurations: All Intra\\n(AI) and Random Access (RA) [ 16 ]. In the AI configuration, each\\nframe is compressed independently, exploring redundancies only\\nwithin the current frame. This simplifies processing but may result\\nin lower compression efficiency. In contrast, the RA configuration\\nexplores redundancies between frames and can reuse data from\\nneighboring frames, allowing for more efficient compression with\\nhigher complexity.\\n### **2.1 HEVC**\\n\\nHigh-Efficiency Video Coding (HEVC) [ 20 ] is a video coding standard developed by the MPEG group and finalized in 2013. HEVC\\nbrought about a 50% increase in encoding efficiency compared to\\nits predecessor, H.264/MPEG-AVC [ 9 ]. A portion of these gains\\nis due to the use of a new Coding Tree Units (CTU) partitioning\\nscheme based on recursive quad-trees [ 10 ], which can range in size\\nfrom 8x8 to 64x64 samples. A CTU can be recursively partitioned\\ninto multiple Coding Units (CUs) until they reach a size of 8x8 (or\\n4x4 for chrominance samples). Figure 4 shows an example of the\\npartitioning of a 64x64 CTU along with the Coding Tree structure.\\nIn Figure 4, 1’s indicate that a subdivision (or split) of CUs occurred,\\nand 0’s indicate that there was no subdivision at that level.\\n### **2.2 V-PCC complexity analysis**\\n\\nAn analysis was conducted to identify the complex behavior of\\nV-PCC stages. For this, the Test Model Category 2 (TMC2) reference software for V-PCC jointly with the GNU profiling (G-Prof)\\nsoftware version 2.9.1 was used [ 6 ]. In these experiments, the Random Access temporal configuration was used to encode all 10-bit\\ntest sequences of the V-PCC Common Test Conditions (CTCs) [ 13 ],\\nfive point clouds sequences in total. All five bitrate configurations\\navailable on TMC2 (r1, r2, r3, r4, and r5) were used, r1 being the\\nconfiguration with the lowest bitrate, and r5 being the one with the\\nhighest bitrate and quality, respectively.\\nFigure 5 presents the results (in percentage) of the average time\\nfor each coding step, considering the five point cloud sequences.\\n\\n\\n21\\n\\n\\n-----\\n\\nA Machine-Learning-Driven Fast Video-based Point Cloud Compression (V-PCC) WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n**Figure 2: V-PCC encoder diagram [16].**\\n\\nin Figure 5. This complexity is observed through the sum of the\\n\"Video Encoder Steps\" and \"Video Encoder ME\" percentage values.\\nTable 1 presents the average results in percentage of time used\\nby HEVC to encode each type of video sub-stream (Occupation,\\nGeometry, and Attributes). It is possible to see that the encoding of\\nGeometry and Attribute videos accounts for approximately 99.18%\\nof the total time spent by HEVC, while the Occupation video encoding accounts for only 0.82%.\\n\\n\\n**(a)** **(b)** **(c)**\\n\\n**Figure 3: Example of an Occupation image (a), Geometry**\\n**image (b) and Attribute image (c) extracted from V-PCC.**\\n\\n\\n\\n**Table 1: Percentage of time on the HEVC encoding of each**\\n**sub-stream in V-PCC.**\\n\\nRate settin g Occu p ation Geometr y Attribute\\n\\nr1 0.59% 54.89% 44.52%\\n\\nr2 0.57% 55.10% 44.33%\\n\\nr3 0.53% 54.05% 45.42%\\n\\nr4 0.49% 53.36% 46.15%\\n\\nr5 1.95% 50.56% 47.49%\\n\\nAvera g e 0.82% 53.59% 45.58%\\n\\nIn this regard, improvements to reduce the video encoding complexity are primarily required to make real-time point cloud encoding feasible, especially on devices with limited processing power\\nand/or energy autonomy, such as smartphones, laptops, embedded\\nsystems, robotic devices, virtual and mixed-reality glasses, among\\nothers.\\n### **3 RELATED WORKS**\\n\\nThere are several related works in the literature focusing on complexity reduction in HEVC for accelerating video encoding with\\nmachine learning, such as [ 3 ] and [ 11 ], which target at complexity\\n\\n\\n\\n64x64\\n\\n32x32\\n\\n16x16\\n\\n\\n8x8\\n\\n**Figure 4: Example of a 64x64 CTU and Coding Tree structure**\\n**being split into smaller CUs (adapted from [10]).**\\n\\nThese results were separated into bitrate settings. In this analysis,\\nit was identified that the video encoding is the most complex stage\\namong the processes of the V-PCC standard. The HEVC demands\\n90% of the average encoding time of the V-PCC, as it can be seen\\n\\n\\n22\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Rehbein et al.\\n\\n\\n**GeneratePatchFrame** **GenerateSegments**\\n\\n**VideoEncoder Steps** **Others**\\n\\n**VideoEncoder ME**\\n\\n\\n**r1**\\n\\n**r2**\\n\\n**r3**\\n\\n**r4**\\n\\n**r5**\\n\\n**Avg.**\\n\\n\\n\\n\\n\\n|Col1|5.8 5.5 5.2 4.9 4.4 5.1|Col3|4 4 4 4 5 4|6.0 7.1 8.0 9.4 0.8 8.3|Col6|7. 7. 7. 7 7|6 6 7 .8 8.2 .8|3 3 3 3 3|7.3 6.7 6.2 5.2 34.0 5.9|Col11|\\n|---|---|---|---|---|---|---|---|---|---|---|\\n||||||||||||\\n||||||||||||\\n||||||||||||\\n||||||||||||\\n||||||||||||\\n||||||||||||\\n\\n\\n0 10 20 30 40 50 60 70 80 90 100\\n\\n**Figure 5: Encoding time percentage of high-complexity tools**\\n**of V-PCC.**\\n\\nreduction at the CTU level for 2D videos. Also, there are some\\nworks targeting at V-PCC complexity reduction, such as [ 22 ], [ 7 ],\\nand [ 12 ]. However, all these studies targeting V-PCC are limited to\\nthe All Intra temporal configuration.\\nIn [ 3 ], machine learning was applied to reduce complexity by\\nearly terminating the block size decision process of HEVC Coding Tree Units. The Random Access temporal configuration was\\nemployed in the experiments. The model proposed in [ 3 ] assigns\\nthe best block sizes based on the input frame characteristics, avoiding testing all possible block sizes (64x64, 32x32, and 16x16). The\\nwork achieved an average computational complexity reduction of\\n37% compared to the original encoder, with a marginal increase in\\nBD-Rate of only 0.28%.\\nIn [ 11 ], the author employs convolutional neural networks to\\nreduce the complexity of HEVC, predicting the decisions of the\\nCoding Tree Unit (CTU) in the All Intra temporal configuration.\\nThe model proposed in [ 11 ] was trained and tested using largescale database with diversiform patterns of CTU partition for each\\nCTU depth. This approach reduces encoding time by 62.25% with\\nBD-Rate increases of 2.12%.\\n\\nAmong the studies focused on machine learning to reduce the\\nV-PCC encoding time, [ 22 ] introduces a machine learning approach\\nfor early termination targeting complexity reduction of geometry\\nand attribute map coding at CU level. Instead of using the HEVC\\nvideo encoder, the authors opted for the Versatile Video Coding\\nstandard [ 2 ], due to its ability to maintain subjective image quality, although with computational complexity up to nineteen times\\nhigher than HEVC in the All Intra configuration. The work achieved\\napproximately 55% reduction in encoding time in V-PCC when using the modified VVC as the video encoder, compared to using VVC\\nwithout the proposed method.\\nIn [ 7 ], the author applies a cross-projection algorithm with ratedistortion-oriented decision-making at the geometry level of CU in\\nV-PCC, focusing solely on the All Intra configuration. This approach\\nreduced the average total encoding time by 57.8%, with BD-Rate\\n\\n\\nlosses for point-to-point (D1) and point-to-plane (D2) geometries\\nof 0.08% and 0.33%, respectively, and for luma attribute a BD-Rate\\nof 0.16%.\\n\\nIn [ 12 ], an unsupervised ML solution using hierarchical clustering is presented as a fast CU size decision method. Its training\\nwas based on the geometry stream, similar to other works aiming\\nto reduce complexity in V-PCC. The presented work achieved an\\naverage reduction in encoding time of 56.7% to 69.3%, with only a\\nslight increase in BD-Rate (D2), ranging from 0.1% to 0.5%.\\nTo the best of the authors’ knowledge, there are no works evaluating the impact of using machine learning models in the Random\\nAccess temporal configuration of V-PCC. The fast V-PCC implementation proposed in this paper is focused on Random Access\\ndue this configuration provides the best compression efficiency, but\\nalso the highest encoding complexity.\\n### **4 PROPOSED METHOD**\\n\\nAs the compression of 2D projections of point clouds in V-PCC is\\nperformed with a 2D video encoder, we explore the impact of using\\nexisting complexity reduction solutions, developed for reducing\\nthe encoding time of 2D videos, in the context of point clouds.\\nThus, we employ the machine learning models proposed in [ 3 ],\\nwhich utilizes decision trees to find the best Coding Tree for a\\nCTU, thereby shortening the block partitioning process. Once the\\nbest depth of a Coding Unit is found, it is no longer necessary\\nto continue testing at lower levels. This reduces the number of\\npossibilities tested during the Rate-Distortion Optimization (RDO)\\nprocess and consequently decreases the time required for encoding.\\nThe HEVC standard allows up to four Coding Trees depths: 0\\n(64x64 CU), 1 (32x32 CU), 2 (16x16 CU), and 3 (8x8 CU). Since it\\nis not possible to subdivide a Coding Unit at the last level, the\\nmachine learning models in [ 3 ] work over the first three depths.\\nThese models were trained using data extracted from test sequences\\nof 2D videos, encoded with HEVC in the Random Access (RA)\\ntemporal configuration. The results obtained in [ 3 ] show an average\\ncomplexity reduction of 37% with an increase in BD-Rate of 0.28%.\\nThese same machine learning models were now adopted for use\\nin the context of point cloud compression with V-PCC. To enable\\nthis, modifications were made to the version of the HEVC test\\nmodel (HM), the HEVC reference software [ 19 ], used in the V-PCC\\nreference software [ 16 ]. Since the version of HEVC used is newer\\nthan the one originally referenced in [ 3 ], precautions were taken to\\nensure equivalence between the versions. This step was necessary\\ndue to changes in the HEVC code. These models were employed in\\nthe encoding of color attributes and geometric sub-streams (Figure\\n2). Since more than 99% of the total V-PCC video encoding time\\nis related to these two video streaming, we chose not to use the\\nmodels in the encoding of the occupancy maps, since it does not\\nhave significant impact in the total encoding time of V-PCC, as\\nshown in Table 1.\\n### **5 EXPERIMENTAL RESULTS**\\n\\nTo evaluate the modifications made to the V-PCC reference software, experiments were conducted using the five 10-bit dynamic\\npoint clouds indicated in the V-PCC CTC [ 13 ]: *longdress*, *loot*, *queen*,\\n*redandblack*, and *soldier* . Figure 6 presents the first frame of each\\n\\n\\n23\\n\\n\\n-----\\n\\nA Machine-Learning-Driven Fast Video-based Point Cloud Compression (V-PCC) WebMedia’2024, Juiz de Fora, Brazil\\n\\n**Table 2: Experimental results obtained from the proposed method.**\\n\\n|Test Sequence|Geometry ETR (%) ΔBitrate (%) ΔPSNR (dB)|Attributes ETR (%) ΔBitrate (%) ΔPSNR (dB)|TTR (%)|\\n|---|---|---|---|\\n|longdress loot queen redandblack soldier|34.74 3.31 -0.128 40.06 3.28 -0.106 55.40 2.10 -0.160 34.45 4.03 -0.119 49.03 1.58 -0.150|43.02 -0.02 -0.089 65.37 -0.30 -0.050 59.26 -1.45 -0.325 46.31 0.15 -0.081 62.56 -0.57 -0.087|33.90 43.25 46.60 34.10 44.33|\\n|Average|42.73 2.86 -0.133|55.30 -0.44 -0.126|40.44|\\n\\n\\ntest point cloud sequence used in the experiments. Each test sequence was encoded with the modified and original V-PCC reference sofware, using the five bitrate configurations (r1, r2, r3, r4, and\\nr5), as specified in the CTC. The first 64 frames of each sequence\\nwere encoded using the Random Access temporal configuration of\\nV-PCC.\\n\\nFor each experiment, the Peak Signal-to-Noise Ratio (PSNR)\\nof point-to-point (D1) and point-to-plane (D2) metrics [ 13 ] were\\ncalculated over the geometry information of the reconstructed point\\nclouds. To evaluate the color information, the PSNR was calculated\\n\\non the luminance channel of the attributes. These metrics were\\n\\ncalculated using the Distortion Metric tool, as indicated in the CTC\\n\\n[ 13 ]. The experiments were conducted using version 22.1 of TMC2\\n\\n[ 14 ] on a computer with an AMD Opteron 8276s processor and\\n120GB of RAM.\\n\\nFigures 7-11 present the Rate-Distortion (RD) curves all 5 test\\nsequences used. In these figures, each point represents a V-PCC\\nencoding using one bitrate setting (r1 to r5) as well as the bitrate\\nresults (kbps) and objective quality (PSNR). By analyzing the five\\nsequences, it is possible to observe that the greatest impacts on\\nbitrate and objective quality were obtained in the *queen* sequence\\n(Figure 9), as can be seen by the larger gap between the RD curves.\\nIt is also noticeable that the best RD results were obtained in the\\n\\n*soldier* sequence, where both geometry-related curves (D1 and D2,\\nFigure 11 (a) and (b), respectively) and attribute curves (Figure 11\\n(c)) achieved results close to those obtained with the original VPCC. Figure 12 shows the average RD curves for the geometry and\\nattribute metrics. It is possible to see that the results for attributes\\npresent a lower loss in encoding efficiency compared to the results\\nof V-PCC without the complexity reduction solution implemented.\\n\\nlongdress loot queen redandblack soldier\\n\\n**Figure 6: First frame of the test sequences used.**\\n\\n\\nThe Encoding Time Reduction (ETR) results for each sub-stream\\nencoded and the variations in bitrate and PSNR for geometry and\\nattribute sub-streams encoding are presented in Table 2 for each\\ntest sequence. For the geometry video sub-streams, the proposed\\nmethod achieved an ETR ranging from 34.45% to 55.4%, with an\\naverage of 42.73%. For the attribute video sub-stream, an even higher\\nETR was obtained, ranging from 42.02% to 65.37%, with an average\\nof 55.3%. In both cases, the average reduction was above the value\\nobtained in [ 3 ], which was 37%. When considering the impact on\\nthe total encoding time of the V-PCC, that is, the sum of the total\\ntime spent by V-PCC, including the data reading, projection to the\\n2D space, and the time spent on the sub-streams HEVC encoding,\\nthe average Total Time Reduction (TTR) was 40.44%.\\nWhen evaluating the average bitrate impact on the encoded\\nvideos sub-streams, we observed an increase of 2.86% for geometry\\nand a bitrate decrease of 0.44% for attributes. This indicates that the\\n\\ndecisions made by the machine learning models performed better on\\nthe attribute sub-streams. This was expected, as the characteristics\\nof the attribute videos generated in V-PCC (Figure 3) are closer\\nto the content used (2D videos) when training the models. The\\nobjective quality (PSNR) results also support this hypothesis, with\\nslightly better outcomes for video attribute streams.\\nTable 3 presents the BD-Rate results for reconstructed point\\nclouds using the point-to-point (D1) and point-to-plane (D2) geometry metrics, as well as the BD-Rate results obtained from the\\nLuminance channel of the color attributes. The average BD-Rate\\nresults for the D1 and D2 metrics were 4.2% and 4.75%, respectively,\\nindicating a slightly negative impact on the encoding efficiency. The\\naverage impact on the attribute encoding efficiency was slightly\\nbetter.\\n\\n**Table 3: BD-Rate results.**\\n\\n|Test Sequence|D1 D2 Luma BD-Rate (%) BD-Rate (%) BD-Rate (%)|\\n|---|---|\\n|longdress loot queen redandblack soldier|4.631 5.615 2.348 4.536 4.960 1.259 3.907 4.189 8.580 5.526 6.174 2.255 2.400 2.816 1.606|\\n|Average|4.200 4.751 3.209|\\n\\n\\n\\nHowever, when analyzing the results obtained for each test sequence, it is noticeable that the Luma BD-Rate results for the *queen*\\nsequence show a discrepant value, with an 8.58%, compared to\\n\\n\\n24\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Rehbein et al.\\n\\n\\n|Col1|Col2|Col3|\\n|---|---|---|\\n||||\\n||||\\n||||\\n|V-PC||Ours|\\n||C|Ours|\\n||||\\n||||\\n\\n\\n0 8000 16000 24000 32000\\n\\nbitrate (kbps)\\n#### (c)\\n\\n\\n74\\n\\n72\\n\\n70\\n\\n68\\n\\n\\n\\n71\\n\\n70\\n\\n69\\n\\n68\\n\\n67\\n\\n66\\n\\n\\n\\n36\\n\\n34\\n\\n32\\n\\n30\\n\\n28\\n\\n26\\n\\n|Col1|Col2|\\n|---|---|\\n|||\\n|||\\n|||\\n|||\\n|V-PCC|Ours|\\n\\n\\n1500 3000 4500 6000 7500\\n\\nbitrate (kbps)\\n#### (a)\\n\\n\\n1500 3000 4500 6000 7500\\n\\nbitrate (kbps)\\n#### (b)\\n\\n\\n74\\n\\n72\\n\\n70\\n\\n76\\n\\n74\\n\\n72\\n\\n70\\n\\n\\n**Figure 7: RD curves of geometry D1 (a), D2 (b) and Luma (attribute) (c) obtained from sequence longdress.**\\n\\n72\\n\\n40\\n\\n71\\n\\n\\n|Col1|Col2|Col3|\\n|---|---|---|\\n||||\\n||||\\n||||\\n||||\\n|V-PCC||Ours|\\n||||\\n\\n\\n1500 3000 4500 6000\\n\\nbitrate (kbps)\\n#### (c)\\n\\n\\n|Col1|Col2|Col3|\\n|---|---|---|\\n||||\\n||||\\n||||\\n||||\\n|V-PCC|Our|s|\\n||||\\n\\n\\n2000 3000 4000 5000 6000\\n\\nbitrate (kbps)\\n#### (b)\\n\\n\\n2000 3000 4000 5000 6000\\n\\nbitrate (kbps)\\n#### (a)\\n\\n\\n70\\n\\n69\\n\\n68\\n\\n67\\n\\n\\n38\\n\\n36\\n\\n34\\n\\n32\\n\\n\\n**Figure 8: RD curves of geometry D1 (a), D2 (b) and Luma (attribute) (c) obtained from sequence loot.**\\n\\n72\\n\\n36\\n\\n\\n71\\n\\n70\\n\\n69\\n\\n68\\n\\n\\n|Col1|Col2|Col3|\\n|---|---|---|\\n||||\\n||||\\n||||\\n||||\\n||||\\n|V-PCC|||\\n||Ours||\\n||||\\n\\n\\n0 2500 5000 7500 10000 12500\\n\\nbitrate (kbps)\\n#### (c)\\n\\n\\n1600 2400 3200 4000 4800\\n\\nbitrate (kbps)\\n#### (a)\\n\\n\\n|Col1|Col2|\\n|---|---|\\n|||\\n|||\\n|||\\n|V-PCC|Ours|\\n\\n\\n1600 2400 3200 4000 4800\\n\\nbitrate (kbps)\\n#### (b)\\n\\n\\n35\\n\\n34\\n\\n33\\n\\n32\\n\\n31\\n\\n30\\n\\n\\n**Figure 9: RD curves of geometry D1 (a), D2 (b) and Luma (attribute) (c) obtained from sequence queen.**\\n\\n\\n1.26%-2.35% for the other point clouds. When analyzing the texture characteristics of the sequences used (Figure 6), it is evident\\nthat the other sequences ( *longdress*, *loot*, *redandblack*, and *soldier* )\\nfeature content captured from real life, while the *queen* sequence\\ncontains synthetic content. This may justify the discrepant values,\\n\\n\\nsince the machine learning models were trained with data from\\ntest sequences containing real-life content [3].\\nWhen comparing the BD-Rate results obtained in [ 3 ], which\\nshowed an increase of 0.28%, we notice that the change of context\\npresents significant impact. This indicates that although the models\\ntrained for 2D videos show interesting results in terms of time\\n\\n\\n25\\n\\n\\n-----\\n\\nA Machine-Learning-Driven Fast Video-based Point Cloud Compression (V-PCC) WebMedia’2024, Juiz de Fora, Brazil\\n\\n71\\n\\n74\\n\\n\\n38\\n\\n36\\n\\n34\\n\\n32\\n\\n\\n72\\n\\n70\\n\\n68\\n\\n\\n\\n70\\n\\n69\\n\\n68\\n\\n67\\n\\n66\\n\\n\\n|Col1|Col2|Col3|\\n|---|---|---|\\n||||\\n||||\\n||||\\n||V-PCC|Ours|\\n\\n\\n1500 3000 4500 6000 7500 9000\\n\\nbitrate (kbps)\\n#### (a)\\n\\n\\n1500 3000 4500 6000 7500 9000\\n\\nbitrate (kbps)\\n#### (b)\\n\\n\\n2500 5000 7500 10000 12500 15000\\n\\nbitrate (kbps)\\n#### (c)\\n\\n\\n**Figure 10: RD curves of geometry D1 (a), D2 (b) and Luma (attribute) (c) obtained from sequence redandblack.**\\n\\n\\n|Col1|Col2|\\n|---|---|\\n|||\\n|||\\n|||\\n|||\\n|V-PCC|Ours|\\n\\n\\n0 2500 5000 7500 10000\\n\\nbitrate (kbps)\\n#### (c)\\n\\n\\n74\\n\\n72\\n\\n70\\n\\n68\\n\\n71\\n\\n70\\n\\n69\\n\\n68\\n\\n67\\n\\n\\n71\\n\\n70\\n\\n69\\n\\n68\\n\\n67\\n\\n\\n1500 3000 4500 6000 7500\\n\\nbitrate (kbps)\\n#### (a)\\n\\n\\n1500 3000 4500 6000 7500\\n\\nbitrate (kbps)\\n#### (b)\\n\\n\\n38\\n\\n36\\n\\n34\\n\\n32\\n\\n30\\n\\n\\n**Figure 11: RD curves of geometry D1 (a), D2 (b) and Luma (attribute) (c) obtained from sequence soldier.**\\n\\n38\\n\\n74\\n\\n36\\n\\n\\n1500 3000 4500 6000 7500\\n\\nbitrate (kbps)\\n#### (b)\\n\\n\\n|Col1|Col2|Col3|\\n|---|---|---|\\n||||\\n||||\\n||||\\n||||\\n||||\\n|V-PCC||Ours|\\n\\n\\n0 3000 6000 9000 12000 15000\\n\\nbitrate (kbps)\\n#### (c)\\n\\n\\n1500 3000 4500 6000 7500\\n\\nbitrate (kbps)\\n#### (a)\\n\\n\\n72\\n\\n70\\n\\n68\\n\\n\\n34\\n\\n32\\n\\n30\\n\\n\\n**Figure 12: RD curve of average results of geometry D1 (a), D2 (b) and Luma (attribute) (c) obtained from experiments.**\\n\\n\\nsavings, they do not exhibit the same accuracy when used outside\\ntheir original context. Furthermore, the results suggest that due\\nto the unique characteristics of the geometry and attribute substreams, specialized models should achieve better results in both\\nencoding efficiency and ETR.\\n\\n\\nFigure 13 provides a visual comparison between the point clouds\\nreconstructed using the V-PCC reference software and our proposed\\nfast implementation. Figure 13 (a) and (b) show the first frame\\nof the *queen* sequence encoded with the r3 bitrate configuration.\\nFigure 13 (c) and (d) show the first frame of the *soldier* sequence\\nencoded with the r5 bitrate configuration. Note that in both cases\\n\\n\\n26\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Rehbein et al.\\n\\n**(a)** **(b)** **(c)** **(d)**\\n\\n**Figure 13: Reconstructed point clouds: queen using rate setting r3 ((a) V-PCC, (b) Our Fast V-PCC), and soldier using rate setting**\\n**r5 ((c) V-PCC, (d) Our Fast V-PCC)).**\\n\\n\\n(the test sequences with the highest and lowest impact on encoding\\nefficiency, respectively), there is no noticeable drop in visual quality.\\n### **6 CONCLUSION**\\n\\nThis work presented a solution for reducing the encoding time of\\nthe video encoding stage of the V-PCC reference software, utilizing\\nan existing machine learning model from the literature, trained for\\nthe context of 2D videos. This model was incorporated into the\\nfunctionality of the V-PCC reference software. The experiments\\nwere conducted using the Random Access temporal configuration\\nof V-PCC, delivering a reduction in encoding time of 42.73% for\\ngeometry streams and 55.3% for attribute sub-streams. The method\\nachieved significant encoding time reduction with a minimal impact\\non bitrate and objective quality. The results also demonstrated that\\nmodels trained for common 2D videos may not perform as well\\nin the context of geometry sub-stream encoding as they do in\\nattribute sub-stream encoding, indicating that specialized models\\ncould yield even better results. As future work, we plan to explore\\nthe use of specialized machine learning models for point cloud\\nencoding, utilizing data extracted from V-PCC, with the expectation\\nof achieving better results in encoding time reduction and BD-Rate.\\n### **REFERENCES**\\n\\n[1] Gisle Bjontegaard. 2001. Calculation of average PSNR differences between RDcurves. *ITU SG16 Doc. VCEG-M33* (2001).\\n\\n[2] Benjamin Bross, Ye-Kui Wang, Yan Ye, Shan Liu, Jianle Chen, Gary J Sullivan,\\nand Jens-Rainer Ohm. 2021. Overview of the versatile video coding (VVC)\\nstandard and its applications. *IEEE Transactions on Circuits and Systems for Video*\\n*Technology* 31, 10 (2021), 3736–3764.\\n\\n[3] Guilherme Correa, Pedro Assuncao, Luis A. da Silva Cruz, and Luciano Agostini.\\n2014. Classification-based early termination for coding tree structure decision\\nin HEVC. In *2014 21st IEEE International Conference on Electronics, Circuits and*\\n*Systems (ICECS)* . 239–242.\\n\\n[4] Guilherme Correa, Pedro A. Assuncao, Luciano Volcan Agostini, and Luis A.\\nda Silva Cruz. 2015. Fast HEVC Encoding Decisions Using Data Mining. *IEEE*\\n*Transactions on Circuits and Systems for Video Technology* 25, 4 (2015), 660–673.\\n\\n[5] Tianyu Dong, Kyutae Kim, and Euee S. Jang. 2021. Performance Evaluation of\\nthe Codec Agnostic Approach in MPEG-I Video-Based Point Cloud Compression.\\n*IEEE Access* 9 (2021), 167990–168003.\\n\\n[6] Jay Fenlason. 2024. Gprof. https://ftp.gnu.org/old-gnu/Manuals/gprof-2.9.1/\\nhtml_mono/gprof.html. Accessed: 2024-04-15.\\n\\n[7] Wei Gao, Hang Yuan, Ge Li, Zhu Li, and Hui Yuan. 2023. Low Complexity Coding\\nUnit Decision for Video-Based Point Cloud Compression. *IEEE Transactions on*\\n\\n\\n*Image Processing* 33 (2023), 149–162.\\n\\n[8] Danillo Graziosi, Ohji Nakagami, Satoru Kuma, Alexandre Zaghetto, Teruhiko\\nSuzuki, and Ali Tabatabai. 2020. An overview of ongoing point cloud compression\\nstandardization activities: Video-based (V-PCC) and geometry-based (G-PCC).\\n*APSIPA Transactions on Signal and Information Processing* 9 (2020), e13.\\n\\n[9] Dan Grois, Detlev Marpe, Amit Mulayoff, Benaya Itzhaky, and Ofer Hadar. 2013.\\nPerformance comparison of H.265/MPEG-HEVC, VP9, and H.264/MPEG-AVC\\nencoders. In *2013 Picture Coding Symposium (PCS)* . 394–397.\\n\\n[10] Il-Koo Kim, Junghye Min, Tammy Lee, Woo-Jin Han, and JeongHoon Park. 2012.\\nBlock partitioning structure in the HEVC standard. *IEEE transactions on circuits*\\n*and systems for video technology* 22, 12 (2012), 1697–1706.\\n\\n[11] Tianyi Li, Mai Xu, and Xin Deng. 2017. A deep convolutional neural network approach for complexity reduction on intra-mode HEVC. In *2017 IEEE International*\\n*Conference on Multimedia and Expo (ICME)* . IEEE, 1255–1260.\\n\\n[12] Yue Li, Jun Huang, Chaofeng Wang, and Hongyue Huang. 2024. Unsupervised\\nlearning-based fast CU size decision for geometry videos in V-PCC. *Journal of*\\n*Real-Time Image Processing* 21, 1 (2024), 11.\\n\\n[13] MPEG. 2020. Common Test Conditions for V3C and V-PCC. *ISO/IEC JTC 1/SC*\\n*29/WG 11* (2020).\\n\\n[14] MPEG. 2024. Video Point Cloud Compression - VPCC - mpeg-pcc-tmc2 test\\nmodel candidate software. https://github.com/MPEGGroup/mpeg-pcc-tmc2.\\n\\n[15] Sang-hyo Park and Je-Won Kang. 2020. Fast multi-type tree partitioning for\\nversatile video coding using a lightweight neural network. *IEEE Transactions on*\\n*Multimedia* 23 (2020), 4388–4399.\\n\\n[16] Marius Preda. 2020. V-PCC codec description. *ISO/IEC JTC 1/SC 29/WG 7, Virtual*\\n(2020).\\n\\n[17] R Schaefer. 2017. Call for proposals for point cloud compression V2. In *ISO/IEC*\\n*JTC1 SC29/WG11 MPEG, 117th Meeting. Hobart, TAS* .\\n\\n[18] Yun Song, Biao Zeng, Miaohui Wang, and Zelin Deng. 2022. An efficient lowcomplexity block partition scheme for VVC intra coding. *Journal of Real-Time*\\n*Image Processing* (2022), 1–12.\\n\\n[19] Gary J Sullivan, Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand. 2012.\\nOverview of the high efficiency video coding (HEVC) standard. *IEEE Transactions*\\n*on circuits and systems for video technology* 22, 12 (2012), 1649–1668.\\n\\n[20] Vivienne Sze, Madhukar Budagavi, and Gary J Sullivan. 2014. High efficiency\\nvideo coding (HEVC). In *Integrated circuit and systems, algorithms and architec-*\\n*tures* . Vol. 39. Springer, 40.\\n\\n[21] Alexandre Tissier, Wassim Hamidouche, Jarno Vanne, F Galpin, and Daniel\\nMenard. 2020. CNN oriented complexity reduction of VVC intra encoder. In *2020*\\n*IEEE International Conference on Image Processing (ICIP)* . IEEE, 3139–3143.\\n\\n[22] Yihan Wang, Yongfang Wang, Tengyao Cui, and Zhijun Fang. 2024. Fast VideoBased Point Cloud Compression Based on Early Termination and Transformer\\nModel. *IEEE Transactions on Emerging Topics in Computational Intelligence* (2024).\\n\\n[23] Natasha Westland, André Seixas Dias, and Marta Mrak. 2019. Decision trees for\\ncomplexity reduction in video compression. In *2019 IEEE International Conference*\\n*on Image Processing (ICIP)* . IEEE, 2666–2670.\\n\\n[24] Yun Zhang, Sam Kwong, and Shiqi Wang. 2020. Machine learning based video\\ncoding optimizations: A survey. *Information Sciences* 506 (2020), 395–423.\\n\\n\\n27\\n\\n\\n-----\\n\\n',\n",
       "  'artigo_tokenizado': ['#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'A',\n",
       "   'Machine',\n",
       "   '-',\n",
       "   'Learning',\n",
       "   '-',\n",
       "   'Driven',\n",
       "   'Fast',\n",
       "   'Video',\n",
       "   '-',\n",
       "   'based',\n",
       "   'Point',\n",
       "   'Cloud',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Compression',\n",
       "   '(',\n",
       "   'V',\n",
       "   '-',\n",
       "   'PCC',\n",
       "   ')',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Gustavo',\n",
       "   'Rehbein',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'ghrehbein@inf.ufpel.edu.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Pelotas',\n",
       "   'Video',\n",
       "   'Technology',\n",
       "   'Research',\n",
       "   'Group',\n",
       "   '–',\n",
       "   'Vitech',\n",
       "   'Graduate',\n",
       "   'Program',\n",
       "   'in',\n",
       "   'Computing',\n",
       "   'Pelotas',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Eduardo',\n",
       "   'Costa',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'edfcosta@inf.ufpel.edu.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Pelotas',\n",
       "   'Video',\n",
       "   'Technology',\n",
       "   'Research',\n",
       "   'Group',\n",
       "   '–',\n",
       "   'Vitech',\n",
       "   'Pelotas',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Guilherme',\n",
       "   'Corrêa',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'gcorrea@inf.ufpel.edu.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Pelotas',\n",
       "   'Video',\n",
       "   'Technology',\n",
       "   'Research',\n",
       "   'Group',\n",
       "   '–',\n",
       "   'Vitech',\n",
       "   'Graduate',\n",
       "   'Program',\n",
       "   'in',\n",
       "   'Computing',\n",
       "   'Pelotas',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Cristiano',\n",
       "   'Santos',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'cfdsantos@inf.ufpel.edu.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Pelotas',\n",
       "   'Video',\n",
       "   'Technology',\n",
       "   'Research',\n",
       "   'Group',\n",
       "   '–',\n",
       "   'Vitech',\n",
       "   'Graduate',\n",
       "   'Program',\n",
       "   'in',\n",
       "   'Computing',\n",
       "   'Pelotas',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'ABSTRACT',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   'recent',\n",
       "   'years',\n",
       "   ',',\n",
       "   '3D',\n",
       "   'point',\n",
       "   'cloud',\n",
       "   'content',\n",
       "   'has',\n",
       "   'gained',\n",
       "   'attention',\n",
       "   'due',\n",
       "   '\\n',\n",
       "   'to',\n",
       "   'its',\n",
       "   'application',\n",
       "   'possibilities',\n",
       "   ',',\n",
       "   'such',\n",
       "   'as',\n",
       "   'multimedia',\n",
       "   'systems',\n",
       "   ',',\n",
       "   'virtual',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'augmented',\n",
       "   ',',\n",
       "   'and',\n",
       "   'mixed',\n",
       "   'reality',\n",
       "   ',',\n",
       "   'through',\n",
       "   'the',\n",
       "   'mapping',\n",
       "   'and',\n",
       "   'visualization',\n",
       "   'of',\n",
       "   'environments',\n",
       "   'and/or',\n",
       "   '3D',\n",
       "   'objects',\n",
       "   ',',\n",
       "   'real',\n",
       "   '-',\n",
       "   'time',\n",
       "   'immersive',\n",
       "   '\\n',\n",
       "   'communications',\n",
       "   ',',\n",
       "   'and',\n",
       "   'autonomous',\n",
       "   'driving',\n",
       "   'systems',\n",
       "   '.',\n",
       "   'However',\n",
       "   ',',\n",
       "   'raw',\n",
       "   '\\n',\n",
       "   'point',\n",
       "   'clouds',\n",
       "   'demand',\n",
       "   'a',\n",
       "   'large',\n",
       "   'amount',\n",
       "   'of',\n",
       "   'data',\n",
       "   'for',\n",
       "   'their',\n",
       "   'representation',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'and',\n",
       "   'compression',\n",
       "   'is',\n",
       "   'mandatory',\n",
       "   'to',\n",
       "   'allow',\n",
       "   'efficient',\n",
       "   'transmission',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'storage',\n",
       "   '.',\n",
       "   'The',\n",
       "   'MPEG',\n",
       "   'group',\n",
       "   'proposed',\n",
       "   'the',\n",
       "   'Video',\n",
       "   '-',\n",
       "   'based',\n",
       "   'Point',\n",
       "   'Cloud',\n",
       "   '\\n',\n",
       "   'Compression',\n",
       "   '(',\n",
       "   'V',\n",
       "   '-',\n",
       "   'PCC',\n",
       "   ')',\n",
       "   'standard',\n",
       "   ',',\n",
       "   'which',\n",
       "   'is',\n",
       "   'a',\n",
       "   'dynamic',\n",
       "   'point',\n",
       "   'cloud',\n",
       "   '\\n',\n",
       "   'encoder',\n",
       "   'based',\n",
       "   'on',\n",
       "   'the',\n",
       "   'use',\n",
       "   'of',\n",
       "   'video',\n",
       "   'encoders',\n",
       "   'through',\n",
       "   'projections',\n",
       "   '\\n',\n",
       "   'into',\n",
       "   '2D',\n",
       "   'space',\n",
       "   '.',\n",
       "   'However',\n",
       "   ',',\n",
       "   'V',\n",
       "   '-',\n",
       "   'PCC',\n",
       "   'demands',\n",
       "   'a',\n",
       "   'high',\n",
       "   'computational',\n",
       "   '\\n',\n",
       "   'cost',\n",
       "   ',',\n",
       "   'demanding',\n",
       "   'fast',\n",
       "   'implementations',\n",
       "   'for',\n",
       "   'real',\n",
       "   '-',\n",
       "   'time',\n",
       "   'processing',\n",
       "   'and',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'especially',\n",
       "   ',',\n",
       "   'for',\n",
       "   'mobile',\n",
       "   'device',\n",
       "   'applications',\n",
       "   '.',\n",
       "   'In',\n",
       "   'this',\n",
       "   'paper',\n",
       "   ',',\n",
       "   'a',\n",
       "   'machinelearning',\n",
       "   '-',\n",
       "   'based',\n",
       "   'fast',\n",
       "   'implementation',\n",
       "   'of',\n",
       "   'V',\n",
       "   '-',\n",
       "   'PCC',\n",
       "   'is',\n",
       "   'proposed',\n",
       "   ',',\n",
       "   'where',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'main',\n",
       "   'approach',\n",
       "   'is',\n",
       "   'the',\n",
       "   'use',\n",
       "   'of',\n",
       "   'trained',\n",
       "   'decision',\n",
       "   'trees',\n",
       "   'to',\n",
       "   'speed',\n",
       "   'up',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'block',\n",
       "   'partitioning',\n",
       "   'process',\n",
       "   'during',\n",
       "   'the',\n",
       "   'point',\n",
       "   'cloud',\n",
       "   'compression',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'The',\n",
       "   'results',\n",
       "   'show',\n",
       "   'that',\n",
       "   'the',\n",
       "   'proposed',\n",
       "   'fast',\n",
       "   'V',\n",
       "   '-',\n",
       "   'PCC',\n",
       "   'solution',\n",
       "   'is',\n",
       "   'able',\n",
       "   'to',\n",
       "   '\\n',\n",
       "   'achieve',\n",
       "   'an',\n",
       "   'encoding',\n",
       "   'time',\n",
       "   'reduction',\n",
       "   'of',\n",
       "   '42.73',\n",
       "   '%',\n",
       "   'for',\n",
       "   'the',\n",
       "   'geometry',\n",
       "   '\\n',\n",
       "   'video',\n",
       "   'sub',\n",
       "   '-',\n",
       "   'stream',\n",
       "   'and',\n",
       "   '55.3',\n",
       "   '%',\n",
       "   'for',\n",
       "   'the',\n",
       "   'attribute',\n",
       "   'video',\n",
       "   'sub',\n",
       "   '-',\n",
       "   'stream',\n",
       "   ',',\n",
       "   'with',\n",
       "   '\\n',\n",
       "   'a',\n",
       "   'minimal',\n",
       "   'impact',\n",
       "   'on',\n",
       "   'bitrate',\n",
       "   'and',\n",
       "   'objective',\n",
       "   'quality',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'KEYWORDS',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'point',\n",
       "   'clouds',\n",
       "   ',',\n",
       "   'machine',\n",
       "   'learning',\n",
       "   ',',\n",
       "   'V',\n",
       "   '-',\n",
       "   'PCC',\n",
       "   ',',\n",
       "   'complexity',\n",
       "   'reduction',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   '1',\n",
       "   'INTRODUCTION',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Point',\n",
       "   'clouds',\n",
       "   'can',\n",
       "   'be',\n",
       "   'used',\n",
       "   'in',\n",
       "   'many',\n",
       "   'applications',\n",
       "   ',',\n",
       "   'such',\n",
       "   'as',\n",
       "   '3D',\n",
       "   'environment',\n",
       "   'mapping',\n",
       "   ',',\n",
       "   'representation',\n",
       "   'of',\n",
       "   'historical',\n",
       "   'objects',\n",
       "   'or',\n",
       "   'monuments',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'and',\n",
       "   'virtual',\n",
       "   ',',\n",
       "   'augmented',\n",
       "   ',',\n",
       "   'and',\n",
       "   'mixed',\n",
       "   'realities',\n",
       "   ',',\n",
       "   'providing',\n",
       "   'detailed',\n",
       "   '3D',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   ':',\n",
       "   'Proceedings',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Web',\n",
       "   '(',\n",
       "   'WebMe',\n",
       "   '\\n',\n",
       "   'dia’2024',\n",
       "   ')',\n",
       "   '.',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '.',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   ':',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   ',',\n",
       "   '2024',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'SBC',\n",
       "   '–',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '-',\n",
       "   '2753',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Marcelo',\n",
       "   'Porto',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'porto@inf.ufpel.edu.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Pelotas',\n",
       "   'Video',\n",
       "   'Technology',\n",
       "   'Research',\n",
       "   'Group',\n",
       "   '–',\n",
       "   'Vitech',\n",
       "   'Graduate',\n",
       "   'Program',\n",
       "   'in',\n",
       "   'Computing',\n",
       "   'Pelotas',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '\\n\\n',\n",
       "   'representation',\n",
       "   'and',\n",
       "   'multiple',\n",
       "   'points',\n",
       "   'of',\n",
       "   'view',\n",
       "   'of',\n",
       "   'objects',\n",
       "   'and',\n",
       "   'scenes',\n",
       "   '.',\n",
       "   'Regarding',\n",
       "   'the',\n",
       "   'application',\n",
       "   'context',\n",
       "   ',',\n",
       "   'point',\n",
       "   'clouds',\n",
       "   'can',\n",
       "   'be',\n",
       "   'categorized',\n",
       "   'into',\n",
       "   '\\n',\n",
       "   'three',\n",
       "   'types',\n",
       "   ':',\n",
       "   '(',\n",
       "   'Category',\n",
       "   '1',\n",
       "   ')',\n",
       "   'static',\n",
       "   ',',\n",
       "   'which',\n",
       "   'represents',\n",
       "   'the',\n",
       "   'capture',\n",
       "   'of',\n",
       "   'a',\n",
       "   '\\n',\n",
       "   'scene',\n",
       "   'or',\n",
       "   'object',\n",
       "   'at',\n",
       "   'a',\n",
       "   'single',\n",
       "   'time',\n",
       "   'instant',\n",
       "   ';',\n",
       "   '(',\n",
       "   'Category',\n",
       "   '2',\n",
       "   ')',\n",
       "   'dynamic',\n",
       "   ',',\n",
       "   'when',\n",
       "   '\\n',\n",
       "   'a',\n",
       "   'sequence',\n",
       "   'of',\n",
       "   'point',\n",
       "   'clouds',\n",
       "   'is',\n",
       "   'captured',\n",
       "   'over',\n",
       "   'time',\n",
       "   ';',\n",
       "   'and',\n",
       "   '(',\n",
       "   'Category',\n",
       "   '3',\n",
       "   ')',\n",
       "   '\\n',\n",
       "   'dynamically',\n",
       "   'captured',\n",
       "   'through',\n",
       "   'multiple',\n",
       "   'partially',\n",
       "   'overlapping',\n",
       "   'scans',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'typically',\n",
       "   'during',\n",
       "   'dynamic',\n",
       "   'mapping',\n",
       "   'of',\n",
       "   'a',\n",
       "   'large',\n",
       "   '-',\n",
       "   'scale',\n",
       "   'environment',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Nevertheless',\n",
       "   ',',\n",
       "   'as',\n",
       "   'non',\n",
       "   '-',\n",
       "   'compressed',\n",
       "   'point',\n",
       "   'clouds',\n",
       "   'comprise',\n",
       "   'a',\n",
       "   'large',\n",
       "   '\\n',\n",
       "   'amount',\n",
       "   'of',\n",
       "   'data',\n",
       "   ',',\n",
       "   'storage',\n",
       "   'and',\n",
       "   'transmission',\n",
       "   'become',\n",
       "   'challenging',\n",
       "   'tasks',\n",
       "   '\\n',\n",
       "   'depending',\n",
       "   'on',\n",
       "   'the',\n",
       "   'point',\n",
       "   'cloud',\n",
       "   'resolution',\n",
       "   'and',\n",
       "   'the',\n",
       "   'device',\n",
       "   'on',\n",
       "   'which',\n",
       "   '\\n',\n",
       "   'it',\n",
       "   'is',\n",
       "   'processed',\n",
       "   '.',\n",
       "   'Thus',\n",
       "   ',',\n",
       "   'compression',\n",
       "   'becomes',\n",
       "   'mandatory',\n",
       "   'to',\n",
       "   'allow',\n",
       "   'for',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'efficient',\n",
       "   'transmission',\n",
       "   'and',\n",
       "   'storage',\n",
       "   'of',\n",
       "   'this',\n",
       "   'type',\n",
       "   'of',\n",
       "   '3D',\n",
       "   'content',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'The',\n",
       "   'Motion',\n",
       "   'Picture',\n",
       "   'Experts',\n",
       "   'Group',\n",
       "   '(',\n",
       "   'MPEG',\n",
       "   ')',\n",
       "   'defined',\n",
       "   'two',\n",
       "   'Point',\n",
       "   'Cloud',\n",
       "   '\\n',\n",
       "   'Compression',\n",
       "   '(',\n",
       "   'PCC',\n",
       "   ')',\n",
       "   'standard',\n",
       "   'specifications',\n",
       "   ':',\n",
       "   'the',\n",
       "   'Video',\n",
       "   '-',\n",
       "   'based',\n",
       "   'PCC',\n",
       "   '\\n',\n",
       "   '(',\n",
       "   'V',\n",
       "   '-',\n",
       "   'PCC',\n",
       "   ')',\n",
       "   '[',\n",
       "   '16',\n",
       "   ']',\n",
       "   ',',\n",
       "   'related',\n",
       "   'to',\n",
       "   'dynamic',\n",
       "   'point',\n",
       "   'clouds',\n",
       "   ',',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Geometrybased',\n",
       "   'PCC',\n",
       "   '(',\n",
       "   'G',\n",
       "   '-',\n",
       "   'PCC',\n",
       "   ')',\n",
       "   ',',\n",
       "   'designed',\n",
       "   'to',\n",
       "   'compress',\n",
       "   'static',\n",
       "   'and',\n",
       "   'dynamically',\n",
       "   '\\n',\n",
       "   'acquired',\n",
       "   'point',\n",
       "   'clouds',\n",
       "   '[',\n",
       "   '8',\n",
       "   ']',\n",
       "   '.',\n",
       "   'While',\n",
       "   'the',\n",
       "   'V',\n",
       "   '-',\n",
       "   'PCC',\n",
       "   'coding',\n",
       "   'approach',\n",
       "   'is',\n",
       "   '\\n',\n",
       "   'based',\n",
       "   'on',\n",
       "   '2D',\n",
       "   'projections',\n",
       "   'of',\n",
       "   'the',\n",
       "   'point',\n",
       "   'clouds',\n",
       "   'to',\n",
       "   'be',\n",
       "   'further',\n",
       "   'compressed',\n",
       "   'with',\n",
       "   'a',\n",
       "   'regular',\n",
       "   '2D',\n",
       "   'video',\n",
       "   'encoder',\n",
       "   ',',\n",
       "   'G',\n",
       "   '-',\n",
       "   'PCC',\n",
       "   'encodes',\n",
       "   'the',\n",
       "   'content',\n",
       "   'directly',\n",
       "   'in',\n",
       "   'the',\n",
       "   '3D',\n",
       "   'space',\n",
       "   '[',\n",
       "   '8',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'V',\n",
       "   '-',\n",
       "   'PCC',\n",
       "   'is',\n",
       "   'an',\n",
       "   'interesting',\n",
       "   'alternative',\n",
       "   'to',\n",
       "   'point',\n",
       "   'clouds',\n",
       "   'compression',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'since',\n",
       "   'the',\n",
       "   'already',\n",
       "   'existing',\n",
       "   'video',\n",
       "   'encoders',\n",
       "   'can',\n",
       "   'be',\n",
       "   'used',\n",
       "   '.',\n",
       "   'Although',\n",
       "   '\\n',\n",
       "   'V',\n",
       "   '-',\n",
       "   'PCC',\n",
       "   'provides',\n",
       "   'an',\n",
       "   'efficient',\n",
       "   'solution',\n",
       "   'for',\n",
       "   'compressing',\n",
       "   'the',\n",
       "   'data',\n",
       "   'generated',\n",
       "   'by',\n",
       "   'point',\n",
       "   'clouds',\n",
       "   ',',\n",
       "   'this',\n",
       "   'encoder',\n",
       "   'inherits',\n",
       "   'the',\n",
       "   'complexity',\n",
       "   'of',\n",
       "   'video',\n",
       "   '\\n',\n",
       "   'encoding',\n",
       "   '.',\n",
       "   'This',\n",
       "   'occurs',\n",
       "   'because',\n",
       "   'V',\n",
       "   '-',\n",
       "   'PCC',\n",
       "   ',',\n",
       "   'after',\n",
       "   'the',\n",
       "   'point',\n",
       "   'cloud',\n",
       "   '2D',\n",
       "   '\\n',\n",
       "   'projection',\n",
       "   'step',\n",
       "   ',',\n",
       "   'uses',\n",
       "   'an',\n",
       "   'HEVC',\n",
       "   '[',\n",
       "   '19',\n",
       "   ']',\n",
       "   'video',\n",
       "   'encoder',\n",
       "   '.',\n",
       "   'In',\n",
       "   'this',\n",
       "   'process',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'HEVC',\n",
       "   'video',\n",
       "   'encoder',\n",
       "   'needs',\n",
       "   'to',\n",
       "   'handle',\n",
       "   'three',\n",
       "   'video',\n",
       "   'sub',\n",
       "   '-',\n",
       "   'streams',\n",
       "   '\\n\\n',\n",
       "   'generated',\n",
       "   'in',\n",
       "   'the',\n",
       "   '2D',\n",
       "   'projection',\n",
       "   'step',\n",
       "   'by',\n",
       "   'V',\n",
       "   '-',\n",
       "   'PCC',\n",
       "   '[',\n",
       "   '16',\n",
       "   ']',\n",
       "   '.',\n",
       "   'One',\n",
       "   'video',\n",
       "   'substream',\n",
       "   'is',\n",
       "   'generated',\n",
       "   'to',\n",
       "   'indicate',\n",
       "   'the',\n",
       "   'occupancy',\n",
       "   'map',\n",
       "   ',',\n",
       "   'which',\n",
       "   'shows',\n",
       "   '\\n',\n",
       "   'where',\n",
       "   'points',\n",
       "   'actually',\n",
       "   'exist',\n",
       "   'due',\n",
       "   'to',\n",
       "   'the',\n",
       "   'sparse',\n",
       "   'and',\n",
       "   'irregular',\n",
       "   'nature',\n",
       "   '\\n',\n",
       "   'of',\n",
       "   'a',\n",
       "   'point',\n",
       "   'cloud',\n",
       "   '.',\n",
       "   'Additionally',\n",
       "   ',',\n",
       "   'a',\n",
       "   'second',\n",
       "   'sub',\n",
       "   '-',\n",
       "   'stream',\n",
       "   'is',\n",
       "   'necessary',\n",
       "   'to',\n",
       "   '\\n',\n",
       "   'indicate',\n",
       "   'the',\n",
       "   'geometric',\n",
       "   'position',\n",
       "   'of',\n",
       "   'each',\n",
       "   'point',\n",
       "   ',',\n",
       "   'i.e.',\n",
       "   ',',\n",
       "   'the',\n",
       "   'position',\n",
       "   'on',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'Z',\n",
       "   '-',\n",
       "   'axis',\n",
       "   'or',\n",
       "   'depth',\n",
       "   '.',\n",
       "   'A',\n",
       "   'third',\n",
       "   'sub',\n",
       "   '-',\n",
       "   'stream',\n",
       "   ',',\n",
       "   'called',\n",
       "   'Attribute',\n",
       "   'sub',\n",
       "   '-',\n",
       "   'stream',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'indicates',\n",
       "   'the',\n",
       "   'color',\n",
       "   'or',\n",
       "   'appearance',\n",
       "   'assigned',\n",
       "   'to',\n",
       "   'each',\n",
       "   'point',\n",
       "   '[',\n",
       "   '16',\n",
       "   ...],\n",
       "  'pos_tagger': '',\n",
       "  'lema': ['a',\n",
       "   'machine',\n",
       "   'Learning',\n",
       "   'Driven',\n",
       "   'Fast',\n",
       "   'Video',\n",
       "   'base',\n",
       "   'Point',\n",
       "   'Cloud',\n",
       "   'compression',\n",
       "   'V',\n",
       "   'PCC',\n",
       "   'Gustavo',\n",
       "   'Rehbein',\n",
       "   'ghrehbein@inf.ufpel.edu.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Pelotas',\n",
       "   'Video',\n",
       "   'Technology',\n",
       "   'Research',\n",
       "   'Group',\n",
       "   'Vitech',\n",
       "   'Graduate',\n",
       "   'Program',\n",
       "   'in',\n",
       "   'Computing',\n",
       "   'Pelotas',\n",
       "   'Brazil',\n",
       "   'Eduardo',\n",
       "   'Costa',\n",
       "   'edfcosta@inf.ufpel.edu.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Pelotas',\n",
       "   'Video',\n",
       "   'Technology',\n",
       "   'Research',\n",
       "   'Group',\n",
       "   'Vitech',\n",
       "   'Pelotas',\n",
       "   'Brazil',\n",
       "   'Guilherme',\n",
       "   'Corrêa',\n",
       "   'gcorrea@inf.ufpel.edu.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Pelotas',\n",
       "   'Video',\n",
       "   'Technology',\n",
       "   'Research',\n",
       "   'Group',\n",
       "   'Vitech',\n",
       "   'Graduate',\n",
       "   'Program',\n",
       "   'in',\n",
       "   'Computing',\n",
       "   'Pelotas',\n",
       "   'Brazil',\n",
       "   'Cristiano',\n",
       "   'santos',\n",
       "   'cfdsantos@inf.ufpel.edu.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Pelotas',\n",
       "   'Video',\n",
       "   'Technology',\n",
       "   'Research',\n",
       "   'Group',\n",
       "   'Vitech',\n",
       "   'Graduate',\n",
       "   'Program',\n",
       "   'in',\n",
       "   'Computing',\n",
       "   'Pelotas',\n",
       "   'Brazil',\n",
       "   'ABSTRACT',\n",
       "   'in',\n",
       "   'recent',\n",
       "   'year',\n",
       "   '3d',\n",
       "   'point',\n",
       "   'cloud',\n",
       "   'content',\n",
       "   'have',\n",
       "   'gain',\n",
       "   'attention',\n",
       "   'due',\n",
       "   'to',\n",
       "   'its',\n",
       "   'application',\n",
       "   'possibility',\n",
       "   'such',\n",
       "   'as',\n",
       "   'multimedia',\n",
       "   'system',\n",
       "   'virtual',\n",
       "   'augmented',\n",
       "   'and',\n",
       "   'mixed',\n",
       "   'reality',\n",
       "   'through',\n",
       "   'the',\n",
       "   'mapping',\n",
       "   'and',\n",
       "   'visualization',\n",
       "   'of',\n",
       "   'environment',\n",
       "   'and/or',\n",
       "   '3d',\n",
       "   'object',\n",
       "   'real',\n",
       "   'time',\n",
       "   'immersive',\n",
       "   'communication',\n",
       "   'and',\n",
       "   'autonomous',\n",
       "   'driving',\n",
       "   'system',\n",
       "   'however',\n",
       "   'raw',\n",
       "   'point',\n",
       "   'cloud',\n",
       "   'demand',\n",
       "   'a',\n",
       "   'large',\n",
       "   'amount',\n",
       "   'of',\n",
       "   'datum',\n",
       "   'for',\n",
       "   'their',\n",
       "   'representation',\n",
       "   'and',\n",
       "   'compression',\n",
       "   'be',\n",
       "   'mandatory',\n",
       "   'to',\n",
       "   'allow',\n",
       "   'efficient',\n",
       "   'transmission',\n",
       "   'and',\n",
       "   'storage',\n",
       "   'the',\n",
       "   'MPEG',\n",
       "   'group',\n",
       "   'propose',\n",
       "   'the',\n",
       "   'Video',\n",
       "   'base',\n",
       "   'Point',\n",
       "   'Cloud',\n",
       "   'Compression',\n",
       "   'V',\n",
       "   'PCC',\n",
       "   'standard',\n",
       "   'which',\n",
       "   'be',\n",
       "   'a',\n",
       "   'dynamic',\n",
       "   'point',\n",
       "   'cloud',\n",
       "   'encoder',\n",
       "   'base',\n",
       "   'on',\n",
       "   'the',\n",
       "   'use',\n",
       "   'of',\n",
       "   'video',\n",
       "   'encoder',\n",
       "   'through',\n",
       "   'projection',\n",
       "   'into',\n",
       "   '2D',\n",
       "   'space',\n",
       "   'however',\n",
       "   'V',\n",
       "   'pcc',\n",
       "   'demand',\n",
       "   'a',\n",
       "   'high',\n",
       "   'computational',\n",
       "   'cost',\n",
       "   'demand',\n",
       "   'fast',\n",
       "   'implementation',\n",
       "   'for',\n",
       "   'real',\n",
       "   'time',\n",
       "   'processing',\n",
       "   'and',\n",
       "   'especially',\n",
       "   'for',\n",
       "   'mobile',\n",
       "   'device',\n",
       "   'application',\n",
       "   'in',\n",
       "   'this',\n",
       "   'paper',\n",
       "   'a',\n",
       "   'machinelearning',\n",
       "   'base',\n",
       "   'fast',\n",
       "   'implementation',\n",
       "   'of',\n",
       "   'V',\n",
       "   'PCC',\n",
       "   'be',\n",
       "   'propose',\n",
       "   'where',\n",
       "   'the',\n",
       "   'main',\n",
       "   'approach',\n",
       "   'be',\n",
       "   'the',\n",
       "   'use',\n",
       "   'of',\n",
       "   'train',\n",
       "   'decision',\n",
       "   'tree',\n",
       "   'to',\n",
       "   'speed',\n",
       "   'up',\n",
       "   'the',\n",
       "   'block',\n",
       "   'partitioning',\n",
       "   'process',\n",
       "   'during',\n",
       "   'the',\n",
       "   'point',\n",
       "   'cloud',\n",
       "   'compression',\n",
       "   'the',\n",
       "   'result',\n",
       "   'show',\n",
       "   'that',\n",
       "   'the',\n",
       "   'propose',\n",
       "   'fast',\n",
       "   'v',\n",
       "   'pcc',\n",
       "   'solution',\n",
       "   'be',\n",
       "   'able',\n",
       "   'to',\n",
       "   'achieve',\n",
       "   'an',\n",
       "   'encoding',\n",
       "   'time',\n",
       "   'reduction',\n",
       "   'of',\n",
       "   '42.73',\n",
       "   'for',\n",
       "   'the',\n",
       "   'geometry',\n",
       "   'video',\n",
       "   'sub',\n",
       "   'stream',\n",
       "   'and',\n",
       "   '55.3',\n",
       "   'for',\n",
       "   'the',\n",
       "   'attribute',\n",
       "   'video',\n",
       "   'sub',\n",
       "   'stream',\n",
       "   'with',\n",
       "   'a',\n",
       "   'minimal',\n",
       "   'impact',\n",
       "   'on',\n",
       "   'bitrate',\n",
       "   'and',\n",
       "   'objective',\n",
       "   'quality',\n",
       "   'keyword',\n",
       "   'point',\n",
       "   'cloud',\n",
       "   'machine',\n",
       "   'learning',\n",
       "   'V',\n",
       "   'PCC',\n",
       "   'complexity',\n",
       "   'reduction',\n",
       "   '1',\n",
       "   'introduction',\n",
       "   'Point',\n",
       "   'cloud',\n",
       "   'can',\n",
       "   'be',\n",
       "   'use',\n",
       "   'in',\n",
       "   'many',\n",
       "   'application',\n",
       "   'such',\n",
       "   'as',\n",
       "   '3d',\n",
       "   'environment',\n",
       "   'mapping',\n",
       "   'representation',\n",
       "   'of',\n",
       "   'historical',\n",
       "   'object',\n",
       "   'or',\n",
       "   'monument',\n",
       "   'and',\n",
       "   'virtual',\n",
       "   'augmented',\n",
       "   'and',\n",
       "   'mixed',\n",
       "   'reality',\n",
       "   'provide',\n",
       "   'detailed',\n",
       "   '3d',\n",
       "   'in',\n",
       "   'proceeding',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'web',\n",
       "   'WebMe',\n",
       "   'dia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   '2024',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'sbc',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '2753',\n",
       "   'Marcelo',\n",
       "   'Porto',\n",
       "   'porto@inf.ufpel.edu.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Pelotas',\n",
       "   'Video',\n",
       "   'Technology',\n",
       "   'Research',\n",
       "   'Group',\n",
       "   'Vitech',\n",
       "   'Graduate',\n",
       "   'Program',\n",
       "   'in',\n",
       "   'Computing',\n",
       "   'Pelotas',\n",
       "   'Brazil',\n",
       "   'representation',\n",
       "   'and',\n",
       "   'multiple',\n",
       "   'point',\n",
       "   'of',\n",
       "   'view',\n",
       "   'of',\n",
       "   'object',\n",
       "   'and',\n",
       "   'scene',\n",
       "   'regard',\n",
       "   'the',\n",
       "   'application',\n",
       "   'context',\n",
       "   'point',\n",
       "   'cloud',\n",
       "   'can',\n",
       "   'be',\n",
       "   'categorize',\n",
       "   'into',\n",
       "   'three',\n",
       "   'type',\n",
       "   'category',\n",
       "   '1',\n",
       "   'static',\n",
       "   'which',\n",
       "   'represent',\n",
       "   'the',\n",
       "   'capture',\n",
       "   'of',\n",
       "   'a',\n",
       "   'scene',\n",
       "   'or',\n",
       "   'object',\n",
       "   'at',\n",
       "   'a',\n",
       "   'single',\n",
       "   'time',\n",
       "   'instant',\n",
       "   'category',\n",
       "   '2',\n",
       "   'dynamic',\n",
       "   'when',\n",
       "   'a',\n",
       "   'sequence',\n",
       "   'of',\n",
       "   'point',\n",
       "   'cloud',\n",
       "   'be',\n",
       "   'capture',\n",
       "   'over',\n",
       "   'time',\n",
       "   'and',\n",
       "   'category',\n",
       "   '3',\n",
       "   'dynamically',\n",
       "   'capture',\n",
       "   'through',\n",
       "   'multiple',\n",
       "   'partially',\n",
       "   'overlap',\n",
       "   'scan',\n",
       "   'typically',\n",
       "   'during',\n",
       "   'dynamic',\n",
       "   'mapping',\n",
       "   'of',\n",
       "   'a',\n",
       "   'large',\n",
       "   'scale',\n",
       "   'environment',\n",
       "   'nevertheless',\n",
       "   'as',\n",
       "   'non',\n",
       "   'compressed',\n",
       "   'point',\n",
       "   'cloud',\n",
       "   'comprise',\n",
       "   'a',\n",
       "   'large',\n",
       "   'amount',\n",
       "   'of',\n",
       "   'datum',\n",
       "   'storage',\n",
       "   'and',\n",
       "   'transmission',\n",
       "   'become',\n",
       "   'challenging',\n",
       "   'task',\n",
       "   'depend',\n",
       "   'on',\n",
       "   'the',\n",
       "   'point',\n",
       "   'cloud',\n",
       "   'resolution',\n",
       "   'and',\n",
       "   'the',\n",
       "   'device',\n",
       "   'on',\n",
       "   'which',\n",
       "   'it',\n",
       "   'be',\n",
       "   'process',\n",
       "   'thus',\n",
       "   'compression',\n",
       "   'become',\n",
       "   'mandatory',\n",
       "   'to',\n",
       "   'allow',\n",
       "   'for',\n",
       "   'the',\n",
       "   'efficient',\n",
       "   'transmission',\n",
       "   'and',\n",
       "   'storage',\n",
       "   'of',\n",
       "   'this',\n",
       "   'type',\n",
       "   'of',\n",
       "   '3d',\n",
       "   'content',\n",
       "   'the',\n",
       "   'Motion',\n",
       "   'Picture',\n",
       "   'Experts',\n",
       "   'Group',\n",
       "   'MPEG',\n",
       "   'define',\n",
       "   'two',\n",
       "   'Point',\n",
       "   'Cloud',\n",
       "   'Compression',\n",
       "   'PCC',\n",
       "   'standard',\n",
       "   'specification',\n",
       "   'the',\n",
       "   'video',\n",
       "   'base',\n",
       "   'PCC',\n",
       "   'V',\n",
       "   'PCC',\n",
       "   '16',\n",
       "   'relate',\n",
       "   'to',\n",
       "   'dynamic',\n",
       "   'point',\n",
       "   'cloud',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Geometrybased',\n",
       "   'PCC',\n",
       "   'G',\n",
       "   'PCC',\n",
       "   'design',\n",
       "   'to',\n",
       "   'compress',\n",
       "   'static',\n",
       "   'and',\n",
       "   'dynamically',\n",
       "   'acquire',\n",
       "   'point',\n",
       "   'cloud',\n",
       "   '8',\n",
       "   'while',\n",
       "   'the',\n",
       "   'v',\n",
       "   'PCC',\n",
       "   'code',\n",
       "   'approach',\n",
       "   'be',\n",
       "   'base',\n",
       "   'on',\n",
       "   '2D',\n",
       "   'projection',\n",
       "   'of',\n",
       "   'the',\n",
       "   'point',\n",
       "   'cloud',\n",
       "   'to',\n",
       "   'be',\n",
       "   'far',\n",
       "   'compress',\n",
       "   'with',\n",
       "   'a',\n",
       "   'regular',\n",
       "   '2d',\n",
       "   'video',\n",
       "   'encoder',\n",
       "   'G',\n",
       "   'PCC',\n",
       "   'encode',\n",
       "   'the',\n",
       "   'content',\n",
       "   'directly',\n",
       "   'in',\n",
       "   'the',\n",
       "   '3d',\n",
       "   'space',\n",
       "   '8',\n",
       "   'V',\n",
       "   'PCC',\n",
       "   'be',\n",
       "   'an',\n",
       "   'interesting',\n",
       "   'alternative',\n",
       "   'to',\n",
       "   'point',\n",
       "   'cloud',\n",
       "   'compression',\n",
       "   'since',\n",
       "   'the',\n",
       "   'already',\n",
       "   'exist',\n",
       "   'video',\n",
       "   'encoder',\n",
       "   'can',\n",
       "   'be',\n",
       "   'use',\n",
       "   'although',\n",
       "   'V',\n",
       "   'pcc',\n",
       "   'provide',\n",
       "   'an',\n",
       "   'efficient',\n",
       "   'solution',\n",
       "   'for',\n",
       "   'compress',\n",
       "   'the',\n",
       "   'datum',\n",
       "   'generate',\n",
       "   'by',\n",
       "   'point',\n",
       "   'cloud',\n",
       "   'this',\n",
       "   'encoder',\n",
       "   'inherit',\n",
       "   'the',\n",
       "   'complexity',\n",
       "   'of',\n",
       "   'video',\n",
       "   'encoding',\n",
       "   'this',\n",
       "   'occur',\n",
       "   'because',\n",
       "   'V',\n",
       "   'PCC',\n",
       "   'after',\n",
       "   'the',\n",
       "   'point',\n",
       "   'cloud',\n",
       "   '2D',\n",
       "   'projection',\n",
       "   'step',\n",
       "   'use',\n",
       "   'an',\n",
       "   'hevc',\n",
       "   '19',\n",
       "   'video',\n",
       "   'encoder',\n",
       "   'in',\n",
       "   'this',\n",
       "   'process',\n",
       "   'the',\n",
       "   'HEVC',\n",
       "   'video',\n",
       "   'encoder',\n",
       "   'need',\n",
       "   'to',\n",
       "   'handle',\n",
       "   'three',\n",
       "   'video',\n",
       "   'sub',\n",
       "   'stream',\n",
       "   'generate',\n",
       "   'in',\n",
       "   'the',\n",
       "   '2D',\n",
       "   'projection',\n",
       "   'step',\n",
       "   'by',\n",
       "   'V',\n",
       "   'PCC',\n",
       "   '16',\n",
       "   'one',\n",
       "   'video',\n",
       "   'substream',\n",
       "   'be',\n",
       "   'generate',\n",
       "   'to',\n",
       "   'indicate',\n",
       "   'the',\n",
       "   'occupancy',\n",
       "   'map',\n",
       "   'which',\n",
       "   'show',\n",
       "   'where',\n",
       "   'point',\n",
       "   'actually',\n",
       "   'exist',\n",
       "   'due',\n",
       "   'to',\n",
       "   'the',\n",
       "   'sparse',\n",
       "   'and',\n",
       "   'irregular',\n",
       "   'nature',\n",
       "   'of',\n",
       "   'a',\n",
       "   'point',\n",
       "   'cloud',\n",
       "   'additionally',\n",
       "   'a',\n",
       "   'second',\n",
       "   'sub',\n",
       "   'stream',\n",
       "   'be',\n",
       "   'necessary',\n",
       "   'to',\n",
       "   'indicate',\n",
       "   'the',\n",
       "   'geometric',\n",
       "   'position',\n",
       "   'of',\n",
       "   'each',\n",
       "   'point',\n",
       "   'i.e.',\n",
       "   'the',\n",
       "   'position',\n",
       "   'on',\n",
       "   'the',\n",
       "   'z',\n",
       "   'axis',\n",
       "   'or',\n",
       "   'depth',\n",
       "   'a',\n",
       "   'third',\n",
       "   'sub',\n",
       "   'stream',\n",
       "   'call',\n",
       "   'Attribute',\n",
       "   'sub',\n",
       "   'stream',\n",
       "   'indicate',\n",
       "   'the',\n",
       "   'color',\n",
       "   'or',\n",
       "   'appearance',\n",
       "   'assign',\n",
       "   'to',\n",
       "   'each',\n",
       "   'point',\n",
       "   '16',\n",
       "   '20',\n",
       "   'WebMedia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Rehbein',\n",
       "   'et',\n",
       "   'al',\n",
       "   'there',\n",
       "   'be',\n",
       "   'many',\n",
       "   'work',\n",
       "   'in',\n",
       "   'literature',\n",
       "   'propose',\n",
       "   'the',\n",
       "   'use',\n",
       "   'of',\n",
       "   'machine',\n",
       "   'learning',\n",
       "   'to',\n",
       "   'reduce',\n",
       "   'video',\n",
       "   'compression',\n",
       "   'complexity',\n",
       "   'as',\n",
       "   '21',\n",
       "   '15',\n",
       "   '24',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'most',\n",
       "   'efficient',\n",
       "   'approach',\n",
       "   'in',\n",
       "   'to',\n",
       "   'predict',\n",
       "   'the',\n",
       "   'block',\n",
       "   'partition',\n",
       "   'during',\n",
       "   'the',\n",
       "   'encoding',\n",
       "   'process',\n",
       "   '3',\n",
       "   '18',\n",
       "   '23',\n",
       "   '4',\n",
       "   'this',\n",
       "   'type',\n",
       "   'of',\n",
       "   'prediction',\n",
       "   'prevent',\n",
       "   'the',\n",
       "   'encoder',\n",
       "   'to',\n",
       "   'evaluate',\n",
       "   'small',\n",
       "   'block',\n",
       "   'size',\n",
       "   'thereby',\n",
       "   'reduce',\n",
       "   'complexity',\n",
       "   'and',\n",
       "   'achieve',\n",
       "   'time',\n",
       "   'saving',\n",
       "   'this',\n",
       "   'work',\n",
       "   'propose',\n",
       "   'a',\n",
       "   'machine',\n",
       "   'learn',\n",
       "   'base',\n",
       "   'fast',\n",
       "   'v',\n",
       "   'pcc',\n",
       "   'implementation',\n",
       "   'that',\n",
       "   'employ',\n",
       "   'an',\n",
       "   'already',\n",
       "   'train',\n",
       "   'model',\n",
       "   'originally',\n",
       "   'propose',\n",
       "   'for',\n",
       "   '2D',\n",
       "   'video',\n",
       "   'compression',\n",
       "   '3',\n",
       "   'to',\n",
       "   'reduce',\n",
       "   'the',\n",
       "   'complexity',\n",
       "   'of',\n",
       "   'point',\n",
       "   'cloud',\n",
       "   'compression',\n",
       "   'in',\n",
       "   'the',\n",
       "   'V',\n",
       "   'PCC',\n",
       "   'standard',\n",
       "   'to',\n",
       "   'this',\n",
       "   'end',\n",
       "   'the',\n",
       "   'use',\n",
       "   'and',\n",
       "   'adaptation',\n",
       "   'of',\n",
       "   'a',\n",
       "   'pre',\n",
       "   'trained',\n",
       "   'model',\n",
       "   'to',\n",
       "   'accelerate',\n",
       "   'video',\n",
       "   'compression',\n",
       "   'be',\n",
       "   'propose',\n",
       "   'as',\n",
       "   'an',\n",
       "   'alternative',\n",
       "   'to',\n",
       "   'speed',\n",
       "   'up',\n",
       "   'the',\n",
       "   'point',\n",
       "   'cloud',\n",
       "   'compression',\n",
       "   'process',\n",
       "   'in',\n",
       "   'the',\n",
       "   'Test',\n",
       "   'Model',\n",
       "   'Category',\n",
       "   '2',\n",
       "   'TMC2',\n",
       "   '14',\n",
       "   'the',\n",
       "   'reference',\n",
       "   'software',\n",
       "   'of',\n",
       "   'the',\n",
       "   'V',\n",
       "   'PCC',\n",
       "   'standard',\n",
       "   'the',\n",
       "   'machine',\n",
       "   'learning',\n",
       "   'model',\n",
       "   'use',\n",
       "   'achieve',\n",
       "   '84',\n",
       "   'accuracy',\n",
       "   'on',\n",
       "   'the',\n",
       "   'testing',\n",
       "   'dataset',\n",
       "   '3',\n",
       "   'this',\n",
       "   'enable',\n",
       "   'a',\n",
       "   '42.73',\n",
       "   'time',\n",
       "   'reduction',\n",
       "   'in',\n",
       "   'the',\n",
       "   'video',\n",
       "   'encode',\n",
       "   'step',\n",
       "   'for',\n",
       "   'the',\n",
       "   'geometry',\n",
       "   'sub',\n",
       "   'stream',\n",
       "   'and',\n",
       "   'a',\n",
       "   '55.3',\n",
       "   'reduction',\n",
       "   'for',\n",
       "   'the',\n",
       "   'attribute',\n",
       "   'sub',\n",
       "   'stream',\n",
       "   'the',\n",
       "   'loss',\n",
       "   'in',\n",
       "   'code',\n",
       "   'efficiency',\n",
       "   'measure',\n",
       "   'with',\n",
       "   'the',\n",
       "   'BD',\n",
       "   'rate',\n",
       "   'metric',\n",
       "   '1',\n",
       "   'be',\n",
       "   'of',\n",
       "   '4.2',\n",
       "   'for',\n",
       "   'geometry',\n",
       "   'use',\n",
       "   'D2',\n",
       "   'metric',\n",
       "   'and',\n",
       "   '3.2',\n",
       "   'for',\n",
       "   'color',\n",
       "   'attribute',\n",
       "   'respectively',\n",
       "   'the',\n",
       "   'result',\n",
       "   'show',\n",
       "   'that',\n",
       "   'while',\n",
       "   'there',\n",
       "   'be',\n",
       "   'an',\n",
       "   'impact',\n",
       "   'on',\n",
       "   'encode',\n",
       "   'efficiency',\n",
       "   'this',\n",
       "   'can',\n",
       "   'be',\n",
       "   'outweigh',\n",
       "   'by',\n",
       "   'the',\n",
       "   'significant',\n",
       "   'gain',\n",
       "   'in',\n",
       "   'complexity',\n",
       "   'reduction',\n",
       "   'this',\n",
       "   'paper',\n",
       "   'be',\n",
       "   'organize',\n",
       "   'as',\n",
       "   'follow',\n",
       "   'section',\n",
       "   '2',\n",
       "   'present',\n",
       "   'related',\n",
       "   'work',\n",
       "   'section',\n",
       "   '3',\n",
       "   'introduce',\n",
       "   'the',\n",
       "   'V',\n",
       "   'PCC',\n",
       "   'standard',\n",
       "   'and',\n",
       "   'its',\n",
       "   'operation',\n",
       "   'section',\n",
       "   '4',\n",
       "   'outline',\n",
       "   'the',\n",
       "   'propose',\n",
       "   'method',\n",
       "   'section',\n",
       "   '5',\n",
       "   'present',\n",
       "   'the',\n",
       "   'result',\n",
       "   'of',\n",
       "   'the',\n",
       "   'experiment',\n",
       "   'conduct',\n",
       "   'and',\n",
       "   'section',\n",
       "   '6',\n",
       "   'provide',\n",
       "   'the',\n",
       "   'conclusion',\n",
       "   '2',\n",
       "   ...]},\n",
       " {'titulo': 'E-BELA: Enhanced Embedding-Based Entity Linking Approach',\n",
       "  'informacoes_url': '',\n",
       "  'idioma': 'english',\n",
       "  'storage_key': '../articles/original/english/985-24750-1-10-20240923.pdf',\n",
       "  'author': 'Ítalo M. Pereira and Anderson A. Ferreira',\n",
       "  'data_publicacao': '11-09-2024',\n",
       "  'resumo': 'Entity linking is the process of connecting mentions of entities in natural language texts, such as references to people or places, to specific entities in knowledge graphs, such as DBpedia or Wikidata. This process is crucial in the natural language processing tasks since it facilitates disambiguating entities in unstructured data, enhancing understanding and semantic processing. However, entity linking faces challenges due to the complexity and ambiguity of natural languages, as well as the discrepancy between the form of textual entity mentions and entity representations. Considering that entity mentions are in natural language and entity representations in knowledge graphs have object nodes that describe them in the same way, in this work we propose E-BELA, an effective approach based on literal embeddings. We aim to put close vector representations of mentions and entities in a vector space, allowing linking of mentions and entities by using a similarity or distance metric. The results demonstrate that our approach outperforms previous ones, contributing to the field of natural language processing. ###',\n",
       "  'keywords': 'Natural Language Processing, Entity Linking, Linked Open Data, Entity Similarity, Embedding, Disambiguation, DBpedia',\n",
       "  'referencias': ['[1] Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St.\\nJohn, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, YunHsuan Sung, Brian Strope, and Ray Kurzweil. 2018. Universal Sentence Encoder.\\n*CoRR* abs/1803.11175 (2018). arXiv:1803.11175 http://arxiv.org/abs/1803.11175',\n",
       "   '[2] Lihan Chen, Tinghui Zhu, Jingping Liu, Jiaqing Liang, and Yanghua Xiao. 2023.\\nEnd-to-End Entity Linking with Hierarchical Reinforcement Learning. *Proceed-*\\n*ings of the AAAI Conference on Artificial Intelligence* 37, 4 (Jun. 2023), 4173–4181.\\nhttps://doi.org/10.1609/aaai.v37i4.25534',\n",
       "   '[3] Lucas Colucci, Prachi Doshi, Kun-Lin Lee, Jiajie Liang, Yin Lin, Ishan Vashishtha,\\nJia Zhang, and Alvin Jude. 2016. Evaluating Item-Item Similarity Algorithms\\nfor Movies. In *Proceedings of the 2016 CHI Conference Extended Abstracts on*\\n*Human Factors in Computing Systems* (San Jose, California, USA) *(CHI EA*\\n*’16)* . Association for Computing Machinery, New York, NY, USA, 2141–2147.\\nhttps://doi.org/10.1145/2851581.2892362',\n",
       "   '[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In *Proceedings of the 2019 Conference of the North American Chap-*\\n*ter of the Association for Computational Linguistics: Human Language Technolo-*\\n*gies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long*\\n*and Short Papers)*, Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational Linguistics, Minneapolis, MN, USA, 4171–4186.\\nhttps://doi.org/10.18653/V1/N19-1423',\n",
       "   '[5] Tommaso Di Noia, Roberto Mirizzi, Vito Claudio Ostuni, Davide Romito, and\\nMarkus Zanker. 2012. Linked Open Data to Support Content-Based Recommender\\nSystems. In *Proceedings of the 8th International Conference on Semantic Systems*\\n(Graz, Austria) *(I-SEMANTICS ’12)* . Association for Computing Machinery, New\\nYork, NY, USA, 1–8. https://doi.org/10.1145/2362499.2362501',\n",
       "   '[6] Tommaso Di Noia and Vito Claudio Ostuni. 2015. *Recommender Systems and*\\n*Linked Open Data* . Springer International Publishing, Cham, 88–113.',\n",
       "   '[7] Mohnish Dubey, Debayan Banerjee, Debanjan Chaudhuri, and Jens Lehmann.\\n2018. EARL: Joint Entity and Relation Linking for Question Answering over\\nKnowledge Graphs. In *The Semantic Web – ISWC 2018*, Denny Vrandečić, Kalina\\nBontcheva, Mari Carmen Suárez-Figueroa, Valentina Presutti, Irene Celino, Marta\\nSabou, Lucie-Aimée Kaffee, and Elena Simperl (Eds.). Springer International Publishing, Cham, 108–126.',\n",
       "   '[8] Jorão Gomes, Rômulo Chrispim de Mello, Victor Ströele, and Jairo Francisco de\\nSouza. 2022. A Hereditary Attentive Template-based Approach for Complex\\nKnowledge Base Question Answering Systems. *Expert Systems with Applications*\\n205 (2022), 117725. https://doi.org/10.1016/j.eswa.2022.117725',\n",
       "   '[9] Ningning Jia, Xiang Cheng, Sen Su, and Liyuan Ding. 2021. CoGCN: Combining\\nco-attention with graph convolutional network for entity linking with knowledge\\ngraphs. *Expert Systems* 38, 1 (2021), e12606. https://doi.org/10.1111/exsy.12606\\narXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/exsy.12606',\n",
       "   '[10] Hongkun Leng, Caleb De La Cruz Paulino, Momina Haider, Rui Lu, Zhehui Zhou,\\nOle Mengshoel, Per-Erik Brodin, Julien Forgeat, and Alvin Jude. 2018. Finding\\nsimilar movies: dataset, tools, and methods. In *Proceedings of the 8th Interna-*\\n*tional Conference on Semantic Systems (WSCG’2018)* . Václav Skala-UNION Agency,\\nPlzen, Czech Republic, 115–124.',\n",
       "   '[11] Huiying Li, Wenqi Yu, and Xinbang Dai. 2023. Joint linking of entity and relation\\nfor question answering over knowledge graph. *Multimedia Tools and Applications*\\n82, 29 (01 Dec 2023), 44801–44818. https://doi.org/10.1007/s11042-023-15646-w',\n",
       "   '[12] Qijia Li, Feng Li, Shuchao Li, Xiaoyu Li, Kang Liu, Qing Liu, and Pengcheng\\nDong. 2022. Improving Entity Linking by Introducing Knowledge Graph\\nStructure Information. *Applied Sciences* 12, 5 (2022), 44801–44818. https:\\n//doi.org/10.3390/app12052702',\n",
       "   '[13] Tomás Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Estimation of Word Representations in Vector Space. In *1st International Conference on*\\n*Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Work-*\\n*shop Track Proceedings*, Yoshua Bengio and Yann LeCun (Eds.). Association for\\nComputing Machinery, Scottsdale, Arizona, USA. http://arxiv.org/abs/1301.3781',\n",
       "   '[14] Tomás Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.\\nDistributed Representations of Words and Phrases and their Compositionality.\\n*CoRR* abs/1310.4546 (2013). arXiv:1310.4546 http://arxiv.org/abs/1310.4546',\n",
       "   '[15] Roberto Mirizzi, Tommaso Di Noia, Azzurra Ragone, Vito Ostuni, and Eugenio\\nDi Sciascio. 2012. Movie recommendation with DBpedia, In Movie recommendation with DBpedia. *CEUR Workshop Proceedings* 835, 101–112.',\n",
       "   '[16] Jean Gabriel Nguema Ngomo, Giseli Rabello Lopes, Maria Luiza Machado Campos,\\nand Maria Claudia Reis Cavalcanti. 2020. An Approach for Improving DBpedia\\nas a Research Data Hub. In *Proceedings of the Brazilian Symposium on Multime-*\\n*dia and the Web* (São Luís, Brazil) *(WebMedia ’20)* . Association for Computing\\nMachinery, New York, NY, USA, 65–72. https://doi.org/10.1145/3428658.3431075',\n",
       "   '[17] Ítalo M. Pereira and Anderson A. Ferreira. 2019. An Item-Item Similarity Approach Based on Linked Open Data Semantic Relationship. In *Proceedings of the*\\n*25th Brazillian Symposium on Multimedia and the Web* (Rio de Janeiro, Brazil)\\n*(WebMedia ’19)* . Association for Computing Machinery, New York, NY, USA,\\n425–432. https://doi.org/10.1145/3323503.3349547',\n",
       "   '[18] Maria Pershina, Yifan He, and Ralph Grishman. 2015. Personalized Page Rank\\nfor Named Entity Disambiguation. In *Proceedings of the 2015 Conference of*\\n*the North American Chapter of the Association for Computational Linguistics:*\\n*Human Language Technologies*, Rada Mihalcea, Joyce Chai, and Anoop Sarkar\\n(Eds.). Association for Computational Linguistics, Denver, Colorado, 238–243.\\nhttps://doi.org/10.3115/v1/N15-1026',\n",
       "   '[19] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings\\nusing Siamese BERT-Networks. In *Proceedings of the 2019 Conference on Empirical*\\n*Methods in Natural Language Processing and the 9th International Joint Confer-*\\n*ence on Natural Language Processing (EMNLP-IJCNLP)*, Kentaro Inui, Jing Jiang,\\nVincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics,\\nHong Kong, China, 3982–3992. https://doi.org/10.18653/v1/D19-1410',\n",
       "   '[20] Petar Ristoski and Heiko Paulheim. 2016. RDF2Vec: RDF Graph Embeddings\\nfor Data Mining. In *The Semantic Web – ISWC 2016*, Paul Groth, Elena Simperl,\\nAlasdair Gray, Marta Sabou, Markus Krötzsch, Freddy Lecue, Fabian Flöck, and\\nYolanda Gil (Eds.). Springer International Publishing, Cham, 498–514.',\n",
       "   '[21] Petar Ristoski, Jessica Rosati, Tommaso Di Noia, Renato De Leone, and Heiko\\nPaulheim. 2019. RDF2Vec: RDF graph embeddings and their applications. *Se-*\\n*mantic Web* 10, 4 (2019), 721–752.',\n",
       "   '[22] Ahmad Sakor, Kuldeep Singh, Anery Patel, and Maria-Esther Vidal. 2020. Falcon\\n2.0: An Entity and Relation Linking Tool over Wikidata. In *Proceedings of the 29th*\\n*ACM International Conference on Information & Knowledge Management* (Virtual\\nEvent, Ireland) *(CIKM ’20)* . Association for Computing Machinery, New York, NY,\\nUSA, 3141–3148. https://doi.org/10.1145/3340531.3412777',\n",
       "   '[23] W. Shen, Y. Li, Y. Liu, J. Han, J. Wang, and X. Yuan. 2023. Entity Linking Meets\\nDeep Learning: Techniques and Solutions. *IEEE Transactions on Knowledge; Data*\\n*Engineering* 35, 03 (mar 2023), 2556–2578. https://doi.org/10.1109/TKDE.2021.\\n3117715',\n",
       "   '[24] Wei Shen, Jianyong Wang, and Jiawei Han. 2015. Entity Linking with a Knowledge Base: Issues, Techniques, and Solutions. *IEEE Transactions on Knowledge*\\n*and Data Engineering* 27, 2 (Feb 2015), 443–460. https://doi.org/10.1109/TKDE.\\n2014.2327028',\n",
       "   '[25] Uma Srinivasan and Chidambaram Mani. 2018. Diversity-Ensured Semantic\\nMovie Recommendation by Applying Linked Open Data. *International Journal*\\n*of Intelligent Engineering and Systems* 11 (04 2018), 275–286.',\n",
       "   '[26] Priyansh Trivedi, Gaurav Maheshwari, Mohnish Dubey, and Jens Lehmann. 2017.\\nLC-QuAD: A Corpus for Complex Question Answering over Knowledge Graphs.\\nIn *The Semantic Web – ISWC 2017*, Claudia d’Amato, Miriam Fernandez, Valentina\\nTamma, Freddy Lecue, Philippe Cudré-Mauroux, Juan Sequeda, Christoph Lange,\\nand Jeff Heflin (Eds.). Springer International Publishing, Cham, 210–218.',\n",
       "   '[27] Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, and Yoshiyasu Takefuji. 2016.\\nJoint learning of the embedding of words and entities for named entity disambiguation. In *CoNLL 2016 - 20th SIGNLL Conference on Computational Natural*\\n*Language Learning, Proceedings (CoNLL 2016 - 20th SIGNLL Conference on Compu-*\\n*tational Natural Language Learning, Proceedings)* . Association for Computational\\nLinguistics (ACL), United States, 250–259. https://doi.org/10.18653/v1/k16-1025\\nPublisher Copyright: © 2016 Association for Computational Linguistics.; 20th\\nSIGNLL Conference on Computational Natural Language Learning, CoNLL 2016\\n; Conference date: 11-08-2016 Through 12-08-2016.',\n",
       "   '[28] Ikuya Yamada, Koki Washio, Hiroyuki Shindo, and Yuji Matsumoto. 2022. Global\\nEntity Disambiguation with BERT. In *Proceedings of the 2022 Conference of the*\\n*North American Chapter of the Association for Computational Linguistics: Hu-*\\n*man Language Technologies*, Marine Carpuat, Marie-Catherine de Marneffe, and\\nIvan Vladimir Meza Ruiz (Eds.). Association for Computational Linguistics, Seattle, United States, 3264–3271. https://doi.org/10.18653/v1/2022.naacl-main.238\\n\\n\\n123\\n\\n\\n-----'],\n",
       "  'text': '# **E-BELA: Enhanced Embedding-Based Entity Linking Approach**\\n\\n## Ítalo M. Pereira\\n#### italo.pereira@ifmg.edu.br Instituto Federal de Minas Gerais - SJE São João Evangelista, MG, Brasil\\n### **ABSTRACT**\\n\\nEntity linking is the process of connecting mentions of entities in\\nnatural language texts, such as references to people or places, to\\nspecific entities in knowledge graphs, such as DBpedia or Wikidata.\\nThis process is crucial in the natural language processing tasks since\\nit facilitates disambiguating entities in unstructured data, enhancing\\nunderstanding and semantic processing. However, entity linking\\nfaces challenges due to the complexity and ambiguity of natural\\nlanguages, as well as the discrepancy between the form of textual\\nentity mentions and entity representations. Considering that entity\\nmentions are in natural language and entity representations in\\nknowledge graphs have object nodes that describe them in the same\\nway, in this work we propose E-BELA, an effective approach based\\non literal embeddings. We aim to put close vector representations\\nof mentions and entities in a vector space, allowing linking of\\nmentions and entities by using a similarity or distance metric. The\\nresults demonstrate that our approach outperforms previous ones,\\ncontributing to the field of natural language processing.\\n### **KEYWORDS**\\n\\nNatural Language Processing, Entity Linking, Linked Open Data,\\nEntity Similarity, Embedding, Disambiguation, DBpedia\\n### **1 INTRODUCTION**\\n\\nThe increasing volume of data published on the web has led to an\\nera of information overload. Persons generate more information\\nthan they can actually process and consume [ 3, 5, 6, 10, 15, 25 ].\\nAdditionally, natural language, one of the most important forms of\\ndata on the web, is inherently complex and ambiguous. To address\\nthis scenario, many efforts have been made. Among these efforts,\\nthe creation of the Web of Data stands out, achieved through the\\ndevelopment of Linked Open Data (LOD) datasets. These datasets\\nare massive Knowledge Bases (KBs), also referred to as Knowledge\\nGraphs (KGs), containing millions of entities and billions of factual\\nrelationships [ 16 ]. Moreover, a key characteristic is their machinereadable nature [ 9 ]. Examples of such datasets include DBpedia [1],\\nWikidata [2], YAGO [3], among others.\\nRelating data published on the web to data in such KGs may\\nmitigate the complexity and inherent ambiguity of natural language, as well as enrich these KGs. This purpose may be achieved\\nby aligning entity mentions obtained from the data published on\\n\\n1 https://www.dbpedia.org/\\n2 https://www.wikidata.org/\\n3 https://yago-knowledge.org/\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n© 2024 SBC – Brazilian Computing Society.\\nISSN 2966-2753\\n\\n## Anderson A. Ferreira\\n#### anderson.ferreira@ufop.edu.br Universidade Federal de Ouro Preto Ouro Preto, MG, Brasil\\n\\nthe web, such as references to people or places, and entity representations in knowledge bases, hereinafter referred to as *𝑒𝑛𝑡𝑖𝑡𝑖𝑒𝑠* .\\nIn addition, this alignment aid a wide variety of problems, such as\\npopulating knowledge bases, content analysis, relation extraction,\\nand question-answering systems [ 23 ]. This alignment is one of the\\nmain tasks in Natural Language Processing (NLP) [ 7, 9 ] and is defined as Entity Linking (EL), also known as Named Entity Linking,\\nNamed Entity Disambiguation, or Entity Disambiguation.\\nFormally, given a document *𝐷* containing a set of named entity\\nmentions *𝑀* = { *𝑚* 1 *,𝑚* 2 *, ...,𝑚* | *𝑀* | } along with its context, and a KG\\ncontaining a set of named entities *𝐸* = { *𝑒* 1 *,𝑒* 2 *, ...,𝑒* | *𝐸* | }, the goal is\\nto define a function *𝑓* that maps each entity mention *𝑚* *𝑖* ∈ *𝑀* to its\\ncorresponding entity *𝑒* *𝑗* ∈ *𝐸* [23].\\nHowever, the entity mentions present in sentences are in natural\\nlanguage, whereas the KG data in LOD is represented by graphs.\\nSpecifically, these graphs adhere to the *Resource Description Frame-*\\n*work* (RDF [4] ), in which information or facts are represented as interconnected nodes linked by edges. The connection between two\\nnodes, via an edge, constitutes a triple, composed of a subject (the\\nfirst node), a predicate or property (the edge), and an object (the\\nsecond node) ( *𝑠𝑢𝑏𝑗𝑒𝑐𝑡* → *𝑝𝑟𝑒𝑑𝑖𝑐𝑎𝑡𝑒* → *𝑜𝑏𝑗𝑒𝑐𝑡* ). The edge denotes\\nthe relationship between the two nodes, which can represent entities or literals (entity data). A collection of these triples constitutes\\na directed and labeled graph known as an RDF graph, which also\\nserves as a Knowledge Graph [8, 17].\\nFigure 1 illustrates the Entity Linking process. The rectangle on\\nthe left contains a snippet of text in natural language. There are two\\nmentions of entities (people). In this case, the mentions are *Barack*\\n*Obama* and *Lolo Soetero* . On the right, we illustrate a part of a KG\\nwith several RDF triples. In this example, the entities are *Q4115068*\\n( *Lolo Soetero* ) and *Q76* ( *Barack Obama* ) and their properties are\\nrepresented by the *Label* and *Description* edges. The connections\\nbetween the mentions of entities and their corresponding entities in\\nthe KG are symbolized by dashed arrows. EL employs the contexts\\nof the mentions, represented by the full text in the rectangle, and\\nthe contexts of the entities in the KG, indicated by the literal nodes\\nlinked to the entities, in its processing\\nMany current NLP tasks rely on vector representations of their\\ndata. In this context, each element (term, phrase, object, node, ...)\\nneeds to be represented by feature vectors ⟨ *𝑓* 1 *, 𝑓* 2 *, ..., 𝑓* *𝑛* ⟩ . These\\nvectors usually contain binary ( *𝑓* *𝑖* ∈{ *𝑇𝑟𝑢𝑒, 𝐹𝑎𝑙𝑠𝑒* } ), numerical\\n( *𝐹* *𝑖* ∈ R ) or nominal ( *𝑓* *𝑖* ∈ *𝑆*, where *𝑆* is a finite set of symbols)\\nvalues [ 20, 21 ]. The task of mapping this data to vectors can be\\naccomplished through embedding. The main idea is to represent\\nthe meaning of a piece of natural language (such as text, images,\\nor audio) using dense, low-dimensional vectors with real-valued\\n\\n4 https://www.w3.org/RDF/\\n\\n\\n115\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Ítalo M. Pereira and Anderson A. Ferreira\\n\\nintegrates entity mentions from a text with the entities present in\\nthe KG. We use embedding representations to put mention and its\\nentity close in a common vector space, considering their semantic\\ncontext. Our hypothesis is that this approach will allow for more\\nprecise and coherent linking between mentions and entities by\\ncalculating a similarity or distance metric. In simplified terms, based\\non the calculated similarity or distance between the representation\\nof a mention and the representations of entities, it will be capable\\nof linking the mention to the closest entity, whether considering\\nsimilarity or distance metrics. It is essential to emphasize that the\\ncontext surrounding the mention must be taken into account.\\nConsidering that entity mentions are in the form of natural language and the entities present in the KG have object nodes that\\ndescribe them in the same way, our hypothesis is that obtaining\\nrepresentations of these entities based on their literals can guarantee that their vector representations are in the same vector space\\nof the mentions, and, we hope, are also close in that space.\\nBased on the context, problem, and objectives presented, the\\nfollowing research questions have been raised:\\n\\n\\n**Figure 1: Illustration of Entity Linking. The rectangle on the**\\n**left contains a snippet of text in natural language (mention**\\n**context) and highlights two entity mentions in rounded rect-**\\n**angles. The cylinder on the right represents a portion of a KG.**\\n**The dashed arrows represent the linkages from the mentions**\\n**to the entities in the KG.**\\n\\ncomponents. These vectors are also referred to as latent representations [23].\\nFurthermore, these vectors are designed to capture semantic\\nsimilarity. In the context of words, this means that similar words\\nshould be close to each other in the vector space [ 27 ]. Additionally,\\ndistributed representations of words in dense vectors help learning\\nalgorithms achieve better results in NLP tasks, due, for example, to\\nthe proximity of similar words, considering the distance between\\nthem in the vector space [14].\\nIn addition to the challenge of obtaining vector representations\\nof mentions and entities, EL must address the complexity and ambiguity of natural language, since entity mentions may be written\\nin many ways. For instance, a name can be written in full, partial,\\nabbreviated, or even nickname forms (such as the abbreviation “LA”\\nand the nickname “City of Angels” for the city of “Los Angeles”).\\nMoreover, mentions can ambiguously refer to multiple entities.\\nFor instance, the mention “Washington” could refer to the American state of “Washington”, the capital city of the United States,\\n“ Washington DC ”, the actor “Denzel Washington”, or even the first\\nU.S. president, “George Washington” [7, 9, 12, 23, 27].\\nFor the reasons mentioned above, we propose in this work a\\nsimple and effective approach for EL, called E-BELA, an acronym for\\n“Enhanced Embedding-Based Entity Linking Approach”. E-BELA\\n\\n\\n\\n  - Question 1: Does adopting embeddings of entities from a\\nKG in a vector space, based on the embeddings of their literals, provide advantages in terms of effectiveness for the\\nEL task compared with other approaches documented in the\\nliterature?\\n\\n  - Question 2: Will the vector representations of KG literals\\nand KG entities retain the semantic contexts of their corre\\nsponding literals?\\n\\n  - Question 3: Does the number of properties and, consequently,\\nliterals associated with KG entities impacts EL accuracy?\\n\\nThus, our main contributions include: the proposal of an EL\\napproach that in an experimental study outperforms existing approaches; the representation of DBpedia through embeddings, applicable to tasks that include semantic similarity, such as clustering,\\ntextual similarity, semantic search, even allows recovering entities\\nsemantically similar to a mention in this KG.\\nThe rest of this paper is organized as follows. Section 2 describes\\nrelated work. Section 3 describes our approach. Section 4 describes\\nthe experimental evaluation and discusses its results. And Section 5\\npresents the conclusion and future work.\\n### **2 RELATED WORK**\\n\\nEL is an essential task in Natural Language Processing, widely used\\nin several downstream applications, such as question-answering\\nsystems, relation extraction, knowledge base population, content\\nanalysis, and so on [ 23 ]. Therefore, over time, this challenge has\\nattracted a wide variety of solutions [12].\\nInitially, many works in this field were based on traditional\\nmachine learning techniques, relying on local context compatibility, global coherence, manually designed features (such as entity\\npopularity), and rule-based methods [ 24 ]. However, the rapid development of deep learning techniques has led to new approaches\\nthat outperform the results of previous ones [ 12 ]. In general, the EL\\nprocess involves two subtasks: (1) candidate entity generation, and\\n(2) entity disambiguation. Different solutions have been employed\\nto address these subtasks.\\n\\n\\n116\\n\\n\\n-----\\n\\nE-BELA: Enhanced Embedding-Based Entity Linking Approach WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\nThe following works share the common characteristic of jointly\\nperforming entity and relation linking. Additionally, they obtain\\nthe candidate entities using indices generated by ElasticSearch [5],\\nwith data from various sources, including Wikidata and DBpedia.\\nIn [ 11 ], a method called JLEAR is proposed. JLEAR aims to explore\\nboth independent and joint features of the candidates for disambiguation. Independent features include Entity Popularity, Literal\\nSimilarity (Levenshtein distance), and Semantic Similarity. Joint\\nfeatures leverage the correlation between candidate entities and\\nrelations. In [ 22 ], the authors propose Falcon 2.0, which employs a\\ndisambiguation method based on two modules: the first one called\\n“Matching and Classification”, and the second called “Relevant Rule\\nSelection”. “Matching and Classification” module combines entities\\nand relations from the candidate list into RDF triples and verifies\\ntheir existence in the knowledge graph (KG). Relations and entities\\nrelated to existing triples receive higher scores. The “Relevant Rule\\nSelection” module interacts with the previous module, suggesting\\nscore adjustments for some candidates based on a pre-built rule\\ncatalog. Finally, Dubey et al. [ 7 ] propose a framework called EARL.\\nIn EARL, disambiguation relies on two strategies: Generalized Travelling Salesman Problem (GTSP) and Connection Density. GTSP\\nevaluates the shortest path between combinations of entities and\\nrelations in the candidate lists, while Connection Density is based\\non features such as the number of connections and hops in the\\nknowledge graph.\\nOther works employ a variety of strategies. In [ 2 ], the authors\\npropose modeling the EL task using reinforcement learning. They\\nintroduce high-level and low-level procedures and policies for mention detection and disambiguation. In [ 12 ], an approach called KGEL\\n(Knowledge Graph-based Entity Linking) is proposed to enrich the\\nEL process by incorporating structural information from the knowledge graph. KGEL utilizes both local and global features to evaluate\\nthe mapping between mentions and entities. In [ 28 ], the authors\\npropose entity disambiguation using a model based on BERT [ 4 ].\\nThis model is similar to the BERT Masked Language Model but is\\ntrained to predict masked entities. Finally, in [ 27 ], a method based\\non embeddings is proposed. It jointly embeds words and entities\\ninto the same vector space using three Skip-gram models[ 13, 14 ].\\nIts disambiguation relies on two contexts: textual context similarity\\nthat is based on the similarity between entity and word vectors,\\nand coherence that is based on a target entity and other related\\nentities. That method utilizes candidate lists created in [18].\\nAmong the mentioned works, a variety of resources are employed for the EL task. Highlights include: use of local and global\\nfeatures [ 7, 9, 12, 27, 28 ]; embeddings [ 2, 12, 27, 28 ]; algorithms\\nbased on Neural Networks and Reinforcement Learning [2, 7, 9].\\nIn E-BELA, we obtain the embeddings of literals by transfer\\nlearning, a simplified process that eliminates the need for additional\\ntraining or fine tuning. This approach allows E-BELA to obtain the\\nlist of candidate entities to be linked to a mention directly from\\nthe KG. Many works depend on external data. Furthermore, disambiguation is performed using the contexts of mentions and entities.\\nThis enables E-BELA to achieve effective results when dealing with\\nthe dynamic contexts of KGs, demonstrating the model’s robustness\\n\\n5 https://www.elastic.co/products/elasticsearch\\n\\n\\n**Figure 2: Overview of E-BELA. The top lane illustrates the**\\n**process of embedding the KG entities. The bottom lane illus-**\\n**trates the entity linking process, from generating candidate**\\n**entities to subsequent ambiguity resolution.**\\n\\nin adapting to different scenarios without the need for complex\\ntraining interventions.\\n### **3 E-BELA** **3.1 Overview**\\n\\nOur proposed approach, E-BELA, aims to put representations of\\nmentions and their corresponding entities close together in a vector\\nspace, enabling to perform EL by applying a similarity metric. All\\nartifacts comprising E-BELA are available for download at the\\nfollowing address: https://github.com/italompereira/E-BELA.\\nFigure 2 provides a general overview of E-BELA. In the upper\\nlane, the embedding process takes place, which includes the selection and preprocessing of a KG data, followed by the actual\\nembedding process that obtains the entity vector representation.\\nNext, E-BELA stores these vector representation into a vector database. The lower lane illustrates the entity linking process. E-BELA\\nobtains candidate entities for mentions, as well as their vector\\nrepresentation. And, finally, It uses the context information to disambiguate and provide the corresponding entities to the mentions.\\nIt is worth highlighting that it is not part of the scope of our work to\\nidentify mentions in a natural language text; tools such as spaCy [6]\\n\\nor NLTK [7] can be used for this task.\\n### **3.2 Embedding Process**\\n\\nThis section discusses the process of embedding, from data selection\\nand preprocessing to the acquisition of vector representations for\\n\\nentities.\\n\\n6 https://spacy.io/\\n7 https://www.nltk.org/\\n\\n\\n117\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Ítalo M. Pereira and Anderson A. Ferreira\\n\\n\\nSince entity mentions in texts are in natural language and the\\nentities in KGs have object nodes that describe them also in natural\\nlanguage, as said before, it is possible to obtain their representations\\nbased on their literals and thus ensure that their vector representations are in the same vector space. Additionally, we hope that\\nsemantically similar mentions and entities would have close vector\\nrepresentations.\\n\\n*3.2.1* *Data Selection and Preprocessing.* The KG datasets, such as\\nDBpedia and Wikidata, are available for download through specific\\nweb pages. In this work, we used the DBpedia data containing\\nontology information, and links to other datasets, beyond to the\\ndata. We collected files from “https://downloads.dbpedia.org/201610/” that contain literal data describing the entities, as well as those\\ncontaining the necessary references for disambiguating the entities.\\nWe structure those files into an extensive three-column Apache\\nSpark [8] DataFrame: ⟨ *𝑠𝑢𝑏𝑗𝑒𝑐𝑡* ⟩⟨ *𝑝𝑟𝑒𝑑𝑖𝑐𝑎𝑡𝑒* ⟩⟨ *𝑜𝑏𝑗𝑒𝑐𝑡* ⟩ . Apache Spark\\nwas chosen for its ability to handle large volumes of data and its\\nefficiency in performing queries, similar to a relational database\\nmanagement systems.\\n\\n*3.2.2* *Embedding Process.* In this subsection, we discuss the process\\nof obtaining vector representations for literals and entities by EBELA.\\n\\nThese vector representations can be obtained by training a language model or by transferring learning from pre-trained models.\\nIn this work, we choose transfer learning from pre-trained models.\\nThose models are trained on large data corpora, ensuring higher\\naccuracy, and making possible to reduce the computational cost\\nand time.\\n\\nWe straightly obtain the vector representation for the entity\\nmentions by using those models, and, for the KG entities, we apply a two-step process: In the first step, E-BELA obtains vector\\nrepresentations for the literal nodes. In the second step, it obtains\\na vector representation for each entity by aggregating the vector\\nrepresentations from its literals.\\n\\n*Literal Embeddings.* For the first step, we evaluated the USE [ 1 ] [9]\\n\\nand *all-mpnet-base-v2* [10] models. The latter is one of the original\\nmodels from SBERT [ 19 ] for sentence encoding. Both USE and\\nSBERT encode text into low-dimensional vectors, which can be\\nutilized for tasks such as text classification, semantic similarity,\\nclustering, and other natural language processing tasks.\\nWe chose these models because they are considered state-of-theart in encoding sentences into embedding vectors, according to the\\nresults presented [ 1 ] and [ 19 ]. Furthermore, they specifically aim\\nto transfer learning to other NLP tasks.\\nThe strategy for obtaining embeddings involves providing literals as input to the model, which processes them and generates\\nrepresentative multidimensional vectors. Equation 1 expresses this\\noperation, where *𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔* is the function that takes a literal *𝑙* *𝑖*\\n(e.g., a word or sentence) as input and returns its vector representation *𝑙* [�] *𝑖* as output. These vectors encapsulate the semantics contained\\nwithin the literals.\\n\\n8 https://spark.apache.org/\\n9 https://tfhub.dev/google/universal-sentence-encoder/4\\n10 https://huggingface.co/sentence-transformers/all-mpnet-base-v2\\n\\n\\n�\\n*𝑙* *𝑖* = *𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔* ( *𝑙* *𝑖* ) (1)\\n\\nTo conduct the experiments in this work, we sort the data previously assigned to the Spark DataFrame (see Subsection 3.2.1) by the\\nsubject attribute. This sorting allows us iterate on the data frame\\nentity by entity. Next, E-BELA selects the literals in English language. DBpedia identifies such literals by using the ‘@en’ language\\ntag, such as “ *story and dialogues* ”@en, for instance. Additionally,\\nE-BELA preprocesses the literals by removing characters belonging\\nto {( *,* ) *,* [′] *,* [′′] *, .,,,* : *, <, >,* ? *,* ! *,* @ *,* $ *,* % *,* &}.\\nAccording to the USE documentation, there is no limitation on\\nthe input size. However, as longer as the input text more “diluted”\\nis its embedding. The SBERT model, specifically *all-mpnet-base-v2*,\\nrestricts input size to 384 characters. The USE model produces realvalued vectors with 512 dimensions, whereas the *all-mpnet-base-v2*\\nmodel generates real-valued vectors with 768 dimensions. These\\nvectors are primarily useful for semantic search tasks and semantic\\ntextual similarity.\\n\\n*Entity Embeddings.* Aiming to get an entity vector representation\\nclose to its corresponding mention, we chose to obtain it from vector\\nrepresentations from its literals, instead of using a graph-specific\\nembedding technique such as RDF2Vec[ 20 ], for instance. E-BELA\\nobtains the entity vector representation by averaging its literal\\nassociated vectors. We also evaluated other strategies for obtaining\\nthe entity vector representation, including vector summation and\\nweighting based on literal frequencies in the KG. But, averaging\\nyielded the best results. Equation 2 expresses this operation for\\nobtaining the vector representation of the j-th entity *𝑒* *𝑗*, where *𝐿* *𝑒* *𝑗*\\n*𝑙* � represents the set of literal vectors associated with that entity, and *𝑖* is the i-th vector of a literal in that set.\\n\\n\\n� 1\\n*𝑒* *𝑗* =\\n| *𝐿* *𝑒* *𝑗* |\\n\\n\\n| *𝐿* *𝑒𝑗* |\\n\\n*𝑖* =1\\n\\n∑︁\\n\\nFigure 3 shows a simplified illustration of our proposed approach.\\n\\n\\n�\\n*𝑙* *𝑖* *,* ∀ *𝑙* ∈ *𝐿* *𝑒* *𝑗* (2)\\n\\n*𝑖* =1\\n\\n∑︁\\n\\n\\nFigure 3 shows a simplified illustration of our proposed approach.\\nIn this figure, entities Q76 and Q4115068 from Wikidata represent *“Barack Obama”* and his stepfather *“Lolo Soetoro”*, respectively,\\nalong with their literals. Initially, literal vector representations for\\n*“Barack Obama”, “44th president of the United States of America”,*\\n*“Stepfather of Barack Obama”* are obtained using Equation 1. In the\\nfigure, these representations appear as inner circles within orange\\nbackground. After obtaining the vector representations of the literals, E-BELA obtains the entity vectors by using Equation 2. In\\nFigure 3, the vector representation of entity Q76 is calculated based\\non the average of the vectors of its literals. The vectors involved\\nin this calculation are highlighted by green arrows whose labels\\nare *𝑒𝑄* 76. The vector for entity Q4115068 is also obtained based\\non the average of its vectors, in this case, only the literal vector of\\n*“Stepfather of Barack Obama”* .\\nConsidering the amount of vector generated using a dataset\\nsuch as DBpedia, which implies in a vast search space during the\\nentity linking process, we adopted PostgreSQL 14.12 and along with\\nthe pgvector [11] plugin as our vector database management system.\\nThis plugin enables PostgreSQL to perform exact and approximate\\n\\n\\n11 https://github.com/pgvector/pgvector\\n\\n\\n118\\n\\n\\n-----\\n\\nE-BELA: Enhanced Embedding-Based Entity Linking Approach WebMedia’2024, Juiz de Fora, Brazil\\n\\nTo illustrate, consider the following sentence: “Japan scored two\\ngoals against China”. In this sentence, the mentions “Japan” and\\n“China” individually describe Asian countries. However, within the\\ncontext of the sentence, their meaning is related to soccer teams.\\nCosine similarity enables to retrieve soccer teams as candidate\\nentities even when they are more distant, considering Euclidean\\ndistance.\\n\\nEquation 4 calculates the cosine similarity,\\n\\n*𝑠𝑖𝑚* *𝑐𝑜𝑠* (� *𝑣* *𝑒* *,* � *𝑣* *𝑚* ) = *𝑣* � *𝑒*                                       - � *𝑣* *𝑚* (4)\\n∥� *𝑣* *𝑒* ∥∥� *𝑣* *𝑚* ∥\\n\\n\\n**Figure 3: The figure illustrates the acquisition of vector rep-**\\n**resentations for entities. In this example, entities Q76 and**\\n**Q4115068 are displayed, along with their literals. The ob-**\\n**tained vectors are represented by rectangles containing mul-**\\n**tiple circles.**\\n\\nsearches (indexing the data) for nearest neighbors, using metrics\\nsuch as L2 distance (Euclidean), inner product, cosine distance,\\nL1 distance (Manhattan), Hamming distance, and Jaccard distance.\\nPostgreSQL efficiently indexes and retrieves close vectors using a\\ndistance function.\\n### **3.3 Entity Linking**\\n\\nGenerally, the EL process is carried out in two steps: (i) obtaining\\nthe candidate list and (ii) performing the disambiguation process.\\nSince the vector representations of the entities and their literals are\\narranged in a vector space, these tasks can be accomplished using\\nsimilarity or distance metrics.\\n\\n*3.3.1* *Candidate Generation.* We gather the candidate list for a\\nmention comparing mention vector with entity vectors, by using\\ncosine similarity. We also compare a mention vector with the literal\\nvector to compose the candidate list, i.e., the candidate list contains\\nthe *𝑛* most similar vectors, among entity and literal vectors, compared with the mention vector. Formally, Equation 3 obtains such a\\nlist.\\n\\n*𝑐𝑎𝑛𝑑𝑖𝑑𝑎𝑡𝑒𝑠* *𝑚* *𝑖* = *𝑠𝑜𝑟𝑡* ( *𝑠𝑖𝑚* ( *𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔* ( *𝑚* *𝑖* ) *, 𝐸* ∪ *𝐿* ))[: *𝑛* ] *,* (3)\\n\\nwhere the function *𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔* ( *𝑚* *𝑖* ) returns the mention vector of\\nthe mention *𝑚* *𝑖*, the function *𝑠𝑖𝑚* returns the similarity values between the mention vector and all entity and literal vectors, *𝐸* is the\\nset of entity vectors, *𝐿* is the set of literal vectors, *𝑠𝑜𝑟𝑡* orders the\\nsimilarity results, and [: *𝑛* ] obtains the top *𝑛* sorted results.\\nWe chose the cosine similarity function based on: (i) usually entities from a KG have properties, as *𝑛𝑎𝑚𝑒* and *𝑙𝑎𝑏𝑒𝑙*, whose vectors\\nare close to the corresponding mention vector. For instance, the\\nvector mention of “Japan” tends to be more similar with entities\\nwhose properties *𝑛𝑎𝑚𝑒* or *𝑙𝑎𝑏𝑒𝑙* contain the term “Japan”. (ii) the\\ncosine similarity function allows us retrieval distant candidates considering Euclidean distance but the angles to the mention vector\\nare small. The usefulness of this method is related to the fact that\\n\\nthe embeddings carry relevant semantic information, and some\\nmodels position these distant entities with a small angle.\\n\\n\\nwhere � *𝑣* *𝑒* represents the vector of an entity and � *𝑣* *𝑚* represents the\\nvector of a mention, - (dot) represents the scalar product between\\nthe vectors, and ∥� *𝑣* ∥ represents the Euclidean norm of a vector.\\n\\n*3.3.2* *Disambiguation.* To perform disambiguation, it is necessary\\nto consider the context of the mentions. In the previous example,\\n“Japan scored two goals against China”, “Japan” and “China” are the\\nmentions, and the whole sentence constitutes the context. Although\\nthe mentions seem to refer to Asian countries, the semantic context\\nreveals that they correspond, in fact, to soccer teams representing\\nthose countries.\\n\\nAs seen, performing the disambiguation of a mention using its\\nrepresentation in isolation is not a good alternative, since entities\\nrepresenting their Asian countries would be considered more similar. Thus, we must use the vector representation of their context.\\nBut, in such a sentence, we have two mentions and we must generate different candidate lists for both mentions. If we use only\\na vector representation of the context, entities representing the\\nsoccer teams of these countries could be retrieved as well as soccer\\n\\nteams of other countries. Alternatively, we could retain the mention\\ntarget and remove other mentions from the sentence, but removing\\na mention within this sentence implies in a context less informative.\\nThus, we choose to combine the mention vector of a mention\\ntarget with a context vector based on the whole sentence. We average such vectors for combining them, similar to how we obtain the\\nentity vectors.\\nFurthermore, in the disambiguation process, we use Euclidean\\ndistance as distance function, instead of the cosine similarity function. Mean vectors represent central points among vectors involved\\nin the operation. Thus, applying Euclidean distance seems more\\npromising than using the cosine similarity function. Nonetheless,\\nwe evaluated both functions.\\n\\nIn this scenario, the closest entity, considering the Euclidean distance, is the entity to be linked to the mention. Equation 5 performs\\nthe Euclidean distance,\\n\\n\\n*𝑑* (� *𝑣* *𝑒* *,* � *𝑣* *𝑚* ) =\\n\\n\\n� *𝑛*\\n\\n(� *𝑣* *𝑒* *𝑖* −� *𝑣* *𝑚* *𝑖* ) [2] (5)\\n\\n� *𝑖* =1\\n∑︁\\n\\n\\nwhere *𝑛* is the number of dimensions of the vectors � *𝑣* and *𝑖* is the\\n\\nindex of each dimension.\\n\\n\\n119\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Ítalo M. Pereira and Anderson A. Ferreira\\n\\n**Table 1: The DBpedia files used in this work**\\n\\n\\nFiles\\n\\ninfobox_properties_en.ttl.bz2\\ninstance_types_en.ttl.bz2\\nlabels_en.ttl.bz2\\nlong_abstracts_en.ttl.bz2\\nmappingbased_literals_en.ttl.bz2\\nmappingbased_objects_en.ttl.bz2\\npersondata_en.ttl.bz2\\ndisambiguations_en.ttl.bz2\\ninfobox_property_definitions_en.ttl.bz2\\ns p ecific _ ma pp in g based _p ro p erties _ en.ttl.bz2\\n### **4 EVALUATION** **4.1 Experimental Setup** **Datasets**\\n\\nFor evaluating our proposal, we use DBpedia (version 2016-10) [12],\\na robust knowledge graph composed of millions of RDF triples.\\nSpecifically, we focus on files containing literal data that describes\\nentities, including the *disambiguations_en.ttl.bz2* file, which disambiguates different Uniform Resource Identifiers (URIs) about\\nthe same entities. Table 1 details the files (downloaded from https:\\n//downloads.dbpedia.org/2016-10/core/) used in our experiments.\\nThose files contain 119 *,* 157 *,* 509 RDF triples, about 35 *,* 318 *,* 483 entities and 27 *,* 370 *,* 487 literals.\\nWe evaluated E-BELA using the LC-QuAD dataset [ 26 ]. The LCQuAD is a dataset of complex questions available for evaluating\\nQuestion Answering Systems over KGs. It contains 5,000 questions\\nand their respective SPARQL queries over the DBpedia dataset. In\\n\\n[ 7 ], the authors adapted LC-QuAD as a benchmark dataset for entity\\nand relation linking. Each question contains the mapping between\\nmentions and URIs of entities and relations from the KG, along with\\nthe corresponding part of the text in the question. According to the\\nauthors, the annotation process was carried out semi-automatically\\nand then manually reviewed. The annotated dataset is publicly\\navailable at https://figshare.com/articles/dataset/Full_Annotated.\\n_LC_QuAD_dataset/5782197.\\nLC-QuAD has 6,612 links between mentions and entities, with\\n3,963 unique entities. We did not find 64 entities in the KG, reducing\\nthe number to 3,899 unique entities. We randomly selected 370 of the\\naforementioned links as a sample of the population for evaluating\\nE-BELA. Adopting a confidence level of 95%, the estimated margin\\nof error for this sample is 5%.\\n### **Evaluation Metric**\\n\\nTo quantify the efficiency of E-BELA, we employed accuracy as\\nthe performance metric, aligning with methodological guidelines\\nfrom previous studies. Accuracy is defined as the ratio between the\\ntotal number of correctly identified entities and the total number of\\nentities. In general, it represents the proportion of correct predictions (both true positives and true negatives) relative to the total\\nevaluated observations, as expressed by Equation 6.\\n\\n12 https://downloads.dbpedia.org/2016-10/\\n\\n\\nAccuracy = [Number of Correct Predictions] (6)\\n\\nTotal Predictions\\n### **Baselines**\\n\\nWe selected recognized state-of-the-art works as baselines for the EL\\ntask: EARL[ 7 ], Falcon 2.0[ 22 ], and JLEAR[ 11 ]. A common feature\\nof these works is performing EL alongside relation linking, an\\napproach we intend to explore in the future.\\nIn these works, EL involves processing a list of candidates followed by disambiguation. Notably, none of them directly performed\\nsemantic entity candidate search in the KG. However, E-BELA does\\nperform this search. For this purpose, they used ElasticSearch to create an index of mention-URI pairs with data from external sources\\nand other KGs.\\n\\nEARL and JLEAR evaluated EL performance using accuracy on\\nthe LC-QuAD evaluation set. Falcon 2.0 adopted precision, recall,\\nand *f-score* as evaluation metrics. The evaluation was conducted on\\nthe LC-QuAD 2.0 test set. However, the authors did not describe\\nhow mention-entity alignment was performed, as this set is not\\nannotated for the EL task. Their performance, based on LC-QuAD\\naccuracy, was obtained from the results reported in [11].\\nIn [ 11 ], the authors used entity disambiguation information from\\nDBpedia. This information is available through a file containing\\nmappings of disambiguated URIs. Such ambiguity exists in the KG\\nand can impact model results. Consider the following RDF triples\\nas an example:\\n\\ndbr:David_Bowen, disambiguates, dbr:David_Bowen_(cricketer),\\ndbr:David_Bowen, disambiguates, dbr:David_Bowens\\n\\nThe URIs dbr:David_Bowen, dbr:David_Bowen_(cricketer) e dbr:\\nDavid_Bowens ambiguously represent the same entity.\\n### **Computational Environment**\\n\\nWe conducted the experimental evaluation of this work on a Dell\\nAlienware R15 computer, with Windows 11 Home Edition operating system, equipped with an Intel i9-13900K processor, an NVIDIA\\nGeForce RTX 4070Ti graphics card, and 32GB of RAM. For programming, we used Python 3.7.16 as the programming language.\\nThe libraries we used include Apache Spark 3.3.4 with Hadoop 3,\\nTensorFlow 2.10.1, Torch 1.10.1+cu113, and Numpy 1.21.6. Spark\\ndepends on the availability of the installed Java Virtual Machine,\\nin this case, JVM 17.0.9 was available. As the Database Management System, we utilized PostgreSQL 14.12 along with the pgvector\\nplugin.\\n### **4.2 Results and Discussion**\\n\\nIn this study, we conducted experiments structured around the established research questions, as outlined in Section 1. The method\\nemployed aimed to obtain empirical data to systematically address\\nthese inquiries. We designed each experiment specifically to test\\nthe related hypotheses and collect relevant information, ensuring\\nthe validity and reliability of the obtained results.\\n\\n\\n120\\n\\n\\n-----\\n\\nE-BELA: Enhanced Embedding-Based Entity Linking Approach WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\n**Table 2: E-BELA compared with baselines**\\n\\nA pp roach Accurac y\\n\\nEARL 0.65\\n\\nFalcon 0.74\\n\\nJLEAR 0.83\\nE-BELA (USE) without disambiguation 0.68\\nE-BELA (USE) with disambiguation 0.74\\nE-BELA (SBERT) without disambiguation 0.78\\n**E-BELA** **(** **SBERT** **)** with disambi g uation **0.84**\\n\\n*4.2.1* *Experiment 1:* Question 1: Does adopting embeddings of\\nentities from a KG in a vector space, based on the embeddings of\\ntheir literals, provide advantages in terms of effectiveness for the EL\\ntask compared with other approaches documented in the literature?\\nWe assessed the effectiveness of E-BELA by conducting a comparative analysis with previously established baselines. We used\\naccuracy as the performance metric to quantify the efficiency of\\nE-BELA. Table 2 shows the results.\\nAlthough we cannot make a direct comparison due to differences in the sample space used in this work and the baselines, the\\ndata shown in Table 2 highlight the validity of the proposed approach, considering the higher accuracy achieved by E-BELA. It\\nis worth noting that the transfer learning model whose accuracy\\noutperformed the others was the *all-mpnet-base-v2*, associated with\\nSBERT. We theorize that the performance of the USE model may\\nhave been compromised during the embedding process because\\nwe did not limit the size of the input sentences. Despite the lower\\naccuracy achieved by the USE model, its result is comparable to\\nthat obtained in Falcon[ 22 ]. Furthermore, the results of E-BELA,\\nusing both models, are substantially superior to those presented in\\nEARL[7].\\nIt is also important to emphasize that unlike the JLEAR approach,\\nproposed in [ 11 ], no training process or fine-tuning of the model\\nwas adopted. However, we have the hypothesis that the process of\\nfine-tuning the models can improve the results obtained by E-BELA,\\nwe intend to evaluate this hypothesis in future work.\\nThus, as in [ 11 ], we utilized the disambiguation information provided by DBpedia. This information allows us validating whether a\\nprediction made by the model, initially incorrect, is an ambiguous\\nreference.\\n\\nHowever, due to the volume of data or the constant updates\\nin DBpedia, this file cannot resolve all the ambiguous references\\npresent. For instance, our model linked the mention “Us congress”\\nto the entity “http://dbpedia.org/resource/Us_congress”, but the\\ncorrect entity labeled by LC-QuAD is “http://dbpedia.org/resource/\\nUnited_States_Congress”. Upon manual verification, we note that\\nboth URIs refer to the same entity, and the disambiguation file does\\nnot contain information about this ambiguity. This suggests that\\nthe accuracy of the model may be even higher than reported in\\nTable 2.\\n\\nFurthermore, it is worth mentioning that due to the nature of\\nthe approach proposed by E-BELA, we can search for candidate\\nentities directly in DBpedia, unlike the baselines, which use a list\\nof mention-URI pairs constructed externally. The baselines depend\\non data external to KG.\\n\\n\\n**Table 3: The E-BELA performance in different scenarios**\\n\\nA pp roach Accurac y\\nE-BELA (USE) (literals only) 0.742\\nE-BELA (USE) (entities only) 0.711\\nE-BELA (USE) (both) 0.745\\nE-BELA (SBERT) (literals only) 0.836\\nE-BELA (SBERT) (entities only) 0.829\\nE-BELA ( SBERT ) ( both ) 0.840\\n\\n*4.2.2* *Experiment 2:* Question 2: Will the vector representations\\nof KG literals and KG entities retain the semantic contexts of their\\n\\ncorresponding literals?\\nTo investigate this question, we conducted experiments to compare the accuracy of E-BELA on the EL task in different scenarios.\\nSince we obtain vector representations of entities from their literals\\nand store both entity and literal representations, we can evaluate\\nthe performance of E-BELA using vector representations in exclusive or integrated way. The analyzed scenarios include: (i) using\\nonly literal vectors; (ii) using only entity vectors; and (iii) using\\nboth literal and entity vectors. This experiment aims to clarify how\\nrepresentative the aggregation approach used by E-BELA is compared with isolated literal representations and using both literal and\\nentity representations. Table 3 shows the results of the experiments\\nconducted, considering the different proposed scenarios.\\nImportantly, we can obtain candidate entities for mentions from\\nliteral representations because each vector representation of a literal\\nis associated with the literal itself and the corresponding KG entity.\\nThis allows us to retrieve lists of candidate entities directly from\\nliteral representations.\\nTable 3 shows that entity vectors, when used in isolation, are\\ncapable of maintaining much of the context obtained from their\\nliterals, despite the slightly inferior result. It is worth noting that the\\nbest result was obtained through the use of both vector representations, including literals and entities. Considering the USE model,\\nthe percentage difference between the best result and the isolated\\nrepresentation of entities was approximately 4 *.* 55%. Considering\\nthe model used by SBERT, the difference was approximately 1 *.* 31%.\\nThese results demonstrate that vector representations, both of\\nliterals and KG entities, are capable of retaining the semantic context\\nof the literals. Furthermore, the use of both representations allowed\\nthe model to achieve a better result.\\n\\n*4.2.3* *Experiment 3:* Question 3: Does the number of properties\\nand, consequently, literals associated with KG entities impacts EL\\naccuracy?\\nTo explore this question, we collected quantitative data related\\nto the literal values associated with KG entities. We conducted the\\n\\nanalysis from three perspectives: First, we considered descriptive\\nmetrics (see Table 4). Second, we examine the performance variation of E-BELA based on the number of literals per entity (see\\nTable 5). Lastly, we analyzed the relationship between the number\\nof literals in correct and predicted entities within the set of incorrect\\npredictions (see Figure 4).\\nTable 4 summarizes the statistical metrics of the literals of the\\n\\nentities in the KG, present in the LC-QuAD set. The metrics include\\n\\n\\n121\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Ítalo M. Pereira and Anderson A. Ferreira\\n\\n**Table 4: Descriptive Statistics of Literals per Entity in the KG.**\\n\\nSet Entities Mean Std Dev\\n\\nPopulation 3899 4.691 3.348\\nSample 358 4.782 2.464\\n\\nErrors 59 4.500 2.760\\n\\n**Table 5: Analysis of EL Performance Based on the Number**\\n**of Literals per Entity**\\n\\nNumber of Number of\\n%\\nLiterals Entities\\n\\n1 3 0.666\\n\\n2 59 0.813\\n\\n3 107 0.850\\n\\n4 30 0.700\\n\\n5 24 0.791\\n\\n6 25 0.840\\n\\n7+ 122 0.893\\n\\n\\nthe total number of entities, the average, and the standard deviation\\nof the number of literals per entity.\\nThe data presented in Table 4 reveal that the distribution of\\nthe literals associated with the entities shows a limited variation,\\nfluctuating between 4.50 and 4.78 literals per entity. We observed the\\nlargest deviation in the total population. Moreover, the deviation in\\nthe set of incorrect predictions slightly exceeded that in the sample\\n\\nset.\\n\\nTable 5 presents the variation in E-BELA’s EL performance based\\non the number of literals per entity. Each row contains: the number\\nof literals under analysis in the first column; the number of entities\\ncontaining exactly that quantity in the second column; and the\\npercentage of correct predictions in the third column. The last row\\nconsiders entities that have 7 or more literals.\\n\\nThe data in Table 5 does not allow us to infer a direct relation\\nship between the number of literals per entity and the percentage\\nof correct predictions. We notice that entities with three literals\\nhave a slightly higher percentage of correct predictions than those\\nwith only two literals. However, this value decreases for entities\\ncontaining four or five literals. The accuracy rate increases again\\nfor entities with six or more literals, but the numbers remain statistically insignificant.\\nFigure 4 presents a matrix, analogous to a confusion matrix,\\nwhich instead of displaying classes, shows the actual and predicted\\nquantities of literals per entity in the set of incorrect predictions\\nmade during the experiments.\\nBased on the data presented in this matrix, we can observe a\\nsignificantly higher number of incorrectly predicted entities in\\nthe upper left corner, corresponding to those with fewer available\\nliterals. However, the number of entities with a smaller number of\\nassociated literals (2 and 3 literals) is significantly larger, and the\\nnumber of errors is proportional.\\nAfter analyzing the data from Tables 4 and 5, as well as Figure\\n4, we noticed that although entities with 7 or more literals exhibit\\na slightly higher accuracy, we cannot definitively assert that the\\nnumber of literals significantly impacts the effectiveness of EL.\\n\\n\\n**Figure 4: This figure illustrates a matrix showing the rela-**\\n**tionship between the number of literals in the correct and**\\n**predicted entities in the set of incorrect predictions.**\\n### **5 CONCLUSIONS AND FUTURE WORK**\\n\\nIn this work, we introduce E-BELA (Enhanced Embedding-Based\\nEntity Linking Approach), a straightforward and effective approach\\nfor EL. E-BELA obtains entity embeddings from a KG using its\\nliteral data presented in the form of natural language, the same\\nform used in texts and entity mentions, and stores them in a vector\\ndatabase management system. These embeddings aim to position\\nmentions and entities close in the vector space, enabling their linkage through some similarity metric. The EL process occurs through\\nthe search for a list of candidate entities, from the embedding of a\\nmention, followed by disambiguation. For disambiguation, context\\ninformation from the mention and candidate entities is used. Our\\n\\nresults demonstrate that this methodology outperforms previous\\napproaches, making a significant contribution to the field of NLP.\\nIn future work, we intend to perform the task of Relation Linking\\nin an integrated manner through the embeddings of the entities\\nand relations of the KG. Additionally, we plan to evaluate the EL\\ntask performance after a fine-tuning the all-mpnet-base-v2 model\\nwith mention and entity contexts. We also intend to apply E-BELA\\nto other problems that involve EL in their pipeline. Furthermore,\\nwe aim to enhace the evaluation process by using a sample with a\\nbalanced quantity of literal data.\\n### **ACKNOWLEDGMENTS**\\n\\nThis research is partially sponsored by the Universidade Federal de\\nOuro Preto - UFOP, the Instituto Federal de Minas Gerais - IFMG,\\nCNPq, FAPEMIG and CAPES.\\n\\n\\n122\\n\\n\\n-----\\n\\nE-BELA: Enhanced Embedding-Based Entity Linking Approach WebMedia’2024, Juiz de Fora, Brazil\\n\\n### **REFERENCES**\\n\\n[1] Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St.\\nJohn, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, YunHsuan Sung, Brian Strope, and Ray Kurzweil. 2018. Universal Sentence Encoder.\\n*CoRR* abs/1803.11175 (2018). arXiv:1803.11175 http://arxiv.org/abs/1803.11175\\n\\n[2] Lihan Chen, Tinghui Zhu, Jingping Liu, Jiaqing Liang, and Yanghua Xiao. 2023.\\nEnd-to-End Entity Linking with Hierarchical Reinforcement Learning. *Proceed-*\\n*ings of the AAAI Conference on Artificial Intelligence* 37, 4 (Jun. 2023), 4173–4181.\\nhttps://doi.org/10.1609/aaai.v37i4.25534\\n\\n[3] Lucas Colucci, Prachi Doshi, Kun-Lin Lee, Jiajie Liang, Yin Lin, Ishan Vashishtha,\\nJia Zhang, and Alvin Jude. 2016. Evaluating Item-Item Similarity Algorithms\\nfor Movies. In *Proceedings of the 2016 CHI Conference Extended Abstracts on*\\n*Human Factors in Computing Systems* (San Jose, California, USA) *(CHI EA*\\n*’16)* . Association for Computing Machinery, New York, NY, USA, 2141–2147.\\nhttps://doi.org/10.1145/2851581.2892362\\n\\n[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In *Proceedings of the 2019 Conference of the North American Chap-*\\n*ter of the Association for Computational Linguistics: Human Language Technolo-*\\n*gies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long*\\n*and Short Papers)*, Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational Linguistics, Minneapolis, MN, USA, 4171–4186.\\nhttps://doi.org/10.18653/V1/N19-1423\\n\\n[5] Tommaso Di Noia, Roberto Mirizzi, Vito Claudio Ostuni, Davide Romito, and\\nMarkus Zanker. 2012. Linked Open Data to Support Content-Based Recommender\\nSystems. In *Proceedings of the 8th International Conference on Semantic Systems*\\n(Graz, Austria) *(I-SEMANTICS ’12)* . Association for Computing Machinery, New\\nYork, NY, USA, 1–8. https://doi.org/10.1145/2362499.2362501\\n\\n[6] Tommaso Di Noia and Vito Claudio Ostuni. 2015. *Recommender Systems and*\\n*Linked Open Data* . Springer International Publishing, Cham, 88–113.\\n\\n[7] Mohnish Dubey, Debayan Banerjee, Debanjan Chaudhuri, and Jens Lehmann.\\n2018. EARL: Joint Entity and Relation Linking for Question Answering over\\nKnowledge Graphs. In *The Semantic Web – ISWC 2018*, Denny Vrandečić, Kalina\\nBontcheva, Mari Carmen Suárez-Figueroa, Valentina Presutti, Irene Celino, Marta\\nSabou, Lucie-Aimée Kaffee, and Elena Simperl (Eds.). Springer International Publishing, Cham, 108–126.\\n\\n[8] Jorão Gomes, Rômulo Chrispim de Mello, Victor Ströele, and Jairo Francisco de\\nSouza. 2022. A Hereditary Attentive Template-based Approach for Complex\\nKnowledge Base Question Answering Systems. *Expert Systems with Applications*\\n205 (2022), 117725. https://doi.org/10.1016/j.eswa.2022.117725\\n\\n[9] Ningning Jia, Xiang Cheng, Sen Su, and Liyuan Ding. 2021. CoGCN: Combining\\nco-attention with graph convolutional network for entity linking with knowledge\\ngraphs. *Expert Systems* 38, 1 (2021), e12606. https://doi.org/10.1111/exsy.12606\\narXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/exsy.12606\\n\\n[10] Hongkun Leng, Caleb De La Cruz Paulino, Momina Haider, Rui Lu, Zhehui Zhou,\\nOle Mengshoel, Per-Erik Brodin, Julien Forgeat, and Alvin Jude. 2018. Finding\\nsimilar movies: dataset, tools, and methods. In *Proceedings of the 8th Interna-*\\n*tional Conference on Semantic Systems (WSCG’2018)* . Václav Skala-UNION Agency,\\nPlzen, Czech Republic, 115–124.\\n\\n[11] Huiying Li, Wenqi Yu, and Xinbang Dai. 2023. Joint linking of entity and relation\\nfor question answering over knowledge graph. *Multimedia Tools and Applications*\\n82, 29 (01 Dec 2023), 44801–44818. https://doi.org/10.1007/s11042-023-15646-w\\n\\n[12] Qijia Li, Feng Li, Shuchao Li, Xiaoyu Li, Kang Liu, Qing Liu, and Pengcheng\\nDong. 2022. Improving Entity Linking by Introducing Knowledge Graph\\nStructure Information. *Applied Sciences* 12, 5 (2022), 44801–44818. https:\\n//doi.org/10.3390/app12052702\\n\\n[13] Tomás Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Estimation of Word Representations in Vector Space. In *1st International Conference on*\\n*Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Work-*\\n*shop Track Proceedings*, Yoshua Bengio and Yann LeCun (Eds.). Association for\\nComputing Machinery, Scottsdale, Arizona, USA. http://arxiv.org/abs/1301.3781\\n\\n[14] Tomás Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.\\nDistributed Representations of Words and Phrases and their Compositionality.\\n*CoRR* abs/1310.4546 (2013). arXiv:1310.4546 http://arxiv.org/abs/1310.4546\\n\\n[15] Roberto Mirizzi, Tommaso Di Noia, Azzurra Ragone, Vito Ostuni, and Eugenio\\nDi Sciascio. 2012. Movie recommendation with DBpedia, In Movie recommendation with DBpedia. *CEUR Workshop Proceedings* 835, 101–112.\\n\\n\\n\\n[16] Jean Gabriel Nguema Ngomo, Giseli Rabello Lopes, Maria Luiza Machado Campos,\\nand Maria Claudia Reis Cavalcanti. 2020. An Approach for Improving DBpedia\\nas a Research Data Hub. In *Proceedings of the Brazilian Symposium on Multime-*\\n*dia and the Web* (São Luís, Brazil) *(WebMedia ’20)* . Association for Computing\\nMachinery, New York, NY, USA, 65–72. https://doi.org/10.1145/3428658.3431075\\n\\n[17] Ítalo M. Pereira and Anderson A. Ferreira. 2019. An Item-Item Similarity Approach Based on Linked Open Data Semantic Relationship. In *Proceedings of the*\\n*25th Brazillian Symposium on Multimedia and the Web* (Rio de Janeiro, Brazil)\\n*(WebMedia ’19)* . Association for Computing Machinery, New York, NY, USA,\\n425–432. https://doi.org/10.1145/3323503.3349547\\n\\n[18] Maria Pershina, Yifan He, and Ralph Grishman. 2015. Personalized Page Rank\\nfor Named Entity Disambiguation. In *Proceedings of the 2015 Conference of*\\n*the North American Chapter of the Association for Computational Linguistics:*\\n*Human Language Technologies*, Rada Mihalcea, Joyce Chai, and Anoop Sarkar\\n(Eds.). Association for Computational Linguistics, Denver, Colorado, 238–243.\\nhttps://doi.org/10.3115/v1/N15-1026\\n\\n[19] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings\\nusing Siamese BERT-Networks. In *Proceedings of the 2019 Conference on Empirical*\\n*Methods in Natural Language Processing and the 9th International Joint Confer-*\\n*ence on Natural Language Processing (EMNLP-IJCNLP)*, Kentaro Inui, Jing Jiang,\\nVincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics,\\nHong Kong, China, 3982–3992. https://doi.org/10.18653/v1/D19-1410\\n\\n[20] Petar Ristoski and Heiko Paulheim. 2016. RDF2Vec: RDF Graph Embeddings\\nfor Data Mining. In *The Semantic Web – ISWC 2016*, Paul Groth, Elena Simperl,\\nAlasdair Gray, Marta Sabou, Markus Krötzsch, Freddy Lecue, Fabian Flöck, and\\nYolanda Gil (Eds.). Springer International Publishing, Cham, 498–514.\\n\\n[21] Petar Ristoski, Jessica Rosati, Tommaso Di Noia, Renato De Leone, and Heiko\\nPaulheim. 2019. RDF2Vec: RDF graph embeddings and their applications. *Se-*\\n*mantic Web* 10, 4 (2019), 721–752.\\n\\n[22] Ahmad Sakor, Kuldeep Singh, Anery Patel, and Maria-Esther Vidal. 2020. Falcon\\n2.0: An Entity and Relation Linking Tool over Wikidata. In *Proceedings of the 29th*\\n*ACM International Conference on Information & Knowledge Management* (Virtual\\nEvent, Ireland) *(CIKM ’20)* . Association for Computing Machinery, New York, NY,\\nUSA, 3141–3148. https://doi.org/10.1145/3340531.3412777\\n\\n[23] W. Shen, Y. Li, Y. Liu, J. Han, J. Wang, and X. Yuan. 2023. Entity Linking Meets\\nDeep Learning: Techniques and Solutions. *IEEE Transactions on Knowledge; Data*\\n*Engineering* 35, 03 (mar 2023), 2556–2578. https://doi.org/10.1109/TKDE.2021.\\n3117715\\n\\n[24] Wei Shen, Jianyong Wang, and Jiawei Han. 2015. Entity Linking with a Knowledge Base: Issues, Techniques, and Solutions. *IEEE Transactions on Knowledge*\\n*and Data Engineering* 27, 2 (Feb 2015), 443–460. https://doi.org/10.1109/TKDE.\\n2014.2327028\\n\\n[25] Uma Srinivasan and Chidambaram Mani. 2018. Diversity-Ensured Semantic\\nMovie Recommendation by Applying Linked Open Data. *International Journal*\\n*of Intelligent Engineering and Systems* 11 (04 2018), 275–286.\\n\\n[26] Priyansh Trivedi, Gaurav Maheshwari, Mohnish Dubey, and Jens Lehmann. 2017.\\nLC-QuAD: A Corpus for Complex Question Answering over Knowledge Graphs.\\nIn *The Semantic Web – ISWC 2017*, Claudia d’Amato, Miriam Fernandez, Valentina\\nTamma, Freddy Lecue, Philippe Cudré-Mauroux, Juan Sequeda, Christoph Lange,\\nand Jeff Heflin (Eds.). Springer International Publishing, Cham, 210–218.\\n\\n[27] Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, and Yoshiyasu Takefuji. 2016.\\nJoint learning of the embedding of words and entities for named entity disambiguation. In *CoNLL 2016 - 20th SIGNLL Conference on Computational Natural*\\n*Language Learning, Proceedings (CoNLL 2016 - 20th SIGNLL Conference on Compu-*\\n*tational Natural Language Learning, Proceedings)* . Association for Computational\\nLinguistics (ACL), United States, 250–259. https://doi.org/10.18653/v1/k16-1025\\nPublisher Copyright: © 2016 Association for Computational Linguistics.; 20th\\nSIGNLL Conference on Computational Natural Language Learning, CoNLL 2016\\n; Conference date: 11-08-2016 Through 12-08-2016.\\n\\n[28] Ikuya Yamada, Koki Washio, Hiroyuki Shindo, and Yuji Matsumoto. 2022. Global\\nEntity Disambiguation with BERT. In *Proceedings of the 2022 Conference of the*\\n*North American Chapter of the Association for Computational Linguistics: Hu-*\\n*man Language Technologies*, Marine Carpuat, Marie-Catherine de Marneffe, and\\nIvan Vladimir Meza Ruiz (Eds.). Association for Computational Linguistics, Seattle, United States, 3264–3271. https://doi.org/10.18653/v1/2022.naacl-main.238\\n\\n\\n123\\n\\n\\n-----\\n\\n',\n",
       "  'artigo_tokenizado': ['#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'E',\n",
       "   '-',\n",
       "   'BELA',\n",
       "   ':',\n",
       "   'Enhanced',\n",
       "   'Embedding',\n",
       "   '-',\n",
       "   'Based',\n",
       "   'Entity',\n",
       "   'Linking',\n",
       "   'Approach',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Ítalo',\n",
       "   'M.',\n",
       "   'Pereira',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'italo.pereira@ifmg.edu.br',\n",
       "   'Instituto',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   '-',\n",
       "   'SJE',\n",
       "   'São',\n",
       "   'João',\n",
       "   'Evangelista',\n",
       "   ',',\n",
       "   'MG',\n",
       "   ',',\n",
       "   'Brasil',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'ABSTRACT',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Entity',\n",
       "   'linking',\n",
       "   'is',\n",
       "   'the',\n",
       "   'process',\n",
       "   'of',\n",
       "   'connecting',\n",
       "   'mentions',\n",
       "   'of',\n",
       "   'entities',\n",
       "   'in',\n",
       "   '\\n',\n",
       "   'natural',\n",
       "   'language',\n",
       "   'texts',\n",
       "   ',',\n",
       "   'such',\n",
       "   'as',\n",
       "   'references',\n",
       "   'to',\n",
       "   'people',\n",
       "   'or',\n",
       "   'places',\n",
       "   ',',\n",
       "   'to',\n",
       "   '\\n',\n",
       "   'specific',\n",
       "   'entities',\n",
       "   'in',\n",
       "   'knowledge',\n",
       "   'graphs',\n",
       "   ',',\n",
       "   'such',\n",
       "   'as',\n",
       "   'DBpedia',\n",
       "   'or',\n",
       "   'Wikidata',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'This',\n",
       "   'process',\n",
       "   'is',\n",
       "   'crucial',\n",
       "   'in',\n",
       "   'the',\n",
       "   'natural',\n",
       "   'language',\n",
       "   'processing',\n",
       "   'tasks',\n",
       "   'since',\n",
       "   '\\n',\n",
       "   'it',\n",
       "   'facilitates',\n",
       "   'disambiguating',\n",
       "   'entities',\n",
       "   'in',\n",
       "   'unstructured',\n",
       "   'data',\n",
       "   ',',\n",
       "   'enhancing',\n",
       "   '\\n',\n",
       "   'understanding',\n",
       "   'and',\n",
       "   'semantic',\n",
       "   'processing',\n",
       "   '.',\n",
       "   'However',\n",
       "   ',',\n",
       "   'entity',\n",
       "   'linking',\n",
       "   '\\n',\n",
       "   'faces',\n",
       "   'challenges',\n",
       "   'due',\n",
       "   'to',\n",
       "   'the',\n",
       "   'complexity',\n",
       "   'and',\n",
       "   'ambiguity',\n",
       "   'of',\n",
       "   'natural',\n",
       "   '\\n',\n",
       "   'languages',\n",
       "   ',',\n",
       "   'as',\n",
       "   'well',\n",
       "   'as',\n",
       "   'the',\n",
       "   'discrepancy',\n",
       "   'between',\n",
       "   'the',\n",
       "   'form',\n",
       "   'of',\n",
       "   'textual',\n",
       "   '\\n',\n",
       "   'entity',\n",
       "   'mentions',\n",
       "   'and',\n",
       "   'entity',\n",
       "   'representations',\n",
       "   '.',\n",
       "   'Considering',\n",
       "   'that',\n",
       "   'entity',\n",
       "   '\\n',\n",
       "   'mentions',\n",
       "   'are',\n",
       "   'in',\n",
       "   'natural',\n",
       "   'language',\n",
       "   'and',\n",
       "   'entity',\n",
       "   'representations',\n",
       "   'in',\n",
       "   '\\n',\n",
       "   'knowledge',\n",
       "   'graphs',\n",
       "   'have',\n",
       "   'object',\n",
       "   'nodes',\n",
       "   'that',\n",
       "   'describe',\n",
       "   'them',\n",
       "   'in',\n",
       "   'the',\n",
       "   'same',\n",
       "   '\\n',\n",
       "   'way',\n",
       "   ',',\n",
       "   'in',\n",
       "   'this',\n",
       "   'work',\n",
       "   'we',\n",
       "   'propose',\n",
       "   'E',\n",
       "   '-',\n",
       "   'BELA',\n",
       "   ',',\n",
       "   'an',\n",
       "   'effective',\n",
       "   'approach',\n",
       "   'based',\n",
       "   '\\n',\n",
       "   'on',\n",
       "   'literal',\n",
       "   'embeddings',\n",
       "   '.',\n",
       "   'We',\n",
       "   'aim',\n",
       "   'to',\n",
       "   'put',\n",
       "   'close',\n",
       "   'vector',\n",
       "   'representations',\n",
       "   '\\n',\n",
       "   'of',\n",
       "   'mentions',\n",
       "   'and',\n",
       "   'entities',\n",
       "   'in',\n",
       "   'a',\n",
       "   'vector',\n",
       "   'space',\n",
       "   ',',\n",
       "   'allowing',\n",
       "   'linking',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'mentions',\n",
       "   'and',\n",
       "   'entities',\n",
       "   'by',\n",
       "   'using',\n",
       "   'a',\n",
       "   'similarity',\n",
       "   'or',\n",
       "   'distance',\n",
       "   'metric',\n",
       "   '.',\n",
       "   'The',\n",
       "   '\\n',\n",
       "   'results',\n",
       "   'demonstrate',\n",
       "   'that',\n",
       "   'our',\n",
       "   'approach',\n",
       "   'outperforms',\n",
       "   'previous',\n",
       "   'ones',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'contributing',\n",
       "   'to',\n",
       "   'the',\n",
       "   'field',\n",
       "   'of',\n",
       "   'natural',\n",
       "   'language',\n",
       "   'processing',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'KEYWORDS',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Natural',\n",
       "   'Language',\n",
       "   'Processing',\n",
       "   ',',\n",
       "   'Entity',\n",
       "   'Linking',\n",
       "   ',',\n",
       "   'Linked',\n",
       "   'Open',\n",
       "   'Data',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'Entity',\n",
       "   'Similarity',\n",
       "   ',',\n",
       "   'Embedding',\n",
       "   ',',\n",
       "   'Disambiguation',\n",
       "   ',',\n",
       "   'DBpedia',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   '1',\n",
       "   'INTRODUCTION',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'The',\n",
       "   'increasing',\n",
       "   'volume',\n",
       "   'of',\n",
       "   'data',\n",
       "   'published',\n",
       "   'on',\n",
       "   'the',\n",
       "   'web',\n",
       "   'has',\n",
       "   'led',\n",
       "   'to',\n",
       "   'an',\n",
       "   '\\n',\n",
       "   'era',\n",
       "   'of',\n",
       "   'information',\n",
       "   'overload',\n",
       "   '.',\n",
       "   'Persons',\n",
       "   'generate',\n",
       "   'more',\n",
       "   'information',\n",
       "   '\\n',\n",
       "   'than',\n",
       "   'they',\n",
       "   'can',\n",
       "   'actually',\n",
       "   'process',\n",
       "   'and',\n",
       "   'consume',\n",
       "   '[',\n",
       "   '3',\n",
       "   ',',\n",
       "   '5',\n",
       "   ',',\n",
       "   '6',\n",
       "   ',',\n",
       "   '10',\n",
       "   ',',\n",
       "   '15',\n",
       "   ',',\n",
       "   '25',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Additionally',\n",
       "   ',',\n",
       "   'natural',\n",
       "   'language',\n",
       "   ',',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'most',\n",
       "   'important',\n",
       "   'forms',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'data',\n",
       "   'on',\n",
       "   'the',\n",
       "   'web',\n",
       "   ',',\n",
       "   'is',\n",
       "   'inherently',\n",
       "   'complex',\n",
       "   'and',\n",
       "   'ambiguous',\n",
       "   '.',\n",
       "   'To',\n",
       "   'address',\n",
       "   '\\n',\n",
       "   'this',\n",
       "   'scenario',\n",
       "   ',',\n",
       "   'many',\n",
       "   'efforts',\n",
       "   'have',\n",
       "   'been',\n",
       "   'made',\n",
       "   '.',\n",
       "   'Among',\n",
       "   'these',\n",
       "   'efforts',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'creation',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Web',\n",
       "   'of',\n",
       "   'Data',\n",
       "   'stands',\n",
       "   'out',\n",
       "   ',',\n",
       "   'achieved',\n",
       "   'through',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'development',\n",
       "   'of',\n",
       "   'Linked',\n",
       "   'Open',\n",
       "   'Data',\n",
       "   '(',\n",
       "   'LOD',\n",
       "   ')',\n",
       "   'datasets',\n",
       "   '.',\n",
       "   'These',\n",
       "   'datasets',\n",
       "   '\\n',\n",
       "   'are',\n",
       "   'massive',\n",
       "   'Knowledge',\n",
       "   'Bases',\n",
       "   '(',\n",
       "   'KBs',\n",
       "   ')',\n",
       "   ',',\n",
       "   'also',\n",
       "   'referred',\n",
       "   'to',\n",
       "   'as',\n",
       "   'Knowledge',\n",
       "   '\\n',\n",
       "   'Graphs',\n",
       "   '(',\n",
       "   'KGs',\n",
       "   ')',\n",
       "   ',',\n",
       "   'containing',\n",
       "   'millions',\n",
       "   'of',\n",
       "   'entities',\n",
       "   'and',\n",
       "   'billions',\n",
       "   'of',\n",
       "   'factual',\n",
       "   '\\n',\n",
       "   'relationships',\n",
       "   '[',\n",
       "   '16',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Moreover',\n",
       "   ',',\n",
       "   'a',\n",
       "   'key',\n",
       "   'characteristic',\n",
       "   'is',\n",
       "   'their',\n",
       "   'machinereadable',\n",
       "   'nature',\n",
       "   '[',\n",
       "   '9',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Examples',\n",
       "   'of',\n",
       "   'such',\n",
       "   'datasets',\n",
       "   'include',\n",
       "   'DBpedia',\n",
       "   '[',\n",
       "   '1',\n",
       "   ']',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'Wikidata',\n",
       "   '[',\n",
       "   '2',\n",
       "   ']',\n",
       "   ',',\n",
       "   'YAGO',\n",
       "   '[',\n",
       "   '3',\n",
       "   ']',\n",
       "   ',',\n",
       "   'among',\n",
       "   'others',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Relating',\n",
       "   'data',\n",
       "   'published',\n",
       "   'on',\n",
       "   'the',\n",
       "   'web',\n",
       "   'to',\n",
       "   'data',\n",
       "   'in',\n",
       "   'such',\n",
       "   'KGs',\n",
       "   'may',\n",
       "   '\\n',\n",
       "   'mitigate',\n",
       "   'the',\n",
       "   'complexity',\n",
       "   'and',\n",
       "   'inherent',\n",
       "   'ambiguity',\n",
       "   'of',\n",
       "   'natural',\n",
       "   'language',\n",
       "   ',',\n",
       "   'as',\n",
       "   'well',\n",
       "   'as',\n",
       "   'enrich',\n",
       "   'these',\n",
       "   'KGs',\n",
       "   '.',\n",
       "   'This',\n",
       "   'purpose',\n",
       "   'may',\n",
       "   'be',\n",
       "   'achieved',\n",
       "   '\\n',\n",
       "   'by',\n",
       "   'aligning',\n",
       "   'entity',\n",
       "   'mentions',\n",
       "   'obtained',\n",
       "   'from',\n",
       "   'the',\n",
       "   'data',\n",
       "   'published',\n",
       "   'on',\n",
       "   '\\n\\n',\n",
       "   '1',\n",
       "   'https://www.dbpedia.org/',\n",
       "   '\\n',\n",
       "   '2',\n",
       "   'https://www.wikidata.org/',\n",
       "   '\\n',\n",
       "   '3',\n",
       "   'https://yago-knowledge.org/',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   ':',\n",
       "   'Proceedings',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Web',\n",
       "   '(',\n",
       "   'WebMe',\n",
       "   '\\n',\n",
       "   'dia’2024',\n",
       "   ')',\n",
       "   '.',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '.',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   ':',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   ',',\n",
       "   '2024',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'SBC',\n",
       "   '–',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '-',\n",
       "   '2753',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Anderson',\n",
       "   'A.',\n",
       "   'Ferreira',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'anderson.ferreira@ufop.edu.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Ouro',\n",
       "   'Preto',\n",
       "   'Ouro',\n",
       "   'Preto',\n",
       "   ',',\n",
       "   'MG',\n",
       "   ',',\n",
       "   'Brasil',\n",
       "   '\\n\\n',\n",
       "   'the',\n",
       "   'web',\n",
       "   ',',\n",
       "   'such',\n",
       "   'as',\n",
       "   'references',\n",
       "   'to',\n",
       "   'people',\n",
       "   'or',\n",
       "   'places',\n",
       "   ',',\n",
       "   'and',\n",
       "   'entity',\n",
       "   'representations',\n",
       "   'in',\n",
       "   'knowledge',\n",
       "   'bases',\n",
       "   ',',\n",
       "   'hereinafter',\n",
       "   'referred',\n",
       "   'to',\n",
       "   'as',\n",
       "   '*',\n",
       "   '𝑒𝑛𝑡𝑖𝑡𝑖𝑒𝑠',\n",
       "   '*',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'In',\n",
       "   'addition',\n",
       "   ',',\n",
       "   'this',\n",
       "   'alignment',\n",
       "   'aid',\n",
       "   'a',\n",
       "   'wide',\n",
       "   'variety',\n",
       "   'of',\n",
       "   'problems',\n",
       "   ',',\n",
       "   'such',\n",
       "   'as',\n",
       "   '\\n',\n",
       "   'populating',\n",
       "   'knowledge',\n",
       "   'bases',\n",
       "   ',',\n",
       "   'content',\n",
       "   'analysis',\n",
       "   ',',\n",
       "   'relation',\n",
       "   'extraction',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'and',\n",
       "   'question',\n",
       "   '-',\n",
       "   'answering',\n",
       "   'systems',\n",
       "   '[',\n",
       "   '23',\n",
       "   ']',\n",
       "   '.',\n",
       "   'This',\n",
       "   'alignment',\n",
       "   'is',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'main',\n",
       "   'tasks',\n",
       "   'in',\n",
       "   'Natural',\n",
       "   'Language',\n",
       "   'Processing',\n",
       "   '(',\n",
       "   'NLP',\n",
       "   ')',\n",
       "   '[',\n",
       "   '7',\n",
       "   ',',\n",
       "   '9',\n",
       "   ']',\n",
       "   'and',\n",
       "   'is',\n",
       "   'defined',\n",
       "   'as',\n",
       "   'Entity',\n",
       "   'Linking',\n",
       "   '(',\n",
       "   'EL',\n",
       "   ')',\n",
       "   ',',\n",
       "   'also',\n",
       "   'known',\n",
       "   'as',\n",
       "   'Named',\n",
       "   'Entity',\n",
       "   'Linking',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'Named',\n",
       "   'Entity',\n",
       "   'Disambiguation',\n",
       "   ',',\n",
       "   'or',\n",
       "   'Entity',\n",
       "   'Disambiguation',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Formally',\n",
       "   ',',\n",
       "   'given',\n",
       "   'a',\n",
       "   'document',\n",
       "   '*',\n",
       "   '𝐷',\n",
       "   '*',\n",
       "   'containing',\n",
       "   'a',\n",
       "   'set',\n",
       "   'of',\n",
       "   'named',\n",
       "   'entity',\n",
       "   '\\n',\n",
       "   'mentions',\n",
       "   '*',\n",
       "   '𝑀',\n",
       "   '*',\n",
       "   '=',\n",
       "   '{',\n",
       "   '*',\n",
       "   '𝑚',\n",
       "   '*',\n",
       "   '1',\n",
       "   '*',\n",
       "   ',',\n",
       "   '𝑚',\n",
       "   '*',\n",
       "   '2',\n",
       "   '*',\n",
       "   ',',\n",
       "   '...',\n",
       "   ',',\n",
       "   '𝑚',\n",
       "   '*',\n",
       "   '|',\n",
       "   '*',\n",
       "   '𝑀',\n",
       "   '*',\n",
       "   '|',\n",
       "   '}',\n",
       "   'along',\n",
       "   'with',\n",
       "   'its',\n",
       "   'context',\n",
       "   ',',\n",
       "   'and',\n",
       "   'a',\n",
       "   'KG',\n",
       "   '\\n',\n",
       "   'containing',\n",
       "   'a',\n",
       "   'set',\n",
       "   'of',\n",
       "   'named',\n",
       "   'entities',\n",
       "   '*',\n",
       "   '𝐸',\n",
       "   '*',\n",
       "   '=',\n",
       "   '{',\n",
       "   '*',\n",
       "   '𝑒',\n",
       "   '*',\n",
       "   '1',\n",
       "   '*',\n",
       "   ',',\n",
       "   '𝑒',\n",
       "   '*',\n",
       "   '2',\n",
       "   '*',\n",
       "   ',',\n",
       "   '...',\n",
       "   ',',\n",
       "   '𝑒',\n",
       "   '*',\n",
       "   '|',\n",
       "   '*',\n",
       "   '𝐸',\n",
       "   '*',\n",
       "   '|',\n",
       "   '}',\n",
       "   ',',\n",
       "   'the',\n",
       "   'goal',\n",
       "   'is',\n",
       "   '\\n',\n",
       "   'to',\n",
       "   'define',\n",
       "   'a',\n",
       "   'function',\n",
       "   '*',\n",
       "   '𝑓',\n",
       "   '*',\n",
       "   'that',\n",
       "   'maps',\n",
       "   'each',\n",
       "   'entity',\n",
       "   'mention',\n",
       "   '*',\n",
       "   '𝑚',\n",
       "   '*',\n",
       "   '*',\n",
       "   '𝑖',\n",
       "   '*',\n",
       "   '∈',\n",
       "   '*',\n",
       "   '𝑀',\n",
       "   '*',\n",
       "   'to',\n",
       "   'its',\n",
       "   '\\n',\n",
       "   'corresponding',\n",
       "   'entity',\n",
       "   '*',\n",
       "   '𝑒',\n",
       "   '*',\n",
       "   '*',\n",
       "   '𝑗',\n",
       "   '*',\n",
       "   '∈',\n",
       "   '*',\n",
       "   '𝐸',\n",
       "   '*',\n",
       "   '[',\n",
       "   '23',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'However',\n",
       "   ',',\n",
       "   'the',\n",
       "   'entity',\n",
       "   'mentions',\n",
       "   'present',\n",
       "   'in',\n",
       "   'sentences',\n",
       "   'are',\n",
       "   'in',\n",
       "   'natural',\n",
       "   '\\n',\n",
       "   'language',\n",
       "   ',',\n",
       "   'whereas',\n",
       "   'the',\n",
       "   'KG',\n",
       "   'data',\n",
       "   'in',\n",
       "   'LOD',\n",
       "   'is',\n",
       "   'represented',\n",
       "   'by',\n",
       "   'graphs',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Specifically',\n",
       "   ',',\n",
       "   'these',\n",
       "   'graphs',\n",
       "   'adhere',\n",
       "   'to',\n",
       "   'the',\n",
       "   '*',\n",
       "   'Resource',\n",
       "   'Description',\n",
       "   'Frame-',\n",
       "   '*',\n",
       "   '\\n',\n",
       "   '*',\n",
       "   'work',\n",
       "   '*',\n",
       "   '(',\n",
       "   'RDF',\n",
       "   '[',\n",
       "   '4',\n",
       "   ']',\n",
       "   ')',\n",
       "   ',',\n",
       "   'in',\n",
       "   'which',\n",
       "   'information',\n",
       "   'or',\n",
       "   'facts',\n",
       "   'are',\n",
       "   'represented',\n",
       "   'as',\n",
       "   'interconnected',\n",
       "   'nodes',\n",
       "   'linked',\n",
       "   'by',\n",
       "   'edges',\n",
       "   '.',\n",
       "   'The',\n",
       "   'connection',\n",
       "   'between',\n",
       "   'two',\n",
       "   '\\n',\n",
       "   'nodes',\n",
       "   ',',\n",
       "   'via',\n",
       "   'an',\n",
       "   'edge',\n",
       "   ',',\n",
       "   'constitutes',\n",
       "   'a',\n",
       "   'triple',\n",
       "   ',',\n",
       "   'composed',\n",
       "   'of',\n",
       "   'a',\n",
       "   'subject',\n",
       "   '(',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'first',\n",
       "   'node',\n",
       "   ')',\n",
       "   ',',\n",
       "   'a',\n",
       "   'predicate',\n",
       "   'or',\n",
       "   'property',\n",
       "   '(',\n",
       "   'the',\n",
       "   'edge',\n",
       "   ')',\n",
       "   ',',\n",
       "   'and',\n",
       "   'an',\n",
       "   'object',\n",
       "   '(',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'second',\n",
       "   'node',\n",
       "   ')',\n",
       "   '(',\n",
       "   '*',\n",
       "   '𝑠𝑢𝑏𝑗𝑒𝑐𝑡',\n",
       "   '*',\n",
       "   '→',\n",
       "   '*',\n",
       "   '𝑝𝑟𝑒𝑑𝑖𝑐𝑎𝑡𝑒',\n",
       "   '*',\n",
       "   '→',\n",
       "   '*',\n",
       "   '𝑜𝑏𝑗𝑒𝑐𝑡',\n",
       "   '*',\n",
       "   ')',\n",
       "   '.',\n",
       "   'The',\n",
       "   'edge',\n",
       "   'denotes',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'relationship',\n",
       "   'between',\n",
       "   'the',\n",
       "   'two',\n",
       "   'nodes',\n",
       "   ',',\n",
       "   'which',\n",
       "   'can',\n",
       "   'represent',\n",
       "   'entities',\n",
       "   'or',\n",
       "   'literals',\n",
       "   '(',\n",
       "   'entity',\n",
       "   'data',\n",
       "   ')',\n",
       "   '.',\n",
       "   'A',\n",
       "   'collection',\n",
       "   'of',\n",
       "   'these',\n",
       "   'triples',\n",
       "   'constitutes',\n",
       "   '\\n',\n",
       "   'a',\n",
       "   'directed',\n",
       "   'and',\n",
       "   'labeled',\n",
       "   'graph',\n",
       "   ...],\n",
       "  'pos_tagger': '',\n",
       "  'lema': ['e',\n",
       "   'bela',\n",
       "   'enhance',\n",
       "   'Embedding',\n",
       "   'base',\n",
       "   'Entity',\n",
       "   'Linking',\n",
       "   'Approach',\n",
       "   'Ítalo',\n",
       "   'M.',\n",
       "   'Pereira',\n",
       "   'italo.pereira@ifmg.edu.br',\n",
       "   'Instituto',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Minas',\n",
       "   'Gerais',\n",
       "   'SJE',\n",
       "   'São',\n",
       "   'João',\n",
       "   'Evangelista',\n",
       "   'MG',\n",
       "   'Brasil',\n",
       "   'ABSTRACT',\n",
       "   'Entity',\n",
       "   'link',\n",
       "   'be',\n",
       "   'the',\n",
       "   'process',\n",
       "   'of',\n",
       "   'connect',\n",
       "   'mention',\n",
       "   'of',\n",
       "   'entity',\n",
       "   'in',\n",
       "   'natural',\n",
       "   'language',\n",
       "   'text',\n",
       "   'such',\n",
       "   'as',\n",
       "   'reference',\n",
       "   'to',\n",
       "   'people',\n",
       "   'or',\n",
       "   'place',\n",
       "   'to',\n",
       "   'specific',\n",
       "   'entity',\n",
       "   'in',\n",
       "   'knowledge',\n",
       "   'graph',\n",
       "   'such',\n",
       "   'as',\n",
       "   'DBpedia',\n",
       "   'or',\n",
       "   'Wikidata',\n",
       "   'this',\n",
       "   'process',\n",
       "   'be',\n",
       "   'crucial',\n",
       "   'in',\n",
       "   'the',\n",
       "   'natural',\n",
       "   'language',\n",
       "   'processing',\n",
       "   'task',\n",
       "   'since',\n",
       "   'it',\n",
       "   'facilitate',\n",
       "   'disambiguate',\n",
       "   'entity',\n",
       "   'in',\n",
       "   'unstructured',\n",
       "   'datum',\n",
       "   'enhance',\n",
       "   'understanding',\n",
       "   'and',\n",
       "   'semantic',\n",
       "   'processing',\n",
       "   'however',\n",
       "   'entity',\n",
       "   'link',\n",
       "   'face',\n",
       "   'challenge',\n",
       "   'due',\n",
       "   'to',\n",
       "   'the',\n",
       "   'complexity',\n",
       "   'and',\n",
       "   'ambiguity',\n",
       "   'of',\n",
       "   'natural',\n",
       "   'language',\n",
       "   'as',\n",
       "   'well',\n",
       "   'as',\n",
       "   'the',\n",
       "   'discrepancy',\n",
       "   'between',\n",
       "   'the',\n",
       "   'form',\n",
       "   'of',\n",
       "   'textual',\n",
       "   'entity',\n",
       "   'mention',\n",
       "   'and',\n",
       "   'entity',\n",
       "   'representation',\n",
       "   'consider',\n",
       "   'that',\n",
       "   'entity',\n",
       "   'mention',\n",
       "   'be',\n",
       "   'in',\n",
       "   'natural',\n",
       "   'language',\n",
       "   'and',\n",
       "   'entity',\n",
       "   'representation',\n",
       "   'in',\n",
       "   'knowledge',\n",
       "   'graph',\n",
       "   'have',\n",
       "   'object',\n",
       "   'node',\n",
       "   'that',\n",
       "   'describe',\n",
       "   'they',\n",
       "   'in',\n",
       "   'the',\n",
       "   'same',\n",
       "   'way',\n",
       "   'in',\n",
       "   'this',\n",
       "   'work',\n",
       "   'we',\n",
       "   'propose',\n",
       "   'e',\n",
       "   'BELA',\n",
       "   'an',\n",
       "   'effective',\n",
       "   'approach',\n",
       "   'base',\n",
       "   'on',\n",
       "   'literal',\n",
       "   'embedding',\n",
       "   'we',\n",
       "   'aim',\n",
       "   'to',\n",
       "   'put',\n",
       "   'close',\n",
       "   'vector',\n",
       "   'representation',\n",
       "   'of',\n",
       "   'mention',\n",
       "   'and',\n",
       "   'entity',\n",
       "   'in',\n",
       "   'a',\n",
       "   'vector',\n",
       "   'space',\n",
       "   'allow',\n",
       "   'link',\n",
       "   'of',\n",
       "   'mention',\n",
       "   'and',\n",
       "   'entity',\n",
       "   'by',\n",
       "   'use',\n",
       "   'a',\n",
       "   'similarity',\n",
       "   'or',\n",
       "   'distance',\n",
       "   'metric',\n",
       "   'the',\n",
       "   'result',\n",
       "   'demonstrate',\n",
       "   'that',\n",
       "   'our',\n",
       "   'approach',\n",
       "   'outperform',\n",
       "   'previous',\n",
       "   'one',\n",
       "   'contribute',\n",
       "   'to',\n",
       "   'the',\n",
       "   'field',\n",
       "   'of',\n",
       "   'natural',\n",
       "   'language',\n",
       "   'processing',\n",
       "   'keyword',\n",
       "   'Natural',\n",
       "   'Language',\n",
       "   'Processing',\n",
       "   'Entity',\n",
       "   'Linking',\n",
       "   'Linked',\n",
       "   'Open',\n",
       "   'Data',\n",
       "   'Entity',\n",
       "   'Similarity',\n",
       "   'Embedding',\n",
       "   'Disambiguation',\n",
       "   'DBpedia',\n",
       "   '1',\n",
       "   'introduction',\n",
       "   'the',\n",
       "   'increase',\n",
       "   'volume',\n",
       "   'of',\n",
       "   'datum',\n",
       "   'publish',\n",
       "   'on',\n",
       "   'the',\n",
       "   'web',\n",
       "   'have',\n",
       "   'lead',\n",
       "   'to',\n",
       "   'an',\n",
       "   'era',\n",
       "   'of',\n",
       "   'information',\n",
       "   'overload',\n",
       "   'person',\n",
       "   'generate',\n",
       "   'more',\n",
       "   'information',\n",
       "   'than',\n",
       "   'they',\n",
       "   'can',\n",
       "   'actually',\n",
       "   'process',\n",
       "   'and',\n",
       "   'consume',\n",
       "   '3',\n",
       "   '5',\n",
       "   '6',\n",
       "   '10',\n",
       "   '15',\n",
       "   '25',\n",
       "   'additionally',\n",
       "   'natural',\n",
       "   'language',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'most',\n",
       "   'important',\n",
       "   'form',\n",
       "   'of',\n",
       "   'datum',\n",
       "   'on',\n",
       "   'the',\n",
       "   'web',\n",
       "   'be',\n",
       "   'inherently',\n",
       "   'complex',\n",
       "   'and',\n",
       "   'ambiguous',\n",
       "   'to',\n",
       "   'address',\n",
       "   'this',\n",
       "   'scenario',\n",
       "   'many',\n",
       "   'effort',\n",
       "   'have',\n",
       "   'be',\n",
       "   'make',\n",
       "   'among',\n",
       "   'these',\n",
       "   'effort',\n",
       "   'the',\n",
       "   'creation',\n",
       "   'of',\n",
       "   'the',\n",
       "   'web',\n",
       "   'of',\n",
       "   'Data',\n",
       "   'stand',\n",
       "   'out',\n",
       "   'achieve',\n",
       "   'through',\n",
       "   'the',\n",
       "   'development',\n",
       "   'of',\n",
       "   'Linked',\n",
       "   'Open',\n",
       "   'Data',\n",
       "   'LOD',\n",
       "   'dataset',\n",
       "   'these',\n",
       "   'dataset',\n",
       "   'be',\n",
       "   'massive',\n",
       "   'Knowledge',\n",
       "   'Bases',\n",
       "   'KBs',\n",
       "   'also',\n",
       "   'refer',\n",
       "   'to',\n",
       "   'as',\n",
       "   'Knowledge',\n",
       "   'Graphs',\n",
       "   'KGs',\n",
       "   'contain',\n",
       "   'million',\n",
       "   'of',\n",
       "   'entity',\n",
       "   'and',\n",
       "   'billion',\n",
       "   'of',\n",
       "   'factual',\n",
       "   'relationship',\n",
       "   '16',\n",
       "   'moreover',\n",
       "   'a',\n",
       "   'key',\n",
       "   'characteristic',\n",
       "   'be',\n",
       "   'their',\n",
       "   'machinereadable',\n",
       "   'nature',\n",
       "   '9',\n",
       "   'example',\n",
       "   'of',\n",
       "   'such',\n",
       "   'dataset',\n",
       "   'include',\n",
       "   'DBpedia',\n",
       "   '1',\n",
       "   'Wikidata',\n",
       "   '2',\n",
       "   'YAGO',\n",
       "   '3',\n",
       "   'among',\n",
       "   'other',\n",
       "   'relate',\n",
       "   'datum',\n",
       "   'publish',\n",
       "   'on',\n",
       "   'the',\n",
       "   'web',\n",
       "   'to',\n",
       "   'datum',\n",
       "   'in',\n",
       "   'such',\n",
       "   'kg',\n",
       "   'may',\n",
       "   'mitigate',\n",
       "   'the',\n",
       "   'complexity',\n",
       "   'and',\n",
       "   'inherent',\n",
       "   'ambiguity',\n",
       "   'of',\n",
       "   'natural',\n",
       "   'language',\n",
       "   'as',\n",
       "   'well',\n",
       "   'as',\n",
       "   'enrich',\n",
       "   'these',\n",
       "   'kg',\n",
       "   'this',\n",
       "   'purpose',\n",
       "   'may',\n",
       "   'be',\n",
       "   'achieve',\n",
       "   'by',\n",
       "   'align',\n",
       "   'entity',\n",
       "   'mention',\n",
       "   'obtain',\n",
       "   'from',\n",
       "   'the',\n",
       "   'datum',\n",
       "   'publish',\n",
       "   'on',\n",
       "   '1',\n",
       "   'https://www.dbpedia.org/',\n",
       "   '2',\n",
       "   'https://www.wikidata.org/',\n",
       "   '3',\n",
       "   'https://yago-knowledge.org/',\n",
       "   'in',\n",
       "   'proceeding',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'web',\n",
       "   'WebMe',\n",
       "   'dia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   '2024',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'sbc',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '2753',\n",
       "   'Anderson',\n",
       "   'A.',\n",
       "   'Ferreira',\n",
       "   'anderson.ferreira@ufop.edu.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Ouro',\n",
       "   'Preto',\n",
       "   'Ouro',\n",
       "   'Preto',\n",
       "   'MG',\n",
       "   'Brasil',\n",
       "   'the',\n",
       "   'web',\n",
       "   'such',\n",
       "   'as',\n",
       "   'reference',\n",
       "   'to',\n",
       "   'people',\n",
       "   'or',\n",
       "   'place',\n",
       "   'and',\n",
       "   'entity',\n",
       "   'representation',\n",
       "   'in',\n",
       "   'knowledge',\n",
       "   'basis',\n",
       "   'hereinafter',\n",
       "   'refer',\n",
       "   'to',\n",
       "   'as',\n",
       "   '𝑒𝑛𝑡𝑖𝑡𝑖𝑒𝑠',\n",
       "   'in',\n",
       "   'addition',\n",
       "   'this',\n",
       "   'alignment',\n",
       "   'aid',\n",
       "   'a',\n",
       "   'wide',\n",
       "   'variety',\n",
       "   'of',\n",
       "   'problem',\n",
       "   'such',\n",
       "   'as',\n",
       "   'populate',\n",
       "   'knowledge',\n",
       "   'basis',\n",
       "   'content',\n",
       "   'analysis',\n",
       "   'relation',\n",
       "   'extraction',\n",
       "   'and',\n",
       "   'question',\n",
       "   'answer',\n",
       "   'system',\n",
       "   '23',\n",
       "   'this',\n",
       "   'alignment',\n",
       "   'be',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'main',\n",
       "   'task',\n",
       "   'in',\n",
       "   'Natural',\n",
       "   'Language',\n",
       "   'Processing',\n",
       "   'NLP',\n",
       "   '7',\n",
       "   '9',\n",
       "   'and',\n",
       "   'be',\n",
       "   'define',\n",
       "   'as',\n",
       "   'Entity',\n",
       "   'Linking',\n",
       "   'EL',\n",
       "   'also',\n",
       "   'know',\n",
       "   'as',\n",
       "   'name',\n",
       "   'Entity',\n",
       "   'Linking',\n",
       "   'name',\n",
       "   'Entity',\n",
       "   'Disambiguation',\n",
       "   'or',\n",
       "   'Entity',\n",
       "   'Disambiguation',\n",
       "   'formally',\n",
       "   'give',\n",
       "   'a',\n",
       "   'document',\n",
       "   '𝐷',\n",
       "   'contain',\n",
       "   'a',\n",
       "   'set',\n",
       "   'of',\n",
       "   'name',\n",
       "   'entity',\n",
       "   'mention',\n",
       "   '𝑀',\n",
       "   '=',\n",
       "   '𝑚',\n",
       "   '1',\n",
       "   '𝑚',\n",
       "   '2',\n",
       "   '𝑚',\n",
       "   '|',\n",
       "   '𝑀',\n",
       "   '|',\n",
       "   'along',\n",
       "   'with',\n",
       "   'its',\n",
       "   'context',\n",
       "   'and',\n",
       "   'a',\n",
       "   'KG',\n",
       "   'contain',\n",
       "   'a',\n",
       "   'set',\n",
       "   'of',\n",
       "   'name',\n",
       "   'entity',\n",
       "   '𝐸',\n",
       "   '=',\n",
       "   '𝑒',\n",
       "   '1',\n",
       "   '𝑒',\n",
       "   '2',\n",
       "   '𝑒',\n",
       "   '|',\n",
       "   '𝐸',\n",
       "   '|',\n",
       "   'the',\n",
       "   'goal',\n",
       "   'be',\n",
       "   'to',\n",
       "   'define',\n",
       "   'a',\n",
       "   'function',\n",
       "   '𝑓',\n",
       "   'that',\n",
       "   'map',\n",
       "   'each',\n",
       "   'entity',\n",
       "   'mention',\n",
       "   '𝑚',\n",
       "   '𝑖',\n",
       "   '∈',\n",
       "   '𝑀',\n",
       "   'to',\n",
       "   'its',\n",
       "   'correspond',\n",
       "   'entity',\n",
       "   '𝑒',\n",
       "   '𝑗',\n",
       "   '∈',\n",
       "   '𝐸',\n",
       "   '23',\n",
       "   'however',\n",
       "   'the',\n",
       "   'entity',\n",
       "   'mention',\n",
       "   'present',\n",
       "   'in',\n",
       "   'sentence',\n",
       "   'be',\n",
       "   'in',\n",
       "   'natural',\n",
       "   'language',\n",
       "   'whereas',\n",
       "   'the',\n",
       "   'KG',\n",
       "   'datum',\n",
       "   'in',\n",
       "   'LOD',\n",
       "   'be',\n",
       "   'represent',\n",
       "   'by',\n",
       "   'graph',\n",
       "   'specifically',\n",
       "   'these',\n",
       "   'graph',\n",
       "   'adhere',\n",
       "   'to',\n",
       "   'the',\n",
       "   'Resource',\n",
       "   'Description',\n",
       "   'Frame-',\n",
       "   'work',\n",
       "   'RDF',\n",
       "   '4',\n",
       "   'in',\n",
       "   'which',\n",
       "   'information',\n",
       "   'or',\n",
       "   'fact',\n",
       "   'be',\n",
       "   'represent',\n",
       "   'as',\n",
       "   'interconnect',\n",
       "   'node',\n",
       "   'link',\n",
       "   'by',\n",
       "   'edge',\n",
       "   'the',\n",
       "   'connection',\n",
       "   'between',\n",
       "   'two',\n",
       "   'node',\n",
       "   'via',\n",
       "   'an',\n",
       "   'edge',\n",
       "   'constitute',\n",
       "   'a',\n",
       "   'triple',\n",
       "   'compose',\n",
       "   'of',\n",
       "   'a',\n",
       "   'subject',\n",
       "   'the',\n",
       "   'first',\n",
       "   'node',\n",
       "   'a',\n",
       "   'predicate',\n",
       "   'or',\n",
       "   'property',\n",
       "   'the',\n",
       "   'edge',\n",
       "   'and',\n",
       "   'an',\n",
       "   'object',\n",
       "   'the',\n",
       "   'second',\n",
       "   'node',\n",
       "   '𝑠𝑢𝑏𝑗𝑒𝑐𝑡',\n",
       "   '→',\n",
       "   '𝑝𝑟𝑒𝑑𝑖𝑐𝑎𝑡𝑒',\n",
       "   '→',\n",
       "   '𝑜𝑏𝑗𝑒𝑐𝑡',\n",
       "   'the',\n",
       "   'edge',\n",
       "   'denote',\n",
       "   'the',\n",
       "   'relationship',\n",
       "   'between',\n",
       "   'the',\n",
       "   'two',\n",
       "   'node',\n",
       "   'which',\n",
       "   'can',\n",
       "   'represent',\n",
       "   'entity',\n",
       "   'or',\n",
       "   'literal',\n",
       "   'entity',\n",
       "   'datum',\n",
       "   'a',\n",
       "   'collection',\n",
       "   'of',\n",
       "   'these',\n",
       "   'triple',\n",
       "   'constitute',\n",
       "   'a',\n",
       "   'directed',\n",
       "   'and',\n",
       "   'label',\n",
       "   'graph',\n",
       "   'know',\n",
       "   'as',\n",
       "   'an',\n",
       "   'RDF',\n",
       "   'graph',\n",
       "   'which',\n",
       "   'also',\n",
       "   'serve',\n",
       "   'as',\n",
       "   'a',\n",
       "   'Knowledge',\n",
       "   'Graph',\n",
       "   '8',\n",
       "   '17',\n",
       "   'figure',\n",
       "   '1',\n",
       "   'illustrate',\n",
       "   'the',\n",
       "   'Entity',\n",
       "   'Linking',\n",
       "   'process',\n",
       "   'the',\n",
       "   'rectangle',\n",
       "   'on',\n",
       "   'the',\n",
       "   'left',\n",
       "   'contain',\n",
       "   'a',\n",
       "   'snippet',\n",
       "   'of',\n",
       "   'text',\n",
       "   'in',\n",
       "   'natural',\n",
       "   'language',\n",
       "   'there',\n",
       "   'be',\n",
       "   'two',\n",
       "   'mention',\n",
       "   'of',\n",
       "   'entity',\n",
       "   'people',\n",
       "   'in',\n",
       "   'this',\n",
       "   'case',\n",
       "   'the',\n",
       "   'mention',\n",
       "   'be',\n",
       "   'barack',\n",
       "   'Obama',\n",
       "   'and',\n",
       "   'Lolo',\n",
       "   'Soetero',\n",
       "   'on',\n",
       "   'the',\n",
       "   'right',\n",
       "   'we',\n",
       "   'illustrate',\n",
       "   'a',\n",
       "   'part',\n",
       "   'of',\n",
       "   'a',\n",
       "   'KG',\n",
       "   'with',\n",
       "   'several',\n",
       "   'RDF',\n",
       "   'triple',\n",
       "   'in',\n",
       "   'this',\n",
       "   'example',\n",
       "   'the',\n",
       "   'entity',\n",
       "   'be',\n",
       "   'Q4115068',\n",
       "   'Lolo',\n",
       "   'Soetero',\n",
       "   'and',\n",
       "   'Q76',\n",
       "   'Barack',\n",
       "   'Obama',\n",
       "   'and',\n",
       "   'their',\n",
       "   'property',\n",
       "   'be',\n",
       "   'represent',\n",
       "   'by',\n",
       "   'the',\n",
       "   'Label',\n",
       "   'and',\n",
       "   'description',\n",
       "   'edge',\n",
       "   'the',\n",
       "   'connection',\n",
       "   'between',\n",
       "   'the',\n",
       "   'mention',\n",
       "   'of',\n",
       "   'entity',\n",
       "   'and',\n",
       "   'their',\n",
       "   'correspond',\n",
       "   'entity',\n",
       "   'in',\n",
       "   'the',\n",
       "   'KG',\n",
       "   'be',\n",
       "   'symbolize',\n",
       "   'by',\n",
       "   'dash',\n",
       "   'arrow',\n",
       "   'EL',\n",
       "   'employ',\n",
       "   'the',\n",
       "   'contexts',\n",
       "   'of',\n",
       "   'the',\n",
       "   'mention',\n",
       "   'represent',\n",
       "   'by',\n",
       "   'the',\n",
       "   'full',\n",
       "   'text',\n",
       "   'in',\n",
       "   'the',\n",
       "   'rectangle',\n",
       "   'and',\n",
       "   'the',\n",
       "   'contexts',\n",
       "   'of',\n",
       "   'the',\n",
       "   'entity',\n",
       "   'in',\n",
       "   'the',\n",
       "   'KG',\n",
       "   'indicate',\n",
       "   'by',\n",
       "   'the',\n",
       "   'literal',\n",
       "   'node',\n",
       "   'link',\n",
       "   'to',\n",
       "   'the',\n",
       "   'entity',\n",
       "   'in',\n",
       "   'its',\n",
       "   'processing',\n",
       "   'many',\n",
       "   'current',\n",
       "   'NLP',\n",
       "   'task',\n",
       "   'rely',\n",
       "   'on',\n",
       "   'vector',\n",
       "   'representation',\n",
       "   'of',\n",
       "   'their',\n",
       "   'datum',\n",
       "   'in',\n",
       "   'this',\n",
       "   'context',\n",
       "   'each',\n",
       "   'element',\n",
       "   'term',\n",
       "   'phrase',\n",
       "   'object',\n",
       "   'node',\n",
       "   'need',\n",
       "   'to',\n",
       "   'be',\n",
       "   'represent',\n",
       "   'by',\n",
       "   'feature',\n",
       "   'vector',\n",
       "   '𝑓',\n",
       "   '1',\n",
       "   '𝑓',\n",
       "   '2',\n",
       "   '𝑓',\n",
       "   '𝑛',\n",
       "   'these',\n",
       "   'vector',\n",
       "   'usually',\n",
       "   'contain',\n",
       "   'binary',\n",
       "   '𝑓',\n",
       "   '𝑖',\n",
       "   '∈',\n",
       "   '𝑇𝑟𝑢𝑒',\n",
       "   '𝐹𝑎𝑙𝑠𝑒',\n",
       "   'numerical',\n",
       "   '𝐹',\n",
       "   '𝑖',\n",
       "   '∈',\n",
       "   'r',\n",
       "   'or',\n",
       "   'nominal',\n",
       "   '𝑓',\n",
       "   '𝑖',\n",
       "   '∈',\n",
       "   '𝑆',\n",
       "   'where',\n",
       "   '𝑆',\n",
       "   'be',\n",
       "   'a',\n",
       "   'finite',\n",
       "   'set',\n",
       "   'of',\n",
       "   'symbol',\n",
       "   'value',\n",
       "   '20',\n",
       "   '21',\n",
       "   'the',\n",
       "   'task',\n",
       "   'of',\n",
       "   'map',\n",
       "   'this',\n",
       "   'datum',\n",
       "   'to',\n",
       "   'vector',\n",
       "   'can',\n",
       "   'be',\n",
       "   'accomplish',\n",
       "   'through',\n",
       "   'embed',\n",
       "   'the',\n",
       "   'main',\n",
       "   'idea',\n",
       "   'be',\n",
       "   'to',\n",
       "   'represent',\n",
       "   'the',\n",
       "   'meaning',\n",
       "   'of',\n",
       "   'a',\n",
       "   'piece',\n",
       "   'of',\n",
       "   'natural',\n",
       "   'language',\n",
       "   'such',\n",
       "   'as',\n",
       "   'text',\n",
       "   'image',\n",
       "   'or',\n",
       "   'audio',\n",
       "   'use',\n",
       "   'dense',\n",
       "   'low',\n",
       "   'dimensional',\n",
       "   'vector',\n",
       "   'with',\n",
       "   'real',\n",
       "   'value',\n",
       "   '4',\n",
       "   'https://www.w3.org/rdf/',\n",
       "   '115',\n",
       "   'WebMedia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Ítalo',\n",
       "   'M.',\n",
       "   'Pereira',\n",
       "   'and',\n",
       "   'Anderson',\n",
       "   'A.',\n",
       "   'Ferreira',\n",
       "   'integrate',\n",
       "   'entity',\n",
       "   'mention',\n",
       "   'from',\n",
       "   'a',\n",
       "   'text',\n",
       "   'with',\n",
       "   'the',\n",
       "   'entity',\n",
       "   'present',\n",
       "   'in',\n",
       "   'the',\n",
       "   'KG',\n",
       "   'we',\n",
       "   'use',\n",
       "   'embed',\n",
       "   'representation',\n",
       "   'to',\n",
       "   'put',\n",
       "   'mention',\n",
       "   'and',\n",
       "   'its',\n",
       "   'entity',\n",
       "   'close',\n",
       "   'in',\n",
       "   'a',\n",
       "   'common',\n",
       "   'vector',\n",
       "   'space',\n",
       "   'consider',\n",
       "   'their',\n",
       "   'semantic',\n",
       "   'context',\n",
       "   'our',\n",
       "   'hypothesis',\n",
       "   'be',\n",
       "   'that',\n",
       "   'this',\n",
       "   'approach',\n",
       "   'will',\n",
       "   'allow',\n",
       "   'for',\n",
       "   'more',\n",
       "   'precise',\n",
       "   'and',\n",
       "   'coherent',\n",
       "   ...]},\n",
       " {'titulo': 'Elderly Fall Monitoring in Smart Homes Using Wearable Device',\n",
       "  'informacoes_url': '',\n",
       "  'idioma': 'english',\n",
       "  'storage_key': '../articles/original/english/985-24751-1-10-20240923.pdf',\n",
       "  'author': 'Júlia M. P. Moreira; Raphael W. Bettio; André P. Freire; Luciano M. Santos; and Marluce R. Pereira',\n",
       "  'data_publicacao': '11-09-2024',\n",
       "  'resumo': 'The constant progress of technology, especially in the area of health, brings numerous benefits, one of which is the increase in human life expectancy. However, problems that occur recurrently among the elderly age group are now on the radar of studies that also seek to improve the quality of life of these people. The number of cases of falls among elderly people is worrying, even more so as this is a portion of the population that tends to live alone. In the context of smart homes, several solutions have emerged for monitoring elderly people to increase safety and provide faster assistance, if necessary. One of these solutions is the use of wearable devices responsible for identifying the person’s movements. This work presents the study and development of a wearable device capable of detecting falls and, if they occur, automatically notifying the necessary people through alert messages via the Telegram application so that they can help the person who has suffered a fall. In this work, a Wi-Fi network, MQTT protocol, accelerometer and gyroscope inertial sensors and an ESP32 board programmed using the Arduino IDE were used. Preliminary tests indicated good performance in recognizing falls, based on tilt angle analysis, gyroscope readings and accelerometer readings. The proof of concept and preliminary tests carried out demonstrate the potential for using low-cost technologies for wearable resources for application in smart homes and monitoring the health of elderly people. ###',\n",
       "  'keywords': 'fall detection, wearable technologies, accelerometer and gyroscope, alert messages',\n",
       "  'referencias': ['[1] Ibukun Awolusi, Eric Marks, and Matthew Hallowell. 2018. Wearable technology for personalized construction safety monitoring and trending: Review of\\napplicable devices. *Automation in Construction* 85 (2018).',\n",
       "   '[2] Sejal Badgujar and Anju S. Pillai. 2020. Fall Detection for Elderly People using\\nMachine Learning. In *2020 11th International Conference on Computing, Commu-*\\n*nication and Networking Technologies (ICCCNT)* . 1–4. https://doi.org/10.1109/\\nICCCNT49239.2020.9225494',\n",
       "   '[3] Shirley Basílio. 2021. O que é Node-RED? Conhecendo e instalando.\\nDisponível em: https://blogmasterwalkershop.com.br/outros/o-que-e-node-redconhecendo-e-instalando, Acesso em: ago de 2022.',\n",
       "   '[4] Tingting Chen, Zhenglong Ding, and Biao Li. 2022. Elderly Fall Detection Based\\non Improved YOLOv5s Network. *IEEE Access* 10 (2022), 91273–91282. https:\\n//doi.org/10.1109/ACCESS.2022.3202293',\n",
       "   '[5] Ministério da Saúde. 2009. Quedas de idosos. Disponível em: https://bvsms.\\nsaude.gov.br/quedas-de-idosos. Acesso em: junho, 2024.',\n",
       "   '[6] Koldo De Miguel, Alberto Brunete, Miguel Hernando, and Ernesto Gambao. 2017.\\nHome Camera-Based Fall Detection System for the Elderly. *Sensors* 17, 12 (2017).\\nhttps://doi.org/10.3390/s17122864',\n",
       "   '[7] Luciana De Nardin, Kamila RH Rodrigues, Larissa C Zimmermann, Brunela DM\\nOrlandi, and Maria da Graça C. Pimentel. 2020. Recognition of human activities\\nvia wearable sensors: variables identified in a systematic mapping. In *Proceedings*\\n*of the Brazilian Symposium on Multimedia and the Web* . 49–56.',\n",
       "   '[8] Thiago de Quadros, André Eugenio Lazzaretti, and Fábio Kürt Schneider. 2018. A\\nMovement Decomposition and Machine Learning-Based Fall Detection System\\nUsing Wrist Wearable Device. *IEEE SENSORS* 18, 12 (2018).',\n",
       "   '[9] Irene Gomes e Vinícius Britto. 2024. Censo 2022: número de pessoas com\\n65 anos ou mais de idade cresceu 57,4% em 12 anos. Disponível\\nem: https://agenciadenoticias.ibge.gov.br/agencia-detalhe-de-midia.html?view=\\nmediaibge&catid=2101&id=6730, Acesso em: jun. de 2024.',\n",
       "   '[10] Andressa B Ferreira, Leonardo S Piva, Reinaldo B Braga, and Rossana MC Andrade.\\n2015. Avaliação da confiança no funcionamento de sistemas de detecção e alerta\\nde quedas. *Revista de Informática Aplicada* 11, 2 (2015).',\n",
       "   '[11] Andressa Bezerra Ferreira, Leonardo Sabadini Piva, Reinaldo Bezerra Braga, and\\nRossana Maria de Castro Andrade. 2014. Trust Evaluation in an Android System\\nfor Detection and Alert Falls. In *Proceedings of the 20th Brazilian Symposium on*\\n*Multimedia and the Web* . 111–114.',\n",
       "   '[12] Ligia FREITAS, Elizabete Viana de; PY. 2016. *Tratado de geriatria e gerontologia.*\\n(4th ed.). Guanabara Koogan. 1651 pages.',\n",
       "   '[13] Usman M. Daud S. Kabir A. Nawaz Q. Gilani, S.M.M. and O. Judit. 2024. SDN-based\\nmulti-level framework for smart home services. (2024), 327–347.',\n",
       "   '[14] Akash Gupta, Rohini Srivastava, Himanshu Gupta, and Basant Kumar. 2020. IoT\\nBased Fall Detection Monitoring and Alarm System For Elderly. In *2020 IEEE*\\n*7th Uttar Pradesh Section International Conference on Electrical, Electronics and*\\n*Computer Engineering (UPCON)* . 1–5. https://doi.org/10.1109/UPCON50219.2020.\\n9376569',\n",
       "   '[15] Sardor Juraev, Akash Ghimire, Jumabek Alikhanov, Vijay Kakani, and Hakil\\nKim. 2022. Exploring Human Pose Estimation and the Usage of Synthetic Data\\nfor Elderly Fall Detection in Real-World Surveillance. *IEEE Access* 10 (2022),\\n94249–94261. https://doi.org/10.1109/ACCESS.2022.3203174',\n",
       "   '[16] Alexandre Kalache. 2010. WHO Global Report on Falls Prevention in Older\\nAge. Disponível em: https://www.who.int/ageing/publications/Falls_\\nprevention7March.pdf, Acesso em: jun de 2024.',\n",
       "   '[17] Suchitporn Lersilp, Supawadee Putthinoi, Peerasak Lerttrakarnnon, and Patima\\nSilsupadol. 2020. Development and Usability Testing of an Emergency Alert\\nDevice for Elderly People and People with Disabilities. *e Scientific World Journal*\\n2020 (2020).',\n",
       "   '[18] Muhammad Mubashir, Ling Shao, and Luke Seed. 2013. A survey on fall detection:\\nPrinciples and approaches. *Neurocomputing* 100 (2013).',\n",
       "   '[19] Md. Jaber Al Nahian, Tapotosh Ghosh, Md. Hasan Al Banna, Mohammed A.\\nAseeri, Mohammed Nasir Uddin, Muhammad Raisuddin Ahmed, Mufti Mahmud,\\nand M. Shamim Kaiser. 2021. Towards an Accelerometer-Based Elderly Fall\\nDetection System Using Cross-Disciplinary Time Series Features. *IEEE Access* 9\\n(2021), 39413–39431. https://doi.org/10.1109/ACCESS.2021.3056441',\n",
       "   '[20] Camila Pereira de Oliveira, Cristiano da Silveira Colombo, and Daniel José Ventorim Nunes. 2024. Machine Learning Applied To Fall Detection in the Elderly. In\\n*Proceedings of the 20th Brazilian Symposium on Information Systems* (<conf-loc>,\\n<city>Juiz de Fora</city>, <country>Brazil</country>, </conf-loc>) *(SBSI ’24)* .\\nAssociation for Computing Machinery, New York, NY, USA, Article 58, 9 pages.\\nhttps://doi.org/10.1145/3658271.3658330',\n",
       "   '[21] Guilherme Bruno Araújo Pimenta. 2019. *Uso da Ferramenta Node-RED em Pro-*\\n*cessos de Automatização no Cenário da Quarta Revolução Industrial* . Monografia.\\nUniversidade Federal de Lavras, Lavras - MG.',\n",
       "   '[22] Mahsa T. Pourazad, Anahita Shojaei-Hashemi, Panos Nasiopoulos, Maryam Azimi, Michelle Mak, Jennifer Grace, Doojin Jung, and Taran Bains. 2020. A NonIntrusive Deep Learning Based Fall Detection Scheme Using Video Cameras.\\nIn *2020 International Conference on Information Networking (ICOIN)* . 443–446.\\n\\n\\nhttps://doi.org/10.1109/ICOIN48656.2020.9016455',\n",
       "   '[23] Luis GS Rodrigues, Diego RC Dias, Marcelo P Guimarães, Alexandre F Brandão,\\nLeonardo CD Rocha, Rogério L Iope, and José RF Brega. 2021. Upper limb motion\\ntracking and classification: A smartphone approach. In *Proceedings of the Brazilian*\\n*Symposium on Multimedia and the Web* . 61–64.',\n",
       "   '[24] Thanos G. Stavropoulos, Asterios Papastergiou, Lampros Mpaltadoros, Spiros\\nNikolopoulos, and Ioannis Kompatsiaris. 2020. IoT Wearable Sensors and Devices\\nin Elderly Care: A Literature Review. *Sensors* 20 (2020).',\n",
       "   '[25] Deen MJ. Subramaniam S, Faisal AI. 2022. Wearable Sensor Systems for Fall Risk\\nAssessment: A Review. Front Digit Health.',\n",
       "   '[26] Guilherme Gerzson Torres. 2018. *Tecnologia Assistiva para Detecção de Quedas:*\\n*Desenvolvimento de Sensor Vestível Integrado ao Sistema de Casa Inteligente* . Dissertação de mestrado. Universidade Federal do Rio Grande do Sul, Porto Alegre RS. Disponível em: http://hdl.handle.net/10183/180824.',\n",
       "   '[27] Xueyi Wang, Joshua Ellul, and George Azzopardi. 2020. Elderly Fall Detection\\nSystems: A Literature Survey. *Frontiers in Robotics and AI* 7 (2020). https:\\n//doi.org/10.3389/frobt.2020.00071',\n",
       "   '[28] Jing Zhang, Jia Li, and Weibing Wang. 2021. A Class-Imbalanced Deep Learning\\nFall Detection Algorithm Using Wearable Sensors. *Sensors* 21, 19 (2021). https:\\n//doi.org/10.3390/s21196511\\n\\n\\n132\\n\\n\\n-----'],\n",
       "  'text': '# **Elderly Fall Monitoring in Smart Homes Using Wearable Device**\\n\\n## Júlia M. P. Moreira\\n#### juliampmoreira7@gmail.com Department of Applyed Computing Federal University of Lavras - Lavras - Brazil\\n\\n## Raphael W. Bettio\\n#### raphaelwb@ufla.br Department of Applyed Computing Federal University of Lavras - Lavras - Brazil\\n\\n## André P. Freire\\n#### apfreire@ufla.br Department of Computer Science Federal University of Lavras - Lavras - Brazil\\n\\n## Luciano M. Santos\\n#### mendesluciano@ufla.br Department of Computer Science Federal University of Lavras - Lavras - Brazil\\n### **ABSTRACT**\\n\\nThe constant progress of technology, especially in the area of health,\\nbrings numerous benefits, one of which is the increase in human\\nlife expectancy. However, problems that occur recurrently among\\nthe elderly age group are now on the radar of studies that also seek\\nto improve the quality of life of these people. The number of cases\\nof falls among elderly people is worrying, even more so as this is a\\nportion of the population that tends to live alone. In the context of\\nsmart homes, several solutions have emerged for monitoring elderly\\npeople to increase safety and provide faster assistance, if necessary.\\nOne of these solutions is the use of wearable devices responsible for\\nidentifying the person’s movements. This work presents the study\\nand development of a wearable device capable of detecting falls and,\\nif they occur, automatically notifying the necessary people through\\nalert messages via the Telegram application so that they can help\\nthe person who has suffered a fall. In this work, a Wi-Fi network,\\nMQTT protocol, accelerometer and gyroscope inertial sensors and\\nan ESP32 board programmed using the Arduino IDE were used.\\nPreliminary tests indicated good performance in recognizing falls,\\nbased on tilt angle analysis, gyroscope readings and accelerometer\\nreadings. The proof of concept and preliminary tests carried out\\ndemonstrate the potential for using low-cost technologies for wearable resources for application in smart homes and monitoring the\\nhealth of elderly people.\\n### **KEYWORDS**\\n\\nfall detection, wearable technologies, accelerometer and gyroscope,\\nalert messages\\n### **1 INTRODUCTION**\\n\\nWith the encouragement of innovation and technology as an important pillar to improve people’s quality of life, there is also an\\nincrease in life expectancy and the aging rate of the world’s population. In Brazil, according to the 2022 IBGE census, the population\\nover 60 years of age reached 32,113,490 (15.6%) [ 9 ]. The best thermometer to measure an elderly person’s health is checking their\\nfunctional capacity, which is a person’s real ability to carry out the\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n© 2024 SBC – Brazilian Computing Society.\\nISSN 2966-2753\\n\\n## Marluce R. Pereira\\n#### marluce@ufla.br Department of Applyed Computing Federal University of Lavras - Lavras - Brazil\\n\\ndaily activities possible to live independently and self-sufficiently,\\nwhich tends to reduce with aging. There are scales that measure\\nthis functional capacity, such as the Barthel scale, the Katz scale,\\nthe Lawton and Brody scale, the Pfeffer scale, among others [ 12 ],\\nwhich assess the independence of the elderly in basic activities of\\ndaily living: eating, personal hygiene, getting dressed, using the\\ntoilet, moving from bed to chair and vice versa, cooking, managing\\nmedications, shopping, washing clothes, cleaning the house, using\\nthe telephone, remembering appointments, among others. Risks\\nsuch as falling further increase the concern of family members and\\npeople close to elderly people, as they reduce functional capacity\\n\\n[28].\\nA smart home has devices connected to the Internet that allow\\n\\nremote monitoring and control of appliances and systems such as\\nlighting and heating, as well as other types of sensors, using the\\ninternet of things (IoT)[ 13 ]. Furthermore, there may be wearable\\nsensors that allow the implementation of *Human Activity Recogni-*\\n*tion (HAR)* systems in smart homes, helping to improve people’s\\nquality of life with autonomy and safety [ 7 ], [ 23 ]. In this context,\\na solution to alleviate the consequences of these difficulties in the\\nlives of elderly people, while rescuing independence in daily activities and reassuring people close to them, consists of installing\\nmonitoring systems in the home, in order to guarantee assistance\\nif an incident occurs such as: a fire, fainting, fall or even illness\\ncaused by changes in vital signs. There are different types of emergency alert devices that have been developed for dependent people,\\nincluding children, people with disabilities, the sick and the elderly.\\nMost of them send an alarm only when help is needed, but some\\nare developed to monitor the health of sick patients, such as blood\\npressure, heart rate, among others [ 17 ]. Monitoring an environment means using different devices that obtain desired data, such\\nas a camera, motion sensor, temperature, humidity or brightness,\\namong others, so that from this data it is possible to make decisions\\nin order to achieve the objectives of monitoring.\\nThe problem addressed in this work is how to identify that an\\nelderly person who is alone in their home has suffered a fall and, in\\naddition to identifying, use this information to alert people nearby\\nthrough warning messages [ 11 ]. Therefore, an effective, portable\\nsolution is sought, with integration with the internet network, to\\nmaintain connectivity and provide warnings when necessary. One\\nof the ways to monitor situations that happen to human beings is by\\n\"wearing\" sensors and devices capable of identifying variations and\\n\\n\\n124\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Moreira et al.\\n\\n\\ncommunicating important information in such a way that readings\\nand data collection are not harmed and are useful for the necessary\\nanalysis.\\nThe main objective of this work is the design and implementation of a wearable device prototype for fall monitoring, using\\naccelerometer and gyroscope sensors, and connecting this monitoring devices to a smart home ecosystem using the Node-RED\\nplatform to manage warnings when a fall has been detected. To\\nachieve this objective, it was necessary to research and understand\\nthe technologies involved in existing wearable devices and, from\\nthis, choose the best and most affordable ones to implement in a\\nreal prototype. Then, an accelerometer and a gyroscope were integrated with the ESP32 module to build the wearable device. Finally,\\nsoftware was developed in a smart home environment capable of\\nreliably collecting sensor data and interpreting it to identify different movements that determine a fall and manage alerts sent over\\nthe internet to registered people.\\nSection 2 presents the references about elderly falls and domestic\\naccidents, wearable devices, Node-RED and related work. Section\\n3 describes the methodology adopted to build the prototype and\\ndefines the test cases studied, as well as the standard followed to\\ncarry out these tests. Section 4 presents all the results obtained\\nwith the developed solution and, finally, Section 5 deals with the\\nconclusions about the prototype and implemented solution and\\nindicates development opportunities to improve the project.\\n### **2 THEORICAL REFERENCE** **2.1 Elderly falls and domestic accidents**\\n\\nA fall is defined as *\"inadvertently coming to rest on the ground or*\\n*at another lower level, excluding intentional changes of position to*\\n*lean on furniture, walls or other objects\"* [ 16 ]. As this is an increasingly present problem in the lives of families around the world, the\\nWorld Health Organization (WHO), which defines the age group of\\nelderly people as 60 years or older, publishes global reports aimed\\nat preventing falls in old age. The frequency of falls increases with\\nage and level of frailty. Approximately 28% to 35% of people over\\n65 years of age suffer falls each year, and the proportion rises from\\n32% to 42% for people over 70 years of age [16].\\nFalls can occur due to factors such as: decreased muscle strength;\\nosteoporosis; walking abnormalities; change in blood pressure; depression; senility; osteoarthritis, hip fragility or balance changes;\\nneurological changes (stroke, Parkinson’s disease, multiple sclerosis and Alzheimer’s disease); decreased vision and/or hearing;\\namong others. There are also factors related to the environment\\nsuch as: poorly lit environments; poorly planned houses; inadequate arrangement of furniture that hinders movement; slippery\\nobjects scattered around the house; among others [5].\\nFurthermore, a higher frequency of falls has been reported in\\nnursing homes, in some cases including recurrent falls. It is possible to conclude, then, that even if nursing homes are adapted to\\nthe reality of the elderly’s tasks, they are still not out of danger.\\nTherefore, the implementation of integrated solutions for these\\npeople’s homes is increasingly being studied so that assistance can\\nbe provided quickly if falls occur, such as the development of intelligent surveillance systems for monitoring, since the medical\\n\\n\\nconsequences of accidents like this depend on much of the rescue\\nresponse time [18].\\n### **2.2 Wearable devices**\\n\\nWearable devices are technologies developed to be used directly\\nin contact with the body, whether as sensors embedded in clothing, as accessories to wear on the head, badges or even as wrist\\naccessories. Several brands already have so-called “smartwatches”,\\nwhich are watches with the functions of well-known smartphones\\nand which increasingly have applications that are interesting for\\nmonitoring the health of the individual who uses them. There are\\n“smartwhatches” which are capable of measuring body temperature, blood glucose or oxygen concentration, blood pH and blood\\npressure through biometric sensors [24].\\nThe benefits of this type of technology are very interesting from\\nthe point of view of people’s quality of life, especially the elderly\\npopulation.\\nIn the specific case of fall detection, as discussed in [ 1 ], there is\\nalso a study on the best location for the wearable device considering\\ntwo main factors: the accuracy of the sensors and the need to be\\ncomfortable for the person using it. Among the places available to\\nattach them, there are waist, head, wrist, front of the waist, thigh,\\nchest, ankle and arm. According to [ 8 ] and [ 1 ], the best places to\\nguarantee correct analysis of sensor data are the chest, ankle and\\nwaist, however these are strongly linked to the stigma of using a\\nmedical device and, combined with the fact needing to be used 24\\nhours a day, they can be uncomfortable and less accepted [8].\\nThe most comfortable place from the user’s point of view is\\nthe wrist, however solutions based on wearable devices attached\\nto the wrist have the worst accuracy results, with values lower\\nthan 90%, greatly increasing the level of complexity of the analysis\\nand calculations involving the data obtained. For devices used in\\nthis location, a more in-depth study on the use of this data ends\\nup being necessary. Some studies include, for example, machine\\nlearning applications as a resource to increase the precision and\\nquality of analyzes [8].\\nAccelerometers and gyroscopes are inertial sensors used to monitor movements that can be attached as wearables [ 26 ]. The accelerometer measures the acceleration of a body in relation to gravity, that is, it measures the acceleration exerted on certain objects,\\nperforming some type of action depending on the movement performed on it. The gyroscope uses the force of gravity to indicate the\\nposition of a given object in space, being able to identify whether\\nsomething is rotating on its own axis or whether it is pointing\\nup or down. When the objective is to accurately determine the\\nmovement of an object or user, it may be necessary to combine\\nthe signals from sensors accelerometers and gyroscopes, because\\naccelerometers present as an output signal the linear acceleration\\nof the object and gyroscopes the angular velocity. The combination\\nof these signals results in more complete information about the\\nmovement and allows for more assertive analysis.\\n### **2.3 Node-RED**\\n\\nIn order to connect and implement IoT devices, it is possible to find\\nsome tools that facilitate activities at the code and programming\\nlevels. Node-Red is a programming tool, created by IBM Emerging\\n\\n\\n125\\n\\n\\n-----\\n\\nElderly Fall Monitoring in Smart Homes Using Wearable Device WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\nTechnology, based on flow, used to integrate electronic components\\nwith applications of different natures, making it possible to develop\\nprograms quickly and intuitively. It is open source and is based\\non *nodes*, blocks of pre-defined functions available in a platform\\ntab called *palette* . It is possible to build the desired flow just by\\ndragging the blocks to the provided workspace and making the\\ndesired connections and interactions between blocks [21].\\nTo make it possible to assess whether the constructed flow is\\nperforming the functions as determined, the platform also has a\\n“debug\" tab, where messages and actions appear synchronously\\nwith the functioning of the developed system. Node-RED has been\\nused in the scenario of developing IoT and connectivity solutions\\nas it proves to be a very useful and practical platform [3].\\n### **2.4 Related works**\\n\\nGupta et al. [ 14 ] presents an IoT-based fall detection monitoring\\nsystem for the elderly using only a 3-axis accelerometer. Torres [ 26 ]\\nuses two sensors (accelerometer and 3-axis gyroscope), attached\\nto the user’s chest. A threshold analysis algorithm uses the data\\ngenerated by these sensors to detect falls. This approach is the same\\nas that used in this work. However, in the work of [ 26 ] Node-Red\\nwas not used as a tool to integrate devices with the network and a\\nsmart home system.\\nQuadros et al. [ 8 ] show the development and evaluation of a fall\\ndetection solution with the implementation of machine learning to\\nimprove results. The developed solution is worn on the wrist, as it\\nis considered the most comfortable place, and uses an accelerometer, gyroscope and magnetometer to calculate acceleration, speed\\nand displacement which, implemented in methods based on machine learning, can define the best approach for detecting a fall.\\nThe results are based on data acquired from falling and non-falling\\nmovements of 22 volunteers demonstrating the approach used for\\ntesting with real users. The authors of [ 20 ] applied the methodology, focusing on precision and effectiveness, to evaluate the KNN,\\nDecision Tree and MLP algorithms applied to accelerometer data to\\ndetect falls. Other works also applied machine learning algorithms\\nand [2] and deep learning [4], [15] and [19].\\nThe works of [ 18 ] and [ 27 ] make a comprehensive survey of\\ndifferent devices, fall detection systems and the algorithms implemented in each one. Three categories of approaches are identified:\\nsolutions based on wearable devices, devices placed in the environment and solutions using cameras. The analysis on wearable\\ndevices deals with the same sensors used in this work and the approach is very close considering algorithms and location of the\\nwearable device. The analyzes of other approaches are interesting,\\nas they demonstrate alternatives that, if linked to wearable devices,\\nadd even more reliability to monitoring in smart homes, especially\\nin the problem of elderly people falling.\\nIn the work of [ 25 ], wearable devices based on inertial sensors\\nand insoles that were developed for applications related to falls\\nare analyzed, identifying key points, including spatio-temporal\\nparameters, biomechanical parameters of gait, physical activities\\nand methods data analysis relating to developed systems.\\nThe assessment of confidence in the functioning of a ubiquitous\\nsystem that uses a microphone and accelerometer and magnetometer sensors to detect and alert falls called fAlert is presented in the\\n\\n\\nworks [ 10 ] and [ 11 ]. Software quality measures such as precision,\\nsensitivity, availability, specificity, performance, accuracy, among\\nothers, are used.\\nThis work presents a wearable device (on the chest) that collects\\nand analyzes movement data from elderly people wherever they\\nare in a smart home ecosystem. An alert is sent to the responsible\\npersonnel in real time via Telegram in a cell phone, speeding up\\nthe rescue process.\\n### **3 METHODOLOGY**\\n\\nThe wearable device for monitoring falls proposed in this work is\\nbased on two sensors, the accelerometer and the gyroscope, to find\\nmovement patterns that determine a fall and then alert family members of elderly people. The MPU6050 module was chosen because it\\nhas the two sensors mentioned integrated in order to facilitate the\\nuse of its output data when carrying out the tests. This module was\\nconnected to the ESP32 development module, which has a small\\nsize and built-in support for WiFi and Bluetooth networks, as well\\nas integrated flash memory.\\nTo develop the project’s programming, the integrated development environment created to program microcontrollers from the\\nArduino family (Arduino IDE) was used, but which can also be used\\nfor compatible modules, such as the ESP32. The IDE’s programming language is C++, with the installation of specific libraries\\nthat already have ready-made functions for reading and processing sensor data, in addition to libraries that assist in the use of\\nthe ESP32 module’s WiFi connectivity (WiFi, Ethernet, Wire and\\nPubSubClient).\\n### **3.1 Construction of the wearable device**\\n\\nThe construction of the prototype began with the assembly of the\\ncomponents on a bench to carry out basic functional tests, with\\nthe sole objective of reading sensor data in a satisfactory manner,\\nimplementing communication with other clients using the MQTT\\nprotocol ( *Message Queue Telemetry Transport* ) and implement the\\ncomplementary filter in order to increase data reliability [ 26 ]. Figure\\n1 depicts the arrangement of the prototype components in the first\\nstage of construction without considering sensor reading axes.\\n\\n**Figure 1: Wearable device prototype on bench.**\\n\\nSource: From the author (2024).\\n\\nTo build the wearable device prototype, the chest was considered as the best place to attach the components, aiming for more\\nreliable data and better performance. Based on this determination,\\n\\n\\n126\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Moreira et al.\\n\\n\\n**Figure 2: Wearable device.**\\n\\nSource: From the author (2024).\\n\\na comfortable and safe way to use the device was sought, as shown\\nin Figure 2. The ESP32 is powered at this stage by a 9V battery.\\n### **3.2 Algorithm**\\n\\nThe algorithm implemented in Arduino IDE for the ESP32 module is\\nschematized in the flowchart in Figure 3. ESP32 stablish connection\\nwith Wi-fi to access the MQTT broker and activate the MPU6050\\nmodule composed by accelerometer and gyroscope. ESP32 remains\\nin a loop taking sensor readings, converting values of gyroscope\\nto degrees/second and accelerometer to degrees. After that, the\\ncomplementary filter is used. The results are analyzed to identify\\nfalls and the alerts are sent.\\n### **3.3 Test Cases**\\n\\nConsidering the age range of the elderly, it is possible to identify\\nsome scenarios that could result in a fall: falls when walking or\\nstanding, falls when standing on supports (e.g. stairs, benches, etc.),\\nfalls when lying down or getting out of bed or falling when sitting\\non a chair [ 18 ]. Therefore, to carry out the tests, three non-falling\\nmovements and four falling movements were determined for analysis. The non-falling movements are walking, standing and sitting,\\nand the falling movements are forwards, backwards and to the left\\nand right sides. Movement variations occur based on the initialization and stationary values of the MPU6050 module sensors. In\\nthis way, the device is always initialized in the sitting position in\\norder to reproduce a scenario where the user puts on the device in\\nthe morning before getting up, thus maintaining value references\\nwith the smallest possible variations and improving results. Other\\nvoluntary movements of lying down are considered normal and do\\nnot generate alerts.\\n### **3.4 Testing methodology**\\n\\nTo carry out tests, the following methodology was defined:\\n\\n(1) The person responsible for using the wearable device prototype must place it on the body, adjusting the straps so that\\nit is coupled and fixed to the chest, but still in a comfortable\\nway that does not impede movement. The device must be\\nturned off.\\n\\n\\n**Figure 3: Flowchart of the program developed for ESP32.**\\n\\nSource: From author (2024).\\n\\n(2) Once adjusted and dressed properly, the user must sit in a\\nchair with an upright posture and turn on the device.\\n(3) All tests must last two minutes to better visualize the plotted\\ndata and be carried out in a location with a stable Wi-Fi\\n\\nconnection and by the same user.\\n(4) For non-fall tests, data can be collected as soon as the device\\nis turned on, however for fall data the user must get up and\\ngo to the testing location and wait at least thirty seconds until\\nthe data is collected and stabilized for better visualization in\\n\\ngraphs that are plotted simultaneously.\\n(5) After carrying out the falling movement, wait again for at\\nleast thirty seconds for the data to stabilize.\\n(6) At the end of the desired data collection, the user must turn\\noff the device so that the data plotting stops and the graphs\\ncan be saved with the recent data.\\n\\n(7) Ideally, the tests should be repeated three to four times for\\ndata comparison and coherence.\\n### **4 RESULTS**\\n\\nObtaining the results begins with reading the data from the MPU6050\\nmodule. The raw values read by the sensors can be obtained using the Wire library, however it is necessary to process these data,\\nas the values found do not correspond to any physical quantity\\n\\n\\n127\\n\\n\\n-----\\n\\nElderly Fall Monitoring in Smart Homes Using Wearable Device WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\nand therefore it is necessary to convert them. In the case of the\\ngyroscope, the values are converted to degrees per second (º/s) and\\nrepresent the position variations in an angular velocity quantity.\\nAs for the accelerometer, the raw data is used to find an inclination\\nangle, but for the result of the values obtained to be a more accurate\\nvalue, it is necessary to use a relationship between the three axes,\\nmaking a conversion according to equations (1) to (3) implemented\\nin the code.\\n\\n\\n*𝜌* = arctan ���\\n�\\n\\n*𝜙* = arctan ���\\n�\\n\\n*𝜃* = arctan\\n���\\n�\\n\\n\\n*𝐴* *𝑥*\\n~~√~~ *𝐴* [2] *𝑦* + *𝐴* *𝑧* [2]\\n\\n\\n*𝐴* *𝑥*\\n\\n*𝐴* [2] +\\n\\n~~√~~ *𝑦*\\n\\n\\n; (1)\\n\\n\\n*𝐴*\\n*𝑦*\\n\\n*𝐴* *𝑥* [2] +\\n\\n√︃\\n\\n[2]\\n\\n\\n; (2)\\n\\n\\n*𝐴* *𝑥* [2] + *𝐴* *𝑧* [2]\\n\\n\\n√\\n\\n\\n*𝐴* *𝑥* [2] + *𝐴* [2] *𝑦*\\n\\n\\n(3)\\n\\n\\nvalues (sitting, standing and walking) and are demonstrated in\\nFigures 4 and 5. In Figure 4, it can be seen that the tilt angle in the\\nY axis increases, while in the X axis it decreases when the person\\nstands up, but in the Z axis the change is small due to the position of\\nthe body where the wearable is located. The person leans forward\\nto stand, so the values of X and Y vary more. These variations are\\nvisualized in the gyroscope and accelerometer readings. When the\\nperson is walking (Figure 5), the variations in the X and Y readings\\nare greater, as the person varies the position of their body.\\nFigures 6, 7, 8 and 9 present data on falling movements (forward,\\nbackward, right side and left side).\\nIn all plotted graphs, the X axis presents the last two minutes of\\nsensor readings in HH:MM format. The Y axis for the “Accelerometer Readings\" graph presents the angle variations in degrees, for\\nthe “Gyroscope Readings\" graph it presents the angular velocity in\\ndegrees per second (º/s) and, for the “ Tilt angle\" the Y axis presents\\nthe angles in degrees. The values presented were analyzed to verify coherence and are samples chosen from four others for each\\n\\nmovement.\\n\\nIt is possible to infer from the graphs generated that non-falling\\nmovements will not be confused with falling movements, since\\nthe values obtained for angular velocity for both the movement\\nof getting up from a chair and movements for walking are not\\ngreater than 80 or less than -40, whereas for falling movements the\\nvalues obtained from the gyroscope reading exceed these limits. The\\nangular velocity is very important for determining the limits, as the\\nspeed of falling movements is clearly greater than that of everyday\\nmovements. It was expected that in the accelerometer readings the\\nZ axis and the Y axis would present greater variation in degrees for\\nforward and backward falling movements, since these movements\\noccur by rotating the axes around the X axis. The falling movements\\nto the right and left sides, the X and Y axes are those that show the\\ngreatest variations, as the movements occur by rotating the axes\\naround the Z axis. Finally, the inclination angle values, mainly on\\nthe Y axis, are also very specific and clearly incorporate the critical\\ncomponents of the accelerometer and gyroscope.\\nAfter analyzing and identifying limits for each component of the\\ntests: accelerometer, gyroscope and tilt angle readings, these limits\\nare implemented in conditional structures in the code to ensure\\nthat all cases are understood. Each case of an outage is published\\nin a single topic called “ Outage \" and this topic must be accessed on\\nNode-RED, where messages are sent to the people registered for\\nthis purpose.\\nWith the implementation of all cases of failure in the developed\\nalgorithm, integration with Node-RED can be carried out. A flow\\nwas built with a node subscribed to the topic “ Queda \" which, upon\\nreceiving the value “ 1 \" indicating that there was a fall, activates a\\ntrigger that sends a message through the Telegram application. To\\nimplement the sending of messages via Node-RED, it is necessary\\nto create a chat bot on Telegram, that is, a specific programmed\\nconversation window for sending outage alert messages. The creation of the chat bot can be easily done through the application\\nitself by chatting with another bot developed for this purpose. It\\nis necessary to search for “ BotFather \" within the Telegram application and send the “ newbot \" command. With this, it is possible\\nchoose the bot’s name and username, but the username must be\\n\\n\\n*𝐴* *𝑧*\\n\\n\\n���;\\n�\\n\\n���;\\n�\\n\\n���\\n�\\n\\n\\nIn the equations *𝜌* represents the rotation around the Y axis, *𝜙*\\nthe rotation around the X axis and *𝜃* the rotation around the Z\\naxis, “arctan\" refers to finding the arc tangent, the inverse function\\nof the tangent, and the values *𝐴* *𝑥*, *𝐴* *𝑦* and *𝐴* *𝑧* represent the readings obtained with the MPU-6050 module on the X, Y and Z axes,\\nrespectively.\\nAfter the sensor data reading provides treated values with known\\nunits, the implementation of the complementary filter is included\\nwith the function of reducing noise in the measurements by integrating gyroscope data with the angles obtained from the accelerometer\\nreadings. The complementary filter uses two constants in the calculations, *𝛼* (alpha) and Δ t (sampling rate). *𝛼* is determined by a\\nrelationship between a time constant and the sampling rate. For the\\ncalculations in this article, a time constant equal to 1 and a sampling\\nrate of 0.04 are used, values obtained through the specifications of\\nthe chosen sensor module, resulting in an alpha value equal to 0.96.\\nIn this way, the filtered angle is found through the equation (4).\\n\\n*𝐴𝑛𝑔𝑙𝑒* = *𝛼* ∗( *𝐴𝑛𝑔𝐺𝑖𝑟𝑜𝑠𝑐𝑜𝑝𝑦* ) + (1 − *𝛼* ) ∗( *𝐴𝑛𝑔𝐴𝑐𝑐𝑒𝑙𝑒𝑟𝑜𝑚𝑒𝑡𝑒𝑟* ) (4)\\n\\nWith all readings occurring correctly, the aim is to develop the\\nfall detection algorithm based on values obtained for previously\\ndetermined falling and non-falling movements.\\nA dashboard was developed using the Node-RED platform capable of synchronously plotting the values obtained with the device.\\nData readings are published in topics and in the flow built in NodeRED, these topics are subscribed to nodes called “ mqtt in \", this\\nis possible because both the ESP32 development module and the\\nnode are connected to the same broker, according to server and\\nclient concepts, using the MQTT protocol. These nodes responsible\\nfor receiving the reading values are connected to “ chart \" nodes\\ncapable of plotting graphs on the dashboard. This way the data is\\nposted at the same time as the readings take place. The constructed\\nflow is demonstrated in Figure 10, where subscription nodes in the\\ntopics where sensor values are posted in Node-RED.\\nFrom the built dashboard it is possible to monitor the values\\nof the desired readings for subsequent analysis and definition of\\nlimits for each type of fall determined. The graphs present the tests\\nfollowing the methodology described previously, first for non-fall\\n\\n\\n128\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Moreira et al.\\n\\n**Figure 4: Values obtained for user testing initially sitting and then standing.**\\n\\nSource: From author (2024).\\n\\n**Figure 5: Values obtained for walking user test.**\\n\\nSource: From author (2024).\\n\\n**Figure 6: Values obtained for user test initially stopped and then falling forward.**\\n\\nSource: From author (2024).\\n\\n\\nunique. For this work, a bot named “ Monitoramento e saúde \"\\nwith username “monitoramento-saudebot\" was created.\\nWhen creating a bot with “ BotFather \" a token code is provided\\nto access the HTTP API. Node-RED has an extension that directly accesses Telegram named “ telegrambot \", generating five new nodes.\\n\\n\\nOne of these is the “ sender \" node responsible for sending a message to the registered chat when activated. To execute the sending\\naction, a “ mqtt in \" node is used to receive the message posted by\\nESP32 when a fall movement occurs and this event is enough to\\nstart the flow.\\n\\n\\n129\\n\\n\\n-----\\n\\nElderly Fall Monitoring in Smart Homes Using Wearable Device WebMedia’2024, Juiz de Fora, Brazil\\n\\n**Figure 7: Values obtained for user testing initially standing still and then falling backwards.**\\n\\nSource: From author (2024).\\n\\n**Figure 8: Values obtained for user testing initially standing still and then falling to the right side.**\\n\\nSource: From author (2024).\\n\\n**Figure 9: Values obtained for user testing initially standing still and then falling to the left side.**\\n\\nSource: From author (2024).\\n\\n\\nOne message that will be sent with each fall alert can also be\\nconfigured using nodes in Node-RED. The fall alert message is written in the “ funcion \" block and connected to the “ mqtt in \" block\\nwith the chatID number specific to the destination Telegram. The\\nchatId is provided by the Telegram application searching for “ Get\\n\\n\\nMy ID \" in the search field of the Telegram application itself and\\nafter starting the conversation, the number will be provided automatically. The “ sender \" node must be filled in with the chatId\\nprovided and the message configured to indicate which chat bot\\nthe message is intended for.\\n\\n\\n130\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Moreira et al.\\n### **5 FINAL CONSIDERATIONS**\\n\\n\\n**Figure 10: Subscription nodes in the topics where sensor**\\n**values are posted.**\\n\\nFor the person interested in monitoring falls to receive messages\\nvia Telegram on their cell phone, they must search for the name\\nof the chat bot in the application’s search field and start the conversation, which corresponds to sending a “ start \" command to\\nthe bot. An introduction message is also configured for when a\\nperson signs up for the conversation. Using the “ receiver \" node,\\nit is possible to trigger a new process when a message is received\\nby the chat bot. From the “ start \" received, a trigger is activated\\nthat sends an introduction message. The settings are the same as\\nthe fall alert flow. The constructed flows are represented in Figure\\n11. The message sent by Node-RED to Telegram from the person\\nresponsible translate from Portuguese to English is:\\n\"Hello! I’m Mr. José’s falls monitoring assistant.\\nI’m here to warn you if an accident happens.\\nWARNING: Mr. José may have suffered a fall. Try to\\nget in touch as soon as possible\".\\n\\n**Figure 11:** **Message sending flows in Node-RED.**\\n\\nSource: From author (2024).\\n\\n\\nThe main problem addressed in this work is how to identify that an\\nelderly person who is alone in their home has suffered a fall. When\\nidentifying that there has been a fall, it is necessary to alert people\\nnearby through warning messages.\\nThe objective of this project was to implement a wearable device\\nprototype for fall monitoring, using accelerometer and gyroscope\\nsensors, and connect this monitoring to a smart home ecosystem using the Node-RED platform to manage warnings if a fall movement\\nis detected.\\n\\nThe wearable prototype developed in this work is a suitable\\nsolution for monitoring movements, more specifically for detecting\\nfalls. It has been proven that wearable devices developed with\\ninertial sensors are suitable for this type of problem and can be\\nused to reduce rescue time if accidents occur with elderly people,\\nwhile increasing the independence and safety of this age group in\\ntheir own houses.\\n\\nEven if low-cost modules with limited computational capacity\\nare chosen, it is possible to implement data treatments such as\\nthe complementary filter to increase the precision of the values\\nobtained and still develop reliable solutions. The approach used\\nin this work to develop the algorithm responsible for determining\\nfalling movements was based on the limits found in coherent tests,\\nhowever there is room for implementing more complex computational analyzes using machine learning. These are currently topics\\nwith great emphasis in the context of assistive technologies and can\\nbring more performance to solutions such as the one developed.\\nFinally, more and more investment is being made in smart home\\napplications, and the integration of several of them results in more\\ncomplete, reliable and comfortable controls. That said, a solution\\nlike the one developed combined with other systems will guarantee\\ngreater performance and reliability in your activities. Other systems\\nthat have the potential to be integrated may be based on different\\nsensors, networks, communications protocols and algorithms. In\\nthis work, a Wi-Fi network, MQTT protocol, accelerometer and gyroscope inertial sensors and an ESP32 board programmed through\\nthe Arduino IDE were used, but there are more possibilities such as\\nbluetooth connection, cameras, presence and light sensors, HTTP\\nprotocol and other development boards such as Arduino, Raspberry\\nPi, among others.\\nAs future work, we intend to carry out usability testing of the\\nwearable in a smart home environment and with elderly people,\\naiming to further improve the prototype. Other real-world scenarios can be also used in tests. Fall movements can be analyzed along\\nwith movements performed during the person’s daily routine. Historical data generated by sensors associated with machine learning\\ntechniques can be used to more accurately identify fall situations.\\nAnother possibility is to use other forms of fall detection, such as\\nanalyzing images from monitoring cameras, as discussed by [ 6 ] and\\n*deep learning* [22].\\n### **6 ACKNOWLEDGMENTS**\\n\\nThe authors thank FAPESP, CNPq and Fapemig for funding part of\\nthis research.\\n\\n\\n131\\n\\n\\n-----\\n\\nElderly Fall Monitoring in Smart Homes Using Wearable Device WebMedia’2024, Juiz de Fora, Brazil\\n\\n### **REFERENCES**\\n\\n[1] Ibukun Awolusi, Eric Marks, and Matthew Hallowell. 2018. Wearable technology for personalized construction safety monitoring and trending: Review of\\napplicable devices. *Automation in Construction* 85 (2018).\\n\\n[2] Sejal Badgujar and Anju S. Pillai. 2020. Fall Detection for Elderly People using\\nMachine Learning. In *2020 11th International Conference on Computing, Commu-*\\n*nication and Networking Technologies (ICCCNT)* . 1–4. https://doi.org/10.1109/\\nICCCNT49239.2020.9225494\\n\\n[3] Shirley Basílio. 2021. O que é Node-RED? Conhecendo e instalando.\\nDisponível em: https://blogmasterwalkershop.com.br/outros/o-que-e-node-redconhecendo-e-instalando, Acesso em: ago de 2022.\\n\\n[4] Tingting Chen, Zhenglong Ding, and Biao Li. 2022. Elderly Fall Detection Based\\non Improved YOLOv5s Network. *IEEE Access* 10 (2022), 91273–91282. https:\\n//doi.org/10.1109/ACCESS.2022.3202293\\n\\n[5] Ministério da Saúde. 2009. Quedas de idosos. Disponível em: https://bvsms.\\nsaude.gov.br/quedas-de-idosos. Acesso em: junho, 2024.\\n\\n[6] Koldo De Miguel, Alberto Brunete, Miguel Hernando, and Ernesto Gambao. 2017.\\nHome Camera-Based Fall Detection System for the Elderly. *Sensors* 17, 12 (2017).\\nhttps://doi.org/10.3390/s17122864\\n\\n[7] Luciana De Nardin, Kamila RH Rodrigues, Larissa C Zimmermann, Brunela DM\\nOrlandi, and Maria da Graça C. Pimentel. 2020. Recognition of human activities\\nvia wearable sensors: variables identified in a systematic mapping. In *Proceedings*\\n*of the Brazilian Symposium on Multimedia and the Web* . 49–56.\\n\\n[8] Thiago de Quadros, André Eugenio Lazzaretti, and Fábio Kürt Schneider. 2018. A\\nMovement Decomposition and Machine Learning-Based Fall Detection System\\nUsing Wrist Wearable Device. *IEEE SENSORS* 18, 12 (2018).\\n\\n[9] Irene Gomes e Vinícius Britto. 2024. Censo 2022: número de pessoas com\\n65 anos ou mais de idade cresceu 57,4% em 12 anos. Disponível\\nem: https://agenciadenoticias.ibge.gov.br/agencia-detalhe-de-midia.html?view=\\nmediaibge&catid=2101&id=6730, Acesso em: jun. de 2024.\\n\\n[10] Andressa B Ferreira, Leonardo S Piva, Reinaldo B Braga, and Rossana MC Andrade.\\n2015. Avaliação da confiança no funcionamento de sistemas de detecção e alerta\\nde quedas. *Revista de Informática Aplicada* 11, 2 (2015).\\n\\n[11] Andressa Bezerra Ferreira, Leonardo Sabadini Piva, Reinaldo Bezerra Braga, and\\nRossana Maria de Castro Andrade. 2014. Trust Evaluation in an Android System\\nfor Detection and Alert Falls. In *Proceedings of the 20th Brazilian Symposium on*\\n*Multimedia and the Web* . 111–114.\\n\\n[12] Ligia FREITAS, Elizabete Viana de; PY. 2016. *Tratado de geriatria e gerontologia.*\\n(4th ed.). Guanabara Koogan. 1651 pages.\\n\\n[13] Usman M. Daud S. Kabir A. Nawaz Q. Gilani, S.M.M. and O. Judit. 2024. SDN-based\\nmulti-level framework for smart home services. (2024), 327–347.\\n\\n[14] Akash Gupta, Rohini Srivastava, Himanshu Gupta, and Basant Kumar. 2020. IoT\\nBased Fall Detection Monitoring and Alarm System For Elderly. In *2020 IEEE*\\n*7th Uttar Pradesh Section International Conference on Electrical, Electronics and*\\n*Computer Engineering (UPCON)* . 1–5. https://doi.org/10.1109/UPCON50219.2020.\\n9376569\\n\\n[15] Sardor Juraev, Akash Ghimire, Jumabek Alikhanov, Vijay Kakani, and Hakil\\nKim. 2022. Exploring Human Pose Estimation and the Usage of Synthetic Data\\nfor Elderly Fall Detection in Real-World Surveillance. *IEEE Access* 10 (2022),\\n94249–94261. https://doi.org/10.1109/ACCESS.2022.3203174\\n\\n[16] Alexandre Kalache. 2010. WHO Global Report on Falls Prevention in Older\\nAge. Disponível em: https://www.who.int/ageing/publications/Falls_\\nprevention7March.pdf, Acesso em: jun de 2024.\\n\\n[17] Suchitporn Lersilp, Supawadee Putthinoi, Peerasak Lerttrakarnnon, and Patima\\nSilsupadol. 2020. Development and Usability Testing of an Emergency Alert\\nDevice for Elderly People and People with Disabilities. *e Scientific World Journal*\\n2020 (2020).\\n\\n[18] Muhammad Mubashir, Ling Shao, and Luke Seed. 2013. A survey on fall detection:\\nPrinciples and approaches. *Neurocomputing* 100 (2013).\\n\\n[19] Md. Jaber Al Nahian, Tapotosh Ghosh, Md. Hasan Al Banna, Mohammed A.\\nAseeri, Mohammed Nasir Uddin, Muhammad Raisuddin Ahmed, Mufti Mahmud,\\nand M. Shamim Kaiser. 2021. Towards an Accelerometer-Based Elderly Fall\\nDetection System Using Cross-Disciplinary Time Series Features. *IEEE Access* 9\\n(2021), 39413–39431. https://doi.org/10.1109/ACCESS.2021.3056441\\n\\n[20] Camila Pereira de Oliveira, Cristiano da Silveira Colombo, and Daniel José Ventorim Nunes. 2024. Machine Learning Applied To Fall Detection in the Elderly. In\\n*Proceedings of the 20th Brazilian Symposium on Information Systems* (<conf-loc>,\\n<city>Juiz de Fora</city>, <country>Brazil</country>, </conf-loc>) *(SBSI ’24)* .\\nAssociation for Computing Machinery, New York, NY, USA, Article 58, 9 pages.\\nhttps://doi.org/10.1145/3658271.3658330\\n\\n[21] Guilherme Bruno Araújo Pimenta. 2019. *Uso da Ferramenta Node-RED em Pro-*\\n*cessos de Automatização no Cenário da Quarta Revolução Industrial* . Monografia.\\nUniversidade Federal de Lavras, Lavras - MG.\\n\\n[22] Mahsa T. Pourazad, Anahita Shojaei-Hashemi, Panos Nasiopoulos, Maryam Azimi, Michelle Mak, Jennifer Grace, Doojin Jung, and Taran Bains. 2020. A NonIntrusive Deep Learning Based Fall Detection Scheme Using Video Cameras.\\nIn *2020 International Conference on Information Networking (ICOIN)* . 443–446.\\n\\n\\nhttps://doi.org/10.1109/ICOIN48656.2020.9016455\\n\\n[23] Luis GS Rodrigues, Diego RC Dias, Marcelo P Guimarães, Alexandre F Brandão,\\nLeonardo CD Rocha, Rogério L Iope, and José RF Brega. 2021. Upper limb motion\\ntracking and classification: A smartphone approach. In *Proceedings of the Brazilian*\\n*Symposium on Multimedia and the Web* . 61–64.\\n\\n[24] Thanos G. Stavropoulos, Asterios Papastergiou, Lampros Mpaltadoros, Spiros\\nNikolopoulos, and Ioannis Kompatsiaris. 2020. IoT Wearable Sensors and Devices\\nin Elderly Care: A Literature Review. *Sensors* 20 (2020).\\n\\n[25] Deen MJ. Subramaniam S, Faisal AI. 2022. Wearable Sensor Systems for Fall Risk\\nAssessment: A Review. Front Digit Health.\\n\\n[26] Guilherme Gerzson Torres. 2018. *Tecnologia Assistiva para Detecção de Quedas:*\\n*Desenvolvimento de Sensor Vestível Integrado ao Sistema de Casa Inteligente* . Dissertação de mestrado. Universidade Federal do Rio Grande do Sul, Porto Alegre RS. Disponível em: http://hdl.handle.net/10183/180824.\\n\\n[27] Xueyi Wang, Joshua Ellul, and George Azzopardi. 2020. Elderly Fall Detection\\nSystems: A Literature Survey. *Frontiers in Robotics and AI* 7 (2020). https:\\n//doi.org/10.3389/frobt.2020.00071\\n\\n[28] Jing Zhang, Jia Li, and Weibing Wang. 2021. A Class-Imbalanced Deep Learning\\nFall Detection Algorithm Using Wearable Sensors. *Sensors* 21, 19 (2021). https:\\n//doi.org/10.3390/s21196511\\n\\n\\n132\\n\\n\\n-----\\n\\n',\n",
       "  'artigo_tokenizado': ['#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Elderly',\n",
       "   'Fall',\n",
       "   'Monitoring',\n",
       "   'in',\n",
       "   'Smart',\n",
       "   'Homes',\n",
       "   'Using',\n",
       "   'Wearable',\n",
       "   'Device',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Júlia',\n",
       "   'M.',\n",
       "   'P.',\n",
       "   'Moreira',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'juliampmoreira7@gmail.com',\n",
       "   'Department',\n",
       "   'of',\n",
       "   'Applyed',\n",
       "   'Computing',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Lavras',\n",
       "   '-',\n",
       "   'Lavras',\n",
       "   '-',\n",
       "   'Brazil',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Raphael',\n",
       "   'W.',\n",
       "   'Bettio',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'raphaelwb@ufla.br',\n",
       "   'Department',\n",
       "   'of',\n",
       "   'Applyed',\n",
       "   'Computing',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Lavras',\n",
       "   '-',\n",
       "   'Lavras',\n",
       "   '-',\n",
       "   'Brazil',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'André',\n",
       "   'P.',\n",
       "   'Freire',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'apfreire@ufla.br',\n",
       "   'Department',\n",
       "   'of',\n",
       "   'Computer',\n",
       "   'Science',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Lavras',\n",
       "   '-',\n",
       "   'Lavras',\n",
       "   '-',\n",
       "   'Brazil',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Luciano',\n",
       "   'M.',\n",
       "   'Santos',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'mendesluciano@ufla.br',\n",
       "   'Department',\n",
       "   'of',\n",
       "   'Computer',\n",
       "   'Science',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Lavras',\n",
       "   '-',\n",
       "   'Lavras',\n",
       "   '-',\n",
       "   'Brazil',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'ABSTRACT',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'The',\n",
       "   'constant',\n",
       "   'progress',\n",
       "   'of',\n",
       "   'technology',\n",
       "   ',',\n",
       "   'especially',\n",
       "   'in',\n",
       "   'the',\n",
       "   'area',\n",
       "   'of',\n",
       "   'health',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'brings',\n",
       "   'numerous',\n",
       "   'benefits',\n",
       "   ',',\n",
       "   'one',\n",
       "   'of',\n",
       "   'which',\n",
       "   'is',\n",
       "   'the',\n",
       "   'increase',\n",
       "   'in',\n",
       "   'human',\n",
       "   '\\n',\n",
       "   'life',\n",
       "   'expectancy',\n",
       "   '.',\n",
       "   'However',\n",
       "   ',',\n",
       "   'problems',\n",
       "   'that',\n",
       "   'occur',\n",
       "   'recurrently',\n",
       "   'among',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'elderly',\n",
       "   'age',\n",
       "   'group',\n",
       "   'are',\n",
       "   'now',\n",
       "   'on',\n",
       "   'the',\n",
       "   'radar',\n",
       "   'of',\n",
       "   'studies',\n",
       "   'that',\n",
       "   'also',\n",
       "   'seek',\n",
       "   '\\n',\n",
       "   'to',\n",
       "   'improve',\n",
       "   'the',\n",
       "   'quality',\n",
       "   'of',\n",
       "   'life',\n",
       "   'of',\n",
       "   'these',\n",
       "   'people',\n",
       "   '.',\n",
       "   'The',\n",
       "   'number',\n",
       "   'of',\n",
       "   'cases',\n",
       "   '\\n',\n",
       "   'of',\n",
       "   'falls',\n",
       "   'among',\n",
       "   'elderly',\n",
       "   'people',\n",
       "   'is',\n",
       "   'worrying',\n",
       "   ',',\n",
       "   'even',\n",
       "   'more',\n",
       "   'so',\n",
       "   'as',\n",
       "   'this',\n",
       "   'is',\n",
       "   'a',\n",
       "   '\\n',\n",
       "   'portion',\n",
       "   'of',\n",
       "   'the',\n",
       "   'population',\n",
       "   'that',\n",
       "   'tends',\n",
       "   'to',\n",
       "   'live',\n",
       "   'alone',\n",
       "   '.',\n",
       "   'In',\n",
       "   'the',\n",
       "   'context',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'smart',\n",
       "   'homes',\n",
       "   ',',\n",
       "   'several',\n",
       "   'solutions',\n",
       "   'have',\n",
       "   'emerged',\n",
       "   'for',\n",
       "   'monitoring',\n",
       "   'elderly',\n",
       "   '\\n',\n",
       "   'people',\n",
       "   'to',\n",
       "   'increase',\n",
       "   'safety',\n",
       "   'and',\n",
       "   'provide',\n",
       "   'faster',\n",
       "   'assistance',\n",
       "   ',',\n",
       "   'if',\n",
       "   'necessary',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'One',\n",
       "   'of',\n",
       "   'these',\n",
       "   'solutions',\n",
       "   'is',\n",
       "   'the',\n",
       "   'use',\n",
       "   'of',\n",
       "   'wearable',\n",
       "   'devices',\n",
       "   'responsible',\n",
       "   'for',\n",
       "   '\\n',\n",
       "   'identifying',\n",
       "   'the',\n",
       "   'person',\n",
       "   '’s',\n",
       "   'movements',\n",
       "   '.',\n",
       "   'This',\n",
       "   'work',\n",
       "   'presents',\n",
       "   'the',\n",
       "   'study',\n",
       "   '\\n',\n",
       "   'and',\n",
       "   'development',\n",
       "   'of',\n",
       "   'a',\n",
       "   'wearable',\n",
       "   'device',\n",
       "   'capable',\n",
       "   'of',\n",
       "   'detecting',\n",
       "   'falls',\n",
       "   'and',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'if',\n",
       "   'they',\n",
       "   'occur',\n",
       "   ',',\n",
       "   'automatically',\n",
       "   'notifying',\n",
       "   'the',\n",
       "   'necessary',\n",
       "   'people',\n",
       "   'through',\n",
       "   '\\n',\n",
       "   'alert',\n",
       "   'messages',\n",
       "   'via',\n",
       "   'the',\n",
       "   'Telegram',\n",
       "   'application',\n",
       "   'so',\n",
       "   'that',\n",
       "   'they',\n",
       "   'can',\n",
       "   'help',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'person',\n",
       "   'who',\n",
       "   'has',\n",
       "   'suffered',\n",
       "   'a',\n",
       "   'fall',\n",
       "   '.',\n",
       "   'In',\n",
       "   'this',\n",
       "   'work',\n",
       "   ',',\n",
       "   'a',\n",
       "   'Wi',\n",
       "   '-',\n",
       "   'Fi',\n",
       "   'network',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'MQTT',\n",
       "   'protocol',\n",
       "   ',',\n",
       "   'accelerometer',\n",
       "   'and',\n",
       "   'gyroscope',\n",
       "   'inertial',\n",
       "   'sensors',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'an',\n",
       "   'ESP32',\n",
       "   'board',\n",
       "   'programmed',\n",
       "   'using',\n",
       "   'the',\n",
       "   'Arduino',\n",
       "   'IDE',\n",
       "   'were',\n",
       "   'used',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Preliminary',\n",
       "   'tests',\n",
       "   'indicated',\n",
       "   'good',\n",
       "   'performance',\n",
       "   'in',\n",
       "   'recognizing',\n",
       "   'falls',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'based',\n",
       "   'on',\n",
       "   'tilt',\n",
       "   'angle',\n",
       "   'analysis',\n",
       "   ',',\n",
       "   'gyroscope',\n",
       "   'readings',\n",
       "   'and',\n",
       "   'accelerometer',\n",
       "   '\\n',\n",
       "   'readings',\n",
       "   '.',\n",
       "   'The',\n",
       "   'proof',\n",
       "   'of',\n",
       "   'concept',\n",
       "   'and',\n",
       "   'preliminary',\n",
       "   'tests',\n",
       "   'carried',\n",
       "   'out',\n",
       "   '\\n',\n",
       "   'demonstrate',\n",
       "   'the',\n",
       "   'potential',\n",
       "   'for',\n",
       "   'using',\n",
       "   'low',\n",
       "   '-',\n",
       "   'cost',\n",
       "   'technologies',\n",
       "   'for',\n",
       "   'wearable',\n",
       "   'resources',\n",
       "   'for',\n",
       "   'application',\n",
       "   'in',\n",
       "   'smart',\n",
       "   'homes',\n",
       "   'and',\n",
       "   'monitoring',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'health',\n",
       "   'of',\n",
       "   'elderly',\n",
       "   'people',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'KEYWORDS',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'fall',\n",
       "   'detection',\n",
       "   ',',\n",
       "   'wearable',\n",
       "   'technologies',\n",
       "   ',',\n",
       "   'accelerometer',\n",
       "   'and',\n",
       "   'gyroscope',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'alert',\n",
       "   'messages',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   '1',\n",
       "   'INTRODUCTION',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'With',\n",
       "   'the',\n",
       "   'encouragement',\n",
       "   'of',\n",
       "   'innovation',\n",
       "   'and',\n",
       "   'technology',\n",
       "   'as',\n",
       "   'an',\n",
       "   'important',\n",
       "   'pillar',\n",
       "   'to',\n",
       "   'improve',\n",
       "   'people',\n",
       "   '’s',\n",
       "   'quality',\n",
       "   'of',\n",
       "   'life',\n",
       "   ',',\n",
       "   'there',\n",
       "   'is',\n",
       "   'also',\n",
       "   'an',\n",
       "   '\\n',\n",
       "   'increase',\n",
       "   'in',\n",
       "   'life',\n",
       "   'expectancy',\n",
       "   'and',\n",
       "   'the',\n",
       "   'aging',\n",
       "   'rate',\n",
       "   'of',\n",
       "   'the',\n",
       "   'world',\n",
       "   '’s',\n",
       "   'population',\n",
       "   '.',\n",
       "   'In',\n",
       "   'Brazil',\n",
       "   ',',\n",
       "   'according',\n",
       "   'to',\n",
       "   'the',\n",
       "   '2022',\n",
       "   'IBGE',\n",
       "   'census',\n",
       "   ',',\n",
       "   'the',\n",
       "   'population',\n",
       "   '\\n',\n",
       "   'over',\n",
       "   '60',\n",
       "   'years',\n",
       "   'of',\n",
       "   'age',\n",
       "   'reached',\n",
       "   '32,113,490',\n",
       "   '(',\n",
       "   '15.6',\n",
       "   '%',\n",
       "   ')',\n",
       "   '[',\n",
       "   '9',\n",
       "   ']',\n",
       "   '.',\n",
       "   'The',\n",
       "   'best',\n",
       "   'thermometer',\n",
       "   'to',\n",
       "   'measure',\n",
       "   'an',\n",
       "   'elderly',\n",
       "   'person',\n",
       "   '’s',\n",
       "   'health',\n",
       "   'is',\n",
       "   'checking',\n",
       "   'their',\n",
       "   '\\n',\n",
       "   'functional',\n",
       "   'capacity',\n",
       "   ',',\n",
       "   'which',\n",
       "   'is',\n",
       "   'a',\n",
       "   'person',\n",
       "   '’s',\n",
       "   'real',\n",
       "   'ability',\n",
       "   'to',\n",
       "   'carry',\n",
       "   'out',\n",
       "   'the',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   ':',\n",
       "   'Proceedings',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Web',\n",
       "   '(',\n",
       "   'WebMe',\n",
       "   '\\n',\n",
       "   'dia’2024',\n",
       "   ')',\n",
       "   '.',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '.',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   ':',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   ',',\n",
       "   '2024',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'SBC',\n",
       "   '–',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '-',\n",
       "   '2753',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Marluce',\n",
       "   'R.',\n",
       "   'Pereira',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'marluce@ufla.br',\n",
       "   'Department',\n",
       "   'of',\n",
       "   'Applyed',\n",
       "   'Computing',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Lavras',\n",
       "   '-',\n",
       "   'Lavras',\n",
       "   '-',\n",
       "   'Brazil',\n",
       "   '\\n\\n',\n",
       "   'daily',\n",
       "   'activities',\n",
       "   'possible',\n",
       "   'to',\n",
       "   'live',\n",
       "   'independently',\n",
       "   'and',\n",
       "   'self',\n",
       "   '-',\n",
       "   'sufficiently',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'which',\n",
       "   'tends',\n",
       "   'to',\n",
       "   'reduce',\n",
       "   'with',\n",
       "   'aging',\n",
       "   '.',\n",
       "   'There',\n",
       "   'are',\n",
       "   'scales',\n",
       "   'that',\n",
       "   'measure',\n",
       "   '\\n',\n",
       "   'this',\n",
       "   'functional',\n",
       "   'capacity',\n",
       "   ',',\n",
       "   'such',\n",
       "   'as',\n",
       "   'the',\n",
       "   'Barthel',\n",
       "   'scale',\n",
       "   ',',\n",
       "   'the',\n",
       "   'Katz',\n",
       "   'scale',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'Lawton',\n",
       "   'and',\n",
       "   'Brody',\n",
       "   'scale',\n",
       "   ',',\n",
       "   'the',\n",
       "   'Pfeffer',\n",
       "   'scale',\n",
       "   ',',\n",
       "   'among',\n",
       "   'others',\n",
       "   '[',\n",
       "   '12',\n",
       "   ']',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'which',\n",
       "   'assess',\n",
       "   'the',\n",
       "   'independence',\n",
       "   'of',\n",
       "   'the',\n",
       "   'elderly',\n",
       "   'in',\n",
       "   'basic',\n",
       "   'activities',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'daily',\n",
       "   'living',\n",
       "   ':',\n",
       "   'eating',\n",
       "   ',',\n",
       "   'personal',\n",
       "   'hygiene',\n",
       "   ',',\n",
       "   'getting',\n",
       "   'dressed',\n",
       "   ',',\n",
       "   'using',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'toilet',\n",
       "   ',',\n",
       "   'moving',\n",
       "   'from',\n",
       "   'bed',\n",
       "   'to',\n",
       "   'chair',\n",
       "   'and',\n",
       "   'vice',\n",
       "   'versa',\n",
       "   ',',\n",
       "   'cooking',\n",
       "   ',',\n",
       "   'managing',\n",
       "   '\\n',\n",
       "   'medications',\n",
       "   ',',\n",
       "   'shopping',\n",
       "   ',',\n",
       "   'washing',\n",
       "   'clothes',\n",
       "   ',',\n",
       "   'cleaning',\n",
       "   'the',\n",
       "   'house',\n",
       "   ',',\n",
       "   'using',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'telephone',\n",
       "   ',',\n",
       "   'remembering',\n",
       "   'appointments',\n",
       "   ',',\n",
       "   'among',\n",
       "   'others',\n",
       "   '.',\n",
       "   'Risks',\n",
       "   '\\n',\n",
       "   'such',\n",
       "   'as',\n",
       "   'falling',\n",
       "   'further',\n",
       "   'increase',\n",
       "   'the',\n",
       "   'concern',\n",
       "   'of',\n",
       "   'family',\n",
       "   'members',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'people',\n",
       "   'close',\n",
       "   'to',\n",
       "   'elderly',\n",
       "   'people',\n",
       "   ',',\n",
       "   'as',\n",
       "   'they',\n",
       "   'reduce',\n",
       "   'functional',\n",
       "   'capacity',\n",
       "   '\\n\\n',\n",
       "   '[',\n",
       "   '28',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'A',\n",
       "   'smart',\n",
       "   'home',\n",
       "   'has',\n",
       "   'devices',\n",
       "   'connected',\n",
       "   'to',\n",
       "   'the',\n",
       "   'Internet',\n",
       "   'that',\n",
       "   'allow',\n",
       "   '\\n\\n',\n",
       "   'remote',\n",
       "   'monitoring',\n",
       "   'and',\n",
       "   'control',\n",
       "   'of',\n",
       "   'appliances',\n",
       "   'and',\n",
       "   'systems',\n",
       "   'such',\n",
       "   'as',\n",
       "   '\\n',\n",
       "   'lighting',\n",
       "   'and',\n",
       "   'heating',\n",
       "   ',',\n",
       "   'as',\n",
       "   'well',\n",
       "   'as',\n",
       "   'other',\n",
       "   'types',\n",
       "   'of',\n",
       "   'sensors',\n",
       "   ',',\n",
       "   'using',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'internet',\n",
       "   'of',\n",
       "   'things',\n",
       "   '(',\n",
       "   'IoT',\n",
       "   ')',\n",
       "   '[',\n",
       "   '13',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Furthermore',\n",
       "   ',',\n",
       "   'there',\n",
       "   'may',\n",
       "   'be',\n",
       "   'wearable',\n",
       "   '\\n',\n",
       "   'sensors',\n",
       "   'that',\n",
       "   'allow',\n",
       "   'the',\n",
       "   'implementation',\n",
       "   'of',\n",
       "   '*',\n",
       "   'Human',\n",
       "   'Activity',\n",
       "   'Recogni-',\n",
       "   '*',\n",
       "   '\\n',\n",
       "   '*',\n",
       "   'tion',\n",
       "   '(',\n",
       "   'HAR',\n",
       "   ')',\n",
       "   '*',\n",
       "   'systems',\n",
       "   'in',\n",
       "   'smart',\n",
       "   'homes',\n",
       "   ',',\n",
       "   'helping',\n",
       "   'to',\n",
       "   'improve',\n",
       "   'people',\n",
       "   '’s',\n",
       "   '\\n',\n",
       "   'quality',\n",
       "   'of',\n",
       "   'life',\n",
       "   'with',\n",
       "   'autonomy',\n",
       "   'and',\n",
       "   'safety',\n",
       "   '[',\n",
       "   '7',\n",
       "   ']',\n",
       "   ',',\n",
       "   '[',\n",
       "   '23',\n",
       "   ']',\n",
       "   '.',\n",
       "   'In',\n",
       "   'this',\n",
       "   'context',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'a',\n",
       "   'solution',\n",
       "   'to',\n",
       "   'alleviate',\n",
       "   'the',\n",
       "   'consequences',\n",
       "   'of',\n",
       "   'these',\n",
       "   'difficulties',\n",
       "   'in',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'lives',\n",
       "   'of',\n",
       "   'elderly',\n",
       "   'people',\n",
       "   ',',\n",
       "   'while',\n",
       "   'rescuing',\n",
       "   'independence',\n",
       "   'in',\n",
       "   'daily',\n",
       "   'activities',\n",
       "   'and',\n",
       "   'reassuring',\n",
       "   'people',\n",
       "   'close',\n",
       "   'to',\n",
       "   'them',\n",
       "   ',',\n",
       "   'consists',\n",
       "   'of',\n",
       "   'installing',\n",
       "   '\\n',\n",
       "   'monitoring',\n",
       "   'systems',\n",
       "   'in',\n",
       "   'the',\n",
       "   'home',\n",
       "   ',',\n",
       "   'in',\n",
       "   'order',\n",
       "   'to',\n",
       "   'guarantee',\n",
       "   'assistance',\n",
       "   '\\n',\n",
       "   'if',\n",
       "   'an',\n",
       "   'incident',\n",
       "   'occurs',\n",
       "   'such',\n",
       "   'as',\n",
       "   ':',\n",
       "   'a',\n",
       "   'fire',\n",
       "   ',',\n",
       "   'fainting',\n",
       "   ',',\n",
       "   'fall',\n",
       "   'or',\n",
       "   'even',\n",
       "   'illness',\n",
       "   '\\n',\n",
       "   'caused',\n",
       "   'by',\n",
       "   'changes',\n",
       "   'in',\n",
       "   'vital',\n",
       "   'signs',\n",
       "   '.',\n",
       "   'There',\n",
       "   'are',\n",
       "   'different',\n",
       "   'types',\n",
       "   'of',\n",
       "   'emergency',\n",
       "   'alert',\n",
       "   'devices',\n",
       "   'that',\n",
       "   'have',\n",
       "   'been',\n",
       "   'developed',\n",
       "   'for',\n",
       "   'dependent',\n",
       "   'people',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'including',\n",
       "   'children',\n",
       "   ',',\n",
       "   'people',\n",
       "   'with',\n",
       "   'disabilities',\n",
       "   ',',\n",
       "   'the',\n",
       "   'sick',\n",
       "   'and',\n",
       "   'the',\n",
       "   'elderly',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Most',\n",
       "   'of',\n",
       "   'them',\n",
       "   'send',\n",
       "   'an',\n",
       "   'alarm',\n",
       "   'only',\n",
       "   'when',\n",
       "   'help',\n",
       "   'is',\n",
       "   'needed',\n",
       "   ',',\n",
       "   'but',\n",
       "   'some',\n",
       "   '\\n',\n",
       "   'are',\n",
       "   'developed',\n",
       "   'to',\n",
       "   'monitor',\n",
       "   'the',\n",
       "   'health',\n",
       "   'of',\n",
       "   'sick',\n",
       "   'patients',\n",
       "   ',',\n",
       "   'such',\n",
       "   'as',\n",
       "   'blood',\n",
       "   '\\n',\n",
       "   'pressure',\n",
       "   ',',\n",
       "   'heart',\n",
       "   'rate',\n",
       "   ',',\n",
       "   'among',\n",
       "   'others',\n",
       "   '[',\n",
       "   '17',\n",
       "   ...],\n",
       "  'pos_tagger': '',\n",
       "  'lema': ['elderly',\n",
       "   'fall',\n",
       "   'monitoring',\n",
       "   'in',\n",
       "   'Smart',\n",
       "   'Homes',\n",
       "   'use',\n",
       "   'wearable',\n",
       "   'device',\n",
       "   'Júlia',\n",
       "   'M.',\n",
       "   'P.',\n",
       "   'Moreira',\n",
       "   'juliampmoreira7@gmail.com',\n",
       "   'Department',\n",
       "   'of',\n",
       "   'Applyed',\n",
       "   'Computing',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Lavras',\n",
       "   'Lavras',\n",
       "   'Brazil',\n",
       "   'Raphael',\n",
       "   'W.',\n",
       "   'Bettio',\n",
       "   'raphaelwb@ufla.br',\n",
       "   'Department',\n",
       "   'of',\n",
       "   'Applyed',\n",
       "   'Computing',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Lavras',\n",
       "   'Lavras',\n",
       "   'Brazil',\n",
       "   'André',\n",
       "   'P.',\n",
       "   'Freire',\n",
       "   'apfreire@ufla.br',\n",
       "   'Department',\n",
       "   'of',\n",
       "   'Computer',\n",
       "   'Science',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Lavras',\n",
       "   'Lavras',\n",
       "   'Brazil',\n",
       "   'Luciano',\n",
       "   'M.',\n",
       "   'Santos',\n",
       "   'mendesluciano@ufla.br',\n",
       "   'Department',\n",
       "   'of',\n",
       "   'Computer',\n",
       "   'Science',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Lavras',\n",
       "   'Lavras',\n",
       "   'Brazil',\n",
       "   'ABSTRACT',\n",
       "   'the',\n",
       "   'constant',\n",
       "   'progress',\n",
       "   'of',\n",
       "   'technology',\n",
       "   'especially',\n",
       "   'in',\n",
       "   'the',\n",
       "   'area',\n",
       "   'of',\n",
       "   'health',\n",
       "   'bring',\n",
       "   'numerous',\n",
       "   'benefit',\n",
       "   'one',\n",
       "   'of',\n",
       "   'which',\n",
       "   'be',\n",
       "   'the',\n",
       "   'increase',\n",
       "   'in',\n",
       "   'human',\n",
       "   'life',\n",
       "   'expectancy',\n",
       "   'however',\n",
       "   'problem',\n",
       "   'that',\n",
       "   'occur',\n",
       "   'recurrently',\n",
       "   'among',\n",
       "   'the',\n",
       "   'elderly',\n",
       "   'age',\n",
       "   'group',\n",
       "   'be',\n",
       "   'now',\n",
       "   'on',\n",
       "   'the',\n",
       "   'radar',\n",
       "   'of',\n",
       "   'study',\n",
       "   'that',\n",
       "   'also',\n",
       "   'seek',\n",
       "   'to',\n",
       "   'improve',\n",
       "   'the',\n",
       "   'quality',\n",
       "   'of',\n",
       "   'life',\n",
       "   'of',\n",
       "   'these',\n",
       "   'people',\n",
       "   'the',\n",
       "   'number',\n",
       "   'of',\n",
       "   'case',\n",
       "   'of',\n",
       "   'fall',\n",
       "   'among',\n",
       "   'elderly',\n",
       "   'people',\n",
       "   'be',\n",
       "   'worry',\n",
       "   'even',\n",
       "   'more',\n",
       "   'so',\n",
       "   'as',\n",
       "   'this',\n",
       "   'be',\n",
       "   'a',\n",
       "   'portion',\n",
       "   'of',\n",
       "   'the',\n",
       "   'population',\n",
       "   'that',\n",
       "   'tend',\n",
       "   'to',\n",
       "   'live',\n",
       "   'alone',\n",
       "   'in',\n",
       "   'the',\n",
       "   'context',\n",
       "   'of',\n",
       "   'smart',\n",
       "   'home',\n",
       "   'several',\n",
       "   'solution',\n",
       "   'have',\n",
       "   'emerge',\n",
       "   'for',\n",
       "   'monitor',\n",
       "   'elderly',\n",
       "   'people',\n",
       "   'to',\n",
       "   'increase',\n",
       "   'safety',\n",
       "   'and',\n",
       "   'provide',\n",
       "   'fast',\n",
       "   'assistance',\n",
       "   'if',\n",
       "   'necessary',\n",
       "   'one',\n",
       "   'of',\n",
       "   'these',\n",
       "   'solution',\n",
       "   'be',\n",
       "   'the',\n",
       "   'use',\n",
       "   'of',\n",
       "   'wearable',\n",
       "   'device',\n",
       "   'responsible',\n",
       "   'for',\n",
       "   'identify',\n",
       "   'the',\n",
       "   'person',\n",
       "   '’s',\n",
       "   'movement',\n",
       "   'this',\n",
       "   'work',\n",
       "   'present',\n",
       "   'the',\n",
       "   'study',\n",
       "   'and',\n",
       "   'development',\n",
       "   'of',\n",
       "   'a',\n",
       "   'wearable',\n",
       "   'device',\n",
       "   'capable',\n",
       "   'of',\n",
       "   'detect',\n",
       "   'fall',\n",
       "   'and',\n",
       "   'if',\n",
       "   'they',\n",
       "   'occur',\n",
       "   'automatically',\n",
       "   'notify',\n",
       "   'the',\n",
       "   'necessary',\n",
       "   'people',\n",
       "   'through',\n",
       "   'alert',\n",
       "   'message',\n",
       "   'via',\n",
       "   'the',\n",
       "   'Telegram',\n",
       "   'application',\n",
       "   'so',\n",
       "   'that',\n",
       "   'they',\n",
       "   'can',\n",
       "   'help',\n",
       "   'the',\n",
       "   'person',\n",
       "   'who',\n",
       "   'have',\n",
       "   'suffer',\n",
       "   'a',\n",
       "   'fall',\n",
       "   'in',\n",
       "   'this',\n",
       "   'work',\n",
       "   'a',\n",
       "   'Wi',\n",
       "   'Fi',\n",
       "   'network',\n",
       "   'MQTT',\n",
       "   'protocol',\n",
       "   'accelerometer',\n",
       "   'and',\n",
       "   'gyroscope',\n",
       "   'inertial',\n",
       "   'sensor',\n",
       "   'and',\n",
       "   'an',\n",
       "   'ESP32',\n",
       "   'board',\n",
       "   'program',\n",
       "   'use',\n",
       "   'the',\n",
       "   'Arduino',\n",
       "   'IDE',\n",
       "   'be',\n",
       "   'use',\n",
       "   'preliminary',\n",
       "   'test',\n",
       "   'indicate',\n",
       "   'good',\n",
       "   'performance',\n",
       "   'in',\n",
       "   'recognize',\n",
       "   'fall',\n",
       "   'base',\n",
       "   'on',\n",
       "   'tilt',\n",
       "   'angle',\n",
       "   'analysis',\n",
       "   'gyroscope',\n",
       "   'reading',\n",
       "   'and',\n",
       "   'accelerometer',\n",
       "   'reading',\n",
       "   'the',\n",
       "   'proof',\n",
       "   'of',\n",
       "   'concept',\n",
       "   'and',\n",
       "   'preliminary',\n",
       "   'test',\n",
       "   'carry',\n",
       "   'out',\n",
       "   'demonstrate',\n",
       "   'the',\n",
       "   'potential',\n",
       "   'for',\n",
       "   'use',\n",
       "   'low',\n",
       "   'cost',\n",
       "   'technology',\n",
       "   'for',\n",
       "   'wearable',\n",
       "   'resource',\n",
       "   'for',\n",
       "   'application',\n",
       "   'in',\n",
       "   'smart',\n",
       "   'home',\n",
       "   'and',\n",
       "   'monitor',\n",
       "   'the',\n",
       "   'health',\n",
       "   'of',\n",
       "   'elderly',\n",
       "   'people',\n",
       "   'keyword',\n",
       "   'fall',\n",
       "   'detection',\n",
       "   'wearable',\n",
       "   'technology',\n",
       "   'accelerometer',\n",
       "   'and',\n",
       "   'gyroscope',\n",
       "   'alert',\n",
       "   'message',\n",
       "   '1',\n",
       "   'introduction',\n",
       "   'with',\n",
       "   'the',\n",
       "   'encouragement',\n",
       "   'of',\n",
       "   'innovation',\n",
       "   'and',\n",
       "   'technology',\n",
       "   'as',\n",
       "   'an',\n",
       "   'important',\n",
       "   'pillar',\n",
       "   'to',\n",
       "   'improve',\n",
       "   'people',\n",
       "   '’s',\n",
       "   'quality',\n",
       "   'of',\n",
       "   'life',\n",
       "   'there',\n",
       "   'be',\n",
       "   'also',\n",
       "   'an',\n",
       "   'increase',\n",
       "   'in',\n",
       "   'life',\n",
       "   'expectancy',\n",
       "   'and',\n",
       "   'the',\n",
       "   'age',\n",
       "   'rate',\n",
       "   'of',\n",
       "   'the',\n",
       "   'world',\n",
       "   '’s',\n",
       "   'population',\n",
       "   'in',\n",
       "   'Brazil',\n",
       "   'accord',\n",
       "   'to',\n",
       "   'the',\n",
       "   '2022',\n",
       "   'IBGE',\n",
       "   'census',\n",
       "   'the',\n",
       "   'population',\n",
       "   'over',\n",
       "   '60',\n",
       "   'year',\n",
       "   'of',\n",
       "   'age',\n",
       "   'reach',\n",
       "   '32,113,490',\n",
       "   '15.6',\n",
       "   '9',\n",
       "   'the',\n",
       "   'good',\n",
       "   'thermometer',\n",
       "   'to',\n",
       "   'measure',\n",
       "   'an',\n",
       "   'elderly',\n",
       "   'person',\n",
       "   '’s',\n",
       "   'health',\n",
       "   'be',\n",
       "   'check',\n",
       "   'their',\n",
       "   'functional',\n",
       "   'capacity',\n",
       "   'which',\n",
       "   'be',\n",
       "   'a',\n",
       "   'person',\n",
       "   '’s',\n",
       "   'real',\n",
       "   'ability',\n",
       "   'to',\n",
       "   'carry',\n",
       "   'out',\n",
       "   'the',\n",
       "   'in',\n",
       "   'proceeding',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'web',\n",
       "   'WebMe',\n",
       "   'dia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   '2024',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'sbc',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '2753',\n",
       "   'Marluce',\n",
       "   'R.',\n",
       "   'Pereira',\n",
       "   'marluce@ufla.br',\n",
       "   'Department',\n",
       "   'of',\n",
       "   'Applyed',\n",
       "   'Computing',\n",
       "   'Federal',\n",
       "   'University',\n",
       "   'of',\n",
       "   'Lavras',\n",
       "   'Lavras',\n",
       "   'Brazil',\n",
       "   'daily',\n",
       "   'activity',\n",
       "   'possible',\n",
       "   'to',\n",
       "   'live',\n",
       "   'independently',\n",
       "   'and',\n",
       "   'self',\n",
       "   'sufficiently',\n",
       "   'which',\n",
       "   'tend',\n",
       "   'to',\n",
       "   'reduce',\n",
       "   'with',\n",
       "   'aging',\n",
       "   'there',\n",
       "   'be',\n",
       "   'scale',\n",
       "   'that',\n",
       "   'measure',\n",
       "   'this',\n",
       "   'functional',\n",
       "   'capacity',\n",
       "   'such',\n",
       "   'as',\n",
       "   'the',\n",
       "   'Barthel',\n",
       "   'scale',\n",
       "   'the',\n",
       "   'Katz',\n",
       "   'scale',\n",
       "   'the',\n",
       "   'Lawton',\n",
       "   'and',\n",
       "   'Brody',\n",
       "   'scale',\n",
       "   'the',\n",
       "   'Pfeffer',\n",
       "   'scale',\n",
       "   'among',\n",
       "   'other',\n",
       "   '12',\n",
       "   'which',\n",
       "   'assess',\n",
       "   'the',\n",
       "   'independence',\n",
       "   'of',\n",
       "   'the',\n",
       "   'elderly',\n",
       "   'in',\n",
       "   'basic',\n",
       "   'activity',\n",
       "   'of',\n",
       "   'daily',\n",
       "   'living',\n",
       "   'eating',\n",
       "   'personal',\n",
       "   'hygiene',\n",
       "   'get',\n",
       "   'dress',\n",
       "   'use',\n",
       "   'the',\n",
       "   'toilet',\n",
       "   'move',\n",
       "   'from',\n",
       "   'bed',\n",
       "   'to',\n",
       "   'chair',\n",
       "   'and',\n",
       "   'vice',\n",
       "   'versa',\n",
       "   'cooking',\n",
       "   'manage',\n",
       "   'medication',\n",
       "   'shopping',\n",
       "   'washing',\n",
       "   'clothe',\n",
       "   'clean',\n",
       "   'the',\n",
       "   'house',\n",
       "   'use',\n",
       "   'the',\n",
       "   'telephone',\n",
       "   'remember',\n",
       "   'appointment',\n",
       "   'among',\n",
       "   'other',\n",
       "   'risk',\n",
       "   'such',\n",
       "   'as',\n",
       "   'fall',\n",
       "   'far',\n",
       "   'increase',\n",
       "   'the',\n",
       "   'concern',\n",
       "   'of',\n",
       "   'family',\n",
       "   'member',\n",
       "   'and',\n",
       "   'people',\n",
       "   'close',\n",
       "   'to',\n",
       "   'elderly',\n",
       "   'people',\n",
       "   'as',\n",
       "   'they',\n",
       "   'reduce',\n",
       "   'functional',\n",
       "   'capacity',\n",
       "   '28',\n",
       "   'a',\n",
       "   'smart',\n",
       "   'home',\n",
       "   'have',\n",
       "   'device',\n",
       "   'connect',\n",
       "   'to',\n",
       "   'the',\n",
       "   'internet',\n",
       "   'that',\n",
       "   'allow',\n",
       "   'remote',\n",
       "   'monitoring',\n",
       "   'and',\n",
       "   'control',\n",
       "   'of',\n",
       "   'appliance',\n",
       "   'and',\n",
       "   'system',\n",
       "   'such',\n",
       "   'as',\n",
       "   'lighting',\n",
       "   'and',\n",
       "   'heating',\n",
       "   'as',\n",
       "   'well',\n",
       "   'as',\n",
       "   'other',\n",
       "   'type',\n",
       "   'of',\n",
       "   'sensor',\n",
       "   'use',\n",
       "   'the',\n",
       "   'internet',\n",
       "   'of',\n",
       "   'thing',\n",
       "   'IoT',\n",
       "   '13',\n",
       "   'furthermore',\n",
       "   'there',\n",
       "   'may',\n",
       "   'be',\n",
       "   'wearable',\n",
       "   'sensor',\n",
       "   'that',\n",
       "   'allow',\n",
       "   'the',\n",
       "   'implementation',\n",
       "   'of',\n",
       "   'Human',\n",
       "   'Activity',\n",
       "   'Recogni-',\n",
       "   'tion',\n",
       "   'HAR',\n",
       "   'system',\n",
       "   'in',\n",
       "   'smart',\n",
       "   'home',\n",
       "   'help',\n",
       "   'to',\n",
       "   'improve',\n",
       "   'people',\n",
       "   '’s',\n",
       "   'quality',\n",
       "   'of',\n",
       "   'life',\n",
       "   'with',\n",
       "   'autonomy',\n",
       "   'and',\n",
       "   'safety',\n",
       "   '7',\n",
       "   '23',\n",
       "   'in',\n",
       "   'this',\n",
       "   'context',\n",
       "   'a',\n",
       "   'solution',\n",
       "   'to',\n",
       "   'alleviate',\n",
       "   'the',\n",
       "   'consequence',\n",
       "   'of',\n",
       "   'these',\n",
       "   'difficulty',\n",
       "   'in',\n",
       "   'the',\n",
       "   'life',\n",
       "   'of',\n",
       "   'elderly',\n",
       "   'people',\n",
       "   'while',\n",
       "   'rescue',\n",
       "   'independence',\n",
       "   'in',\n",
       "   'daily',\n",
       "   'activity',\n",
       "   'and',\n",
       "   'reassure',\n",
       "   'people',\n",
       "   'close',\n",
       "   'to',\n",
       "   'they',\n",
       "   'consist',\n",
       "   'of',\n",
       "   'instal',\n",
       "   'monitoring',\n",
       "   'system',\n",
       "   'in',\n",
       "   'the',\n",
       "   'home',\n",
       "   'in',\n",
       "   'order',\n",
       "   'to',\n",
       "   'guarantee',\n",
       "   'assistance',\n",
       "   'if',\n",
       "   'an',\n",
       "   'incident',\n",
       "   'occur',\n",
       "   'such',\n",
       "   'as',\n",
       "   'a',\n",
       "   'fire',\n",
       "   'fainting',\n",
       "   'fall',\n",
       "   'or',\n",
       "   'even',\n",
       "   'illness',\n",
       "   'cause',\n",
       "   'by',\n",
       "   'change',\n",
       "   'in',\n",
       "   'vital',\n",
       "   'sign',\n",
       "   'there',\n",
       "   'be',\n",
       "   'different',\n",
       "   'type',\n",
       "   'of',\n",
       "   'emergency',\n",
       "   'alert',\n",
       "   'device',\n",
       "   'that',\n",
       "   'have',\n",
       "   'be',\n",
       "   'develop',\n",
       "   'for',\n",
       "   'dependent',\n",
       "   'people',\n",
       "   'include',\n",
       "   'child',\n",
       "   'people',\n",
       "   'with',\n",
       "   'disability',\n",
       "   'the',\n",
       "   'sick',\n",
       "   'and',\n",
       "   'the',\n",
       "   'elderly',\n",
       "   'Most',\n",
       "   'of',\n",
       "   'they',\n",
       "   'send',\n",
       "   'an',\n",
       "   'alarm',\n",
       "   'only',\n",
       "   'when',\n",
       "   'help',\n",
       "   'be',\n",
       "   'need',\n",
       "   'but',\n",
       "   'some',\n",
       "   'be',\n",
       "   'develop',\n",
       "   'to',\n",
       "   'monitor',\n",
       "   'the',\n",
       "   'health',\n",
       "   'of',\n",
       "   'sick',\n",
       "   'patient',\n",
       "   'such',\n",
       "   'as',\n",
       "   'blood',\n",
       "   'pressure',\n",
       "   'heart',\n",
       "   'rate',\n",
       "   'among',\n",
       "   'other',\n",
       "   '17',\n",
       "   'monitor',\n",
       "   'an',\n",
       "   'environment',\n",
       "   'mean',\n",
       "   'use',\n",
       "   'different',\n",
       "   'device',\n",
       "   'that',\n",
       "   'obtain',\n",
       "   'desire',\n",
       "   'datum',\n",
       "   'such',\n",
       "   'as',\n",
       "   'a',\n",
       "   'camera',\n",
       "   'motion',\n",
       "   'sensor',\n",
       "   'temperature',\n",
       "   'humidity',\n",
       "   'or',\n",
       "   'brightness',\n",
       "   'among',\n",
       "   'other',\n",
       "   'so',\n",
       "   'that',\n",
       "   'from',\n",
       "   'this',\n",
       "   'datum',\n",
       "   'it',\n",
       "   'be',\n",
       "   'possible',\n",
       "   'to',\n",
       "   'make',\n",
       "   'decision',\n",
       "   'in',\n",
       "   'order',\n",
       "   'to',\n",
       "   'achieve',\n",
       "   'the',\n",
       "   'objective',\n",
       "   'of',\n",
       "   'monitoring',\n",
       "   'the',\n",
       "   'problem',\n",
       "   'address',\n",
       "   'in',\n",
       "   'this',\n",
       "   'work',\n",
       "   'be',\n",
       "   'how',\n",
       "   'to',\n",
       "   'identify',\n",
       "   'that',\n",
       "   'an',\n",
       "   'elderly',\n",
       "   'person',\n",
       "   'who',\n",
       "   'be',\n",
       "   'alone',\n",
       "   'in',\n",
       "   'their',\n",
       "   'home',\n",
       "   'have',\n",
       "   'suffer',\n",
       "   'a',\n",
       "   'fall',\n",
       "   'and',\n",
       "   'in',\n",
       "   'addition',\n",
       "   'to',\n",
       "   'identify',\n",
       "   'use',\n",
       "   'this',\n",
       "   'information',\n",
       "   'to',\n",
       "   'alert',\n",
       "   'people',\n",
       "   'nearby',\n",
       "   'through',\n",
       "   'warning',\n",
       "   'message',\n",
       "   '11',\n",
       "   'therefore',\n",
       "   'an',\n",
       "   'effective',\n",
       "   'portable',\n",
       "   'solution',\n",
       "   'be',\n",
       "   'seek',\n",
       "   'with',\n",
       "   'integration',\n",
       "   'with',\n",
       "   'the',\n",
       "   'internet',\n",
       "   'network',\n",
       "   'to',\n",
       "   'maintain',\n",
       "   'connectivity',\n",
       "   'and',\n",
       "   'provide',\n",
       "   'warning',\n",
       "   'when',\n",
       "   'necessary',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'way',\n",
       "   'to',\n",
       "   'monitor',\n",
       "   'situation',\n",
       "   'that',\n",
       "   'happen',\n",
       "   'to',\n",
       "   'human',\n",
       "   'being',\n",
       "   'be',\n",
       "   'by',\n",
       "   'wear',\n",
       "   'sensor',\n",
       "   'and',\n",
       "   'device',\n",
       "   'capable',\n",
       "   'of',\n",
       "   'identify',\n",
       "   'variation',\n",
       "   'and',\n",
       "   '124',\n",
       "   'WebMedia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Moreira',\n",
       "   'et',\n",
       "   'al',\n",
       "   'communicate',\n",
       "   'important',\n",
       "   'information',\n",
       "   'in',\n",
       "   'such',\n",
       "   'a',\n",
       "   'way',\n",
       "   'that',\n",
       "   'reading',\n",
       "   'and',\n",
       "   'data',\n",
       "   'collection',\n",
       "   'be',\n",
       "   'not',\n",
       "   'harm',\n",
       "   'and',\n",
       "   'be',\n",
       "   'useful',\n",
       "   'for',\n",
       "   'the',\n",
       "   'necessary',\n",
       "   'analysis',\n",
       "   'the',\n",
       "   'main',\n",
       "   'objective',\n",
       "   'of',\n",
       "   'this',\n",
       "   'work',\n",
       "   'be',\n",
       "   'the',\n",
       "   'design',\n",
       "   'and',\n",
       "   'implementation',\n",
       "   'of',\n",
       "   'a',\n",
       "   'wearable',\n",
       "   'device',\n",
       "   'prototype',\n",
       "   'for',\n",
       "   'fall',\n",
       "   'monitoring',\n",
       "   'use',\n",
       "   'accelerometer',\n",
       "   'and',\n",
       "   'gyroscope',\n",
       "   'sensor',\n",
       "   'and',\n",
       "   'connect',\n",
       "   'this',\n",
       "   'monitoring',\n",
       "   'device',\n",
       "   'to',\n",
       "   'a',\n",
       "   'smart',\n",
       "   'home',\n",
       "   'ecosystem',\n",
       "   'use',\n",
       "   'the',\n",
       "   'Node',\n",
       "   'RED',\n",
       "   'platform',\n",
       "   'to',\n",
       "   'manage',\n",
       "   'warning',\n",
       "   'when',\n",
       "   'a',\n",
       "   'fall',\n",
       "   'have',\n",
       "   'be',\n",
       "   'detect',\n",
       "   'to',\n",
       "   'achieve',\n",
       "   'this',\n",
       "   'objective',\n",
       "   'it',\n",
       "   'be',\n",
       "   'necessary',\n",
       "   'to',\n",
       "   'research',\n",
       "   'and',\n",
       "   'understand',\n",
       "   'the',\n",
       "   'technology',\n",
       "   'involve',\n",
       "   'in',\n",
       "   'exist',\n",
       "   'wearable',\n",
       "   'device',\n",
       "   'and',\n",
       "   'from',\n",
       "   'this',\n",
       "   'choose',\n",
       "   'the',\n",
       "   'good',\n",
       "   'and',\n",
       "   'most',\n",
       "   'affordable',\n",
       "   'one',\n",
       "   'to',\n",
       "   'implement',\n",
       "   'in',\n",
       "   'a',\n",
       "   'real',\n",
       "   'prototype',\n",
       "   'then',\n",
       "   'an',\n",
       "   'accelerometer',\n",
       "   'and',\n",
       "   'a',\n",
       "   'gyroscope',\n",
       "   'be',\n",
       "   'integrate',\n",
       "   'with',\n",
       "   'the',\n",
       "   'ESP32',\n",
       "   'module',\n",
       "   'to',\n",
       "   'build',\n",
       "   'the',\n",
       "   'wearable',\n",
       "   'device',\n",
       "   'finally',\n",
       "   ...]},\n",
       " {'titulo': 'A Comprehensive View of the Biases of Toxicity and Sentiment Analysis Methods Towards Utterances with African American English Expressions',\n",
       "  'informacoes_url': '',\n",
       "  'idioma': 'english',\n",
       "  'storage_key': '../articles/original/english/985-24737-1-10-20240923.pdf',\n",
       "  'author': 'Guilherme Andrade; Luiz Nery; Fabrício Benevenuto; Flavio Figueiredo and Savvas Zannettou',\n",
       "  'data_publicacao': '11-09-2024',\n",
       "  'resumo': 'Language is a dynamic aspect of our culture that changes when expressed in different technologies and/or communities. On the Internet, social networks have enabled the diffusion and evolution of different dialects, including African American English (AAE). However, this increased usage of different dialects is not without barriers. One particular barrier, the focus of this paper, is on how sentiment (Vader, TextBlob, and Flair) and toxicity (Google’s Perspective and models from the open-source Detoxify) scoring methods present biases towards utterances with AAE expressions. In particular, AI tools cannot understand the re-appropriation of the terms, leading to false positive scores and biases. Here, we study the bias of Toxicity and Sentiment Analysis models based on experiments performed on Web-and spoken English datasets. ###',\n",
       "  'keywords': 'African American English, AAE, Bias, Toxicity, Sentiment',\n",
       "  'referencias': ['[1] Abubakar Abid, Maheen Farooqi, and James Zou. 2021. Large language models\\nassociate Muslims with violence. *Nature Machine Intelligence* 3, 6 (2021), 461–463.',\n",
       "   '[2] CJ Adams. 2018. New York Times: Using AI to host better conversations. https://blog.google/technology/ai/new-york-times-using-ai-host-betterconversations/.',\n",
       "   '[3] Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, and\\nRoland Vollgraf. 2019. FLAIR: An easy-to-use framework for state-of-the-art\\nNLP. In *NAACL 2019, 2019 Annual Conference of the North American Chapter of*\\n*the Association for Computational Linguistics (Demonstrations)* . 54–59.',\n",
       "   '[4] Marzieh Babaeianjelodar, Stephen Lorenz, Josh Gordon, Jeanna Matthews, and\\nEvan Freitag. 2020. Quantifying gender bias in different corpora. In *Companion*\\n*Proceedings of the Web Conference 2020* . 752–759.',\n",
       "   '[5] Arnetha F Ball. 1992. Cultural preference and the expository writing of AfricanAmerican adolescents. *Written Communication* 9, 4 (1992), 501–532.',\n",
       "   '[6] Ari Ball-Burack, Michelle Seng Ah Lee, Jennifer Cobbe, and Jatinder Singh. 2021.\\nDifferential tweetment: Mitigating racial dialect bias in harmful tweet detection. In *Proceedings of the 2021 ACM Conference on Fairness, Accountability, and*\\n*Transparency* . 116–128.',\n",
       "   '[7] David Bamman, Chris Dyer, and Noah A Smith. 2014. Distributed representations\\nof geographically situated language. In *Proceedings of the 52nd Annual Meeting of*\\n*the Association for Computational Linguistics (Volume 2: Short Papers)* . 828–834.',\n",
       "   '[8] John Baugh. 1981. Runnin’down Some Lines: The Language and Culture of Black\\nTeenagers.',\n",
       "   '[9] Su Lin Blodgett, Lisa Green, and Brendan O’Connor. 2016. Demographic dialectal\\nvariation in social media: A case study of African-American English. *arXiv*\\n*preprint arXiv:1608.08868* (2016).',\n",
       "   '[10] Su Lin Blodgett, Johnny Wei, and Brendan O’Connor. 2018. Twitter universal\\ndependency parsing for African-American and mainstream American English.\\nIn *Proceedings of the 56th Annual Meeting of the Association for Computational*\\n*Linguistics (Volume 1: Long Papers)* . 1415–1425.',\n",
       "   '[11] Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T\\nKalai. 2016. Man is to computer programmer as woman is to homemaker?\\ndebiasing word embeddings. *Advances in neural information processing systems*\\n29 (2016).',\n",
       "   '[12] Zhenpeng Chen, Jie M Zhang, Max Hort, Federica Sarro, and Mark Harman. 2022.\\nFairness testing: A comprehensive survey and analysis of trends. *arXiv preprint*\\n*arXiv:2207.10223* (2022).',\n",
       "   '[13] Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017.\\nAutomated hate speech detection and the problem of offensive language. In\\n*Proceedings of the international AAAI conference on web and social media*, Vol. 11.\\n\\n\\n512–515.',\n",
       "   '[14] Mark Díaz, Isaac Johnson, Amanda Lazar, Anne Marie Piper, and Darren Gergle.\\n2018. Addressing age-related bias in sentiment analysis. In *Proceedings of the*\\n*2018 chi conference on human factors in computing systems* . 1–14.',\n",
       "   '[15] Joey Lee Dillard. 1977. *Lexicon of Black English.* ERIC.',\n",
       "   '[16] Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. 2018.\\nMeasuring and mitigating unintended bias in text classification. In *Proceedings of*\\n*the 2018 AAAI/ACM Conference on AI, Ethics, and Society* . 67–73.',\n",
       "   '[17] Jacob Eisenstein, Brendan O’Connor, Noah A Smith, and Eric P Xing. 2014.\\nDiffusion of lexical change in social media. *PloS one* 9, 11 (2014), e113114.',\n",
       "   '[18] Anjalie Field, Su Lin Blodgett, Zeerak Waseem, and Yulia Tsvetkov. 2021. A\\nSurvey of Race, Racism, and Anti-Racism in NLP. In *Proceedings of the 59th*\\n*Annual Meeting of the Association for Computational Linguistics and the 11th*\\n*International Joint Conference on Natural Language Processing (Volume 1: Long*\\n*Papers)* . 1905–1925.',\n",
       "   '[19] Sarah Florini. 2014. Tweets, Tweeps, and Signifyin’ Communication and Cultural\\nPerformance on “Black Twitter”. *Television & New Media* 15, 3 (2014), 223–237.',\n",
       "   '[20] Patricia Friedrich. 2020. When Englishes go digital. *World Englishes* 39, 1 (2020),\\n67–78.',\n",
       "   '[21] Patricia Friedrich and Eduardo Diniz de Figueiredo. 2016. *The sociolinguistics of*\\n*digital Englishes* . Routledge.',\n",
       "   '[22] David Garcia, Ingmar Weber, and Venkata Rama Kiran Garimella. 2014. Gender\\nasymmetries in reality and fiction: The bechdel test of social media. In *Eighth*\\n*International AAAI Conference on Weblogs and Social Media* .',\n",
       "   '[23] Anastasia Giachanou and Fabio Crestani. 2016. Like it or not: A survey of twitter\\nsentiment analysis methods. *ACM Computing Surveys (CSUR)* 49, 2 (2016), 1–41.',\n",
       "   '[24] Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter sentiment classification\\nusing distant supervision. *CS224N project report, Stanford* 1, 12 (2009), 2009.',\n",
       "   '[25] A Gomes, D Antonialli, and T Dias-Oliva. 2019. Drag queens and artificial\\nintelligence. Should computers decide what is toxic on the internet. *Internet Lab*\\n*blog* (2019).',\n",
       "   '[26] Hila Gonen and Yoav Goldberg. 2019. Lipstick on a pig: Debiasing methods cover\\nup systematic gender biases in word embeddings but do not remove them. *arXiv*\\n*preprint arXiv:1903.03862* (2019).',\n",
       "   '[27] Mark Graham, Bernie Hogan, Ralph K Straumann, and Ahmed Medhat. 2014.\\nUneven geographies of user-generated information: Patterns of increasing informational poverty. *Annals of the Association of American Geographers* 104, 4\\n(2014), 746–764.',\n",
       "   '[28] Lisa J Green. 2002. *African American English: a linguistic introduction* . Cambridge\\nUniversity Press.',\n",
       "   '[29] Tommi Gröndahl, Luca Pajola, Mika Juuti, Mauro Conti, and N Asokan. 2018. All\\nyou need is\" love\" evading hate speech detection. In *Proceedings of the 11th ACM*\\n*workshop on artificial intelligence and security* . 2–12.',\n",
       "   '[30] Laura Hanu and Unitary team. 2020. Detoxify. Github.\\nhttps://github.com/unitaryai/detoxify.',\n",
       "   '[31] Camille Harris, Matan Halevy, Ayanna Howard, Amy Bruckman, and Diyi Yang.\\n2022. Exploring the role of grammar and word choice in bias toward african\\namerican english (aae) in hate speech classification. In *2022 ACM Conference on*\\n*Fairness, Accountability, and Transparency* . 789–798.',\n",
       "   '[32] Hossein Hosseini, Sreeram Kannan, Baosen Zhang, and Radha Poovendran. 2017.\\nDeceiving google’s perspective api built for detecting toxic comments. *arXiv*\\n*preprint arXiv:1702.08138* (2017).',\n",
       "   '[33] Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews.\\nIn *Proceedings of the tenth ACM SIGKDD international conference on Knowledge*\\n*discovery and data mining* . 168–177.',\n",
       "   '[34] Ben Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu\\nZhong, and Stephen Denuyl. 2020. Unintended machine learning biases as\\nsocial barriers for persons with disabilitiess. *ACM SIGACCESS Accessibility and*\\n*Computing* (2020), 1–1.',\n",
       "   '[35] Clayton Hutto and Eric Gilbert. 2014. Vader: A parsimonious rule-based model\\nfor sentiment analysis of social media text. In *Proceedings of the international*\\n*AAAI conference on web and social media*, Vol. 8. 216–225.',\n",
       "   '[36] Sen Jia, Thomas Lansdall-Welfare, and Nello Cristianini. 2015. Measuring gender\\nbias in news images. In *Proceedings of the 24th International Conference on World*\\n*Wide Web* . 893–898.',\n",
       "   '[37] Jigsaw. [n. d.]. Perspective API. https://perspectiveapi.com/. Accessed: 2023-0130.',\n",
       "   '[38] Tyler Kendall and Charlie Farrington. 2021. The Corpus of Regional African\\nAmerican Language (Version 2021.07). Eugene, OR: The Online Resources for\\nAfrican American Language Project.',\n",
       "   '[39] Svetlana Kiritchenko and Saif M Mohammad. 2018. Examining Gender and Race\\nBias in Two Hundred Sentiment Analysis Systems. *NAACL HLT 2018* (2018), 43.',\n",
       "   '[40] Animesh Koratana and Kevin Hu. 2018. Toxic speech detection. *URL: https://web.*\\n*stanford. edu/class/archive/cs/cs224n/cs224n* 1194 (2018).',\n",
       "   '[41] Deepak Kumar, Patrick Gage Kelley, Sunny Consolvo, Joshua Mason, Elie\\nBursztein, Zakir Durumeric, Kurt Thomas, and Michael Bailey. 2021. Designing\\nToxic Content Classification for a Diversity of Perspectives.. In *SOUPS@ USENIX*\\n*Security Symposium* . 299–318.\\n\\n\\n9\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Resende et al.',\n",
       "   '[42] Steven Loria. 2018. textblob Documentation. *Release 0.15* 2 (2018).',\n",
       "   '[43] Patricia Georgiou Marie Pellat. 2018. Perspective Launches In Spanish With\\nEl País. https://medium.com/jigsaw/perspective-launches-in-spanish-with-elpa%C3%ADs-dc2385d734b2.',\n",
       "   '[44] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.\\nDistributed representations of words and phrases and their compositionality. In\\n*Advances in neural information processing systems* . 3111–3119.',\n",
       "   '[45] Meena Devii Muralikumar, Yun Shan Yang, and David W McDonald. 2023. A\\nHuman-Centered Evaluation of a Toxicity Detection API: Testing Transferability\\nand Unpacking Latent Attributes. *ACM Transactions on Social Computing* (2023).',\n",
       "   '[46] Lisa Nakamura. 2013. *Cybertypes: Race, ethnicity, and identity on the Internet* .\\nRoutledge.',\n",
       "   '[47] Nikolaos Pappas, Georgios Katsimpras, and Efstathios Stamatatos. 2013. Distinguishing the popularity between topics: a system for up-to-date opinion retrieval\\nand mining in the web. In *Computational Linguistics and Intelligent Text Process-*\\n*ing: 14th International Conference, CICLing 2013, Samos, Greece, March 24-30, 2013,*\\n*Proceedings, Part II 14* . Springer, 197–209.',\n",
       "   '[48] Daniel Borkan Patricia Georgiou, Marie Pellat. 2019. Parlons-en! Perspective and\\nTune are now available in French. https://medium.com/jigsaw/perspective-tuneare-now-available-in-french-c4cf1ca198f2.',\n",
       "   '[49] James W Pennebaker, Martha E Francis, and Roger J Booth. 2001. Linguistic\\ninquiry and word count: LIWC 2001. *Mahway: Lawrence Erlbaum Associates* 71,\\n2001 (2001), 2001.',\n",
       "   '[50] Mark A Pitt, Keith Johnson, Elizabeth Hume, Scott Kiesling, and William Raymond. 2005. The Buckeye corpus of conversational speech: Labeling conventions\\nand a test of transcriber reliability. *Speech Communication* 45, 1 (2005), 89–95.',\n",
       "   '[51] Filipe N Ribeiro, Matheus Araújo, Pollyanna Gonçalves, Marcos André Gonçalves,\\nand Fabrício Benevenuto. 2016. Sentibench-a benchmark comparison of state-ofthe-practice sentiment analysis methods. *EPJ Data Science* 5, 1 (2016), 1–29.',\n",
       "   '[52] Max Roser, Hannah Ritchie, and Esteban Ortiz-Ospina. 2015. Internet. *Our World*\\n*in Data* (2015). https://ourworldindata.org/internet.',\n",
       "   '[53] Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, and Noah A Smith. 2019.\\nThe risk of racial bias in hate speech detection. In *Proceedings of the 57th annual*\\n*meeting of the association for computational linguistics* . 1668–1678.',\n",
       "   '[54] Geneva Smitherman. 2000. *Black talk: Words and phrases from the hood to the*\\n*amen corner* . Houghton Mifflin Harcourt.',\n",
       "   '[55] Kaikai Song, Ting Yao, Qiang Ling, and Tao Mei. 2018. Boosting image sentiment\\nanalysis with visual attention. *Neurocomputing* 312 (2018), 218–228.',\n",
       "   '[56] Ezekiel Soremekun, Sakshi Udeshi, and Sudipta Chattopadhyay. 2022. Astraea:\\nGrammar-based fairness testing. *IEEE Transactions on Software Engineering* 48,\\n12 (2022), 5188–5211.',\n",
       "   '[57] Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly Voll, and Manfred Stede.\\n2011. Lexicon-based methods for sentiment analysis. *Computational linguistics*\\n37, 2 (2011), 267–307.',\n",
       "   '[58] Rachael Tatman. 2017. Gender and dialect bias in YouTube’s automatic captions.\\nIn *Proceedings of the first ACL workshop on ethics in natural language processing* .\\n53–59.',\n",
       "   '[59] Mike Thelwall. 2014. Heart and soul: Sentiment strength detection in the social\\nweb with sentistrength, 2017. *Cyberemotions: Collective emotions in cyberspace*\\n(2014).',\n",
       "   '[60] Pranav Narayanan Venkit and Shomir Wilson. 2021. Identification of bias against\\npeople with disabilities in sentiment analysis and toxicity detection models. *arXiv*\\n*preprint arXiv:2111.13259* (2021).',\n",
       "   '[61] Hao Wang, Doğan Can, Abe Kazemzadeh, François Bar, and Shrikanth Narayanan.\\n2012. A system for real-time twitter sentiment analysis of 2012 us presidential\\nelection cycle. In *Proceedings of the ACL 2012 system demonstrations* . 115–120.',\n",
       "   '[62] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou.\\n2020. Minilm: Deep self-attention distillation for task-agnostic compression of\\npre-trained transformers. *Advances in Neural Information Processing Systems* 33\\n(2020), 5776–5788.',\n",
       "   '[63] Maciej Widawski. 2015. *African American slang: A linguistic description* . Cambridge University Press.',\n",
       "   '[64] Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Jason Kessler, Janyce\\nWiebe, Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.\\nOpinionFinder: A system for subjectivity analysis. In *Proceedings of HLT/EMNLP*\\n*2005 Interactive Demonstrations* . 34–35.',\n",
       "   '[65] Austin P Wright, Omar Shaikh, Haekyu Park, Will Epperson, Muhammed Ahmed,\\nStephane Pinel, Duen Horng Chau, and Diyi Yang. 2021. RECAST: Enabling\\nuser recourse and interpretability of toxicity detection models with interactive\\nvisualization. *Proceedings of the ACM on Human-Computer Interaction* 5, CSCW1\\n(2021), 1–26.',\n",
       "   '[66] Ali Yadollahi, Ameneh Gholipour Shahraki, and Osmar R Zaiane. 2017. Current\\nstate of text sentiment analysis from opinion to emotion mining. *ACM Computing*\\n*Surveys (CSUR)* 50, 2 (2017), 1–33.',\n",
       "   '[67] Min Yang, Qiang Qu, Xiaojun Chen, Chaoxue Guo, Ying Shen, and Kai Lei. 2018.\\nFeature-enhanced attention network for target-dependent sentiment classification. *Neurocomputing* 307 (2018), 91–97.\\n\\n\\n10\\n\\n\\n-----'],\n",
       "  'text': '# **A Comprehensive View of the Biases of Toxicity and Sentiment** **Analysis Methods Towards Utterances with African American** **English Expressions**\\n\\n## Guilherme Andrade, Luiz Nery, Fabrício Benevenuto, Flavio Figueiredo\\n#### UFMG guilherme.hra,luiznery,fabricio,flavio@dcc.ufmg.br\\n### **ABSTRACT**\\n\\nLanguage is a dynamic aspect of our culture that changes when\\nexpressed in different technologies and/or communities. On the Internet, social networks have enabled the diffusion and evolution of\\ndifferent dialects, including African American English (AAE). However, this increased usage of different dialects is not without barriers.\\nOne particular barrier, the focus of this paper, is on how sentiment\\n(Vader, TextBlob, and Flair) and toxicity (Google’s Perspective and\\nmodels from the open-source Detoxify) scoring methods present\\nbiases towards utterances with AAE expressions. In particular, AI\\ntools cannot understand the re-appropriation of the terms, leading\\nto false positive scores and biases. Here, we study the bias of Toxicity and Sentiment Analysis models based on experiments performed\\non Web-and spoken English datasets.\\n### **KEYWORDS**\\n\\nAfrican American English, AAE, Bias, Toxicity, Sentiment\\n### **1 INTRODUCTION**\\n\\nIn recent decades, we have witnessed a substantial rise in Internet\\nusage. According to [ 52 ], Internet users increased from approximately 400 million in 2000 to 4.7 billion in 2020. With this increase in\\nusage, it is natural that Web applications enable a wide diversity of\\nsocial groups to interact among themselves and with other groups.\\nSince such applications foster a more open and dynamic form of\\nspeech, a natural increase in the written form of dialects that previously were predominantly seen in the spoken form [ 9 ] occurred.\\nHowever, such massive amounts of textual data make manual content moderation impracticable. In other words, the heavy usage of\\nsocial media has evidenced the urge for automatic moderation tools\\nthat measure and moderate improper behavior online. One of the\\nmain concerns is the public display of negative/toxic sentiments\\nagainst a person or specific group, more drastically when the target\\nis a minority group historically marked with discrimination and\\nstereotypes. The necessity of dealing with the increasing number\\nof deviating content has led many researchers and companies to\\nuse AI tools to identify such events [51].\\nConcurrently to the increase in Web usage, African-American\\nEnglish (AAE) has gone from being seen as a marginalized dialect of\\nEnglish to a consolidated vernacular of the language [ 28 ]. Like most\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n© 2024 SBC – Brazilian Computing Society.\\nISSN 2966-2753\\n\\n## Savvas Zannettou\\n#### TU Delft s.zannettou@tudelft.nl\\n\\ndialects, the AAE was initially heavily used in spoken form and\\nhad the Web as a crucial influence on its emergence in the written\\nform [ 9 ]. However, the Web is not only a disseminator of cultural\\naspects of our society but also a vehicle where toxicity campaigns\\nagainst African Americans are prone to occur. Even though several\\nwebsites have well-defined community guidelines, user anonymity\\nand lack of unaccountability leave room for misbehavior.\\nThe aforementioned rise in AI moderation tools (such as Google’s\\nPerspective [ 37 ] and others [ 23, 51, 66 ]) aim to reduce the amount\\nof negative or toxic utterances online. Overall, such tools rely on\\nMachine Learning (ML) models that help determine proper and improper utterances. Nevertheless, as previous research has discussed,\\nautomatic content moderation can backfire and present biases towards minorities [ 11, 34, 53, 60 ]. For instance, a tool for toxicity\\nanalysis may present high scores for non-toxic AAE sentences for\\nno apparent reason. We show examples of toxicity and sentiment\\nanalysis models employed in online text to depict this issue. We\\npoint out that it is quite easy to find problematic utterances when\\nusing slang terms such as *“n****s”* . In Table 1, we contrast three\\npairs of sentences based on their toxicity/negative sentiment levels.\\n*Why does the problem arise?* From a linguistic perspective, dialects may inherently manifest behaviors and cultural aspects of\\nthe groups in which they were created [ 5, 19 – 21 ]. Terms such as\\n“n****r” are problematic for AI models since both the term and its\\nvariations have a historical pejorative usage [1] . Nevertheless, this\\nsame term was re-appropriated by the black community, so its use\\nceased to be considered problematic when used by people inside\\nthe black community. Suppose such a fine line between causal\\nspeaking and offensive discourse is problematic from a human and\\ncomputational perspective. In that case, these interpretations are\\nconfounding to automatic content moderation tools. In other words,\\ntoxicity/sentiment analysis tools are usually developed using manual rules or supervised ML techniques that employ human-labeled\\ndata to extract patterns. The disparate treatment embodied by machine learning models usually replicates discrimination patterns\\nhistorically practiced by humans when interacting with processes\\nin the real world. Due to biases in this process, a lack of context\\nleads models to a concerning scenario where minorities do not\\nreceive equal treatment [1, 14, 26, 58].\\nThis discussion leads to the research question behind our paper: *Is there a systematic bias on toxicity/sentiment analysis towards*\\n*AAE?* To better understand this issue, we present a broad-scale analysis. To do so, we manually curate a dataset of African American\\nEnglish Expressions [ 54, 63 ] (these sources were recommended by\\n\\n1 https://en.wikipedia.org/wiki/Nigga\\n\\n\\n1\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Resende et al.\\n\\n\\n**Score** All my friends on\\nthe porch and never\\nin the house\\n\\n\\nAll my n****s on\\nthe porch and neva\\nina house\\n\\n\\nYou’re white You’re black I can’t forget Cant fuhgit you\\n\\nyou\\n\\n\\n\\nthe organizers of the to-be-released Oxford Dictionary of African\\nAmerican English [2] ). These expressions complement four different\\nutterance datasets with some demographic information on race (i.e.,\\ninterviews from African American individuals, AAE utterance vs\\nnon-AAE utterance labels, and author-supplied labels). *We empha-*\\n*size that we cannot state how a speaker identifies regarding her/his*\\n*race for some datasets. AAE may also be employed by non-African*\\n*Americans. However, we interpret our results using AAE expressions*\\n*from our expressions dataset (made available)* .\\nThe models we study can be divided into toxicity (Google’s Perspective [ 37 ], Detoxify, and Detoxify Unbiased [ 30 ]), and sentiment\\nanalysis (Flair [ 3 ], TextBlob [ 42 ], and Vader [ 35 ]) models, but also\\ncan be segmented into machine learning-based (Google’s Perspective, Flair, Detoxify, and Detoxify Unbiased), and lexical, or rule,\\nbased (Vader, and TextBlob) models. Our main contributions are:\\n\\n(1) We present a broad-scale analysis of biases toward utterances\\nwith AAE expressions in six out-of-the-shelf models;\\n(2) To do so, we focus on unveiling if there is a systematic tendency for AAE utterances to be deemed more toxic or negative sentiment by several models of datasets of different\\nnatures (tweets, closed captions, and spoken interviews). To\\nreach our results, we manually transcribed a dictionary of\\nAAE expressions and used the number of such expressions\\nin an utterance as an explanatory feature;\\n(3) Other control features include Lexical Analysis [ 49 ], and\\ngrammar-based Part of Speech Tagging PoS) labels for words\\nin utterances. Overall, we discuss which characteristics of\\nthe utterance lead the model to deem it as toxic or of negative\\nsentiment. The number of AAE expressions is a recurrent\\nstatistically significant feature;\\n(4) Using recent language models [ 62 ], we show that in some\\ndatasets, even utterances from African-American (AA) speakers that have a similar meaning to those from non-AA speakers, models will, in several cases, score the sentence from\\nnon-Whites with more toxic/more negative scores.\\n\\nBefore continuing, we point out that our work is not the first\\nto study the biases of similar models towards minorities [1, 14, 26,\\n58 ]. However, we complement prior endeavors with broader-scale\\nanalysis. Previous methods focus on a single dataset or model and do\\n\\n2 https://hutchinscenter.fas.harvard.edu/odaae\\n\\n\\nnot perform the lexical and grammar-based analysis we do here. We\\nfurther point out that our goal in this paper is not to pinpoint models\\nwith the best accuracy. We focus on showing AAEs and comparing\\nif there is a tendency across several models. The datasets where\\nwe show this issue range from online texts from Twitter [ 9, 10 ],\\nspoken English datasets gathered by linguists [ 38, 50 ], and online\\nsingle speaker English from YouTube movie reviews. The YouTube\\ndataset (see Section 3) was a manual effort toward gathering data\\nwith fewer confounding factors (i.e., single-speaker videos). This\\ndataset is made available to the community to improve the current\\nand yet-to-come NLP models [3] .\\nOur results show that biases are more prominent on online\\ndatasets, such as Twitter and YouTube, and less strongly but still\\npresent in spoken English interviews. Our research shows that\\nusing AAE expressions will likely lead to sentences being deemed\\nmore toxic, even when sentences are similar to those with non-AAE\\nexpressions. Overall, system developers may use these findings to\\ndetermine what model type shall be employed (sentiment analysis\\nvs. toxicity scoring) or whether ML vs. lexical-based models are\\nmore adequate for their application. More importantly, our findings\\nshow that even considering “unbiased models” [ 30 ], ML models still\\npresent a bias towards utterances with AAE expressions. Indicating\\nthat AAE speakers may still face unwarranted moderation online.\\n### **2 BACKGROUND AND RELATED WORK**\\n\\nThis section presents an overview of the literature. We begin discussing the sociolinguistics of English online, as the increased usage of AAE expressions online is a primary motivation behind our\\nwork. Subsequently, we also discuss the motivation behind sentiment analysis tools, available alternatives, their major strengths and\\nshortcomings, and how toxicity relates to sentiment analysis. Next,\\nwe discuss bias in machine learning methods and how they can\\nnegatively influence individuals online and suppress the discourse\\nof minority groups. Finally, we focus on those papers most related\\nto ours and present a statement on the novelty of our research.\\n\\n3 https://anonymous.4open.science/r/aae_bias-D396/data/aae_terms_black_talk.\\nyaml\\n\\n\\n2\\n\\n\\n-----\\n\\nA Comprehensive View of Biases Towards Utterances with African American English Expressions WebMedia’2024, Juiz de Fora, Brazil\\n\\n### **2.1 Sociolinguistics of AAE Online**\\n\\nAs a research field, sociolinguistics focuses on studying how social context affects the usage and evolution of language. Overall,\\nhumans take part in several speech communities throughout their\\nlives [ 21 ], and even the same human being may communicate in\\ndifferent variations of English depending on the community she/he\\nis interacting with. With the rise of the Social Web over the 2000’s\\nand 2010’s, the field also focused on how Web communities affect\\nlanguage [ 20, 21 ]. In particular, Friedrich and Figueiredo [ 21 ] argue\\nthat hundreds of years after the invention of the printing press, the\\nwritten usage of English as a language appeared to be “evolving” to\\na standard or uniform English. However, with the Web, community\\nand individualized language use has increased over recent years.\\nAAE is an example of such a case [ 9, 20, 21 ], where the dialect has\\nexperienced a rise in usage (particularly online) in recent years.\\nOne example of the expansion of AAE in recent years comes\\nfrom the movement known as *signifyin’* [ 19 ]. In other words, when\\n*signifyin’* one expresses their race via particular dialects, such as\\nAAE, on social media. This expression is utilized to resist the oppression present in one’s day lives [ 19, 46 ]. Regarding how AAE\\nis spread online, some authors argue that the dialect spreads initially from Web users from large cities to smaller communities in\\nwave-like, or viral, patterns [ 17 ]. Frierich and Figueiredo state that:\\n“With the Internet, we have witnessed a change in this scenario.\\nGender and racial/ethnic activism have become quite strong online\\nand have served not only to spread the debates but also to add new\\nlayers to them – such as the complex construction of identities in\\ncyberspace. And again, we must say, English has been quite present\\nin this new picture, mainly because of its lingua franca status.”\\nFor several decades, the AAE dialect has also been studied offline [ 63 ]. Over the years, some attempts have been made to catalog\\ndifferent expressions from the dialect [ 8, 15, 54 ]. Oxford and Harvard are also organizing a dictionary called the Oxford Dictionary\\nof African American English (ODAEE).\\nGiven that language constantly evolves, we aimed to collect\\na recent corpus of AAE expressions. To do so, we contacted the\\norganizers of the ODAAE, asking if they could share the list of\\nexpressions used in their dictionary. The organizers kindly denied\\nit because ODAEE is still a work in progress. Nevertheless, they\\ndid suggest that we use expressions from Smitherman’s Black Talk\\nDictionary [ 54 ] as it is a large and somewhat recent corpus. In\\nour research, we manually transcribed every expression from this\\ndictionary as our AAE expression list.\\n### **2.2 Sentiment Analysis and Toxicity Models**\\n\\nSentiment Analysis identifies sentiments and quantifies their intensity (positive or negative sentiment) in utterances. Current Sentiment Analysis models may be classified into two major categories,\\nnamely, machine learning-based (ML models) and lexical-based\\napproaches, described below. These methods have been employed\\nsince the mid-2000s, and one of the major motivations behind building such approaches was to rate user reviews online.\\nML-based Sentiment Analysis models [ 3, 24, 55, 61, 67 ] are built\\nover a sample of data points comprising as many examples as possible from positive and negative sentences. Usually, the learning\\n\\n\\nprocedure targets data drawn from a context of interest (e.g., Twitter, Facebook, Marketplaces, etc.), and humans manually label this\\ndataset to train models. This family of methods often benefits from\\ncomplex word representations and can grasp deeper relationships\\nimplied in daily conversations. Lexical-based approaches [ 33, 35,\\n49, 57 ], on the other hand, begin by listing seed words considered\\nto be representatives of groups of emotions. Once the seed list\\nis complete, it is incremented with similar words and synonyms.\\nSuch approaches must actively deal with normal word usage that\\nmay change the intent/intensity of the sentence, such as negations,\\npunctuations, capitalization, etc. Since this approach is based on the\\nhuman understanding/application of terms and expressions, their\\nperformance on novel datasets may have less statistical variance.\\nCompared to lexical-based methods, the ML models rely on the\\nvector representation of terms and utterances [ 44 ]. Such vector\\nrepresentations are used as inputs to train supervised methods. Collecting high-quality labels to train such models is a difficult-to-reach\\npre-requisite (discussed below). On the other hand, lexical-based\\napproaches need to explicitly address negations, punctuations, outof-vocabulary occurrences, and more complex relations between\\nterms [ 51 ]. To address the gaps left by each family of methods,\\nauthors have also proposed hybrid solutions [47, 59, 64].\\nMore recently, we have seen a rise in Toxicity Classification\\n(compared to Sentiment Analysis) models [ 30, 37, 40 ]. Most, if not\\nall, of these approaches are ML-based. Toxic speech is usually considered to be an umbrella term that comprises hate speech, abusive\\nlanguage, racism, and so on [ 31, 41 ]. Despite the efforts to address\\ntoxic speech, there is not a clear agreement about what it means\\nfor a sentence to be toxic. Dixon et al [ 16 ] defines toxicity as rude,\\ndisrespectful, or unreasonable language.\\nDue to the lack of consensus on toxicity, the inherent ambiguity\\nof labeling sentences presents an issue to ML models. The vast\\nmajority of datasets use human labelers, which are influenced by\\ntheir previous experiences and, most of the time, do not have access\\nto the underlying context from which the respective sentence was\\ndrawn. This subjectivity and lack of context may cause considerable\\nlabeling issues. For example, Kumar et al. [ 41 ] state that people who\\nhave suffered harassment in the past are more prone to label random\\nsentences from some social networks as toxic than those who did\\nnot face such problems. Maybe due to its less restrictive definition\\nand to the capacity to encompass many types of harassment, toxicity\\nmodels are actively used in practice to moderate discourse in many\\nplatforms [2, 43, 48]; however, with some known bias problems.\\n### **2.3 Biases Towards Minorities Online**\\n\\nWe now discuss prior work on the biases of Web datasets and AI\\nmodels. Starting from Jia et al. [ 36 ], the authors investigated the\\nproportions in which men and women appeared in news articles’\\nimages. The authors found that men are considerably more frequently represented than women. Garcia et al. [ 22 ] also described a\\nconsistent bias towards men in Twitter content. On Twitter, female\\nusers tend to describe more events in which men play important\\nroles. Babaeianjelodar et al. [ 4 ] explored the nuances of gender\\nbiases over ML models. In all datasets considered, models perform\\ndisparately against unprivileged subgroups. Similar findings were\\nraised by several other authors considering countries [ 27 ], age [ 14 ],\\n\\n\\n3\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Resende et al.\\n\\n\\nreligion [ 1 ], and sexual orientation [ 16 ]. Regarding dialects, Blodgett et al. [ 9 ] studied how language characteristics can change\\nconsiderably within the same country. The work focuses on learning distinguishable features between Mainstream American English\\n(MAE) and AAE with a geographic context. In [ 25 ], the authors also\\npresent another clear differentiation between English focusing on\\nDrag queens. Here, the authors find that transgender individuals\\nhave a speaking characteristic consistently seen as more toxic.\\nAs Bamman et al. [ 7 ] states, language is always situated within\\na context. Neglecting this surrounding context leads to disparate\\ntreatments. For example, transgender individuals may be speaking\\na dialect deemed toxic if used by someone outside of the community.\\nHowever, this may be a defense mechanism to cope with tough\\nsituations imposed by society [ 25 ]. Similar language signals are\\npassive within the black community and the AAE dialect. Studies\\nwere already performed to comprehend and measure how much\\nML models are biased against AAE speakers [6, 9, 53].\\nOverall, we can state that nowadays, it is not hard to find discrimination episodes involving AI models [ 1, 14, 16, 34 ]. For example,\\nAbid et al. [ 1 ] interacted with a conversational artificial intelligence\\nmodel touching religion-related subjects and noting the inner associations with the topic. Finally, they found a consistent bias associating Muslims with terrorists (in 23% of the test cases) and the Jewish\\nwith money (in 5% of the test cases). In the opposite direction, mitigating such biases are also common [ 4, 9, 11, 16, 31 ]. Nevertheless,\\nas studied by Gonen et al. [ 26 ], persistent bias may stick with the\\nmodel even after active effort has been applied to remove it. We also\\nobserve this as we use unbiased versions of recent models. Since\\n\\nthe complexity of the ML model has increased in the last few years,\\nwe could expect the bias to be more elaborate and complex to fight\\nagainst. This leads us to the problem of using biased models for\\nsensible tasks that may perpetrate harmful behavior.\\n### **2.4 Related Work and Research Novelty**\\n\\nWe now detail prior work that is most related to ours (e.g., evaluating and unveiling biases in similar models) [ 13, 18, 29, 32, 39, 45, 53,\\n56, 65 ]. Before doing so, we initially point out that the studying the\\nbiases of NLP models towards race is a well-established research\\n\\ntopic and the survey of Field et al. [ 18 ] presents a recent overview\\non this topic. The authors of this survey analyzed 79 different papers on race and NLP systems. Overall, the consensus is that NLP\\nstill encodes racial biases (something we also observe) and that race\\nis commonly studied as a limited categorical variable. Here, we take\\na step towards a broader view of the issues by incorporating in our\\nstudy (1) a novel list of AAE expressions, (2) grammatical features\\n(PoS), and (3) linguistic features (LIWC) to understand biases.\\nOne of the vanguard efforts of looking into the biases of toxic\\nscoring APIs (Perspective in particular) was performed by Sap et\\nal. [ 53 ]. In contrast, Kiritchenko et al. were among the first to\\nstudy the bias of sentiment analysis models [ 39 ]. Starting with the\\nformer, the authors discuss how datasets are biased and how models\\npropagate such biases. However, unlike our work, the authors only\\nstudy Twitter datasets and do not present statistical analysis on\\nhow utterance features (grammar, linguistics, and usage of AAE\\nexpressions) may explain biases. The authors also only focus on\\nPerspective as an out-of-the-shelf model. The former, focused on\\n\\n\\nsentiment analysis, compares over two hundred models from a wellestablished information retrieval sentiment classification challenge.\\nHowever, the author does not use real-world datasets as we do and\\nfocuses their analysis on sentences of similar meaning but with\\nsmall changes in words related to gender, occupation, and race.\\nThis approach is similar to the adversarial attacks described next.\\nAlthough not focused on measuring biases, the work of Hosseini\\net al. [ 32 ], and Gröndahl et al. [ 29 ] both show how small changes\\nto a sentence will change model scores. We present a different view\\non this finding by incorporating semantic similarity using language\\nmodels [ 62 ] and finding that even expressions that are semantically\\nclose to one another will differ in scores depending on the number\\nof AAE expressions used. Similarly, Davidson et al. [13] discussed\\nsome challenges in differentiating hate speech from other offenses.\\nThis provides evidence of how language is nuanced, and models\\nhave problems with these small nuances. Wright et al. [ 65 ] provides\\na tool called RECAST that helps users pinpoint words that need\\nchanging in order to reduce the toxicity of a score.\\nRegarding human evaluation of models, we refer to the recent\\neffort of Muralikumar et al. [ 45 ]. Here, the authors evaluate Perspective and contrast how human scores align with the model. Overall,\\nthe score from Perspective is a good predictor (based on a Logistic\\nregression) of human labels (“toxic”, “hard to say”, “non-toxic”).\\nHowever, agreement is not always present, with the model still\\nmaking mistakes. The authors suggest that using a score cut-off of\\n0.55, i.e. if the model scores over this value classify the utterance as\\ntoxic, will make model outcomes agree with users 50%.\\nTesting different definitions of fairness is also an active field of\\nresearch [ 12 ], with software tools being developed just for this task\\nin NLP models [ 56 ]. We complement these efforts by showing that\\nout-of-the-shelf models and API still require further testing.\\nOur research differs from previous works by investigating biases in models of different families (ML-based and lexical-based\\nmethods) and throughout four datasets representing different contexts (in-person conversations, single-speaker movie reviews, and\\npersonal social media posts). Here, we focus on out-of-the-shelve\\nmethods and those already applied in real-world forums. Unlike\\nprior work, we employ different statistical features of utterances to\\nshow how the presence of AAE expressions will lead models to rate\\nsentences as more toxic or of negative sentiment. Our statistical\\nanalysis also provides insights into what features models explore to\\nreach their scores. Finally, the datasets we employed were collected\\nto isolate confounding factors. That is: (1) we do not use any dataset\\nused to train models or with toxicity/sentiment labels; (2) one of\\nour datasets is focused on single-speaker movie reviews, controlling for discourse as a confounder; (3) we also compare models on\\nwell-established linguistic datasets on single-person interviews.\\n### **3 DATASETS AND PRE-PROCESSING**\\n\\nWe explore four datasets of different natures to understand the\\nextent of biases in toxicity/sentiment analysis models and when\\nthey present themselves more strongly. Initially, we use the Twitter\\nAAE dataset [ 9, 10 ]. This dataset is interesting as it contains tweets\\nclassified as AAE or Mainstream American English (MAE). Tweets\\nwere classified using an ML model, and we consider a subset of\\ntweets where the model predicted over 80% probability for each\\n\\n\\n4\\n\\n\\n-----\\n\\nA Comprehensive View of Biases Towards Utterances with African American English Expressions WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\n**Dataset** **Demographic** **# Documents** **# Sentences** **# AAE Expr.** **AAE Expr. per Doc.**\\n\\n\\nYouTube AA Speaker [∗] 150 17828 18308 122.05 (\\n\\nnon AA Speaker [∗] 484 41464 42729\\n\\nTwitter AAE Tweet 250 250 372 1.49 (\\n\\nMAE Teet 250 250 259\\n\\nCORAAL AA 142 64493 61651 434.16 (\\n\\n\\nAA Speaker [∗] 150 17828 18308 122.05 ( 43%)\\n\\nnon AA Speaker [∗] 484 41464 42729 85.67\\n\\nAAE Tweet 250 250 372 1.49 ( 43%)\\n\\nMAE Teet 250 250 259 1.04\\n\\n\\nCORAAL AA 142 64493 61651 434.16 ( 9%)\\n\\nBuckeye Caucasian 39 19304 18712 479.79\\n**Table 2: Datasets statistics.** [∗] **Indicates that two (agreeing)**\\n**authors inferred the demographic. Still, our analysis does**\\n**not use it as a variable (we rely on the # of AAE Expr.).**\\n\\n\\nclass. This is a well-established dataset for AAE utterances used by\\nother endeavors [ 53 ]. Twitter is one of the major platforms where\\none would expect that toxicity and sentiment analysis models could\\nmitigate unwanted behavior. On the negative side, as the dataset\\ncontains general Tweets, it does not control for confounding factors\\nsuch as dialogues, debates, and potentially controversial topics.\\nThus, we complement this research with two other datasets.\\nOur YouTube dataset comprises subtitles extracted from YouTube\\nmovie reviews with a single speaker discoursing about a unique\\ntopic per video. The topics are movies from Rotten Tomatoes 100\\nBest Movies of All Time. We targeted single-speaker videos to control for any confounding variables that may appear with dialogues.\\nAlso, we focus on acclaimed films [4] to control for the possible negative influence of bad content (speakers may still dislike the movies,\\nthough). YouTube aims to control both content and dialogue.\\nFinally, we explore the linguistic Corpus of Regional African\\nAmerican Language (CORAAL) [ 38 ] as representations of spoken\\nAfrican-American English. For comparisons, we employ the Buckeye [ 50 ] dataset focused on Caucasian [5] speakers from central Ohio.\\nBoth datasets are focused on spoken interviews with transcripts.\\nBuckeye was recommended to us by the curators of CORAAL.\\nWe now discuss how we identified African-American English\\nexpressions (AAE expressions). As stated, we manually transcribed\\nthe Black Talk dictionary [ 54 ]. Since AAE first emerged as an oral\\nlanguage, the main intent of this dictionary was not to define the\\netymological history of terms; instead, it concentrates on the meanings and significance of expressions. The organizers of the not-yetpublished Oxford Dictionary of African American English referred\\nus to the Black Talk dictionary as a reliable source.\\nThe Black Talk dictionary comprises more than 1800 entries.\\nSince some entries are sentences instead of single terms, they may\\napply to different pronouns. In such cases, the possible use cases are\\nlisted. For example, “BREAK HIM/HER/THEM OFF SOMETHING”\\nbecomes three expressions. Our transcription of the entries considers every possible combination presented.\\nIn Table 2, we present a summary of our datasets in the number\\nof sentences (or utterances), number of words (non-unique), and\\nnumber of African-American English expressions present. Over the\\nnext few subsections, we now detail each dataset.\\n**Twitter:** The Twitter dataset comes from the Twitter AAE [6]\\n\\nwebsite. To create the dataset, the authors [ 9, 10 ] developed a Latent\\nDirichlet Allocation (LDA) based topic model that took into account\\nboth the frequency of common terms used in AAE as well as Census\\ndata. An initial race estimate is performed based on the location\\n\\n4 https://www.rottentomatoes.com/top/bestofrt/\\n5 https://buckeyecorpus.osu.edu/php/corpusInfo.php\\n6 http://slanglab.cs.umass.edu/TwitterAAE/\\n\\n\\nfrom which the account was tweeted. This information is combined\\nwith the presence of key terms to derive different latent topics for\\nthe corpus. Topics were then explored to label AAE and non-AAE.\\nAlthough the authors label over 80 *,* 000 tweets, we focus on a\\nsmaller sample of 500 tweets that the authors manually inspected.\\nThese tweets were manually labeled with PoS tags to derive an\\nAfrican-American English PoS model. According to the authors,\\nmore than 18% of the terms used within the African-American\\n\\ntweets are not in the standard English dictionary. It is also very\\ncommon to find words written in their phonological style in AAE e.g. tha (the), iont (I don’t), ova (over), and so on - while the contrary\\nwas found to never happen in the Non-AAE tweets.\\n**YouTube:** The YouTube dataset is a collection of subtitles from\\n\\nYouTube movie review videos. A single speaker talks to the audience\\nabout a movie production listed among the most relevant movies\\never. We considered Rotten Tomatoes’ top 100 best movies of all\\ntime ranking due to their prestige among the audience and because\\nthey have a higher probability of being well-spoken in a review. For\\neach of the top 50 movies from the ranking, two authors manually\\nsearched and cataloged as many videos as possible. The authors determined Demographic labels, namely, African-American Women,\\nAfrican-American Men, non-African-American Women, and nonAfrican-American Men, in order of appearance when querying the\\nmovie name on YouTube. Since YouTube doesn’t naturally disclose\\ndemographic information about its users, we had to restrict our\\nsearch only to producers who happened to appear on the screen at\\nleast once throughout the entire video. The list of movies and the\\nrespective YouTube channels is available [7] .\\nWhen publishing videos on YouTube, the creators can either\\nexplicitly inform their videos’ subtitles or let the YouTube transcription model automatically caption them. Nonetheless, differently from manually informed subtitles, the captioning mechanism,\\nby default [8] . Only 21 videos had manual subtitles in our dataset [9] .\\nThus, for fair comparisons, we use automatic transcriptions as\\nthese are available in **every** video. Finally, it is important to state\\nthat transcriptions are not punctuated by YouTube. We use ML\\nmodels to correct this behavior on the YouTube dataset and the\\n\\nCORAAL/Buckeye dataset (below).\\nConsidering the observational nature of our study, an extensive\\neffort was applied to control the confounding variables’ effect on the\\nconclusions. The selection of the most prestigious movies of all time\\nwas an attempt to reduce the chance of having negative reviews,\\nwhich would comprise higher scores in the toxicity analysis. We\\nalso tried to find at least one single-speaker video review for every\\nmovie to reduce any sampling bias impact. More importantly, the\\nreviews’ first-person nature helps eliminate the possibility of other\\npeople’s opinions influencing the argumentative paths.\\n*On author inferred demographic variables:* It is worth noticing\\nthat identifying race/gender is subjective and prone to errors – we\\nonly have our view and not the content producer’s identification.\\nThus, we avoid using YouTube demographic variables as input in\\n**any** analysis. We employed author-inferred demographics to collect\\na diverse (to the extent possible) dataset of utterances. Instead,\\n\\n7 https://anonymous.4open.science/r/aae_bias-D396/data/youtube_data_description.\\n\\ncsv\\n8 https://support.google.com/youtube/thread/70343381\\n9 Results do not change if we look only into these few videos.\\n\\n\\n5\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Resende et al.\\n\\n\\nwe rely on the # of AAE Expressions as an explanatory variable.\\nNevertheless, we do see an increase in AAE expressions based on\\nthe inferred demographic on YouTube (see Table 2).\\n**CORAAL and Buckeye:** The Corpus of Regional African American Language [ 38 ] is a long-term corpus developed and maintained\\nby the University of Oregon with the support of the National Science\\nFoundation. The dataset comprises more than 150 socio-linguistic\\ninterviews with African-American English speakers born between\\n1891 and 2005. The dataset contains the orthographic transcriptions\\nof interviews, together with the person’s age, gender, and city they\\nlive in. Thus, each interview from the corpus encompasses many\\nsubjects from a given city/community.\\nUnlike the YouTube data, the transcriptions here represent the\\nentire sentence, accounting for complete punctuation, line-level\\nnotes, and even non-linguistic sounds. Beyond that, the data also\\ntracks the interviewer’s voice in the dialog. The interviews allow\\nthe speakers to talk freely about different topics, an interesting\\nfeature that emulates the diversity of daily interactions and mood\\nvariations. The dataset aggregates five major sub-corpora from\\ndifferent locations in the United States of America, namely, Atlanta (2017), Washington (1968 and 2016), Lower East Side (2009),\\nPrinceville (2004), Rochester (2016), and Valdosta (2017).\\nThe Buckeye [ 50 ] corpus is an effort started in 1999 and supported by the National Institute on Deafness and Other Communication Disorders and the Office of Research at Ohio State University.\\nThe initial goal was to gather approximately 300 *,* 000 words of\\nspeech conversation from central Ohio speakers, keeping track of\\ntime and phonetic information. To reach that objective, researchers\\nselected a group of 40 middle-class Caucasian speakers.\\nSimilar to the YouTube dataset, Buckeye sentences are not punctuated. However, instead of automatically generated captions, this\\ncorpus employed transcribers who were explicitly instructed not\\nto use punctuation within the utterances and not to try to correct\\npossible speech “errors” (we segmented sentences ourselves).\\n\\n*3.0.1* *Sentence Segmentation.* Except for CORAAL, the datasets\\ndon’t necessarily follow the correct orthographic rules about punctuation. Considering the other two transcripted corpus (i.e. YouTube\\nand Buckeye), we should expect their sentences to be segmented not\\naccording to their inherent meaning but to silent intervals (not necessarily long ones) after a continuous pronunciation of words. Such\\nsegmentation can drastically misrepresent the sentences’ meanings\\nand consequently derive misleading conclusions about the data.\\nTo reduce the impacts of incorrect segmentation in later analysis,\\nwe employed a machine learning-based segmentation to all corpus,\\nexcept the one from Twitter. We believe tweets are self-contained\\nmessages where punctuation is not necessarily crucial to the audience’s understanding. Consequently, segmentation is not necessary.\\nOn the other hand, we segment the only correctly punctuated corpus, CORAAL. Since we intend to compare the CORAAL dataset\\ndirectly against Buckeye’s, we should try to reduce the confounding factors (segmentation). The segmentation task was performed\\nusing NVIDIA’s NeMo Toolkit [10] .\\n\\n*3.0.2* *On the Impact of Swear Words.* We executed two versions of\\nour experiments, one considering utterances with swear words and\\n\\n10 https://github.com/NVIDIA/NeMo\\n\\n\\nanother without. The swear words we considered were taken from\\nthe No Swearing project [11], a cooperative effort to help programmers\\nremove unwanted language from their applications. At the time of\\nwriting, the project listed 363 curse words. Overall, we found no\\nstatistical difference in our results nor any significant difference in\\nour figures. Here, we present results with swear words.\\n\\n*3.0.3* *Linguistic Features.* One of the most relevant aspects of our\\nanalysis is directly derived from controlling for linguistic and grammatical features from the available utterances. Thus, our research\\nfocuses on word classes, or Part-of-Speech (PoS), (e.g., Verb, Noun,\\nAdjective, etc.) and language dimensions (e.g., Anger, Hate, etc.).\\nThe PoS features consider the function of each word in the sen\\ntence. The word *smile* can be considered a verb; however, it can\\nalso be considered an adjective when used in certain scenarios, as\\nin “ *The smiling baby is really cute* ”. This information can help us understand the sentence’s composition regarding word classes. Only\\non TwitterAEE is it that we have manual PoS features. For such\\n\\na reason and for fair comparisons, we use automatic PoS features\\nin all datasets. Thus, to classify the tokens according to their PoS\\ncategories, we employ a black-box model [12] . We also point out that\\nsimilar results arise when using manual PoS tags on TwitterAAE.\\nTo define linguistic features, we used the Linguistic Inquiry and\\nWord Count (LIWC) software [ 49 ] in its 2015 release. LIWC is\\na research effort that maps words to psychological features (i.e.,\\nlanguage dimensions) of speech. A single word may be assigned\\nto as many suitable categories as necessary. For example, the word\\n*cried* is a 10-categories term (e.g, Affect, Positive Tone, Emotion,\\nNegative Emotion, Sad Emotion, Verbs, Past Focus, etc.). Features\\nare computed based on the number of occurrences.\\n### **4 RESULTS**\\n\\nWe now present our results. Initially, we compare the model scores\\nfor utterances with and without AAE expressions. Next, we determine which factors impact the outcome of different methods. Our\\nfinal analysis compares semantically similar sentences.\\n### **4.1 Scores Per Usage of AAE Expressions**\\n\\nFigure 1a compares toxicity/sentiment scores for utterances with\\nand without AEE expressions. The figure shows the complementary\\ncumulative distributions (CCDF) considering the number of AAE\\nexpressions on the utterance. That is, the x-axis of the figure shows\\nthe score, whereas the y-axis captures the fraction of sentences for\\nthat with scores greater than the one on the x-axis. In this and the\\nother CDFs presented in our study, some care must be taken when\\ninterpreting results from Textblob and Vader. For these methods,\\nnegative values point toward negative sentiment, whereas positive\\nvalues point toward positive sentiment. For the other models, 0\\ncommonly indicates a not-negative (Vader) sentiment or non-toxic,\\nwhereas 1 is a negative sentiment or toxic utterance. This difference comes from sentiment analysis methods commonly measure\\n*polarity* from -1 to 1. Unlike Textblob and Flair, Vader returns the\\nprobability of a negative, positive, or neutral score (adding up to\\none). Here, we focus on the negative probability.\\n\\n11 https://www.noswearing.com/\\n12 https://spacy.io\\n\\n\\n6\\n\\n\\n-----\\n\\nA Comprehensive View of Biases Towards Utterances with African American English Expressions WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\n|Col1|Col2|Flair|Ma (ML)|ajority of uttr|\\n|---|---|---|---|---|\\n||||ar eM pa|ojosritiitvye of uttr|\\n||Negativ|e Sent.|Positive|Sent.|\\n||||||\\n||ar eM naejograittiyv eo|f uttr|||\\n\\n\\n1.0 0.5 0.0 0.5 1.0\\nx - Polarity\\n\\n|Col1|Col2|Flair|Ma (ML)|ajority of uttr|\\n|---|---|---|---|---|\\n||||ar eM pa|ojosritiitvye of uttr|\\n||||||\\n||Negativ|e Sent.|Positive|Sent.|\\n||||||\\n||ar eM naejograittiyv eo|f uttr|||\\n\\n\\n\\n1.0 0.5 0.0 0.5 1.0\\nx - Polarity\\n\\n|Col1|Col2|Flair|Ma (ML)|ajority of uttr|\\n|---|---|---|---|---|\\n||||ar eM pa|ojosritiitvye of uttr|\\n||||||\\n||Negativ|e Sent.|Positive|Sent.|\\n||||||\\n||ar eM naejograittiyv eo|f uttr|||\\n\\n\\n\\n1.0 0.5 0.0 0.5 1.0\\nx - Polarity\\n\\n|0|Col2|Textblob|Ma b (L)|ajority of uttr|\\n|---|---|---|---|---|\\n|8 6 4 2 0|||ar eM pa|ojosritiitvye of uttr|\\n||Negativ|e Sent.|Positive|Sent.|\\n||||||\\n||ar eM naejograittiyv eo|f uttr|||\\n\\n|Col1|Col2|Col3|Col4|ar eM na|ejogr.ity of uttr|\\n|---|---|---|---|---|---|\\n||Bpb|reiaefoss rem en ot0r .e 20||0 A 1 or|AE Expr. more|\\n||p|rob.||3 or 5 or|more more|\\n|||||||\\n|||||||\\n\\n|Col1|Col2|Col3|ha Mvea|hjoigrihty s ocof ruetstr.|Col6|\\n|---|---|---|---|---|---|\\n||||CM20uu2tr- 3aolfifk suu|mgagre sette adl .by:||\\n|||||||\\n|||||||\\n\\n|Col1|Col2|Col3|ha Mvea|hjoigrihty s ocof ruetstr.|\\n|---|---|---|---|---|\\n||||CM20uu2tr- 3aolfifk suu|mgagre sette adl .by:|\\n||||||\\n||||||\\n\\n|Col1|Col2|Col3|ha Mvea|hjoigrihty s ocof ruetstr.|\\n|---|---|---|---|---|\\n||||CM20uu2tr- 3aolfifk suu|mgagre sette adl .by:|\\n||||||\\n||||||\\n\\n|0|Col2|Textblob|Ma b (L)|ajority of uttr|\\n|---|---|---|---|---|\\n|8 6 4 2 0|||ar eM pa|ojosritiitvye of uttr|\\n||||||\\n||Negativ|e Sent.|Positive|Sent.|\\n||||||\\n||ar eM naejograittiyv eo|f uttr|||\\n\\n|Col1|Col2|Col3|ar eM na|ejogr.ity of uttr|\\n|---|---|---|---|---|\\n||B|ias more|||\\n||||0 A|AE Expr.|\\n||pbp|rere ofos bre .en t0.20|1 or 3 or 5 or|more more more|\\n||||7 or|more|\\n||||||\\n||||||\\n\\n|Col1|Col2|Col3|ha Mvea|hjoigrihty s ocof ruetstr.|\\n|---|---|---|---|---|\\n||||Cut-off su|ggested by:|\\n||||M20u2r3aliku|mar et al.|\\n||||||\\n||||||\\n\\n|Col1|Col2|Col3|ha Mvea|hjoigrihty s ocof ruetstr.|\\n|---|---|---|---|---|\\n||||Cut-off su|ggested by:|\\n||||M20u2r3aliku|mar et al.|\\n||||||\\n||||||\\n\\n|Col1|Col2|Col3|ha Mvea|hjoigrihty s ocof ruetstr.|\\n|---|---|---|---|---|\\n||||Cut-off su|ggested by:|\\n||||M20u2r3aliku|mar et al.|\\n||||||\\n||||||\\n\\n|0|Col2|Textblob|Ma b (L)|ajority of uttr|\\n|---|---|---|---|---|\\n|8 6 4 2 0|||ar eM pa|ojosritiitvye of uttr|\\n||||||\\n||Negativ|e Sent.|Positive|Sent.|\\n||||||\\n||ar eM naejograittiyv eo|f uttr|||\\n\\n|Col1|Col2|Col3|ar eM na|ejogr.ity of uttr|\\n|---|---|---|---|---|\\n||B|ias more|||\\n||||0 A|AE Expr.|\\n||pbp|rere ofos bre .en t0.20|1 or 3 or 5 or|more more more|\\n||||7 or|more|\\n||||||\\n||||||\\n\\n|Col1|Col2|Col3|ha Mvea|hjoigrihty s ocof ruetstr.|\\n|---|---|---|---|---|\\n||||Cut-off su|ggested by:|\\n||||M20u2r3aliku|mar et al.|\\n||||||\\n||||||\\n\\n|Col1|Col2|Col3|ha Mvea|hjoigrihty s ocof ruetstr.|\\n|---|---|---|---|---|\\n||||Cut-off su|ggested by:|\\n||||M20u2r3aliku|mar et al.|\\n||||||\\n||||||\\n\\n|Col1|Col2|Col3|ha Mvea|hjoigrihty s ocof ruetstr.|\\n|---|---|---|---|---|\\n||||Cut-off su|ggested by:|\\n||||M20u2r3aliku|mar et al.|\\n||||||\\n||||||\\n\\n\\n0.00 0.25 0.50 0.75 1.00\\nx - Neg. Probability\\n\\n\\nVader (L)\\n\\n\\n0.00 0.25 0.50 0.75 1.00\\nx - Toxic Probability\\n\\n\\nPerspective (ML)\\n\\n\\nDetoxify (ML)\\n\\n\\nDetoxify Unbiased (ML)\\n\\n\\n1.0\\n\\n0.8\\n\\n0.6\\n\\n0.4\\n\\n0.2\\n\\n0.0\\n\\n1.0\\n\\n0.8\\n\\n0.6\\n\\n0.4\\n\\n0.2\\n\\n0.0\\n\\n1.0\\n\\n0.8\\n\\n0.6\\n\\n0.4\\n\\n0.2\\n\\n0.0\\n\\n\\n1.0 0.5 0.0 0.5 1.0\\nx - Polarity\\n\\n1.0 0.5 0.0 0.5 1.0\\nx - Polarity\\n\\n1.0 0.5 0.0 0.5 1.0\\nx - Polarity\\n\\n\\n\\n\\n0.00 0.25 0.50 0.75 1.00\\nx - Neg. Probability\\n\\n\\n0.00 0.25 0.50 0.75 1.00\\nx - Toxic Probability\\n\\n\\n0.00 0.25 0.50 0.75 1.00\\nx - Toxic Probability\\n\\n\\n0.00 0.25 0.50 0.75 1.00\\nx - Toxic Probability\\n\\n\\n**(a) Twitter**\\n\\nVader (L)\\n\\n\\nPerspective (ML)\\n\\n\\nDetoxify (ML)\\n\\n\\nDetoxify Unbiased (ML)\\n\\n\\n\\n\\n0.00 0.25 0.50 0.75 1.00\\nx - Neg. Probability\\n\\n\\n0.00 0.25 0.50 0.75 1.00\\nx - Toxic Probability\\n\\n\\n0.00 0.25 0.50 0.75 1.00\\nx - Toxic Probability\\n\\n\\n0.00 0.25 0.50 0.75 1.00\\nx - Toxic Probability\\n\\n\\n**(b) YouTube**\\n\\nVader (L)\\n\\n\\nPerspective (ML)\\n\\n\\nDetoxify (ML)\\n\\n\\nDetoxify Unbiased (ML)\\n\\n\\n\\n\\n0.00 0.25 0.50 0.75 1.00\\nx - Toxic Probability\\n\\n\\n0.00 0.25 0.50 0.75 1.00\\nx - Toxic Probability\\n\\n\\n**(c) CORAAL/Buckeye**\\n\\n**Figure 1: Score distributions for sentences with and without AAE Expressions**\\n\\n\\nWe can initially see that utterances with AAE expressions receive\\nmuch higher scores for Twitter and toxicity models (Perspective,\\nDetoxify, and Detoxify Unbiased). There is a clear tendency for\\nstatistical dominance – the CDF, or y-value, of utterances with AAE\\nexpressions is above the one without AAE expressions regardless of\\nx-values. This behavior is attenuated as more AAE expressions are\\nconsidered in the utterance (we considered those with at least one,\\nthree, five, or seven expressions). For instance, considering Toxicity\\nmodels on Twitter, the highest 20% scoring utterances *without* AAE\\nexpressions achieve a minimum of 0.25. For utterances *with at least*\\n*one* AAE expression, this minimum is around 0.55.\\nUsing a Kolmogorov-Smirnov test, we compared whether each\\nCDF with AAE expressions differs from those without. Under *𝑝* *<*\\n0 *.* 01, Flair did not show this difference on the Twitter dataset when\\nconsidering utterances with at least one expression. This is the\\n**only** case where we failed to reject the null hypothesis. When we\\nconsider lexical models, Textblob and Vader, it appears they present\\na mild bias on lower-scoring sentences. To help understand this\\nissue, focus on the sentences with scores below 0.5 for Textblob and\\n0.20 for Vader. Whether this is an issue will depend on the cut-off\\ndevelopers employ (e.g., ±0 *.* 5 appears to mitigate the issue).\\nFrom Figures 1b and 1c, we can see the same trend that occurred\\non Twitter for the lexical approaches (Textblob and Vader), also occurs on YouTube and CORAAL/Buckeye. This finding likely stems\\nfrom the fact that such approaches employ manually curated rules\\nthat do not consider AAE, a positive aspect of these approaches. On\\nthese datasets, Flair is biased, achieving lower scores (polarity or\\ntendency to rate as more negative) for utterances with AAE expressions. The ML models still present biases on YouTube (Figure 1b).\\n\\n\\nHowever, it is important to point out two facts: (1) albeit statistically significant, this bias is negligible on Detoxify for YouTube; (2)\\nsuch bias is less present when considering the suggested cut-off\\nof 0.55 [ 45 ] for toxicity models. Biases **may not** be an issue when\\nusing large cut-off values. To further investigate biases, we now\\ncorrelate scores with linguistic features.\\n### **4.2 Impact of Grammatical/Linguistic Features**\\n\\nIn Table 3, we present our Logistic regression results for Twitter.\\nFor each tweet, we counted the number of AAE expressions, LIWC\\ncategories, and PoS tags as features. Being counts, all of these features have positive values only. To present regression coefficients\\non a similar, features were *Min-Max* scaled to the [ 0 *,* 1 ] range before\\nthe regression was executed. Models were executed with an intercept variable and no regularization. Models that output polarity\\nhad such polarity values re-scaled to [ 0 *,* 1 ] also (by adding one and\\ndividing by two). The table presents only the statistically significant\\nfeatures ( *𝑝* *<* 0 *.* 05). LIWC features are identified by LIWC_, and PoS\\ntags by POS_ . The demographic variable (AAE tweet or non-AAE\\ntweet) was used as a Twitter and CORAAL/Buckeye feature.\\nSimilar results are found for the other two datasets. For simplicity,\\ninstead of presenting the tables with such results, we shall discuss\\nthese results throughout the text. We also note similar results when\\nusing the manual POS tags for TwitterAAE (omitted for space).\\nOur feature of most interest is the AAE_EXPR (the number of AAE\\nexpressions on the utterance). The other features act as control\\nvariables to ensure that, on some level, such expressions are not\\nbeing confounded with other grammatical/linguistic attributes.\\n\\n\\n7\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Resende et al.\\n\\n**Table 3: Logistic Regression coefficients for the Twitter dataset with** *𝑝* *<* 0 *.* 05 **. Each model’s five most relevant coefficients are**\\n\\n|Features Textblob (L) Flair (ML), < 0, - > 0|Vader (L) Perspective (ML) Detoxify (ML) Detoxify U. (ML), > 0, - < 0|\\n|---|---|\\n|AAE_EXPR|0.0934 0.2238 0.1779|\\n|LIWC_SWEAR LIWC_SEXUAL -0.3354 -1.3448 LIWC_NETSPEAK -2.6379 LIWC_INFORMAL 2.5228 LIWC_NEGATE -0.8331 LIWC_FILLER -2.1683 LIWC_ASSENT LIWC_MALE -0.3237|0.8567 0.9492 1.2792 0.4657 0.5942 0.5609 -0.4496 -0.8121 -0.9239 0.4386 0.7988 0.946 0.2075 0.1855 -0.6053 -0.6377 -0.1599 -0.2614 -0.1985 -0.1505 -0.1968 -0.191|\\n|POS_X 0.5655 POS_DET|-0.1636 -0.3857 -0.4255 -0.3066 0.1623 0.3369 0.3268|\\n|DEMOGRAPHIC|0.0508 0.1262 0.0762|\\n\\n**presented in bold, whereas not statistically significant coefficients were omitted. When a coefficient pushes towards a negative**\\n\\n**sentiment or toxic score, we color it red (** � **) . Positive sentiment and non-toxic score is colored green (** � **) .**\\n\\n\\nFrom the table, we can see that this feature pushes the polarity\\nof the Vader model towards having fewer sentiments. The feature is not significant for the other sentiment analysis models.\\nNevertheless, this feature is significant for Perspective and Detoxify (but not for Detoxify Unbiased). When we consider YouTube,\\nAAE_EXPR is statistically significant for Vader ~~(~~ 0.0617 ), Perspective ~~(~~ 0.2488 ), and – *suprisingly* – Detoxify Unbiased ( 0.1334 ). On\\nCORALL/Buckeye, it was significant for Flair ~~(~~ -0.9432, Perspective\\n( 0.3291 ), Detoxify ( 0.1754 ), and Detoxify Unbiased ( 0.2126 ).\\nFor Twitter and CORAAL/Buckeye, our demographic variable\\n( DEMOGRAPHIC ) was used as a categorical feature. This feature was\\nstatistically significant for Twitter and not CORAAL/Buckeye. Twitter is the only dataset where the demographic variable was developed to align with AAE utterances. On CORAAL/Buckeye, an\\nAfrican-American may not employ AAE, or a Caucasian may employ AAE (in fact, the usage of expressions is comparable in Table 2).\\nConsidering the other features, some linguistic features are expected to push models towards negative sentiment or toxic scores\\n(this is the case for the feature LIWC_SWEAR in every dataset). Finally, PoS features were only statistically significant for Twitter:\\nquantifiers, POS_DET, and the unknown/other tag, POS_X.\\n### **4.3 Semantic Comparison**\\n\\nWe now seek to answer the following question: *How do highly*\\n*semantically similar pairs of utterances that achieve diverging scores*\\n*differ in their usage of AAE expressions?* Notice that we have two\\nconditions here: (1) being similar in meaning and (2) achieving\\ndiverging scores. Such pairs of utterances are interesting because\\nthey control for confounding factors in semantics.\\nWe employ a large language model as our semantic feature extractor [ 62 ]. With this model, utterances are mapped to an embedding vector. We compare these vectors using a cosine similarity\\nfor pairs of utterances: -1 indicates entirely dissimilar, 0 indicates\\na lack of relationship, and 1 shows completely similar. We deem\\ntwo sentences similar when the cosine score is above 0.5 (this is\\nless than 0.1% of pairs as discussed below).\\nFor each dataset and sentiment/toxicity method of our study,\\nwe isolated the top 2.5% and bottom 2.5% scoring utterances. To\\nperform a single analysis, we standardized scores. For Perspective\\n\\n\\nand Detoxify, the top 2.5% have a higher chance of being toxic,\\nwhereas for Vader, Flair, and Textblob, the top 2.5% have negative\\npolarity. Due to memory constraints sampled 100,000 of such pairs\\nper dataset and method. Next, we focused only on pairs where one\\nof the sentences had at least one AAE expression. If this is not the\\ncase, the difference in score certainly is not due to the number of\\nAAE expressions employed. When this is the case, the usage of AAE\\nexpressions may be the underlying cause. Due to the small sample\\nsize, we did not find any pairs on Twitter that met our conditions.\\nCombining every method, we found 585,679 unique pairs on\\nYouTube with diverging scores (our top 2.5% versus the bottom\\n2.5%). Out of these, 568 had a cosine similarity above 0.5%. On\\nCORAAL/Buckeye, we found 592,606 unique pairs from our diverging scores filter, with 243 being highly similar. For each setting\\n(YouTube or CORAAL/Buckeyey), we computed the number of\\npairs where ( *𝑑* *𝑏𝑖𝑎𝑠* ) the most toxic or most negative had more AAE\\nexpressions; ( *𝑑* *𝑒𝑞* ) both had the same amount of expressions, and\\n( *𝑑* *𝑛𝑜* _ *𝑏𝑖𝑎𝑠* ) the least toxic or most positive had more expressions.\\nFor YouTube, we have that: *𝑑* *𝑏𝑖𝑎𝑠* = 272 ( 48% ), *𝑑* *𝑒𝑞* = 87 ( 15% ),\\nand *𝑑* *𝑛𝑜* _ *𝑏𝑖𝑎𝑠* = 209 ( 37% ) . Whereas on CORAAL/Buckeye: *𝑑* *𝑏𝑖𝑎𝑠* =\\n187 ( 77% ), *𝑑* *𝑒𝑞* = 26 ( 11% ), and *𝑑* *𝑛𝑜* _ *𝑏𝑖𝑎𝑠* = 30 ( 12% ) . Under a Binomial test and *𝑝* *<* 0 *.* 01, in both cases, we find that *𝑑* *𝑏𝑖𝑎𝑠* *> 𝑑* *𝑛𝑜* _ *𝑏𝑖𝑎𝑠* .\\nThus, results show statistical evidence of bias [13] .\\n### **5 CONCLUSIONS AND LIMITATIONS**\\n\\nThis paper investigates the biases of sentiment analysis/toxicity\\nmethods regarding the usage of AAE expressions. We analyzed the\\nperformance of six well-known off-the-shelf methods in light of\\nfour different datasets. Our datasets ranged from online texts from\\nTwitter to single-speaker closed captions from YouTube and spoken\\nEnglish encompassing daily live situations.\\nConsidering the latter, or nonexistent, introduction of AAE in\\nML datasets, the under-representation of such expressions leads ML\\nmodels to present a systemic bias towards AAE. We argue that the\\nbiggest problems derive directly from the absence of context in the\\nutterances. Since they employ human-crafted rules, lexical-based\\n(rule) approaches tend to be less biased than ML models.\\n\\n13 *𝑑* *𝑒𝑞* is not considered as the counts of AAE expressions usage are certainly not the\\nissue in these settings (they are equal)\\n\\n\\n8\\n\\n\\n-----\\n\\nA Comprehensive View of Biases Towards Utterances with African American English Expressions WebMedia’2024, Juiz de Fora, Brazil\\n\\n### **ETHICAL CONSIDERATIONS**\\n\\n**Ethical Concerns:** One of the ethical concerns of our study comes\\nfrom using demographic variables related to race **without** groundtruth self-identification labels for speakers. To mitigate this issue,\\nwe refrained from using author-inferred demographic variables in\\nour study (on the YouTube dataset). CORAAL/Buckeye are wellestablished in linguistics, with CORAAL focusing solely on African\\nAmericans. This issue is not present on Twitter, as labels come from\\nusing AAE or not (regardless of race). We also point out that our\\nmain statistical variable of study is not race. We focus on the usage\\nof AAE expressions, where such expressions came from reliable and\\nsuggested sources (by the organizers of a well-known dictionary).\\n**Unintended Impact:** Readers may interpret our research as\\nagainst ML models or automatic utterance scoring tools. We point\\nout that this is **not** our statement. Our research advances both re\\ncent and large literature on the unintended biases of Lexical/AI/ML\\nmodels. We hope our findings will improve how such tools are used;\\nmodel advances towards fewer biases or both.\\n\\n**Researcher Background:** The majority of authors of this study\\nare from a region where racial discrimination is still very present\\nin the population’s day-to-day lives. Our research aims to foster\\nthe ongoing discussion on how AI impacts the lives of different\\nhistorically segregated communities.\\nIf readers deem any terms or expressions used in this paper\\noffensive, we point out that it was not deliberate.\\n### **REFERENCES**\\n\\n[1] Abubakar Abid, Maheen Farooqi, and James Zou. 2021. Large language models\\nassociate Muslims with violence. *Nature Machine Intelligence* 3, 6 (2021), 461–463.\\n\\n[2] CJ Adams. 2018. New York Times: Using AI to host better conversations. https://blog.google/technology/ai/new-york-times-using-ai-host-betterconversations/.\\n\\n[3] Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, and\\nRoland Vollgraf. 2019. FLAIR: An easy-to-use framework for state-of-the-art\\nNLP. In *NAACL 2019, 2019 Annual Conference of the North American Chapter of*\\n*the Association for Computational Linguistics (Demonstrations)* . 54–59.\\n\\n[4] Marzieh Babaeianjelodar, Stephen Lorenz, Josh Gordon, Jeanna Matthews, and\\nEvan Freitag. 2020. Quantifying gender bias in different corpora. In *Companion*\\n*Proceedings of the Web Conference 2020* . 752–759.\\n\\n[5] Arnetha F Ball. 1992. Cultural preference and the expository writing of AfricanAmerican adolescents. *Written Communication* 9, 4 (1992), 501–532.\\n\\n[6] Ari Ball-Burack, Michelle Seng Ah Lee, Jennifer Cobbe, and Jatinder Singh. 2021.\\nDifferential tweetment: Mitigating racial dialect bias in harmful tweet detection. In *Proceedings of the 2021 ACM Conference on Fairness, Accountability, and*\\n*Transparency* . 116–128.\\n\\n[7] David Bamman, Chris Dyer, and Noah A Smith. 2014. Distributed representations\\nof geographically situated language. In *Proceedings of the 52nd Annual Meeting of*\\n*the Association for Computational Linguistics (Volume 2: Short Papers)* . 828–834.\\n\\n[8] John Baugh. 1981. Runnin’down Some Lines: The Language and Culture of Black\\nTeenagers.\\n\\n[9] Su Lin Blodgett, Lisa Green, and Brendan O’Connor. 2016. Demographic dialectal\\nvariation in social media: A case study of African-American English. *arXiv*\\n*preprint arXiv:1608.08868* (2016).\\n\\n[10] Su Lin Blodgett, Johnny Wei, and Brendan O’Connor. 2018. Twitter universal\\ndependency parsing for African-American and mainstream American English.\\nIn *Proceedings of the 56th Annual Meeting of the Association for Computational*\\n*Linguistics (Volume 1: Long Papers)* . 1415–1425.\\n\\n[11] Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T\\nKalai. 2016. Man is to computer programmer as woman is to homemaker?\\ndebiasing word embeddings. *Advances in neural information processing systems*\\n29 (2016).\\n\\n[12] Zhenpeng Chen, Jie M Zhang, Max Hort, Federica Sarro, and Mark Harman. 2022.\\nFairness testing: A comprehensive survey and analysis of trends. *arXiv preprint*\\n*arXiv:2207.10223* (2022).\\n\\n[13] Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017.\\nAutomated hate speech detection and the problem of offensive language. In\\n*Proceedings of the international AAAI conference on web and social media*, Vol. 11.\\n\\n\\n512–515.\\n\\n[14] Mark Díaz, Isaac Johnson, Amanda Lazar, Anne Marie Piper, and Darren Gergle.\\n2018. Addressing age-related bias in sentiment analysis. In *Proceedings of the*\\n*2018 chi conference on human factors in computing systems* . 1–14.\\n\\n[15] Joey Lee Dillard. 1977. *Lexicon of Black English.* ERIC.\\n\\n[16] Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. 2018.\\nMeasuring and mitigating unintended bias in text classification. In *Proceedings of*\\n*the 2018 AAAI/ACM Conference on AI, Ethics, and Society* . 67–73.\\n\\n[17] Jacob Eisenstein, Brendan O’Connor, Noah A Smith, and Eric P Xing. 2014.\\nDiffusion of lexical change in social media. *PloS one* 9, 11 (2014), e113114.\\n\\n[18] Anjalie Field, Su Lin Blodgett, Zeerak Waseem, and Yulia Tsvetkov. 2021. A\\nSurvey of Race, Racism, and Anti-Racism in NLP. In *Proceedings of the 59th*\\n*Annual Meeting of the Association for Computational Linguistics and the 11th*\\n*International Joint Conference on Natural Language Processing (Volume 1: Long*\\n*Papers)* . 1905–1925.\\n\\n[19] Sarah Florini. 2014. Tweets, Tweeps, and Signifyin’ Communication and Cultural\\nPerformance on “Black Twitter”. *Television & New Media* 15, 3 (2014), 223–237.\\n\\n[20] Patricia Friedrich. 2020. When Englishes go digital. *World Englishes* 39, 1 (2020),\\n67–78.\\n\\n[21] Patricia Friedrich and Eduardo Diniz de Figueiredo. 2016. *The sociolinguistics of*\\n*digital Englishes* . Routledge.\\n\\n[22] David Garcia, Ingmar Weber, and Venkata Rama Kiran Garimella. 2014. Gender\\nasymmetries in reality and fiction: The bechdel test of social media. In *Eighth*\\n*International AAAI Conference on Weblogs and Social Media* .\\n\\n[23] Anastasia Giachanou and Fabio Crestani. 2016. Like it or not: A survey of twitter\\nsentiment analysis methods. *ACM Computing Surveys (CSUR)* 49, 2 (2016), 1–41.\\n\\n[24] Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter sentiment classification\\nusing distant supervision. *CS224N project report, Stanford* 1, 12 (2009), 2009.\\n\\n[25] A Gomes, D Antonialli, and T Dias-Oliva. 2019. Drag queens and artificial\\nintelligence. Should computers decide what is toxic on the internet. *Internet Lab*\\n*blog* (2019).\\n\\n[26] Hila Gonen and Yoav Goldberg. 2019. Lipstick on a pig: Debiasing methods cover\\nup systematic gender biases in word embeddings but do not remove them. *arXiv*\\n*preprint arXiv:1903.03862* (2019).\\n\\n[27] Mark Graham, Bernie Hogan, Ralph K Straumann, and Ahmed Medhat. 2014.\\nUneven geographies of user-generated information: Patterns of increasing informational poverty. *Annals of the Association of American Geographers* 104, 4\\n(2014), 746–764.\\n\\n[28] Lisa J Green. 2002. *African American English: a linguistic introduction* . Cambridge\\nUniversity Press.\\n\\n[29] Tommi Gröndahl, Luca Pajola, Mika Juuti, Mauro Conti, and N Asokan. 2018. All\\nyou need is\" love\" evading hate speech detection. In *Proceedings of the 11th ACM*\\n*workshop on artificial intelligence and security* . 2–12.\\n\\n[30] Laura Hanu and Unitary team. 2020. Detoxify. Github.\\nhttps://github.com/unitaryai/detoxify.\\n\\n[31] Camille Harris, Matan Halevy, Ayanna Howard, Amy Bruckman, and Diyi Yang.\\n2022. Exploring the role of grammar and word choice in bias toward african\\namerican english (aae) in hate speech classification. In *2022 ACM Conference on*\\n*Fairness, Accountability, and Transparency* . 789–798.\\n\\n[32] Hossein Hosseini, Sreeram Kannan, Baosen Zhang, and Radha Poovendran. 2017.\\nDeceiving google’s perspective api built for detecting toxic comments. *arXiv*\\n*preprint arXiv:1702.08138* (2017).\\n\\n[33] Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews.\\nIn *Proceedings of the tenth ACM SIGKDD international conference on Knowledge*\\n*discovery and data mining* . 168–177.\\n\\n[34] Ben Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu\\nZhong, and Stephen Denuyl. 2020. Unintended machine learning biases as\\nsocial barriers for persons with disabilitiess. *ACM SIGACCESS Accessibility and*\\n*Computing* (2020), 1–1.\\n\\n[35] Clayton Hutto and Eric Gilbert. 2014. Vader: A parsimonious rule-based model\\nfor sentiment analysis of social media text. In *Proceedings of the international*\\n*AAAI conference on web and social media*, Vol. 8. 216–225.\\n\\n[36] Sen Jia, Thomas Lansdall-Welfare, and Nello Cristianini. 2015. Measuring gender\\nbias in news images. In *Proceedings of the 24th International Conference on World*\\n*Wide Web* . 893–898.\\n\\n[37] Jigsaw. [n. d.]. Perspective API. https://perspectiveapi.com/. Accessed: 2023-0130.\\n\\n[38] Tyler Kendall and Charlie Farrington. 2021. The Corpus of Regional African\\nAmerican Language (Version 2021.07). Eugene, OR: The Online Resources for\\nAfrican American Language Project.\\n\\n[39] Svetlana Kiritchenko and Saif M Mohammad. 2018. Examining Gender and Race\\nBias in Two Hundred Sentiment Analysis Systems. *NAACL HLT 2018* (2018), 43.\\n\\n[40] Animesh Koratana and Kevin Hu. 2018. Toxic speech detection. *URL: https://web.*\\n*stanford. edu/class/archive/cs/cs224n/cs224n* 1194 (2018).\\n\\n[41] Deepak Kumar, Patrick Gage Kelley, Sunny Consolvo, Joshua Mason, Elie\\nBursztein, Zakir Durumeric, Kurt Thomas, and Michael Bailey. 2021. Designing\\nToxic Content Classification for a Diversity of Perspectives.. In *SOUPS@ USENIX*\\n*Security Symposium* . 299–318.\\n\\n\\n9\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Resende et al.\\n\\n\\n\\n[42] Steven Loria. 2018. textblob Documentation. *Release 0.15* 2 (2018).\\n\\n[43] Patricia Georgiou Marie Pellat. 2018. Perspective Launches In Spanish With\\nEl País. https://medium.com/jigsaw/perspective-launches-in-spanish-with-elpa%C3%ADs-dc2385d734b2.\\n\\n[44] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.\\nDistributed representations of words and phrases and their compositionality. In\\n*Advances in neural information processing systems* . 3111–3119.\\n\\n[45] Meena Devii Muralikumar, Yun Shan Yang, and David W McDonald. 2023. A\\nHuman-Centered Evaluation of a Toxicity Detection API: Testing Transferability\\nand Unpacking Latent Attributes. *ACM Transactions on Social Computing* (2023).\\n\\n[46] Lisa Nakamura. 2013. *Cybertypes: Race, ethnicity, and identity on the Internet* .\\nRoutledge.\\n\\n[47] Nikolaos Pappas, Georgios Katsimpras, and Efstathios Stamatatos. 2013. Distinguishing the popularity between topics: a system for up-to-date opinion retrieval\\nand mining in the web. In *Computational Linguistics and Intelligent Text Process-*\\n*ing: 14th International Conference, CICLing 2013, Samos, Greece, March 24-30, 2013,*\\n*Proceedings, Part II 14* . Springer, 197–209.\\n\\n[48] Daniel Borkan Patricia Georgiou, Marie Pellat. 2019. Parlons-en! Perspective and\\nTune are now available in French. https://medium.com/jigsaw/perspective-tuneare-now-available-in-french-c4cf1ca198f2.\\n\\n[49] James W Pennebaker, Martha E Francis, and Roger J Booth. 2001. Linguistic\\ninquiry and word count: LIWC 2001. *Mahway: Lawrence Erlbaum Associates* 71,\\n2001 (2001), 2001.\\n\\n[50] Mark A Pitt, Keith Johnson, Elizabeth Hume, Scott Kiesling, and William Raymond. 2005. The Buckeye corpus of conversational speech: Labeling conventions\\nand a test of transcriber reliability. *Speech Communication* 45, 1 (2005), 89–95.\\n\\n[51] Filipe N Ribeiro, Matheus Araújo, Pollyanna Gonçalves, Marcos André Gonçalves,\\nand Fabrício Benevenuto. 2016. Sentibench-a benchmark comparison of state-ofthe-practice sentiment analysis methods. *EPJ Data Science* 5, 1 (2016), 1–29.\\n\\n[52] Max Roser, Hannah Ritchie, and Esteban Ortiz-Ospina. 2015. Internet. *Our World*\\n*in Data* (2015). https://ourworldindata.org/internet.\\n\\n[53] Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, and Noah A Smith. 2019.\\nThe risk of racial bias in hate speech detection. In *Proceedings of the 57th annual*\\n*meeting of the association for computational linguistics* . 1668–1678.\\n\\n[54] Geneva Smitherman. 2000. *Black talk: Words and phrases from the hood to the*\\n*amen corner* . Houghton Mifflin Harcourt.\\n\\n[55] Kaikai Song, Ting Yao, Qiang Ling, and Tao Mei. 2018. Boosting image sentiment\\nanalysis with visual attention. *Neurocomputing* 312 (2018), 218–228.\\n\\n\\n\\n[56] Ezekiel Soremekun, Sakshi Udeshi, and Sudipta Chattopadhyay. 2022. Astraea:\\nGrammar-based fairness testing. *IEEE Transactions on Software Engineering* 48,\\n12 (2022), 5188–5211.\\n\\n[57] Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly Voll, and Manfred Stede.\\n2011. Lexicon-based methods for sentiment analysis. *Computational linguistics*\\n37, 2 (2011), 267–307.\\n\\n[58] Rachael Tatman. 2017. Gender and dialect bias in YouTube’s automatic captions.\\nIn *Proceedings of the first ACL workshop on ethics in natural language processing* .\\n53–59.\\n\\n[59] Mike Thelwall. 2014. Heart and soul: Sentiment strength detection in the social\\nweb with sentistrength, 2017. *Cyberemotions: Collective emotions in cyberspace*\\n(2014).\\n\\n[60] Pranav Narayanan Venkit and Shomir Wilson. 2021. Identification of bias against\\npeople with disabilities in sentiment analysis and toxicity detection models. *arXiv*\\n*preprint arXiv:2111.13259* (2021).\\n\\n[61] Hao Wang, Doğan Can, Abe Kazemzadeh, François Bar, and Shrikanth Narayanan.\\n2012. A system for real-time twitter sentiment analysis of 2012 us presidential\\nelection cycle. In *Proceedings of the ACL 2012 system demonstrations* . 115–120.\\n\\n[62] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou.\\n2020. Minilm: Deep self-attention distillation for task-agnostic compression of\\npre-trained transformers. *Advances in Neural Information Processing Systems* 33\\n(2020), 5776–5788.\\n\\n[63] Maciej Widawski. 2015. *African American slang: A linguistic description* . Cambridge University Press.\\n\\n[64] Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Jason Kessler, Janyce\\nWiebe, Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.\\nOpinionFinder: A system for subjectivity analysis. In *Proceedings of HLT/EMNLP*\\n*2005 Interactive Demonstrations* . 34–35.\\n\\n[65] Austin P Wright, Omar Shaikh, Haekyu Park, Will Epperson, Muhammed Ahmed,\\nStephane Pinel, Duen Horng Chau, and Diyi Yang. 2021. RECAST: Enabling\\nuser recourse and interpretability of toxicity detection models with interactive\\nvisualization. *Proceedings of the ACM on Human-Computer Interaction* 5, CSCW1\\n(2021), 1–26.\\n\\n[66] Ali Yadollahi, Ameneh Gholipour Shahraki, and Osmar R Zaiane. 2017. Current\\nstate of text sentiment analysis from opinion to emotion mining. *ACM Computing*\\n*Surveys (CSUR)* 50, 2 (2017), 1–33.\\n\\n[67] Min Yang, Qiang Qu, Xiaojun Chen, Chaoxue Guo, Ying Shen, and Kai Lei. 2018.\\nFeature-enhanced attention network for target-dependent sentiment classification. *Neurocomputing* 307 (2018), 91–97.\\n\\n\\n10\\n\\n\\n-----\\n\\n',\n",
       "  'artigo_tokenizado': ['#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'A',\n",
       "   'Comprehensive',\n",
       "   'View',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Biases',\n",
       "   'of',\n",
       "   'Toxicity',\n",
       "   'and',\n",
       "   'Sentiment',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Analysis',\n",
       "   'Methods',\n",
       "   'Towards',\n",
       "   'Utterances',\n",
       "   'with',\n",
       "   'African',\n",
       "   'American',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   'English',\n",
       "   'Expressions',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Guilherme',\n",
       "   'Andrade',\n",
       "   ',',\n",
       "   'Luiz',\n",
       "   'Nery',\n",
       "   ',',\n",
       "   'Fabrício',\n",
       "   'Benevenuto',\n",
       "   ',',\n",
       "   'Flavio',\n",
       "   'Figueiredo',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'UFMG',\n",
       "   'guilherme.hra,luiznery,fabricio,flavio@dcc.ufmg.br',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'ABSTRACT',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Language',\n",
       "   'is',\n",
       "   'a',\n",
       "   'dynamic',\n",
       "   'aspect',\n",
       "   'of',\n",
       "   'our',\n",
       "   'culture',\n",
       "   'that',\n",
       "   'changes',\n",
       "   'when',\n",
       "   '\\n',\n",
       "   'expressed',\n",
       "   'in',\n",
       "   'different',\n",
       "   'technologies',\n",
       "   'and/or',\n",
       "   'communities',\n",
       "   '.',\n",
       "   'On',\n",
       "   'the',\n",
       "   'Internet',\n",
       "   ',',\n",
       "   'social',\n",
       "   'networks',\n",
       "   'have',\n",
       "   'enabled',\n",
       "   'the',\n",
       "   'diffusion',\n",
       "   'and',\n",
       "   'evolution',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'different',\n",
       "   'dialects',\n",
       "   ',',\n",
       "   'including',\n",
       "   'African',\n",
       "   'American',\n",
       "   'English',\n",
       "   '(',\n",
       "   'AAE',\n",
       "   ')',\n",
       "   '.',\n",
       "   'However',\n",
       "   ',',\n",
       "   'this',\n",
       "   'increased',\n",
       "   'usage',\n",
       "   'of',\n",
       "   'different',\n",
       "   'dialects',\n",
       "   'is',\n",
       "   'not',\n",
       "   'without',\n",
       "   'barriers',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'One',\n",
       "   'particular',\n",
       "   'barrier',\n",
       "   ',',\n",
       "   'the',\n",
       "   'focus',\n",
       "   'of',\n",
       "   'this',\n",
       "   'paper',\n",
       "   ',',\n",
       "   'is',\n",
       "   'on',\n",
       "   'how',\n",
       "   'sentiment',\n",
       "   '\\n',\n",
       "   '(',\n",
       "   'Vader',\n",
       "   ',',\n",
       "   'TextBlob',\n",
       "   ',',\n",
       "   'and',\n",
       "   'Flair',\n",
       "   ')',\n",
       "   'and',\n",
       "   'toxicity',\n",
       "   '(',\n",
       "   'Google',\n",
       "   '’s',\n",
       "   'Perspective',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'models',\n",
       "   'from',\n",
       "   'the',\n",
       "   'open',\n",
       "   '-',\n",
       "   'source',\n",
       "   'Detoxify',\n",
       "   ')',\n",
       "   'scoring',\n",
       "   'methods',\n",
       "   'present',\n",
       "   '\\n',\n",
       "   'biases',\n",
       "   'towards',\n",
       "   'utterances',\n",
       "   'with',\n",
       "   'AAE',\n",
       "   'expressions',\n",
       "   '.',\n",
       "   'In',\n",
       "   'particular',\n",
       "   ',',\n",
       "   'AI',\n",
       "   '\\n',\n",
       "   'tools',\n",
       "   'can',\n",
       "   'not',\n",
       "   'understand',\n",
       "   'the',\n",
       "   're',\n",
       "   '-',\n",
       "   'appropriation',\n",
       "   'of',\n",
       "   'the',\n",
       "   'terms',\n",
       "   ',',\n",
       "   'leading',\n",
       "   '\\n',\n",
       "   'to',\n",
       "   'false',\n",
       "   'positive',\n",
       "   'scores',\n",
       "   'and',\n",
       "   'biases',\n",
       "   '.',\n",
       "   'Here',\n",
       "   ',',\n",
       "   'we',\n",
       "   'study',\n",
       "   'the',\n",
       "   'bias',\n",
       "   'of',\n",
       "   'Toxicity',\n",
       "   'and',\n",
       "   'Sentiment',\n",
       "   'Analysis',\n",
       "   'models',\n",
       "   'based',\n",
       "   'on',\n",
       "   'experiments',\n",
       "   'performed',\n",
       "   '\\n',\n",
       "   'on',\n",
       "   'Web',\n",
       "   '-',\n",
       "   'and',\n",
       "   'spoken',\n",
       "   'English',\n",
       "   'datasets',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'KEYWORDS',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'African',\n",
       "   'American',\n",
       "   'English',\n",
       "   ',',\n",
       "   'AAE',\n",
       "   ',',\n",
       "   'Bias',\n",
       "   ',',\n",
       "   'Toxicity',\n",
       "   ',',\n",
       "   'Sentiment',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   '1',\n",
       "   'INTRODUCTION',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   'recent',\n",
       "   'decades',\n",
       "   ',',\n",
       "   'we',\n",
       "   'have',\n",
       "   'witnessed',\n",
       "   'a',\n",
       "   'substantial',\n",
       "   'rise',\n",
       "   'in',\n",
       "   'Internet',\n",
       "   '\\n',\n",
       "   'usage',\n",
       "   '.',\n",
       "   'According',\n",
       "   'to',\n",
       "   '[',\n",
       "   '52',\n",
       "   ']',\n",
       "   ',',\n",
       "   'Internet',\n",
       "   'users',\n",
       "   'increased',\n",
       "   'from',\n",
       "   'approximately',\n",
       "   '400',\n",
       "   'million',\n",
       "   'in',\n",
       "   '2000',\n",
       "   'to',\n",
       "   '4.7',\n",
       "   'billion',\n",
       "   'in',\n",
       "   '2020',\n",
       "   '.',\n",
       "   'With',\n",
       "   'this',\n",
       "   'increase',\n",
       "   'in',\n",
       "   '\\n',\n",
       "   'usage',\n",
       "   ',',\n",
       "   'it',\n",
       "   'is',\n",
       "   'natural',\n",
       "   'that',\n",
       "   'Web',\n",
       "   'applications',\n",
       "   'enable',\n",
       "   'a',\n",
       "   'wide',\n",
       "   'diversity',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'social',\n",
       "   'groups',\n",
       "   'to',\n",
       "   'interact',\n",
       "   'among',\n",
       "   'themselves',\n",
       "   'and',\n",
       "   'with',\n",
       "   'other',\n",
       "   'groups',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Since',\n",
       "   'such',\n",
       "   'applications',\n",
       "   'foster',\n",
       "   'a',\n",
       "   'more',\n",
       "   'open',\n",
       "   'and',\n",
       "   'dynamic',\n",
       "   'form',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'speech',\n",
       "   ',',\n",
       "   'a',\n",
       "   'natural',\n",
       "   'increase',\n",
       "   'in',\n",
       "   'the',\n",
       "   'written',\n",
       "   'form',\n",
       "   'of',\n",
       "   'dialects',\n",
       "   'that',\n",
       "   'previously',\n",
       "   'were',\n",
       "   'predominantly',\n",
       "   'seen',\n",
       "   'in',\n",
       "   'the',\n",
       "   'spoken',\n",
       "   'form',\n",
       "   '[',\n",
       "   '9',\n",
       "   ']',\n",
       "   'occurred',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'However',\n",
       "   ',',\n",
       "   'such',\n",
       "   'massive',\n",
       "   'amounts',\n",
       "   'of',\n",
       "   'textual',\n",
       "   'data',\n",
       "   'make',\n",
       "   'manual',\n",
       "   'content',\n",
       "   'moderation',\n",
       "   'impracticable',\n",
       "   '.',\n",
       "   'In',\n",
       "   'other',\n",
       "   'words',\n",
       "   ',',\n",
       "   'the',\n",
       "   'heavy',\n",
       "   'usage',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'social',\n",
       "   'media',\n",
       "   'has',\n",
       "   'evidenced',\n",
       "   'the',\n",
       "   'urge',\n",
       "   'for',\n",
       "   'automatic',\n",
       "   'moderation',\n",
       "   'tools',\n",
       "   '\\n',\n",
       "   'that',\n",
       "   'measure',\n",
       "   'and',\n",
       "   'moderate',\n",
       "   'improper',\n",
       "   'behavior',\n",
       "   'online',\n",
       "   '.',\n",
       "   'One',\n",
       "   'of',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'main',\n",
       "   'concerns',\n",
       "   'is',\n",
       "   'the',\n",
       "   'public',\n",
       "   'display',\n",
       "   'of',\n",
       "   'negative',\n",
       "   '/',\n",
       "   'toxic',\n",
       "   'sentiments',\n",
       "   '\\n',\n",
       "   'against',\n",
       "   'a',\n",
       "   'person',\n",
       "   'or',\n",
       "   'specific',\n",
       "   'group',\n",
       "   ',',\n",
       "   'more',\n",
       "   'drastically',\n",
       "   'when',\n",
       "   'the',\n",
       "   'target',\n",
       "   '\\n',\n",
       "   'is',\n",
       "   'a',\n",
       "   'minority',\n",
       "   'group',\n",
       "   'historically',\n",
       "   'marked',\n",
       "   'with',\n",
       "   'discrimination',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'stereotypes',\n",
       "   '.',\n",
       "   'The',\n",
       "   'necessity',\n",
       "   'of',\n",
       "   'dealing',\n",
       "   'with',\n",
       "   'the',\n",
       "   'increasing',\n",
       "   'number',\n",
       "   '\\n',\n",
       "   'of',\n",
       "   'deviating',\n",
       "   'content',\n",
       "   'has',\n",
       "   'led',\n",
       "   'many',\n",
       "   'researchers',\n",
       "   'and',\n",
       "   'companies',\n",
       "   'to',\n",
       "   '\\n',\n",
       "   'use',\n",
       "   'AI',\n",
       "   'tools',\n",
       "   'to',\n",
       "   'identify',\n",
       "   'such',\n",
       "   'events',\n",
       "   '[',\n",
       "   '51',\n",
       "   ']',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Concurrently',\n",
       "   'to',\n",
       "   'the',\n",
       "   'increase',\n",
       "   'in',\n",
       "   'Web',\n",
       "   'usage',\n",
       "   ',',\n",
       "   'African',\n",
       "   '-',\n",
       "   'American',\n",
       "   '\\n',\n",
       "   'English',\n",
       "   '(',\n",
       "   'AAE',\n",
       "   ')',\n",
       "   'has',\n",
       "   'gone',\n",
       "   'from',\n",
       "   'being',\n",
       "   'seen',\n",
       "   'as',\n",
       "   'a',\n",
       "   'marginalized',\n",
       "   'dialect',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'English',\n",
       "   'to',\n",
       "   'a',\n",
       "   'consolidated',\n",
       "   'vernacular',\n",
       "   'of',\n",
       "   'the',\n",
       "   'language',\n",
       "   '[',\n",
       "   '28',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Like',\n",
       "   'most',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   ':',\n",
       "   'Proceedings',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Web',\n",
       "   '(',\n",
       "   'WebMe',\n",
       "   '\\n',\n",
       "   'dia’2024',\n",
       "   ')',\n",
       "   '.',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '.',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   ':',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   ',',\n",
       "   '2024',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'SBC',\n",
       "   '–',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '-',\n",
       "   '2753',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Savvas',\n",
       "   'Zannettou',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'TU',\n",
       "   'Delft',\n",
       "   's.zannettou@tudelft.nl',\n",
       "   '\\n\\n',\n",
       "   'dialects',\n",
       "   ',',\n",
       "   'the',\n",
       "   'AAE',\n",
       "   'was',\n",
       "   'initially',\n",
       "   'heavily',\n",
       "   'used',\n",
       "   'in',\n",
       "   'spoken',\n",
       "   'form',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'had',\n",
       "   'the',\n",
       "   'Web',\n",
       "   'as',\n",
       "   'a',\n",
       "   'crucial',\n",
       "   'influence',\n",
       "   'on',\n",
       "   'its',\n",
       "   'emergence',\n",
       "   'in',\n",
       "   'the',\n",
       "   'written',\n",
       "   '\\n',\n",
       "   'form',\n",
       "   '[',\n",
       "   '9',\n",
       "   ']',\n",
       "   '.',\n",
       "   'However',\n",
       "   ',',\n",
       "   'the',\n",
       "   'Web',\n",
       "   'is',\n",
       "   'not',\n",
       "   'only',\n",
       "   'a',\n",
       "   'disseminator',\n",
       "   'of',\n",
       "   'cultural',\n",
       "   '\\n',\n",
       "   'aspects',\n",
       "   'of',\n",
       "   'our',\n",
       "   'society',\n",
       "   'but',\n",
       "   'also',\n",
       "   'a',\n",
       "   'vehicle',\n",
       "   'where',\n",
       "   'toxicity',\n",
       "   'campaigns',\n",
       "   '\\n',\n",
       "   'against',\n",
       "   'African',\n",
       "   'Americans',\n",
       "   'are',\n",
       "   'prone',\n",
       "   'to',\n",
       "   'occur',\n",
       "   '.',\n",
       "   'Even',\n",
       "   'though',\n",
       "   'several',\n",
       "   '\\n',\n",
       "   'websites',\n",
       "   'have',\n",
       "   'well',\n",
       "   '-',\n",
       "   'defined',\n",
       "   'community',\n",
       "   'guidelines',\n",
       "   ',',\n",
       "   'user',\n",
       "   'anonymity',\n",
       "   '\\n',\n",
       "   'and',\n",
       "   'lack',\n",
       "   'of',\n",
       "   'unaccountability',\n",
       "   'leave',\n",
       "   'room',\n",
       "   'for',\n",
       "   'misbehavior',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'The',\n",
       "   'aforementioned',\n",
       "   'rise',\n",
       "   'in',\n",
       "   'AI',\n",
       "   'moderation',\n",
       "   'tools',\n",
       "   '(',\n",
       "   'such',\n",
       "   'as',\n",
       "   'Google',\n",
       "   '’s',\n",
       "   '\\n',\n",
       "   'Perspective',\n",
       "   '[',\n",
       "   '37',\n",
       "   ']',\n",
       "   'and',\n",
       "   'others',\n",
       "   '[',\n",
       "   '23',\n",
       "   ',',\n",
       "   '51',\n",
       "   ',',\n",
       "   '66',\n",
       "   ']',\n",
       "   ')',\n",
       "   'aim',\n",
       "   'to',\n",
       "   'reduce',\n",
       "   'the',\n",
       "   'amount',\n",
       "   '\\n',\n",
       "   'of',\n",
       "   'negative',\n",
       "   'or',\n",
       "   'toxic',\n",
       "   'utterances',\n",
       "   'online',\n",
       "   '.',\n",
       "   'Overall',\n",
       "   ',',\n",
       "   'such',\n",
       "   'tools',\n",
       "   'rely',\n",
       "   'on',\n",
       "   '\\n',\n",
       "   'Machine',\n",
       "   'Learning',\n",
       "   '(',\n",
       "   'ML',\n",
       "   ')',\n",
       "   'models',\n",
       "   'that',\n",
       "   'help',\n",
       "   'determine',\n",
       "   'proper',\n",
       "   'and',\n",
       "   'improper',\n",
       "   'utterances',\n",
       "   '.',\n",
       "   'Nevertheless',\n",
       "   ',',\n",
       "   'as',\n",
       "   'previous',\n",
       "   'research',\n",
       "   'has',\n",
       "   'discussed',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'automatic',\n",
       "   'content',\n",
       "   'moderation',\n",
       "   'can',\n",
       "   'backfire',\n",
       "   'and',\n",
       "   'present',\n",
       "   'biases',\n",
       "   'towards',\n",
       "   'minorities',\n",
       "   '[',\n",
       "   '11',\n",
       "   ',',\n",
       "   '34',\n",
       "   ',',\n",
       "   '53',\n",
       "   ',',\n",
       "   '60',\n",
       "   ']',\n",
       "   '.',\n",
       "   'For',\n",
       "   'instance',\n",
       "   ',',\n",
       "   'a',\n",
       "   'tool',\n",
       "   'for',\n",
       "   'toxicity',\n",
       "   '\\n',\n",
       "   'analysis',\n",
       "   'may',\n",
       "   'present',\n",
       "   'high',\n",
       "   'scores',\n",
       "   'for',\n",
       "   'non',\n",
       "   '-',\n",
       "   'toxic',\n",
       "   'AAE',\n",
       "   'sentences',\n",
       "   'for',\n",
       "   '\\n',\n",
       "   'no',\n",
       "   'apparent',\n",
       "   'reason',\n",
       "   '.',\n",
       "   'We',\n",
       "   'show',\n",
       "   'examples',\n",
       "   'of',\n",
       "   'toxicity',\n",
       "   'and',\n",
       "   'sentiment',\n",
       "   '\\n',\n",
       "   'analysis',\n",
       "   'models',\n",
       "   'employed',\n",
       "   'in',\n",
       "   'online',\n",
       "   'text',\n",
       "   'to',\n",
       "   'depict',\n",
       "   'this',\n",
       "   'issue',\n",
       "   '.',\n",
       "   'We',\n",
       "   '\\n',\n",
       "   'point',\n",
       "   'out',\n",
       "   'that',\n",
       "   'it',\n",
       "   'is',\n",
       "   'quite',\n",
       "   'easy',\n",
       "   'to',\n",
       "   'find',\n",
       "   'problematic',\n",
       "   'utterances',\n",
       "   'when',\n",
       "   '\\n',\n",
       "   'using',\n",
       "   'slang',\n",
       "   'terms',\n",
       "   'such',\n",
       "   'as',\n",
       "   '*',\n",
       "   '“',\n",
       "   'n****s',\n",
       "   '”',\n",
       "   '*',\n",
       "   '.',\n",
       "   'In',\n",
       "   'Table',\n",
       "   '1',\n",
       "   ',',\n",
       "   'we',\n",
       "   'contrast',\n",
       "   'three',\n",
       "   '\\n',\n",
       "   'pairs',\n",
       "   'of',\n",
       "   'sentences',\n",
       "   'based',\n",
       "   'on',\n",
       "   'their',\n",
       "   'toxicity',\n",
       "   '/',\n",
       "   'negative',\n",
       "   'sentiment',\n",
       "   'levels',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '*',\n",
       "   'Why',\n",
       "   'does',\n",
       "   'the',\n",
       "   'problem',\n",
       "   'arise',\n",
       "   '?',\n",
       "   '*',\n",
       "   'From',\n",
       "   'a',\n",
       "   'linguistic',\n",
       "   'perspective',\n",
       "   ',',\n",
       "   'dialects',\n",
       "   'may',\n",
       "   'inherently',\n",
       "   'manifest',\n",
       "   'behaviors',\n",
       "   'and',\n",
       "   'cultural',\n",
       "   'aspects',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'groups',\n",
       "   'in',\n",
       "   'which',\n",
       "   'they',\n",
       "   'were',\n",
       "   'created',\n",
       "   '[',\n",
       "   '5',\n",
       "   ',',\n",
       "   '19',\n",
       "   '–',\n",
       "   '21',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Terms',\n",
       "   'such',\n",
       "   'as',\n",
       "   '\\n',\n",
       "   '“',\n",
       "   'n****r',\n",
       "   '”',\n",
       "   'are',\n",
       "   'problematic',\n",
       "   'for',\n",
       "   'AI',\n",
       "   'models',\n",
       "   'since',\n",
       "   'both',\n",
       "   'the',\n",
       "   'term',\n",
       "   'and',\n",
       "   'its',\n",
       "   '\\n',\n",
       "   'variations',\n",
       "   'have',\n",
       "   'a',\n",
       "   'historical',\n",
       "   'pejorative',\n",
       "   'usage',\n",
       "   '[',\n",
       "   '1',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Nevertheless',\n",
       "   ',',\n",
       "   'this',\n",
       "   '\\n',\n",
       "   'same',\n",
       "   'term',\n",
       "   'was',\n",
       "   're',\n",
       "   '-',\n",
       "   'appropriated',\n",
       "   'by',\n",
       "   'the',\n",
       "   'black',\n",
       "   'community',\n",
       "   ',',\n",
       "   'so',\n",
       "   'its',\n",
       "   'use',\n",
       "   '\\n',\n",
       "   'ceased',\n",
       "   'to',\n",
       "   'be',\n",
       "   'considered',\n",
       "   'problematic',\n",
       "   'when',\n",
       "   'used',\n",
       "   'by',\n",
       "   'people',\n",
       "   'inside',\n",
       "   '\\n',\n",
       "   'the',\n",
       "   'black',\n",
       "   'community',\n",
       "   '.',\n",
       "   'Suppose',\n",
       "   'such',\n",
       "   'a',\n",
       "   'fine',\n",
       "   'line',\n",
       "   'between',\n",
       "   'causal',\n",
       "   '\\n',\n",
       "   'speaking',\n",
       "   'and',\n",
       "   'offensive',\n",
       "   'discourse',\n",
       "   'is',\n",
       "   'problematic',\n",
       "   'from',\n",
       "   'a',\n",
       "   'human',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'computational',\n",
       "   'perspective',\n",
       "   '.',\n",
       "   'In',\n",
       "   'that',\n",
       "   'case',\n",
       "   ',',\n",
       "   'these',\n",
       "   'interpretations',\n",
       "   'are',\n",
       "   '\\n',\n",
       "   'confounding',\n",
       "   'to',\n",
       "   'automatic',\n",
       "   'content',\n",
       "   'moderation',\n",
       "   'tools',\n",
       "   '.',\n",
       "   'In',\n",
       "   'other',\n",
       "   'words',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'toxicity',\n",
       "   '/',\n",
       "   'sentiment',\n",
       "   'analysis',\n",
       "   'tools',\n",
       "   'are',\n",
       "   'usually',\n",
       "   'developed',\n",
       "   'using',\n",
       "   'manual',\n",
       "   ...],\n",
       "  'pos_tagger': '',\n",
       "  'lema': ['a',\n",
       "   'comprehensive',\n",
       "   'view',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Biases',\n",
       "   'of',\n",
       "   'Toxicity',\n",
       "   'and',\n",
       "   'Sentiment',\n",
       "   'analysis',\n",
       "   'Methods',\n",
       "   'towards',\n",
       "   'Utterances',\n",
       "   'with',\n",
       "   'African',\n",
       "   'American',\n",
       "   'english',\n",
       "   'expression',\n",
       "   'Guilherme',\n",
       "   'Andrade',\n",
       "   'Luiz',\n",
       "   'Nery',\n",
       "   'Fabrício',\n",
       "   'Benevenuto',\n",
       "   'Flavio',\n",
       "   'Figueiredo',\n",
       "   'ufmg',\n",
       "   'guilherme.hra,luiznery,fabricio,flavio@dcc.ufmg.br',\n",
       "   'ABSTRACT',\n",
       "   'Language',\n",
       "   'be',\n",
       "   'a',\n",
       "   'dynamic',\n",
       "   'aspect',\n",
       "   'of',\n",
       "   'our',\n",
       "   'culture',\n",
       "   'that',\n",
       "   'change',\n",
       "   'when',\n",
       "   'express',\n",
       "   'in',\n",
       "   'different',\n",
       "   'technology',\n",
       "   'and/or',\n",
       "   'community',\n",
       "   'on',\n",
       "   'the',\n",
       "   'internet',\n",
       "   'social',\n",
       "   'network',\n",
       "   'have',\n",
       "   'enable',\n",
       "   'the',\n",
       "   'diffusion',\n",
       "   'and',\n",
       "   'evolution',\n",
       "   'of',\n",
       "   'different',\n",
       "   'dialect',\n",
       "   'include',\n",
       "   'african',\n",
       "   'American',\n",
       "   'English',\n",
       "   'AAE',\n",
       "   'however',\n",
       "   'this',\n",
       "   'increase',\n",
       "   'usage',\n",
       "   'of',\n",
       "   'different',\n",
       "   'dialect',\n",
       "   'be',\n",
       "   'not',\n",
       "   'without',\n",
       "   'barrier',\n",
       "   'one',\n",
       "   'particular',\n",
       "   'barrier',\n",
       "   'the',\n",
       "   'focus',\n",
       "   'of',\n",
       "   'this',\n",
       "   'paper',\n",
       "   'be',\n",
       "   'on',\n",
       "   'how',\n",
       "   'sentiment',\n",
       "   'Vader',\n",
       "   'TextBlob',\n",
       "   'and',\n",
       "   'Flair',\n",
       "   'and',\n",
       "   'toxicity',\n",
       "   'Google',\n",
       "   '’s',\n",
       "   'Perspective',\n",
       "   'and',\n",
       "   'model',\n",
       "   'from',\n",
       "   'the',\n",
       "   'open',\n",
       "   'source',\n",
       "   'Detoxify',\n",
       "   'scoring',\n",
       "   'method',\n",
       "   'present',\n",
       "   'bias',\n",
       "   'towards',\n",
       "   'utterance',\n",
       "   'with',\n",
       "   'AAE',\n",
       "   'expression',\n",
       "   'in',\n",
       "   'particular',\n",
       "   'AI',\n",
       "   'tool',\n",
       "   'can',\n",
       "   'not',\n",
       "   'understand',\n",
       "   'the',\n",
       "   're',\n",
       "   'appropriation',\n",
       "   'of',\n",
       "   'the',\n",
       "   'term',\n",
       "   'lead',\n",
       "   'to',\n",
       "   'false',\n",
       "   'positive',\n",
       "   'score',\n",
       "   'and',\n",
       "   'bias',\n",
       "   'here',\n",
       "   'we',\n",
       "   'study',\n",
       "   'the',\n",
       "   'bias',\n",
       "   'of',\n",
       "   'toxicity',\n",
       "   'and',\n",
       "   'Sentiment',\n",
       "   'Analysis',\n",
       "   'model',\n",
       "   'base',\n",
       "   'on',\n",
       "   'experiment',\n",
       "   'perform',\n",
       "   'on',\n",
       "   'web',\n",
       "   'and',\n",
       "   'speak',\n",
       "   'english',\n",
       "   'dataset',\n",
       "   'keyword',\n",
       "   'african',\n",
       "   'American',\n",
       "   'English',\n",
       "   'AAE',\n",
       "   'Bias',\n",
       "   'Toxicity',\n",
       "   'Sentiment',\n",
       "   '1',\n",
       "   'introduction',\n",
       "   'in',\n",
       "   'recent',\n",
       "   'decade',\n",
       "   'we',\n",
       "   'have',\n",
       "   'witness',\n",
       "   'a',\n",
       "   'substantial',\n",
       "   'rise',\n",
       "   'in',\n",
       "   'internet',\n",
       "   'usage',\n",
       "   'accord',\n",
       "   'to',\n",
       "   '52',\n",
       "   'internet',\n",
       "   'user',\n",
       "   'increase',\n",
       "   'from',\n",
       "   'approximately',\n",
       "   '400',\n",
       "   'million',\n",
       "   'in',\n",
       "   '2000',\n",
       "   'to',\n",
       "   '4.7',\n",
       "   'billion',\n",
       "   'in',\n",
       "   '2020',\n",
       "   'with',\n",
       "   'this',\n",
       "   'increase',\n",
       "   'in',\n",
       "   'usage',\n",
       "   'it',\n",
       "   'be',\n",
       "   'natural',\n",
       "   'that',\n",
       "   'web',\n",
       "   'application',\n",
       "   'enable',\n",
       "   'a',\n",
       "   'wide',\n",
       "   'diversity',\n",
       "   'of',\n",
       "   'social',\n",
       "   'group',\n",
       "   'to',\n",
       "   'interact',\n",
       "   'among',\n",
       "   'themselves',\n",
       "   'and',\n",
       "   'with',\n",
       "   'other',\n",
       "   'group',\n",
       "   'since',\n",
       "   'such',\n",
       "   'application',\n",
       "   'foster',\n",
       "   'a',\n",
       "   'more',\n",
       "   'open',\n",
       "   'and',\n",
       "   'dynamic',\n",
       "   'form',\n",
       "   'of',\n",
       "   'speech',\n",
       "   'a',\n",
       "   'natural',\n",
       "   'increase',\n",
       "   'in',\n",
       "   'the',\n",
       "   'write',\n",
       "   'form',\n",
       "   'of',\n",
       "   'dialect',\n",
       "   'that',\n",
       "   'previously',\n",
       "   'be',\n",
       "   'predominantly',\n",
       "   'see',\n",
       "   'in',\n",
       "   'the',\n",
       "   'speak',\n",
       "   'form',\n",
       "   '9',\n",
       "   'occur',\n",
       "   'however',\n",
       "   'such',\n",
       "   'massive',\n",
       "   'amount',\n",
       "   'of',\n",
       "   'textual',\n",
       "   'datum',\n",
       "   'make',\n",
       "   'manual',\n",
       "   'content',\n",
       "   'moderation',\n",
       "   'impracticable',\n",
       "   'in',\n",
       "   'other',\n",
       "   'word',\n",
       "   'the',\n",
       "   'heavy',\n",
       "   'usage',\n",
       "   'of',\n",
       "   'social',\n",
       "   'medium',\n",
       "   'have',\n",
       "   'evidence',\n",
       "   'the',\n",
       "   'urge',\n",
       "   'for',\n",
       "   'automatic',\n",
       "   'moderation',\n",
       "   'tool',\n",
       "   'that',\n",
       "   'measure',\n",
       "   'and',\n",
       "   'moderate',\n",
       "   'improper',\n",
       "   'behavior',\n",
       "   'online',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'main',\n",
       "   'concern',\n",
       "   'be',\n",
       "   'the',\n",
       "   'public',\n",
       "   'display',\n",
       "   'of',\n",
       "   'negative',\n",
       "   'toxic',\n",
       "   'sentiment',\n",
       "   'against',\n",
       "   'a',\n",
       "   'person',\n",
       "   'or',\n",
       "   'specific',\n",
       "   'group',\n",
       "   'more',\n",
       "   'drastically',\n",
       "   'when',\n",
       "   'the',\n",
       "   'target',\n",
       "   'be',\n",
       "   'a',\n",
       "   'minority',\n",
       "   'group',\n",
       "   'historically',\n",
       "   'mark',\n",
       "   'with',\n",
       "   'discrimination',\n",
       "   'and',\n",
       "   'stereotype',\n",
       "   'the',\n",
       "   'necessity',\n",
       "   'of',\n",
       "   'deal',\n",
       "   'with',\n",
       "   'the',\n",
       "   'increase',\n",
       "   'number',\n",
       "   'of',\n",
       "   'deviate',\n",
       "   'content',\n",
       "   'have',\n",
       "   'lead',\n",
       "   'many',\n",
       "   'researcher',\n",
       "   'and',\n",
       "   'company',\n",
       "   'to',\n",
       "   'use',\n",
       "   'AI',\n",
       "   'tool',\n",
       "   'to',\n",
       "   'identify',\n",
       "   'such',\n",
       "   'event',\n",
       "   '51',\n",
       "   'concurrently',\n",
       "   'to',\n",
       "   'the',\n",
       "   'increase',\n",
       "   'in',\n",
       "   'web',\n",
       "   'usage',\n",
       "   'African',\n",
       "   'American',\n",
       "   'English',\n",
       "   'AAE',\n",
       "   'have',\n",
       "   'go',\n",
       "   'from',\n",
       "   'be',\n",
       "   'see',\n",
       "   'as',\n",
       "   'a',\n",
       "   'marginalized',\n",
       "   'dialect',\n",
       "   'of',\n",
       "   'English',\n",
       "   'to',\n",
       "   'a',\n",
       "   'consolidated',\n",
       "   'vernacular',\n",
       "   'of',\n",
       "   'the',\n",
       "   'language',\n",
       "   '28',\n",
       "   'like',\n",
       "   'most',\n",
       "   'in',\n",
       "   'proceeding',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'web',\n",
       "   'WebMe',\n",
       "   'dia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   '2024',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'sbc',\n",
       "   'Brazilian',\n",
       "   'Computing',\n",
       "   'Society',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '2753',\n",
       "   'Savvas',\n",
       "   'Zannettou',\n",
       "   'TU',\n",
       "   'Delft',\n",
       "   's.zannettou@tudelft.nl',\n",
       "   'dialect',\n",
       "   'the',\n",
       "   'AAE',\n",
       "   'be',\n",
       "   'initially',\n",
       "   'heavily',\n",
       "   'use',\n",
       "   'in',\n",
       "   'speak',\n",
       "   'form',\n",
       "   'and',\n",
       "   'have',\n",
       "   'the',\n",
       "   'web',\n",
       "   'as',\n",
       "   'a',\n",
       "   'crucial',\n",
       "   'influence',\n",
       "   'on',\n",
       "   'its',\n",
       "   'emergence',\n",
       "   'in',\n",
       "   'the',\n",
       "   'write',\n",
       "   'form',\n",
       "   '9',\n",
       "   'however',\n",
       "   'the',\n",
       "   'web',\n",
       "   'be',\n",
       "   'not',\n",
       "   'only',\n",
       "   'a',\n",
       "   'disseminator',\n",
       "   'of',\n",
       "   'cultural',\n",
       "   'aspect',\n",
       "   'of',\n",
       "   'our',\n",
       "   'society',\n",
       "   'but',\n",
       "   'also',\n",
       "   'a',\n",
       "   'vehicle',\n",
       "   'where',\n",
       "   'toxicity',\n",
       "   'campaign',\n",
       "   'against',\n",
       "   'African',\n",
       "   'Americans',\n",
       "   'be',\n",
       "   'prone',\n",
       "   'to',\n",
       "   'occur',\n",
       "   'even',\n",
       "   'though',\n",
       "   'several',\n",
       "   'website',\n",
       "   'have',\n",
       "   'well',\n",
       "   'define',\n",
       "   'community',\n",
       "   'guideline',\n",
       "   'user',\n",
       "   'anonymity',\n",
       "   'and',\n",
       "   'lack',\n",
       "   'of',\n",
       "   'unaccountability',\n",
       "   'leave',\n",
       "   'room',\n",
       "   'for',\n",
       "   'misbehavior',\n",
       "   'the',\n",
       "   'aforementioned',\n",
       "   'rise',\n",
       "   'in',\n",
       "   'AI',\n",
       "   'moderation',\n",
       "   'tool',\n",
       "   'such',\n",
       "   'as',\n",
       "   'Google',\n",
       "   '’s',\n",
       "   'Perspective',\n",
       "   '37',\n",
       "   'and',\n",
       "   'other',\n",
       "   '23',\n",
       "   '51',\n",
       "   '66',\n",
       "   'aim',\n",
       "   'to',\n",
       "   'reduce',\n",
       "   'the',\n",
       "   'amount',\n",
       "   'of',\n",
       "   'negative',\n",
       "   'or',\n",
       "   'toxic',\n",
       "   'utterance',\n",
       "   'online',\n",
       "   'overall',\n",
       "   'such',\n",
       "   'tool',\n",
       "   'rely',\n",
       "   'on',\n",
       "   'Machine',\n",
       "   'Learning',\n",
       "   'ML',\n",
       "   'model',\n",
       "   'that',\n",
       "   'help',\n",
       "   'determine',\n",
       "   'proper',\n",
       "   'and',\n",
       "   'improper',\n",
       "   'utterance',\n",
       "   'nevertheless',\n",
       "   'as',\n",
       "   'previous',\n",
       "   'research',\n",
       "   'have',\n",
       "   'discuss',\n",
       "   'automatic',\n",
       "   'content',\n",
       "   'moderation',\n",
       "   'can',\n",
       "   'backfire',\n",
       "   'and',\n",
       "   'present',\n",
       "   'bias',\n",
       "   'towards',\n",
       "   'minority',\n",
       "   '11',\n",
       "   '34',\n",
       "   '53',\n",
       "   '60',\n",
       "   'for',\n",
       "   'instance',\n",
       "   'a',\n",
       "   'tool',\n",
       "   'for',\n",
       "   'toxicity',\n",
       "   'analysis',\n",
       "   'may',\n",
       "   'present',\n",
       "   'high',\n",
       "   'score',\n",
       "   'for',\n",
       "   'non',\n",
       "   'toxic',\n",
       "   'AAE',\n",
       "   'sentence',\n",
       "   'for',\n",
       "   'no',\n",
       "   'apparent',\n",
       "   'reason',\n",
       "   'we',\n",
       "   'show',\n",
       "   'example',\n",
       "   'of',\n",
       "   'toxicity',\n",
       "   'and',\n",
       "   'sentiment',\n",
       "   'analysis',\n",
       "   'model',\n",
       "   'employ',\n",
       "   'in',\n",
       "   'online',\n",
       "   'text',\n",
       "   'to',\n",
       "   'depict',\n",
       "   'this',\n",
       "   'issue',\n",
       "   'we',\n",
       "   'point',\n",
       "   'out',\n",
       "   'that',\n",
       "   'it',\n",
       "   'be',\n",
       "   'quite',\n",
       "   'easy',\n",
       "   'to',\n",
       "   'find',\n",
       "   'problematic',\n",
       "   'utterance',\n",
       "   'when',\n",
       "   'use',\n",
       "   'slang',\n",
       "   'term',\n",
       "   'such',\n",
       "   'as',\n",
       "   'n****s',\n",
       "   'in',\n",
       "   'table',\n",
       "   '1',\n",
       "   'we',\n",
       "   'contrast',\n",
       "   'three',\n",
       "   'pair',\n",
       "   'of',\n",
       "   'sentence',\n",
       "   'base',\n",
       "   'on',\n",
       "   'their',\n",
       "   'toxicity',\n",
       "   'negative',\n",
       "   'sentiment',\n",
       "   'level',\n",
       "   'why',\n",
       "   'do',\n",
       "   'the',\n",
       "   'problem',\n",
       "   'arise',\n",
       "   'from',\n",
       "   'a',\n",
       "   'linguistic',\n",
       "   'perspective',\n",
       "   'dialect',\n",
       "   'may',\n",
       "   'inherently',\n",
       "   'manifest',\n",
       "   'behavior',\n",
       "   'and',\n",
       "   'cultural',\n",
       "   'aspect',\n",
       "   'of',\n",
       "   'the',\n",
       "   'group',\n",
       "   'in',\n",
       "   'which',\n",
       "   'they',\n",
       "   'be',\n",
       "   'create',\n",
       "   '5',\n",
       "   '19',\n",
       "   '21',\n",
       "   'term',\n",
       "   'such',\n",
       "   'as',\n",
       "   'n****r',\n",
       "   'be',\n",
       "   'problematic',\n",
       "   'for',\n",
       "   'AI',\n",
       "   'model',\n",
       "   'since',\n",
       "   'both',\n",
       "   'the',\n",
       "   'term',\n",
       "   'and',\n",
       "   'its',\n",
       "   'variation',\n",
       "   'have',\n",
       "   'a',\n",
       "   'historical',\n",
       "   'pejorative',\n",
       "   'usage',\n",
       "   '1',\n",
       "   'nevertheless',\n",
       "   'this',\n",
       "   'same',\n",
       "   'term',\n",
       "   'be',\n",
       "   're',\n",
       "   'appropriate',\n",
       "   'by',\n",
       "   'the',\n",
       "   'black',\n",
       "   'community',\n",
       "   'so',\n",
       "   'its',\n",
       "   'use',\n",
       "   'cease',\n",
       "   'to',\n",
       "   'be',\n",
       "   'consider',\n",
       "   'problematic',\n",
       "   'when',\n",
       "   'use',\n",
       "   'by',\n",
       "   'people',\n",
       "   'inside',\n",
       "   'the',\n",
       "   'black',\n",
       "   'community',\n",
       "   'suppose',\n",
       "   'such',\n",
       "   'a',\n",
       "   'fine',\n",
       "   'line',\n",
       "   'between',\n",
       "   'causal',\n",
       "   'speaking',\n",
       "   'and',\n",
       "   'offensive',\n",
       "   'discourse',\n",
       "   'be',\n",
       "   'problematic',\n",
       "   'from',\n",
       "   'a',\n",
       "   'human',\n",
       "   'and',\n",
       "   'computational',\n",
       "   'perspective',\n",
       "   'in',\n",
       "   'that',\n",
       "   'case',\n",
       "   'these',\n",
       "   'interpretation',\n",
       "   'be',\n",
       "   'confound',\n",
       "   'to',\n",
       "   'automatic',\n",
       "   'content',\n",
       "   'moderation',\n",
       "   'tool',\n",
       "   'in',\n",
       "   'other',\n",
       "   'word',\n",
       "   'toxicity',\n",
       "   'sentiment',\n",
       "   'analysis',\n",
       "   'tool',\n",
       "   'be',\n",
       "   'usually',\n",
       "   'develop',\n",
       "   'use',\n",
       "   'manual',\n",
       "   'rule',\n",
       "   'or',\n",
       "   'supervise',\n",
       "   'ML',\n",
       "   'technique',\n",
       "   'that',\n",
       "   'employ',\n",
       "   'human',\n",
       "   'label',\n",
       "   'datum',\n",
       "   'to',\n",
       "   'extract',\n",
       "   'pattern',\n",
       "   'the',\n",
       "   'disparate',\n",
       "   'treatment',\n",
       "   'embody',\n",
       "   'by',\n",
       "   'machine',\n",
       "   'learning',\n",
       "   'model',\n",
       "   'usually',\n",
       "   'replicate',\n",
       "   'discrimination',\n",
       "   'pattern',\n",
       "   'historically',\n",
       "   'practice',\n",
       "   'by',\n",
       "   'human',\n",
       "   'when',\n",
       "   'interact',\n",
       "   'with',\n",
       "   'process',\n",
       "   'in',\n",
       "   'the',\n",
       "   'real',\n",
       "   'world',\n",
       "   'due',\n",
       "   'to',\n",
       "   'bias',\n",
       "   'in',\n",
       "   'this',\n",
       "   'process',\n",
       "   'a',\n",
       "   'lack',\n",
       "   'of',\n",
       "   'context',\n",
       "   'lead',\n",
       "   'model',\n",
       "   'to',\n",
       "   'a',\n",
       "   'concern',\n",
       "   'scenario',\n",
       "   'where',\n",
       "   'minority',\n",
       "   'do',\n",
       "   'not',\n",
       "   'receive',\n",
       "   'equal',\n",
       "   'treatment',\n",
       "   '1',\n",
       "   '14',\n",
       "   '26',\n",
       "   '58',\n",
       "   'this',\n",
       "   'discussion',\n",
       "   'lead',\n",
       "   'to',\n",
       "   'the',\n",
       "   'research',\n",
       "   'question',\n",
       "   'behind',\n",
       "   'our',\n",
       "   'paper',\n",
       "   'be',\n",
       "   'there',\n",
       "   'a',\n",
       "   'systematic',\n",
       "   'bias',\n",
       "   'on',\n",
       "   'toxicity',\n",
       "   'sentiment',\n",
       "   'analysis',\n",
       "   'towards',\n",
       "   'aae',\n",
       "   'to',\n",
       "   'well',\n",
       "   'understand',\n",
       "   'this',\n",
       "   'issue',\n",
       "   'we',\n",
       "   'present',\n",
       "   'a',\n",
       "   'broad',\n",
       "   'scale',\n",
       "   'analysis',\n",
       "   'to',\n",
       "   'do',\n",
       "   'so',\n",
       "   'we',\n",
       "   'manually',\n",
       "   'curate',\n",
       "   'a',\n",
       "   'dataset',\n",
       "   'of',\n",
       "   'african',\n",
       "   'American',\n",
       "   'English',\n",
       "   'Expressions',\n",
       "   '54',\n",
       "   '63',\n",
       "   'these',\n",
       "   'source',\n",
       "   'be',\n",
       "   'recommend',\n",
       "   'by',\n",
       "   '1',\n",
       "   'https://en.wikipedia.org/wiki/Nigga',\n",
       "   '1',\n",
       "   'WebMedia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Resende',\n",
       "   'et',\n",
       "   'al',\n",
       "   'score',\n",
       "   'all',\n",
       "   'my',\n",
       "   'friend',\n",
       "   'on',\n",
       "   'the',\n",
       "   'porch',\n",
       "   'and',\n",
       "   'never',\n",
       "   'in',\n",
       "   'the',\n",
       "   'house',\n",
       "   'all',\n",
       "   'my',\n",
       "   'n****s',\n",
       "   'on',\n",
       "   'the',\n",
       "   'porch',\n",
       "   'and',\n",
       "   'neva',\n",
       "   'ina',\n",
       "   'house',\n",
       "   'you',\n",
       "   '’re',\n",
       "   'white',\n",
       "   'you',\n",
       "   '’re',\n",
       "   'black',\n",
       "   'I',\n",
       "   'can',\n",
       "   'not',\n",
       "   'forget',\n",
       "   'can',\n",
       "   'not',\n",
       "   'fuhgit',\n",
       "   'you',\n",
       "   'you',\n",
       "   'the',\n",
       "   'organizer',\n",
       "   'of',\n",
       "   'the',\n",
       "   'to',\n",
       "   'be',\n",
       "   'release',\n",
       "   'Oxford',\n",
       "   'Dictionary',\n",
       "   'of',\n",
       "   'African',\n",
       "   'american',\n",
       "   'English',\n",
       "   '2',\n",
       "   'these',\n",
       "   'expression',\n",
       "   'complement',\n",
       "   'four',\n",
       "   'different',\n",
       "   'utterance',\n",
       "   'dataset',\n",
       "   'with',\n",
       "   'some',\n",
       "   'demographic',\n",
       "   'information',\n",
       "   'on',\n",
       "   'race',\n",
       "   'i.e.',\n",
       "   'interview',\n",
       "   'from',\n",
       "   'african',\n",
       "   'american',\n",
       "   'individual',\n",
       "   'AAE',\n",
       "   'utterance',\n",
       "   'vs',\n",
       "   'non',\n",
       "   'aae',\n",
       "   'utterance',\n",
       "   'label',\n",
       "   'and',\n",
       "   'author',\n",
       "   'supply',\n",
       "   'label',\n",
       "   'we',\n",
       "   'empha-',\n",
       "   'size',\n",
       "   'that',\n",
       "   'we',\n",
       "   'can',\n",
       "   'not',\n",
       "   'state',\n",
       "   'how',\n",
       "   'a',\n",
       "   'speaker',\n",
       "   'identifie',\n",
       "   'regard',\n",
       "   'she',\n",
       "   'his',\n",
       "   'race',\n",
       "   'for',\n",
       "   'some',\n",
       "   'dataset',\n",
       "   'AAE',\n",
       "   'may',\n",
       "   'also',\n",
       "   'be',\n",
       "   'employ',\n",
       "   'by',\n",
       "   'non',\n",
       "   'african',\n",
       "   'Americans',\n",
       "   'however',\n",
       "   'we',\n",
       "   'interpret',\n",
       "   'our',\n",
       "   'result',\n",
       "   'use',\n",
       "   'AAE',\n",
       "   'expression',\n",
       "   'from',\n",
       "   'our',\n",
       "   'expression',\n",
       "   'dataset',\n",
       "   'make',\n",
       "   'available',\n",
       "   'the',\n",
       "   'model',\n",
       "   'we',\n",
       "   ...]},\n",
       " {'titulo': \"Through the Eyes of Instagram: Analyzing Image Content utilizing Meta's Automatic Alt-Text\",\n",
       "  'informacoes_url': '',\n",
       "  'idioma': 'english',\n",
       "  'storage_key': '../articles/original/english/985-24768-1-10-20240923.pdf',\n",
       "  'author': 'João Francisco Hecksher Olivetti and Philipe de Freitas Melo',\n",
       "  'data_publicacao': '11-09-2024',\n",
       "  'resumo': 'Multimedia communication has become an essential part of social media, with images representing a significant part of the content on most platforms. This study investigates image content on Instagram through Meta’s internal image classification algorithm, Automatic Alt-Text (AAT). Our approach differs from research on data from comments and hashtags because of the use of actual visual descriptions as the means of understanding the kinds of the content published on the network. Our analysis of 200k posts reveals 1,471 unique tags being used to characterize image content on Instagram, representing mostly objects, food, animals, locations and other common components of social media photos. Notably, we found that content about personal aesthetics is highly popular on the platform, with person and selfie being respectively some of the top two most common tag and post categories, being also highly related to other tags such as makeup, lipstick and eyeliner. Furthermore, we explored the connections between tags, representing very popular content trends within the network. Finally, we uncover substantial differences in posting behavior of influencers and news pages when compared to regular users, observing they post more frequently and about more specific content, suggesting what may attract more engagement on Instagram. ###',\n",
       "  'keywords': 'Instagram, alt-text, social media, image classification, image tagging, complex networks, influencers',\n",
       "  'referencias': ['[1] Rakesh Agrawal, Ramakrishnan Srikant, et al . 1994. Fast algorithms for mining\\nassociation rules. In *Proc. 20th int. conf. very large data bases, VLDB*, Vol. 1215.\\nSantiago, 487–499.',\n",
       "   '[2] Wasim Ahmed, Josep Vidal-Alaball, Joseph Downing, and Francesc López Seguí.\\n2020. COVID-19 and the 5G Conspiracy Theory: Social Network Analysis of\\nTwitter Data. *Journal of Medical Internet Research* 22, 5 (2020), e19458.',\n",
       "   '[3] Camila S Araujo, Luiz P D Correa, Ana P C Silva, Raquel O Prates, and Wagner\\nMeira. 2014. It is Not Just a Picture: Revealing Some User Practices in Instagram.\\nIn *9th Latin American Web Congress* . 19–23.',\n",
       "   '[4] Camila S Araújo, Wagner Meira, and Virgilio Almeida. 2016. Identifying Stereotypes in the Online Perception of Physical Attractivenes. In *Proc. of the 8th Intl.*\\n*Conf. of Social Informatics* . Springer, 419–437.',\n",
       "   '[5] Saeideh Bakhshi, David A. Shamma, and Eric Gilbert. 2014. Faces engage us:\\nphotos with faces attract more likes and comments on Instagram. In *Proc. of the*\\n*SIGCHI Conf. on Human Factors in Computing Systems* (Toronto, Canada) *(CHI*\\n*’14)* . ACM, NY, 965–974.',\n",
       "   '[6] Vincent Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre.\\n2008. Fast Unfolding of Communities in Large Networks. *Journal of Statistical*\\n*Mechanics Theory and Experiment* 2008 (04 2008).',\n",
       "   '[7] Meeyoung Cha, Hamed Haddadi, Fabrício Benevenuto, and Krishna Gummadi.\\n2010. Measuring User Influence in Twitter: The Million Follower Fallacy. *Proc. of*\\n*the International AAAI Conf. on Web and Social Media* 4, 1 (2010), 10–17.',\n",
       "   '[8] Koyel Chakraborty, Siddhartha Bhattacharyya, and Rajib Bag. 2020. A Survey of\\nSentiment Analysis from Social Media Data. *IEEE Transactions on Computational*\\n*Social Systems* 7, 2 (2020), 450–464.',\n",
       "   '[9] P Chauhan, N Sharma, and G Sikka. 2020. The emergence of social media data\\nand sentiment analysis in election prediction. *Journal of Ambient Intelligence and*\\n*Humanized Computing* 12, 2 (2020), 2601.',\n",
       "   '[10] Rion Brattig Correia, Lang Li, and Luis M Rocha. 2016. Monitoring potential\\ndrug interactions and reactions via network analysis of instagram user timelines.\\nIn *Biocomputing 2016: Proc. of the Pacific Symposium* . World Scientific, 492–503.',\n",
       "   '[11] Emory J. Edwards, Michael Gilbert, Emily Blank, and Stacy M. Branham. 2023.\\nHow the Alt Text Gets Made: What Roles and Processes of Alt Text Creation Can\\nTeach Us About Inclusive Imagery. *ACM Transactions on Accessible Computing*\\n16, 2, Article 18 (jul 2023), 28 pages.',\n",
       "   '[12] Carlos H G Ferreira, Fabrício Murai, Ana P C, Jussara M Almeida, Martino\\nTrevisan, Luca Vassio, Idilio Drago, and Marco Mellia. 2020. Unveiling Community\\nDynamics on Instagram Political Network. In *Proc. of the 12th ACM Conf. on Web*\\n*Science* (NY). ACM, 231–240.',\n",
       "   '[13] Yuheng Hu, Lydia Manikonda, and Subbarao Kambhampati. 2014. What We\\nInstagram: A First Analysis of Instagram Photo Content and User Types. *Proc. of*\\n*the International AAAI Conf. on Web and Social Media* 8, 1 (2014), 595–598.',\n",
       "   '[14] C. Hutto and Eric Gilbert. 2014. VADER: A Parsimonious Rule-Based Model for\\nSentiment Analysis of Social Media Text. *Proc. of the International AAAI Conf. on*\\n*Web and Social Media* 8, 1 (2014), 216–225.',\n",
       "   '[15] Simona Ibba, Matteo Orrù, Filippo Eros Pani, and Simone Porru. 2015. Hashtag\\nof Instagram: From Folksonomy to Complex Network.. In *KEOD* . 279–284.',\n",
       "   '[16] Jared Katzman, Angelina Wang, Morgan Scheuerman, Su L Blodgett, Kristen\\nLaird, Hanna Wallach, and Solon Barocas. 2023. Taxonomizing and Measuring\\nRepresentational Harms: A Look at Image Tagging. *AAAI Conf. on Artificial*\\n*Intelligence* 37, 12 (2023), 14277–14285.',\n",
       "   '[17] Mirella Lapata. 2006. Automatic evaluation of information ordering: Kendall’s\\ntau. *Computational Linguistics* 32, 4 (2006), 471–484.',\n",
       "   '[18] Sebastián Lozano and Ester Gutiérrez. 2018. A complex network analysis of global\\ntourism flows. *International Journal of Tourism Research* 20, 5 (2018), 588–604.',\n",
       "   '[19] Hafiz A M Malik. 2022. Complex Network Formation and Analysis of Online\\nSocial Media Systems. *Computer Modeling in Engineering & Sciences* 130, 3 (2022),\\n1737–1750.',\n",
       "   '[20] Breno Matos, Fabrício Benevenuto, and Ana P C da Silva. 2021. A Nudez como\\nEstereótipo de Beleza através das Lentes do Instagram. In *X Brazilian Workshop*\\n*on Social Network Analysis and Mining* . 163–168.',\n",
       "   '[21] Philipe Melo, Fabrício Benevenuto, Daniel Kansaon, Vitor Mafra, and Kaio Sá.\\n2021. Monitor de WhatsApp: Um sistema para checagem de fatos no combate à\\ndesinformação. In *Anais do XXVII Simpósio Brasileiro de Sistemas Multimídia e*\\n*Web (WebMedia’21)* . SBC, 79–82.',\n",
       "   '[22] Caroline Muñoz and Terri Towner. 2017. The Image is the Message: Instagram\\nMarketing and the 2016 Presidential Primary Season. *Journal of Political Market-*\\n*ing* 16 (06 2017).',\n",
       "   '[23] László Nemes and Attila Kiss. 2021. Social media sentiment analysis based on\\nCOVID-19. *Journal of Information and Telecommunication* 5, 1 (2021), 1–15.\\nTaylor & Francis.',\n",
       "   '[24] Serge Nyawa, Dieudonné Tchuente, and Samuel Fosso-Wamba. 2022. COVID19 vaccine hesitancy: a social media analysis using deep learning. *Annals of*\\n*Operations Research* (2022).',\n",
       "   '[25] Abubakr H. Ombabi, Wael Ouarda, and Adel M. Alimi. 2020. Deep learning\\nCNN–LSTM framework for Arabic sentiment analysis using textual information\\nshared in social networks. *Social Network Analysis and Mining* 10, 1 (2020), 53.',\n",
       "   '[26] Khubaib A Qureshi, Rauf A S Malick, Muhammad Sabih, and Hocine Cherifi.\\n2021. Complex network and source inspired COVID-19 fake news classification\\non Twitter. *IEEE Access* 9 (2021), 139636–139656.',\n",
       "   '[27] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic\\nImage Captioning. In *Proc. of the 56th Annual Meeting of the Association for*\\n*Computational Linguistics*, Vol. 1. ACL, 2556–2565.',\n",
       "   '[28] Abigale Stangl, Meredith Ringel Morris, and Danna Gurari. 2020. “Person, Shoes,\\nTree. Is the Person Naked?” What People with Vision Impairments Want in\\nImage Descriptions. In *Proc. of the 2020 CHI Conf. on Human Factors in Computing*\\n*Systems* . ACM, 1–13.',\n",
       "   '[29] Shaomei Wu, Jeffrey Wieland, Omid Farivar, and Julie Schiller. 2017. Automatic\\nAlt-text: Computer-generated Image Descriptions for Blind Users on a Social Network Service. In *Proc. of the 2017 ACM Conf. on Computer Supported Cooperative*\\n*Work and Social Computing (CSCW ’17)* . ACM, NY, 1180–1192.',\n",
       "   '[30] Koosha Zarei, Reza Farahbakhsh, and Noël Crespi. 2019. Typification of Impersonated Accounts on Instagram. In *IEEE 38th International Performance Computing*\\n*and Communications Conf. (IPCCC)* . 1–6.\\n\\n\\n282\\n\\n\\n-----'],\n",
       "  'text': '# **Through the Eyes of Instagram: Analyzing Image Content** **utilizing Meta’s Automatic Alt-Text**\\n\\n## João Francisco Hecksher Olivetti\\n#### joao.olivetti@ufv.br Universidade Federal de Viçosa - UFV Florestal, MG, Brasil\\n### **ABSTRACT**\\n\\nMultimedia communication has become an essential part of social\\nmedia, with images representing a significant part of the content\\non most platforms. This study investigates image content on Instagram through Meta’s internal image classification algorithm,\\nAutomatic Alt-Text (AAT). Our approach differs from research on\\ndata from comments and hashtags because of the use of actual\\nvisual descriptions as the means of understanding the kinds of the\\ncontent published on the network. Our analysis of 200k posts reveals 1,471 unique tags being used to characterize image content\\non Instagram, representing mostly objects, food, animals, locations\\nand other common components of social media photos. Notably,\\nwe found that content about personal aesthetics is highly popular\\non the platform, with person and selfie being respectively some\\nof the top two most common tag and post categories, being also\\nhighly related to other tags such as makeup, lipstick and eyeliner.\\nFurthermore, we explored the connections between tags, representing very popular content trends within the network. Finally, we\\nuncover substantial differences in posting behavior of influencers\\nand news pages when compared to regular users, observing they\\npost more frequently and about more specific content, suggesting\\nwhat may attract more engagement on Instagram.\\n### **KEYWORDS**\\n\\nInstagram, alt-text, social media, image classification, image tagging,\\ncomplex networks, influencers\\n### **1 INTRODUCTION**\\n\\nThe Web has become an integral part of daily life for a significant\\nportion of the global population. With the increasing prevalence\\nof online social networks and the multimedia nature of communi\\ncation online, visual content understanding is crucial. To enhance\\naccessibility, various tools have been implemented to assist disabled\\nusers in this task. One notable example is “Alt-text” (alternative\\ntext), a textual description of an image included in the HTML code\\nof a webpage. It describes the appearance and function of an image\\nto users who cannot see it, those using screen readers, or when the\\nimage fails to load.\\nIn 2017, Instagram and Facebook implemented Meta’s Automatic\\nAlt-Text (AAT) algorithm, which automatically extracts features\\nfrom images and photos and provides tags and textual descriptions\\nof visual content. Meta’s AAT has proven effective in aiding blind\\n\\nIn: Proceedings of the Brazilian Symposium on Multimedia and the Web (WebMe\\ndia’2024). Juiz de Fora, Brazil. Porto Alegre: Brazilian Computer Society, 2024.\\n© 2024 SBC – Sociedade Brasileira de Computação.\\nISSN 2966-2753\\n\\n## Philipe de Freitas Melo\\n#### philipe.freitas@ufv.br Universidade Federal de Viçosa - UFV Florestal, MG, Brasil\\n\\nusers to navigate these platforms and understand the image content [ 29 ]. Furthermore, recent extensions to the AAT system created\\na large-scale description of content on Instagram, as this feature provided valuable details to the entire platform, beyond just assisting\\nvisually impaired users. Image analysis poses significant challenges\\nfor researchers due to the inherent difficulty of both manual and\\nautomated processes to categorize its content. However, it is a key\\nsocial currency on the Web [ 13 ], and substantial insights can be\\ngained from analyzing images on Instagram, ranging from social\\nbehavior [ 5 ] to political campaigns [ 22 ] and even typification of\\nfake profiles [ 30 ]. Most existing studies, though, rely on traditional\\nanalysis of textual comments or hashtags from posts, which may\\nnot accurately represent the actual content portrayed in pictures.\\nThis study aims to analyze the structure of Meta’s Automatic\\nAlt-Text (AAT) system in categorizing Instagram images. We seek\\nto answer the following research questions: **RQ1:** What are the\\nmost popular content tags on different Instagram user classes? **RQ2:**\\nWhat is the relationship between these contents? **RQ3:** Is there\\na substantial difference between the content posted by different\\ntypes of users?\\nTo achieve this, we utilized a custom-built data scraper to collect\\n61,944 Instagram accounts and 198,623 posts. This effort extracted\\napproximately 550k Alt-text tags describing the content of Instagram images. After that, by applying complex networks analysis,\\nassociation rules and ranking metrics, we conducted a broad analysis to better understand and categorize the content posted on\\nInstagram, as also the relation of these tags among the users and\\nits popularity.\\nOur results indicate that the current state of the AAT algorithm\\nshares similarities, but also has significant differences from what is\\ndescribed in the literature. Through 1,471 unique tags used by AAT\\nthat we found within our dataset, objects, foods, animals, locations\\nare among the most common topics on Instagram posts. We found\\nthat personal aesthetic content is highly popular on Instagram, often\\nlinked to beauty tags such as “makeup”, “lipstick”, and “eyeliner”.\\nLastly, we observed substantial differences in posting behavior\\nbetween influencers, news pages, and regular users.\\n### **2 BACKGROUND AND RELATED WORK**\\n\\nThis section shows similar, adjacent or related research to the\\npresent work. A discussion on what alt-text is also included as\\ncontext on this papers main datapoint.\\n### **2.1 Automatic Alt-Text**\\n\\nAlt-text is the content present in the alt attribute of HTML documents. Its original usage was to show a brief description of an\\nobject that could not be loaded. Nowadays, it is used mainly for\\n\\n\\n275\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Olivetti and Melo\\n\\n\\naccessibility purposes, allowing screen readers to describe visual\\ncontent to visually impaired users.\\nIn social media, as content is user-uploaded, most images end\\nup having no alt-text associated with them. This has led many\\nOnline Social Networks to allow users to add their own description\\nof the image. This feature, however, remains very underutilized.\\nThere has been an extensive body of work focusing on alt-text,\\nincluding analyses on its manual and automated construction[ 11 ],\\npre-processed image-description pair datasets[ 27 ], inquiries on\\nwhat Blind or Low-Vision (BLVs) people want from these tags[ 28 ],\\nas well as what harms can arise from alt-text tagging[16].\\nThis has motivated Meta’s (by the time called Facebook) researchers to develop the technology known as Automatic Alt-Text\\n(AAT), with its original development being discussed in [ 29 ]. Their\\narticle focuses on the process of assuring the system was useful to\\npeople with vision disabilities.\\n### **2.2 Social Media Analysis**\\n\\nThe analysis and extraction of content from social networks is\\nincreasingly relevant as these platforms have become integral to\\ndaily life. Studies in this field offer insights into social impacts and\\nphenomena on these networks, as evidenced by studies like [ 2,\\n24 ]. One prevalent approach is sentiment analysis through various\\nstrategies, ranging from simplified methods like the influential\\nstudy by [ 14 ] to more advanced neural network techniques [ 23, 25 ].\\nThese studies aim to computationally classify the emotional content\\nof sentences, a complex task with numerous applications [8, 9].\\nOther approaches focus on understanding social media dynamics,\\nsuch as Complex Network modeling, which provides deep explanatory insights into relationships. A notable study in this area by [ 7 ]\\nused Twitter data to show that follower count alone is not a good\\nmetric for influence on the network. Other similar works include\\n\\n[ 18, 19, 26 ]. Twitter has been extensively studied due to its APIs\\nand openness to academic research. However, with recent changes,\\nthere is a growing trend towards studying other social networks.\\nDespite challenges in extracting data from less accessible networks,\\nresearch on these platforms, such as the work by [ 21 ], highlights the\\nvalue of image content and systematizes the investigation of fake\\nnews, revealing how messaging apps function as social networks.\\nThe focus on images, which are computationally more costly to\\nanalyze than texts, partly explains the fewer studies on Instagram\\n\\n[ 13 ]. Nonetheless, significant studies on Instagram do exist. For\\ninstance, [ 12 ] used Complex Network modeling to analyze community interactions during elections. Other studies, like [ 10, 15 ] also\\nuse this approach.\\nSome research examines the social implications of image content\\nacross various platforms, such as [ 3, 4 ]. The foundational study by\\n\\n[ 13 ] is one of the few focusing on understanding Instagram’s image\\ncontent. Using early computer vision mechanisms, the authors\\nidentified challenges like defining relevant categories and the high\\ncomputational cost. This study, by categorizing images from various\\nusers, directly connects to our work. Other studies, such as [ 22 ]\\nand [20] also focus on the image content on Instagram\\n\\n### **2.3 Research Gap**\\n\\nInstagram is predominantly a visual platform; however, most studies focus on the textual content of posts, such as comments, which\\nmay not accurately reflect the platform’s full picture. By analyzing\\nthe alt-text descriptions of images, this study aims to bridge this\\ngap, providing greater transparency and understanding of Instagram. It seeks to deepen the discussion about the topics present in\\nthis environment.\\n### **3 METHODOLOGY**\\n\\nIn this section, we describe the approach and strategies used to\\ngather process and analyze data from Instagram’s webpage. It details the methodology used for data collection, analysis, and also\\nrelevant limitations encountered during the study. The methodology is divided into three main parts: Data Collection, Data Analysis,\\nand Limitations.\\n### **3.1 Data Collection**\\n\\nTo investigate Meta’s Automatic Alt-Text (AAT) data on Instagram,\\nwe utilized a custom-built data scraper developed using Selenium [1],\\na tool that automates web browser navigation, enabling us to collect\\ndata without the use of APIs.\\n\\nThe time period of the data collection was between 09/2023 and\\n02/2024, about six months. This was the time we needed to have a\\nsample close in size to the one Meta’s researchers used to build the\\nAAT algorithm[ 29 ], both having about 200K posts. It is important\\nto note that the our scraper collected posts going from most recent\\nto older, so some posts may be from dates prior to the start of\\nthe extraction, having been posted previously. Since Regular Users\\nmight not post anything for very long periods of time, their posts\\nare from an irregular time period. One observation noted from\\nchecking these is that posts from before 2018 have no AAT tags,\\nmatching the time frame from the original paper, published in 2017.\\nSome of the challenges presented were dynamic content loading, frequent changes in Instagram’s HTML structure, rate limiting\\nand blocking, and obfuscated tags. Dynamic content loading requires waiting for the content to fully load before extracting alt\\ntext. Updates to Instagram’s HTML structure break scraping scripts,\\nrequiring updates. Additionally, Instagram imposes rate limits on\\nrequests, which can lead to IP blocking or suspension. Moreover,\\nInstagram’s web version does not show posts if no account is logged\\nin. Requests made in this context repeatedly return pages reporting\\nan error. In order to collect Instagram data, one must be logged in\\nto a valid, not-banned account. To manage these obstacles, 8 collection accounts were created. The crawler then cyclically changes the\\naccount used to scrape Instagram profiles, through Selenium, preventing individual accounts from exceeding limits without delaying\\nextraction. We observed the most common way to get an account\\nbanned is to log in manually multiple times in a short period.\\nThe data collection loop, which directly interacts with a user’s\\npage on the network, operates as follows: first, it checks if the\\ncurrent account to be extracted is already in the database. If not, a\\nnew data point is created, containing the username and the total\\nnumber of posts from that profile. The algorithm then proceeds to\\nthe next step, scrolling and accessing each post on the account to\\n\\n1 Available at: https://selenium-python.readthedocs.io/\\n\\n\\n276\\n\\n\\n-----\\n\\nThrough the Eyes of Instagram: Analyzing Image Content utilizing Meta’s Automatic Alt-Text WebMedia’2024, Juiz de Fora, Brazil\\n\\n**Figure 1: Flowchart of Instagram data collection methodology.**\\n\\n\\ncheck if they contain alt-text, until the end of the page is reached\\nor one of several code-imposed limits is hit to avoid unnecessary\\noperations. With the web-scraper implemented, we targeted three\\nmain types of Instagram users for data collection:\\n**Influencers:** We start our profile’s collection with the most popular\\naccounts within the network. These influential users were obtained\\nfrom HypeAuditor’s ranking of the top 1000 accounts [2] with the\\nmost engagement, which calculates the ranking as a composite\\nmeasure of total followers and the average number of likes and\\ncomments per post. These profiles mainly belong to celebrities (such\\nas @cristiano and @taylorswift ), but also include some politicians (e.g. @narendramodi ) and a few “ *meme pages* ” (e.g. @9gag ).\\n86 accounts were excluded from top 1000 for reasons as the profiles\\nbeing deactivated, turning private, or archiving posts.\\n**News Pages:** Profiles from this category represent respected news\\noutlets, tabloids, gossip magazines and information-focused pages.\\nThese were selected starting from manually selected \"seeds\", mainstream news outlet users (e.g. @nytimes ), which were then used\\nto acquire more news pages through Instagram’s recommendation\\nsystem. A total of 59 pages were found and collected through this\\nprocess. This fewer number of profiles was enough to reach a similar number of publications, as shown in Table 1, in which the 59\\nnews pages have, in sum, more posts than the 914 Influencers.\\n**Regular Users:** Also included were ordinary Instagram users. Due\\nto the vast size of the Instagram network and its restrictions, we\\ndeveloped a strategy to incorporate a diverse set of profiles to our\\ndata collection. To achieve this, we collected the usernames of\\nfollowers of Influencer and News profiles. This aimed to increase\\nthe randomness and variation on the sample, as popular profiles\\nusually have a set of followers with a wide range of interests and\\ndemographics. This method allowed us to include a sample of 60,971\\ngeneral Instagram user base and 150,662 posts in our analysis.\\n### **3.2 Instagram Alt-text Structure**\\n\\nAfter collecting data from Instagram users and their posts, it is\\nessential to understand the formatting of posts and the placement\\nof alt text for effective data collection. This subsection explains the\\nstructure of a typical Instagram post, how alt text is integrated, and\\nthe challenges associated with extracting this information.\\nA typical Instagram post consists of visual content (images or\\nvideos), textual content (captions, comments, and hashtags), and\\n\\n2 Available at: https://hypeauditor.com/top-instagram/\\n\\n\\n**Figure 2: Example of alt-text structure of Instagram post.**\\n\\n**Figure 3: Features extracted from the Instagram alt-text.**\\n\\nalt-text, which is a hidden layer of textual information associated\\nwith the visual content. The alt-text is embedded within the <img>\\nin the HTML structure of an Instagram post, specifically as the\\nvalue of its alt attribute. After reaching the actual content of alttext, it can be presented in different formats and variations. We\\noutlined the alt-text structure to classify and utilize it effectively.\\nOne of the first observations was that not all images containing\\nalt-text have image “classification tags”. This aligns with Meta’s\\ninformation on [ 29 ], which explains that before the AAT technology\\nwas implemented, alt-texts were either manually added by users,\\nwhich was very rare, or it had the generic “User’s Photo” phrase\\nattached to it. Another alt-text format containing no tags only\\nincludes the preamble, similar to that shown in Figure 2.\\nFinally, most of the AAT content has a structure similar to the\\nformat portrayed in Figure 3, highlighting how the alt text is located\\nwithin the HTML code. The uncovered structure revealed three\\n\\n\\n277\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Olivetti and Melo\\n\\n**Accounts** **Posts** **Tags** **Unique Tags**\\n\\n**Regular Users** 60.971 159.662 420.593 1470\\n**News Pages** 59 20.945 52.426 992\\n**Influencers** 914 18.016 73.133 1095\\n**Total** 61.944 198.623 546.152 1471\\n\\n**Table 1: Summary of the Instagram dataset collected.**\\n\\n\\nmain features in the Instagram alt-text: (1) An “ **Image Class** ” at\\nthe beginning of the AAT, represented by the description “ *May*\\n*be a ***** ”; (2) a set of “ **Description Tags** ”, which are class labels\\nassigned to images with the aim of describing the visual content; and\\noccasionally (3) “ **OCR text** ”, which represent actual text present\\nwithin the images.\\nWith the AAT features defined, we implemented an algorithm to\\nprocess this information. Using Python, we created the TextContent\\nclass. This class receives an alt-text string and divides it into the\\nrelevant features, accessed via the class’ getters. All tag analysis\\noperations were mediated by this structure, reducing errors and\\nsimplifying manipulation. Data collection results are summarized\\nin Table 1. Our final dataset includes over 60,000 users and approximately 200,000 posts, from which we extracted around 550,000\\nalt-text tags that describe the content of the images on Instagram.\\n### **3.3 Limitations**\\n\\nThere are some limitations associated with this collecting methodology and the resulting dataset. Beyond all restrictions imposed by\\nInstagram to gather data from the platform, the main limitation\\nis that, considering that Instagram has more than a billion users\\nworldwide, our collection of 60k users and 200K posts represents a\\nsmall sample, not necessarily representative of the entirety of the\\nnetwork. However other key studies in the field use datasets of similar sizes, with Meta using exactly 200K posts to build sample what\\ntags they would need to initially build AAT algorithm[ 29 ]. Also,\\ngiven the AAT system automatically assigns tags to the images, the\\ndata may present some mislabeled content, and we cannot ensure\\nthe quality of the image labeling. That said, this is the actual tool\\nInstagram uses to provide information for the screen reader users,\\nand thus our view of the content is the same a BLV user may have\\nwithin the platform.\\n### **4 RESULTS AND DISCUSSIONS**\\n\\nIn this section, we present our analysis of the data collected. We\\nexamine the structure of Meta’s AAT system, using tools such as\\ncomplex networks and association rules metrics, focusing on how\\nit categorizes and describes visual content. Our findings include an\\nanalysis of tag popularity, relationships between alt-text tags, and\\na comparison of the content shared by regular users, news pages,\\nand Influencers.\\n### **4.1 Post Types on Instagram**\\n\\nOnce the data was pre-processed and ready, we began the analysis\\nand understanding of Instagram content. Figure 4 illustrates the\\nhigh-level classification of Instagram images by AAT. This classification reveals key differences in content types shared by various\\nuser groups. The “ *Image Class* ” feature is the first description the\\n\\n\\n**Figure 4: Types of posts present on Instagram data.**\\n\\n1.0\\n\\n0.8\\n\\n0.6\\n\\n0.4\\n\\n0.2\\n\\n0.0\\n\\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|\\n|---|---|---|---|---|---|---|\\n||||||||\\n||||||||\\n||||||||\\n||||||||\\n||||||||\\n||||||||\\n\\n\\n\\n10 [0] 10 [1] 10 [2] 10 [3] 10 [4] 10 [5]\\nTag Frequency (log)\\n\\n**Figure 5: CDF of popularity of each tag.**\\n\\nAAT algorithm provides for users, serving as a general guide to\\nwhat type of post the image could be. Although most content is\\nclassified as an image, Instagram differentiates whether the content\\nis also a selfie, a meme, an illustration and other types. Unlike tags,\\na post can belong to only one ImageClass. When considering all 15\\nclasses found, however, the methodology behind their definition is\\nnot clear. About half of the classes are art-related (illustration, doodle, pop art, art, cartoon, anime-style image, and drawing), with no\\nobvious distinction among them. Although some classes have more\\nwell-defined content, such as “selfie”, “back-and-white images” or\\n“anime-style”, manual observation within each class did not clarify\\nthe specific reasoning behind this labeling.\\n### **4.2 RQ1: Content Popularity by User Type**\\n\\nWe also analyzed the popularity of the description tags provided\\nby Meta’s Automatic Alt-Text (AAT) system. The number of tags\\nper post varies, with some posts having only one tag and others\\ncontaining up to ten tags. Our findings show that only a few tags\\nappear just once, and more than 95% of tags are present in at least\\n2 or more posts. This suggests Instagram has limited pre-built tag\\nclasses that they assign to describe images on the platform. Also,\\nabout 65% of the tags appear up to 100 times among the images and\\napproximately 5% of the tags are highly popular, present in more\\nthan 1,000 different posts as shown in CDF of Figure 5.\\n\\n\\n278\\n\\n\\n-----\\n\\nThrough the Eyes of Instagram: Analyzing Image Content utilizing Meta’s Automatic Alt-Text WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\n**Regular Users** **Influencers** **News Pages**\\n\\n1. text (56K) text (13K) text (20K)\\n2. 1 person (50K) 1 person (8K) 1 person (7K)\\n3. one or more ppl (10K) stadium (2K) 2 people (3K)\\n4. 2 people (10K) 2 people (2K) poster (2K)\\n5. smiling (8K) poster (2K) magazine (2K)\\n6. poster (8K) makeup (1K) 3 people (1K)\\n7. top (8K) people playing football (1K) newspaper (738)\\n8. hair (8K) people playing soccer (1K) 4 people (735)\\n9. beard (8K) one or more people (1K) people standing (520)\\n10. makeup (7K) playing football (1K) 5 people (469)\\n\\n**Table 2: Ranking of top 10 most popular tags on Instagram.**\\n\\n**Figure 6: Manual classification of tag categories.**\\n\\n**Figure 7: Tag specificity/generalization level per type.**\\n\\nAdditionally, we examined the rankings of the most popular tags,\\nas shown in Table 2. This table lists the most frequent tags for each\\nuser type. We observed that for all user types, tags like “ *text* ” and “ *1*\\n*person* ” are the most common. This aligns with findings from other\\nstudies, such as [ 5 ] and [ 13 ], indicating that individual photos of\\npeople with some captions are extremely popular on the Instagram.\\nOther frequently tags, such as “ *selfie* ”, “ *hair* ” and “ *beard* ” further\\nhighlights the prevalence of personal photos on the platform.\\nIn total, we extracted 546.152 tags among the posts within our\\ndataset; however, we found those tags comprise a total of only 1,471\\nunique tags used by Instagram to describe the image content. To\\nhave a wide view of what those tags are and the actual content\\nof Instagram images, we manually labeled them into broader categories, following a similar approach of authors from [ 13 ]. This\\ncategorization was done through iterative labeling, in which each\\nstep the categories were grouped, resulting in 14 classes by the end,\\nas illustrated in Figure 6. Interestingly, we also observed and labeled\\na different degree of specificity/generalization of each tag. While\\nsome tags represent very general concepts, such as “ *text* ”, some\\nare very specialized. For example, Instagram differentiates a lot of\\nspecific cat and dog breeds, as well as famous real-world locations.\\n\\n\\nFigure 7 represents the portion of tags defined as general specific.\\nFor example, “ *duck* ” is considered a general tag, whereas “ *mallard* ”\\nis specific. Following this process, we discovered that objects, food\\nand animals are the categories with most tag variation on Instagram. An interesting observation from this is that the categories\\nwith more tags are not necessarily the most popular among posts.\\nFurthermore, we find a redundancy among tags. For instance, tags\\nsuch as “ *soccer* ” is accompanied by tags such as “ *playing soccer* ”\\nand “ *people playing soccer* ”. These specific action tags could reflect\\nresults from [ 29 ] in which impaired users indicated that people are\\nthe most interesting part of an image, especially their mood and\\nwhat they were doing (action).\\nAdditionally, we found typos in tags like “ *acquatic animal* ” and\\n“ *riding a crousel* ”, in which all occurrences have the same error, but\\nthis did not repeat in tags such as “ *carousel* ” and “ *people riding a*\\n*carousel* ”.\\n### **4.3 RQ2: How AAT Content Interacts on** **Instagram**\\n\\nNext, we focused on the relation between kinds of image content\\nposted on Instagram by investigating common co-occurrence of\\ntags within our dataset. To consider what users usually put together\\non their photos on Instagram, we firstly use complex network\\nanalysis to develop the graph of tags presented in Figure 8 and assess\\ncommon paths that posts follow on the network with community\\nalgorithms. In this graph, the nodes represent each of the tags\\ncollected, and each edge represents the co-occurrence of two tags\\nin a post; edges are weighted by how many times the tags appeared\\ntogether. The size of a node is determined by its degree, with the\\nmost well-connected nodes being the largest. As expected, these\\nare nodes such as “ *text* ‘ and “ *1 person* “, which are the top 1 and 2\\nmost frequent within the dataset.\\nThe grouping of nodes was made using the Community Detection algorithm described by [ 6 ], as implemented in Gephi. It resulted in uncovering the 10 communities, 8 of which had a relevant\\nenough size, differentiated by color in the graph. It is interesting to\\nobserve how some types of content are grouped on Instagram and\\ntend to appear together in a post. Examples such as “ *cup* “, “ *bottle* “\\nand “ *tea* “ linked together show how objects that one would expect\\nto be together actually appear in images. In the same direction\\nwe also have “ *lipstick* “, “ *makeup* “, “ *hair* “, “ *eyeliner* “, all around “ *1*\\n*person* “ suggesting the popularity of such kind of selfies within\\nthe network. In general, our findings on this network analysis revealed linked groups of content representing as following: **Red** :\\nGenerally contains objects associated with apparel. Mirrors the\\n“Fashion” category from [ 13 ]; **Pink** : includes most of the “People\\nCount” mentioned in the original AAT paper[ 29 ]. Also includes\\nobjects, notably things that usually appear alongside people; **Cyan** :\\nRepresents landscapes, their components and some outdoor activities; **Light Green** : Contains food and kitchen utensils. Also, a\\ncategory in [ 13 ]; **Dark Green** : Species of animals, plants and types\\nof furniture; **Deep Blue** : Includes only flowers and four other tags,\\nthose being “ *bouquet* ”, “ *vase* ” and “ *flower pot* ”; **Grey** : Vehicles, animals used in transportation and some other apparently unrelated\\ncritters; **Brown** : Eight out of the nine items in this category are\\ncontainers, such as “ *bag* ”, “ *wallet* ”, “ *purse* ” and “ *suitcase* ”.\\n\\n\\n279\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Olivetti and Melo\\n\\n**Figure 8: Network of tag relationships. Nodes are tags and edge means co-ocurrence in same post.**\\n\\n\\n**Association Rules** : The second approach to understand relationships between content is extracting the association rules through\\nApriori Mining algorithm [ 1 ], which can be used to point pairs or\\nsets of tags that have high probabilities to appear together within a\\npost. Since the large differences between user types were shown,\\nthe rule mining was applied separately on news pages, regular users\\nand Influencers.\\nOn the news pages set, there are many high-confidence association rules on pairs, especially those following the pattern (people\\ncount tag) -> (text), with a confidence above 0.9 This helps\\nto explain posts frequently seen in news pages, in which people\\ninvolved in some news-worthy project are depicted along with text\\nthat gives context. Other similar rules are (1 person, poster,\\nmagazine) -> (text) and (2 people, poster, magazine) ->\\n(text) . These show a very similar pattern, but they also include\\nother common tags for news publications. In general, those rules\\nsuggest news pages have a much more homogeneous posting format, depicting usually a person or other object in a specific format\\nof poster accompanied by some text describing the news headline.\\nThe influencer users are those with the most association rules. A\\nvery representative and high confidence rule is (stadium, playing\\nsoccer) -> (text, 1 person, playing football), which includes most of the tags associated with the sport. One reason for\\nthis may come from the AAT redundancy in action tags and the\\n\\n\\nabsence of difference between “ *soccer* ” and “ *football* ”. Additionally,\\nseveral famous football players are among the top 1000 most influential users in our dataset due to the list used to collect them,\\ncontributing to the popularity of this type of content within this\\nuser category. However, these also evidence that thematic posts of\\nsoccer are prevalent for those users. On the other hand, few rules\\nhave the tag “ *ball* ”, suggesting that, despite the theme, the game\\nitself is not the focus of the post. One of the main rules that is not\\nrelated to sport is (makeup, 1 person) -> (text), a rule seemingly more representative of the artists and other influencers of this\\ncategory. Note the presence of textual content in these influencers\\nrules, which is a common format for events, advertises, sponsored\\nposts and other communication tools that celebrity accounts are\\nused to post on Instagram.\\nFinally, the tags associated with the regular user type are much\\nmore diverse than those of the other categories, but generally have\\nlower confidence levels. For instance, the rule (smiling) -> (1\\nperson) has a confidence of 0.96, which is higher than most others.\\nWe also observe pairs such as (ocean) -> (beach), with a confidence of 0.62. Interestingly, rules related to makeup appear more\\nfrequently and with higher confidence. For example, (lipstick)\\n-> (makeup), (eyeliner) -> (makeup), (makeup, eyeliner)\\n-> (1 person), and (makeup, blonde hair) -> (1 person),\\nall of which have confidence levels close to 0.7. Additionally with\\n\\n\\n280\\n\\n\\n-----\\n\\nThrough the Eyes of Instagram: Analyzing Image Content utilizing Meta’s Automatic Alt-Text WebMedia’2024, Juiz de Fora, Brazil\\n\\n\\n1.0\\n\\n0.8\\n\\n0.6\\n\\n0.4\\n\\n0.2\\n\\n0.0\\n\\n\\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|\\n|---|---|---|---|---|---|---|---|---|---|\\n|||||||||||\\n|||||||||||\\n|||||||||||\\n|||||||||||\\n|||||||||||\\n||||||||R N In|egular Ac ews Page fluencers|counts s|\\n\\n\\n0 10 [0] 10 [1] 10 [2] Total Likes (log) 10 [3] 10 [4] 10 [5] 10 [6] 10 [7] 10 [8]\\n\\n**(a) Likes per Post**\\n\\n\\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|\\n|---|---|---|---|---|---|---|---|\\n|||||||||\\n|||||||||\\n|||||||||\\n|||||||||\\n|||||||||\\n|||||||Regular News P Influenc|Accounts ages ers|\\n|||||||||\\n\\n\\n0 10 [0] 10 [1] Total Comments (log) 10 [2] 10 [3] 10 [4] 10 [5] 10 [6]\\n\\n**(b) Comments per Post**\\n\\n\\n50\\n\\n40\\n\\n\\n\\n1.0\\n\\n0.8\\n\\n0.6\\n\\n0.4\\n\\n0.2\\n\\n0.0\\n\\n\\n1.0\\n\\n\\n\\n0.8\\n\\n0.6\\n\\n0.4\\n\\n0.2\\n\\n0.0\\n\\n|R N In|egular Ac ews Page fluencers|counts s|Col4|Col5|Col6|\\n|---|---|---|---|---|---|\\n|||||||\\n|||||||\\n|||||||\\n|||||||\\n|||||||\\n|||||||\\n\\n\\n\\n0 10 [0] 10 [1] Total Posts (log) 10 [2] 10 [3] 10 [4] 10 [5]\\n\\n**(c) Posts per User**\\n\\n**Figure 9: Differences in engagement for different user types.**\\n\\n“person” and “selfie” being the second most common tags and post\\ncategories, it is evident that content about personal aesthetics is\\nhighly popular on Instagram. These findings indicate that users are\\nhighly engaged with content related to personal appearance and\\nself-expression.\\n### **4.4 RQ3: Posting Behavior by User Type**\\n\\nNext, we explore the differences in posting behavior and engagement across different kinds user classes. We analyze the frequency\\nof posts, the number of tags per post, as well as engagement metrics such as likes and comments from regular users, news pages\\nInfluencer profiles. These insights help us understand how different\\nuser types utilize the platform and how their content resonates\\nwith their audience.\\n**Differences in Engagement:** First, we examined some engagement metrics such as likes and comments per post, as shown in\\nFigure 9. Influencers generally receive more likes and comments\\ncompared to regular users and news pages, reflecting their higher\\nengagement levels due to their follower base and the nature of\\ntheir content. It is notable that for regular users, about 20% and 60%\\nof their posts have no likes and no comments at all, respectively.\\nOn the other hand, almost all influencer’s posts reach more than\\n100k likes and 100 comments. For comparison, only 25% of news\\nposts have similar likes. However, looking at the number of posts\\nper type of user (Figure 9c, we can note that influencers are not\\nthe users with the most posts. News pages have a much higher\\nnumber of posts than other types, with all news pages collected\\nhaving more than 1,000 posts in their profiles, while less than 1%\\nof regular users and just 40% of influencers have such numbers.\\nNews pages post most frequently, often several times a day. This\\ncontrasts with Regular users, where almost 40% have zero posts\\nand about 90% have up to 100 posts. Since Regular users typically\\n\\n\\n30\\n\\n20\\n\\n10\\n\\n0\\n\\n0 1 2 3 4 5 6 7 8 9\\n\\nTag Amount\\n\\n**Figure 10: Number of tags per post for each type of user.**\\n\\nhave no economic or professional reasons to post frequently, their\\ntotals are usually much lower.\\n**Differences in Content:** After evaluating some posting behaviors,\\nwe take our attention to the actual content of the publication of\\neach types of users. First, we see the tag frequency of their posts\\nin the histogram of Figure 10. While about 60% of regular users’\\nposts have up to 3 tags, we observe influencers generally have more\\ntags within their publications. This suggests influencers have a\\nhigher variety of content in a single post. Also, it can be attributed\\nto product placement, showing various objects, and the diverse\\nlifestyles of celebrities and politicians. No posts from news pages\\nhave zero tags, but almost 50% have exactly two tags. This is because\\nmost images from news pages include a text, thus heavily increasing\\nthe chances of including the “ *text* ” tagging, consequently also have\\nthe OCR property, which is enough to describe the image without\\nadditional tags required.\\nTo further measure the differences between content published by\\nuser types, we applied Kendall’s Tau rank correlation metric [ 17 ] to\\ncompare rankings of the top 10, 100, and 992 tags of each account\\nclass, as shown in Table 3. The number 992 was used instead of\\n1000 because that was the total number of tags in the news pages\\ndataset. Considering the top 10 tags of each set, the Kendall’s Tau\\nresults show weak negative associations ( *𝜏* *<* 0) between Regular/Influencer and Influencer/News pages. The strongest negative\\ncorrelation is between News pages and Regular users, indicating\\na moderate but significant negative association, suggesting News\\naccounts are the most distinct user category.\\nThe correlation results from Kendall’s Tau show that there are\\nsubstantial differences between all the types of users. Even when\\nconsidering most of the tags, the positive correlation is at best\\nmoderate, suggesting some content is always popular. As comparison, we also apply Kendall Tau it to another set: two randomly\\ndistributed, equally sized partitions of the \"Regular user\" dataset.\\nThe results, shown in Table 4, indicate a strong positive correlation\\nand that random samples would present similar tags ( *𝜏* *>* 0 *.* 87)\\namong their content. This confirms that the negative correlations\\nobserved between user types are not random, and makes it evident\\nthat they have different behavior on Instagram.\\n\\n\\n281\\n\\n\\n-----\\n\\nWebMedia’2024, Juiz de Fora, Brazil Olivetti and Melo\\n\\n#### Account Types Top10 𝜏 Top100 𝜏 Top992 𝜏 -0.077 0.092 0.528 **Regular/Influencer** -0.167 -0.211 0.543 **Influencer/News** News/Regular -0.446 -0.194 0.501\\n\\n**Table 3: Kendall’s Tau Applied between different User Types**\\n#### Account Types Top10 𝜏 Top100 𝜏 Top1000 𝜏 Random A / B 0.918 0.911 0.872\\n\\n**Table 4: K** *𝜏* **of two regular user sets randomly partitioned.**\\n### **5 CONCLUSION**\\n\\nIn this study, we conducted an extensive investigation of image\\ncontent on Instagram through AAT system. By analyzing alt-text\\ndata from nearly 200,000 user posts collected from 60,000 Instagram\\nusers, we provided significant insights into the visual content shared\\non the platform.\\nOur findings reveal 1,471 unique tags used by Instagram to categorize content, composed mostly of objects, food, animals, and\\nlocations. These show that content related to personal aesthetics,\\nsuch as makeup, lipstick, and eyeliner, are highly popular on Instagram. Additionally, we identified several tags that are frequently\\nconnected, forming popular content trends within the network.\\nMoreover, we highlight differences in the content shared by news\\npages, Influencers, and regular users. Notably, news pages tend to\\npost images containing text, while Influencers often share content\\nrelated to personal aesthetics and activities, such sports. Regular\\nusers, on the other hand, post a diverse range of content, although\\nhaving fewer tags per post.\\nOur research also highlighted the evolution of the AAT algorithm, noting the introduction of many new tags, including some\\nseemingly redundant. Future studies could expand on this work by\\nincorporating larger samples and applying AAT context to studies\\non political campaigns, mental health, and disinformation on Instagram. Our approach offers a fresh perspective that complements\\nexisting research, enhancing our understanding of the diverse and\\ndynamic nature of content shared on this social network.\\n### **REFERENCES**\\n\\n[1] Rakesh Agrawal, Ramakrishnan Srikant, et al . 1994. Fast algorithms for mining\\nassociation rules. In *Proc. 20th int. conf. very large data bases, VLDB*, Vol. 1215.\\nSantiago, 487–499.\\n\\n[2] Wasim Ahmed, Josep Vidal-Alaball, Joseph Downing, and Francesc López Seguí.\\n2020. COVID-19 and the 5G Conspiracy Theory: Social Network Analysis of\\nTwitter Data. *Journal of Medical Internet Research* 22, 5 (2020), e19458.\\n\\n[3] Camila S Araujo, Luiz P D Correa, Ana P C Silva, Raquel O Prates, and Wagner\\nMeira. 2014. It is Not Just a Picture: Revealing Some User Practices in Instagram.\\nIn *9th Latin American Web Congress* . 19–23.\\n\\n[4] Camila S Araújo, Wagner Meira, and Virgilio Almeida. 2016. Identifying Stereotypes in the Online Perception of Physical Attractivenes. In *Proc. of the 8th Intl.*\\n*Conf. of Social Informatics* . Springer, 419–437.\\n\\n[5] Saeideh Bakhshi, David A. Shamma, and Eric Gilbert. 2014. Faces engage us:\\nphotos with faces attract more likes and comments on Instagram. In *Proc. of the*\\n*SIGCHI Conf. on Human Factors in Computing Systems* (Toronto, Canada) *(CHI*\\n*’14)* . ACM, NY, 965–974.\\n\\n[6] Vincent Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre.\\n2008. Fast Unfolding of Communities in Large Networks. *Journal of Statistical*\\n*Mechanics Theory and Experiment* 2008 (04 2008).\\n\\n\\n\\n[7] Meeyoung Cha, Hamed Haddadi, Fabrício Benevenuto, and Krishna Gummadi.\\n2010. Measuring User Influence in Twitter: The Million Follower Fallacy. *Proc. of*\\n*the International AAAI Conf. on Web and Social Media* 4, 1 (2010), 10–17.\\n\\n[8] Koyel Chakraborty, Siddhartha Bhattacharyya, and Rajib Bag. 2020. A Survey of\\nSentiment Analysis from Social Media Data. *IEEE Transactions on Computational*\\n*Social Systems* 7, 2 (2020), 450–464.\\n\\n[9] P Chauhan, N Sharma, and G Sikka. 2020. The emergence of social media data\\nand sentiment analysis in election prediction. *Journal of Ambient Intelligence and*\\n*Humanized Computing* 12, 2 (2020), 2601.\\n\\n[10] Rion Brattig Correia, Lang Li, and Luis M Rocha. 2016. Monitoring potential\\ndrug interactions and reactions via network analysis of instagram user timelines.\\nIn *Biocomputing 2016: Proc. of the Pacific Symposium* . World Scientific, 492–503.\\n\\n[11] Emory J. Edwards, Michael Gilbert, Emily Blank, and Stacy M. Branham. 2023.\\nHow the Alt Text Gets Made: What Roles and Processes of Alt Text Creation Can\\nTeach Us About Inclusive Imagery. *ACM Transactions on Accessible Computing*\\n16, 2, Article 18 (jul 2023), 28 pages.\\n\\n[12] Carlos H G Ferreira, Fabrício Murai, Ana P C, Jussara M Almeida, Martino\\nTrevisan, Luca Vassio, Idilio Drago, and Marco Mellia. 2020. Unveiling Community\\nDynamics on Instagram Political Network. In *Proc. of the 12th ACM Conf. on Web*\\n*Science* (NY). ACM, 231–240.\\n\\n[13] Yuheng Hu, Lydia Manikonda, and Subbarao Kambhampati. 2014. What We\\nInstagram: A First Analysis of Instagram Photo Content and User Types. *Proc. of*\\n*the International AAAI Conf. on Web and Social Media* 8, 1 (2014), 595–598.\\n\\n[14] C. Hutto and Eric Gilbert. 2014. VADER: A Parsimonious Rule-Based Model for\\nSentiment Analysis of Social Media Text. *Proc. of the International AAAI Conf. on*\\n*Web and Social Media* 8, 1 (2014), 216–225.\\n\\n[15] Simona Ibba, Matteo Orrù, Filippo Eros Pani, and Simone Porru. 2015. Hashtag\\nof Instagram: From Folksonomy to Complex Network.. In *KEOD* . 279–284.\\n\\n[16] Jared Katzman, Angelina Wang, Morgan Scheuerman, Su L Blodgett, Kristen\\nLaird, Hanna Wallach, and Solon Barocas. 2023. Taxonomizing and Measuring\\nRepresentational Harms: A Look at Image Tagging. *AAAI Conf. on Artificial*\\n*Intelligence* 37, 12 (2023), 14277–14285.\\n\\n[17] Mirella Lapata. 2006. Automatic evaluation of information ordering: Kendall’s\\ntau. *Computational Linguistics* 32, 4 (2006), 471–484.\\n\\n[18] Sebastián Lozano and Ester Gutiérrez. 2018. A complex network analysis of global\\ntourism flows. *International Journal of Tourism Research* 20, 5 (2018), 588–604.\\n\\n[19] Hafiz A M Malik. 2022. Complex Network Formation and Analysis of Online\\nSocial Media Systems. *Computer Modeling in Engineering & Sciences* 130, 3 (2022),\\n1737–1750.\\n\\n[20] Breno Matos, Fabrício Benevenuto, and Ana P C da Silva. 2021. A Nudez como\\nEstereótipo de Beleza através das Lentes do Instagram. In *X Brazilian Workshop*\\n*on Social Network Analysis and Mining* . 163–168.\\n\\n[21] Philipe Melo, Fabrício Benevenuto, Daniel Kansaon, Vitor Mafra, and Kaio Sá.\\n2021. Monitor de WhatsApp: Um sistema para checagem de fatos no combate à\\ndesinformação. In *Anais do XXVII Simpósio Brasileiro de Sistemas Multimídia e*\\n*Web (WebMedia’21)* . SBC, 79–82.\\n\\n[22] Caroline Muñoz and Terri Towner. 2017. The Image is the Message: Instagram\\nMarketing and the 2016 Presidential Primary Season. *Journal of Political Market-*\\n*ing* 16 (06 2017).\\n\\n[23] László Nemes and Attila Kiss. 2021. Social media sentiment analysis based on\\nCOVID-19. *Journal of Information and Telecommunication* 5, 1 (2021), 1–15.\\nTaylor & Francis.\\n\\n[24] Serge Nyawa, Dieudonné Tchuente, and Samuel Fosso-Wamba. 2022. COVID19 vaccine hesitancy: a social media analysis using deep learning. *Annals of*\\n*Operations Research* (2022).\\n\\n[25] Abubakr H. Ombabi, Wael Ouarda, and Adel M. Alimi. 2020. Deep learning\\nCNN–LSTM framework for Arabic sentiment analysis using textual information\\nshared in social networks. *Social Network Analysis and Mining* 10, 1 (2020), 53.\\n\\n[26] Khubaib A Qureshi, Rauf A S Malick, Muhammad Sabih, and Hocine Cherifi.\\n2021. Complex network and source inspired COVID-19 fake news classification\\non Twitter. *IEEE Access* 9 (2021), 139636–139656.\\n\\n[27] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic\\nImage Captioning. In *Proc. of the 56th Annual Meeting of the Association for*\\n*Computational Linguistics*, Vol. 1. ACL, 2556–2565.\\n\\n[28] Abigale Stangl, Meredith Ringel Morris, and Danna Gurari. 2020. “Person, Shoes,\\nTree. Is the Person Naked?” What People with Vision Impairments Want in\\nImage Descriptions. In *Proc. of the 2020 CHI Conf. on Human Factors in Computing*\\n*Systems* . ACM, 1–13.\\n\\n[29] Shaomei Wu, Jeffrey Wieland, Omid Farivar, and Julie Schiller. 2017. Automatic\\nAlt-text: Computer-generated Image Descriptions for Blind Users on a Social Network Service. In *Proc. of the 2017 ACM Conf. on Computer Supported Cooperative*\\n*Work and Social Computing (CSCW ’17)* . ACM, NY, 1180–1192.\\n\\n[30] Koosha Zarei, Reza Farahbakhsh, and Noël Crespi. 2019. Typification of Impersonated Accounts on Instagram. In *IEEE 38th International Performance Computing*\\n*and Communications Conf. (IPCCC)* . 1–6.\\n\\n\\n282\\n\\n\\n-----\\n\\n',\n",
       "  'artigo_tokenizado': ['#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'Through',\n",
       "   'the',\n",
       "   'Eyes',\n",
       "   'of',\n",
       "   'Instagram',\n",
       "   ':',\n",
       "   'Analyzing',\n",
       "   'Image',\n",
       "   'Content',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   '*',\n",
       "   'utilizing',\n",
       "   'Meta',\n",
       "   '’s',\n",
       "   'Automatic',\n",
       "   'Alt',\n",
       "   '-',\n",
       "   'Text',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'João',\n",
       "   'Francisco',\n",
       "   'Hecksher',\n",
       "   'Olivetti',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'joao.olivetti@ufv.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Viçosa',\n",
       "   '-',\n",
       "   'UFV',\n",
       "   'Florestal',\n",
       "   ',',\n",
       "   'MG',\n",
       "   ',',\n",
       "   'Brasil',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'ABSTRACT',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Multimedia',\n",
       "   'communication',\n",
       "   'has',\n",
       "   'become',\n",
       "   'an',\n",
       "   'essential',\n",
       "   'part',\n",
       "   'of',\n",
       "   'social',\n",
       "   '\\n',\n",
       "   'media',\n",
       "   ',',\n",
       "   'with',\n",
       "   'images',\n",
       "   'representing',\n",
       "   'a',\n",
       "   'significant',\n",
       "   'part',\n",
       "   'of',\n",
       "   'the',\n",
       "   'content',\n",
       "   '\\n',\n",
       "   'on',\n",
       "   'most',\n",
       "   'platforms',\n",
       "   '.',\n",
       "   'This',\n",
       "   'study',\n",
       "   'investigates',\n",
       "   'image',\n",
       "   'content',\n",
       "   'on',\n",
       "   'Instagram',\n",
       "   'through',\n",
       "   'Meta',\n",
       "   '’s',\n",
       "   'internal',\n",
       "   'image',\n",
       "   'classification',\n",
       "   'algorithm',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'Automatic',\n",
       "   'Alt',\n",
       "   '-',\n",
       "   'Text',\n",
       "   '(',\n",
       "   'AAT',\n",
       "   ')',\n",
       "   '.',\n",
       "   'Our',\n",
       "   'approach',\n",
       "   'differs',\n",
       "   'from',\n",
       "   'research',\n",
       "   'on',\n",
       "   '\\n',\n",
       "   'data',\n",
       "   'from',\n",
       "   'comments',\n",
       "   'and',\n",
       "   'hashtags',\n",
       "   'because',\n",
       "   'of',\n",
       "   'the',\n",
       "   'use',\n",
       "   'of',\n",
       "   'actual',\n",
       "   '\\n',\n",
       "   'visual',\n",
       "   'descriptions',\n",
       "   'as',\n",
       "   'the',\n",
       "   'means',\n",
       "   'of',\n",
       "   'understanding',\n",
       "   'the',\n",
       "   'kinds',\n",
       "   'of',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'content',\n",
       "   'published',\n",
       "   'on',\n",
       "   'the',\n",
       "   'network',\n",
       "   '.',\n",
       "   'Our',\n",
       "   'analysis',\n",
       "   'of',\n",
       "   '200k',\n",
       "   'posts',\n",
       "   'reveals',\n",
       "   '1,471',\n",
       "   'unique',\n",
       "   'tags',\n",
       "   'being',\n",
       "   'used',\n",
       "   'to',\n",
       "   'characterize',\n",
       "   'image',\n",
       "   'content',\n",
       "   '\\n',\n",
       "   'on',\n",
       "   'Instagram',\n",
       "   ',',\n",
       "   'representing',\n",
       "   'mostly',\n",
       "   'objects',\n",
       "   ',',\n",
       "   'food',\n",
       "   ',',\n",
       "   'animals',\n",
       "   ',',\n",
       "   'locations',\n",
       "   '\\n',\n",
       "   'and',\n",
       "   'other',\n",
       "   'common',\n",
       "   'components',\n",
       "   'of',\n",
       "   'social',\n",
       "   'media',\n",
       "   'photos',\n",
       "   '.',\n",
       "   'Notably',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'we',\n",
       "   'found',\n",
       "   'that',\n",
       "   'content',\n",
       "   'about',\n",
       "   'personal',\n",
       "   'aesthetics',\n",
       "   'is',\n",
       "   'highly',\n",
       "   'popular',\n",
       "   '\\n',\n",
       "   'on',\n",
       "   'the',\n",
       "   'platform',\n",
       "   ',',\n",
       "   'with',\n",
       "   'person',\n",
       "   'and',\n",
       "   'selfie',\n",
       "   'being',\n",
       "   'respectively',\n",
       "   'some',\n",
       "   '\\n',\n",
       "   'of',\n",
       "   'the',\n",
       "   'top',\n",
       "   'two',\n",
       "   'most',\n",
       "   'common',\n",
       "   'tag',\n",
       "   'and',\n",
       "   'post',\n",
       "   'categories',\n",
       "   ',',\n",
       "   'being',\n",
       "   'also',\n",
       "   '\\n',\n",
       "   'highly',\n",
       "   'related',\n",
       "   'to',\n",
       "   'other',\n",
       "   'tags',\n",
       "   'such',\n",
       "   'as',\n",
       "   'makeup',\n",
       "   ',',\n",
       "   'lipstick',\n",
       "   'and',\n",
       "   'eyeliner',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Furthermore',\n",
       "   ',',\n",
       "   'we',\n",
       "   'explored',\n",
       "   'the',\n",
       "   'connections',\n",
       "   'between',\n",
       "   'tags',\n",
       "   ',',\n",
       "   'representing',\n",
       "   'very',\n",
       "   'popular',\n",
       "   'content',\n",
       "   'trends',\n",
       "   'within',\n",
       "   'the',\n",
       "   'network',\n",
       "   '.',\n",
       "   'Finally',\n",
       "   ',',\n",
       "   'we',\n",
       "   '\\n',\n",
       "   'uncover',\n",
       "   'substantial',\n",
       "   'differences',\n",
       "   'in',\n",
       "   'posting',\n",
       "   'behavior',\n",
       "   'of',\n",
       "   'influencers',\n",
       "   '\\n',\n",
       "   'and',\n",
       "   'news',\n",
       "   'pages',\n",
       "   'when',\n",
       "   'compared',\n",
       "   'to',\n",
       "   'regular',\n",
       "   'users',\n",
       "   ',',\n",
       "   'observing',\n",
       "   'they',\n",
       "   '\\n',\n",
       "   'post',\n",
       "   'more',\n",
       "   'frequently',\n",
       "   'and',\n",
       "   'about',\n",
       "   'more',\n",
       "   'specific',\n",
       "   'content',\n",
       "   ',',\n",
       "   'suggesting',\n",
       "   '\\n',\n",
       "   'what',\n",
       "   'may',\n",
       "   'attract',\n",
       "   'more',\n",
       "   'engagement',\n",
       "   'on',\n",
       "   'Instagram',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   'KEYWORDS',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'Instagram',\n",
       "   ',',\n",
       "   'alt',\n",
       "   '-',\n",
       "   'text',\n",
       "   ',',\n",
       "   'social',\n",
       "   'media',\n",
       "   ',',\n",
       "   'image',\n",
       "   'classification',\n",
       "   ',',\n",
       "   'image',\n",
       "   'tagging',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'complex',\n",
       "   'networks',\n",
       "   ',',\n",
       "   'influencers',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '*',\n",
       "   '*',\n",
       "   '1',\n",
       "   'INTRODUCTION',\n",
       "   '*',\n",
       "   '*',\n",
       "   '\\n\\n',\n",
       "   'The',\n",
       "   'Web',\n",
       "   'has',\n",
       "   'become',\n",
       "   'an',\n",
       "   'integral',\n",
       "   'part',\n",
       "   'of',\n",
       "   'daily',\n",
       "   'life',\n",
       "   'for',\n",
       "   'a',\n",
       "   'significant',\n",
       "   '\\n',\n",
       "   'portion',\n",
       "   'of',\n",
       "   'the',\n",
       "   'global',\n",
       "   'population',\n",
       "   '.',\n",
       "   'With',\n",
       "   'the',\n",
       "   'increasing',\n",
       "   'prevalence',\n",
       "   '\\n',\n",
       "   'of',\n",
       "   'online',\n",
       "   'social',\n",
       "   'networks',\n",
       "   'and',\n",
       "   'the',\n",
       "   'multimedia',\n",
       "   'nature',\n",
       "   'of',\n",
       "   'communi',\n",
       "   '\\n',\n",
       "   'cation',\n",
       "   'online',\n",
       "   ',',\n",
       "   'visual',\n",
       "   'content',\n",
       "   'understanding',\n",
       "   'is',\n",
       "   'crucial',\n",
       "   '.',\n",
       "   'To',\n",
       "   'enhance',\n",
       "   '\\n',\n",
       "   'accessibility',\n",
       "   ',',\n",
       "   'various',\n",
       "   'tools',\n",
       "   'have',\n",
       "   'been',\n",
       "   'implemented',\n",
       "   'to',\n",
       "   'assist',\n",
       "   'disabled',\n",
       "   '\\n',\n",
       "   'users',\n",
       "   'in',\n",
       "   'this',\n",
       "   'task',\n",
       "   '.',\n",
       "   'One',\n",
       "   'notable',\n",
       "   'example',\n",
       "   'is',\n",
       "   '“',\n",
       "   'Alt',\n",
       "   '-',\n",
       "   'text',\n",
       "   '”',\n",
       "   '(',\n",
       "   'alternative',\n",
       "   '\\n',\n",
       "   'text',\n",
       "   ')',\n",
       "   ',',\n",
       "   'a',\n",
       "   'textual',\n",
       "   'description',\n",
       "   'of',\n",
       "   'an',\n",
       "   'image',\n",
       "   'included',\n",
       "   'in',\n",
       "   'the',\n",
       "   'HTML',\n",
       "   'code',\n",
       "   '\\n',\n",
       "   'of',\n",
       "   'a',\n",
       "   'webpage',\n",
       "   '.',\n",
       "   'It',\n",
       "   'describes',\n",
       "   'the',\n",
       "   'appearance',\n",
       "   'and',\n",
       "   'function',\n",
       "   'of',\n",
       "   'an',\n",
       "   'image',\n",
       "   '\\n',\n",
       "   'to',\n",
       "   'users',\n",
       "   'who',\n",
       "   'can',\n",
       "   'not',\n",
       "   'see',\n",
       "   'it',\n",
       "   ',',\n",
       "   'those',\n",
       "   'using',\n",
       "   'screen',\n",
       "   'readers',\n",
       "   ',',\n",
       "   'or',\n",
       "   'when',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'image',\n",
       "   'fails',\n",
       "   'to',\n",
       "   'load',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'In',\n",
       "   '2017',\n",
       "   ',',\n",
       "   'Instagram',\n",
       "   'and',\n",
       "   'Facebook',\n",
       "   'implemented',\n",
       "   'Meta',\n",
       "   '’s',\n",
       "   'Automatic',\n",
       "   '\\n',\n",
       "   'Alt',\n",
       "   '-',\n",
       "   'Text',\n",
       "   '(',\n",
       "   'AAT',\n",
       "   ')',\n",
       "   'algorithm',\n",
       "   ',',\n",
       "   'which',\n",
       "   'automatically',\n",
       "   'extracts',\n",
       "   'features',\n",
       "   '\\n',\n",
       "   'from',\n",
       "   'images',\n",
       "   'and',\n",
       "   'photos',\n",
       "   'and',\n",
       "   'provides',\n",
       "   'tags',\n",
       "   'and',\n",
       "   'textual',\n",
       "   'descriptions',\n",
       "   '\\n',\n",
       "   'of',\n",
       "   'visual',\n",
       "   'content',\n",
       "   '.',\n",
       "   'Meta',\n",
       "   '’s',\n",
       "   'AAT',\n",
       "   'has',\n",
       "   'proven',\n",
       "   'effective',\n",
       "   'in',\n",
       "   'aiding',\n",
       "   'blind',\n",
       "   '\\n\\n',\n",
       "   'In',\n",
       "   ':',\n",
       "   'Proceedings',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'Web',\n",
       "   '(',\n",
       "   'WebMe',\n",
       "   '\\n',\n",
       "   'dia’2024',\n",
       "   ')',\n",
       "   '.',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   ',',\n",
       "   'Brazil',\n",
       "   '.',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   ':',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   ',',\n",
       "   '2024',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'SBC',\n",
       "   '–',\n",
       "   'Sociedade',\n",
       "   'Brasileira',\n",
       "   'de',\n",
       "   'Computação',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '-',\n",
       "   '2753',\n",
       "   '\\n\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   'Philipe',\n",
       "   'de',\n",
       "   'Freitas',\n",
       "   'Melo',\n",
       "   '\\n',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   '#',\n",
       "   'philipe.freitas@ufv.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Viçosa',\n",
       "   '-',\n",
       "   'UFV',\n",
       "   'Florestal',\n",
       "   ',',\n",
       "   'MG',\n",
       "   ',',\n",
       "   'Brasil',\n",
       "   '\\n\\n',\n",
       "   'users',\n",
       "   'to',\n",
       "   'navigate',\n",
       "   'these',\n",
       "   'platforms',\n",
       "   'and',\n",
       "   'understand',\n",
       "   'the',\n",
       "   'image',\n",
       "   'content',\n",
       "   '[',\n",
       "   '29',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Furthermore',\n",
       "   ',',\n",
       "   'recent',\n",
       "   'extensions',\n",
       "   'to',\n",
       "   'the',\n",
       "   'AAT',\n",
       "   'system',\n",
       "   'created',\n",
       "   '\\n',\n",
       "   'a',\n",
       "   'large',\n",
       "   '-',\n",
       "   'scale',\n",
       "   'description',\n",
       "   'of',\n",
       "   'content',\n",
       "   'on',\n",
       "   'Instagram',\n",
       "   ',',\n",
       "   'as',\n",
       "   'this',\n",
       "   'feature',\n",
       "   'provided',\n",
       "   'valuable',\n",
       "   'details',\n",
       "   'to',\n",
       "   'the',\n",
       "   'entire',\n",
       "   'platform',\n",
       "   ',',\n",
       "   'beyond',\n",
       "   'just',\n",
       "   'assisting',\n",
       "   '\\n',\n",
       "   'visually',\n",
       "   'impaired',\n",
       "   'users',\n",
       "   '.',\n",
       "   'Image',\n",
       "   'analysis',\n",
       "   'poses',\n",
       "   'significant',\n",
       "   'challenges',\n",
       "   '\\n',\n",
       "   'for',\n",
       "   'researchers',\n",
       "   'due',\n",
       "   'to',\n",
       "   'the',\n",
       "   'inherent',\n",
       "   'difficulty',\n",
       "   'of',\n",
       "   'both',\n",
       "   'manual',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'automated',\n",
       "   'processes',\n",
       "   'to',\n",
       "   'categorize',\n",
       "   'its',\n",
       "   'content',\n",
       "   '.',\n",
       "   'However',\n",
       "   ',',\n",
       "   'it',\n",
       "   'is',\n",
       "   'a',\n",
       "   'key',\n",
       "   '\\n',\n",
       "   'social',\n",
       "   'currency',\n",
       "   'on',\n",
       "   'the',\n",
       "   'Web',\n",
       "   '[',\n",
       "   '13',\n",
       "   ']',\n",
       "   ',',\n",
       "   'and',\n",
       "   'substantial',\n",
       "   'insights',\n",
       "   'can',\n",
       "   'be',\n",
       "   '\\n',\n",
       "   'gained',\n",
       "   'from',\n",
       "   'analyzing',\n",
       "   'images',\n",
       "   'on',\n",
       "   'Instagram',\n",
       "   ',',\n",
       "   'ranging',\n",
       "   'from',\n",
       "   'social',\n",
       "   '\\n',\n",
       "   'behavior',\n",
       "   '[',\n",
       "   '5',\n",
       "   ']',\n",
       "   'to',\n",
       "   'political',\n",
       "   'campaigns',\n",
       "   '[',\n",
       "   '22',\n",
       "   ']',\n",
       "   'and',\n",
       "   'even',\n",
       "   'typification',\n",
       "   'of',\n",
       "   '\\n',\n",
       "   'fake',\n",
       "   'profiles',\n",
       "   '[',\n",
       "   '30',\n",
       "   ']',\n",
       "   '.',\n",
       "   'Most',\n",
       "   'existing',\n",
       "   'studies',\n",
       "   ',',\n",
       "   'though',\n",
       "   ',',\n",
       "   'rely',\n",
       "   'on',\n",
       "   'traditional',\n",
       "   '\\n',\n",
       "   'analysis',\n",
       "   'of',\n",
       "   'textual',\n",
       "   'comments',\n",
       "   'or',\n",
       "   'hashtags',\n",
       "   'from',\n",
       "   'posts',\n",
       "   ',',\n",
       "   'which',\n",
       "   'may',\n",
       "   '\\n',\n",
       "   'not',\n",
       "   'accurately',\n",
       "   'represent',\n",
       "   'the',\n",
       "   'actual',\n",
       "   'content',\n",
       "   'portrayed',\n",
       "   'in',\n",
       "   'pictures',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'This',\n",
       "   'study',\n",
       "   'aims',\n",
       "   'to',\n",
       "   'analyze',\n",
       "   'the',\n",
       "   'structure',\n",
       "   'of',\n",
       "   'Meta',\n",
       "   '’s',\n",
       "   'Automatic',\n",
       "   '\\n',\n",
       "   'Alt',\n",
       "   '-',\n",
       "   'Text',\n",
       "   '(',\n",
       "   'AAT',\n",
       "   ')',\n",
       "   'system',\n",
       "   'in',\n",
       "   'categorizing',\n",
       "   'Instagram',\n",
       "   'images',\n",
       "   '.',\n",
       "   'We',\n",
       "   'seek',\n",
       "   '\\n',\n",
       "   'to',\n",
       "   'answer',\n",
       "   'the',\n",
       "   'following',\n",
       "   'research',\n",
       "   'questions',\n",
       "   ':',\n",
       "   '*',\n",
       "   '*',\n",
       "   'RQ1',\n",
       "   ':*',\n",
       "   '*',\n",
       "   'What',\n",
       "   'are',\n",
       "   'the',\n",
       "   '\\n',\n",
       "   'most',\n",
       "   'popular',\n",
       "   'content',\n",
       "   'tags',\n",
       "   'on',\n",
       "   'different',\n",
       "   'Instagram',\n",
       "   'user',\n",
       "   'classes',\n",
       "   '?',\n",
       "   '*',\n",
       "   '*',\n",
       "   'RQ2',\n",
       "   ':*',\n",
       "   '*',\n",
       "   '\\n',\n",
       "   'What',\n",
       "   'is',\n",
       "   'the',\n",
       "   'relationship',\n",
       "   'between',\n",
       "   'these',\n",
       "   'contents',\n",
       "   '?',\n",
       "   '*',\n",
       "   '*',\n",
       "   'RQ3',\n",
       "   ':*',\n",
       "   '*',\n",
       "   'Is',\n",
       "   'there',\n",
       "   '\\n',\n",
       "   'a',\n",
       "   'substantial',\n",
       "   'difference',\n",
       "   'between',\n",
       "   'the',\n",
       "   'content',\n",
       "   'posted',\n",
       "   'by',\n",
       "   'different',\n",
       "   '\\n',\n",
       "   'types',\n",
       "   'of',\n",
       "   'users',\n",
       "   '?',\n",
       "   '\\n',\n",
       "   'To',\n",
       "   'achieve',\n",
       "   'this',\n",
       "   ',',\n",
       "   'we',\n",
       "   'utilized',\n",
       "   'a',\n",
       "   'custom',\n",
       "   '-',\n",
       "   'built',\n",
       "   'data',\n",
       "   'scraper',\n",
       "   'to',\n",
       "   'collect',\n",
       "   '\\n',\n",
       "   '61,944',\n",
       "   'Instagram',\n",
       "   'accounts',\n",
       "   'and',\n",
       "   '198,623',\n",
       "   'posts',\n",
       "   '.',\n",
       "   'This',\n",
       "   'effort',\n",
       "   'extracted',\n",
       "   '\\n',\n",
       "   'approximately',\n",
       "   '550k',\n",
       "   'Alt',\n",
       "   '-',\n",
       "   'text',\n",
       "   'tags',\n",
       "   'describing',\n",
       "   'the',\n",
       "   'content',\n",
       "   'of',\n",
       "   'Instagram',\n",
       "   'images',\n",
       "   '.',\n",
       "   'After',\n",
       "   'that',\n",
       "   ',',\n",
       "   'by',\n",
       "   'applying',\n",
       "   'complex',\n",
       "   'networks',\n",
       "   'analysis',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'association',\n",
       "   'rules',\n",
       "   'and',\n",
       "   'ranking',\n",
       "   'metrics',\n",
       "   ',',\n",
       "   'we',\n",
       "   'conducted',\n",
       "   'a',\n",
       "   'broad',\n",
       "   'analysis',\n",
       "   'to',\n",
       "   'better',\n",
       "   'understand',\n",
       "   'and',\n",
       "   'categorize',\n",
       "   'the',\n",
       "   'content',\n",
       "   'posted',\n",
       "   'on',\n",
       "   '\\n',\n",
       "   'Instagram',\n",
       "   ',',\n",
       "   'as',\n",
       "   'also',\n",
       "   'the',\n",
       "   'relation',\n",
       "   'of',\n",
       "   'these',\n",
       "   'tags',\n",
       "   'among',\n",
       "   'the',\n",
       "   'users',\n",
       "   'and',\n",
       "   '\\n',\n",
       "   'its',\n",
       "   'popularity',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Our',\n",
       "   'results',\n",
       "   'indicate',\n",
       "   'that',\n",
       "   'the',\n",
       "   'current',\n",
       "   'state',\n",
       "   'of',\n",
       "   'the',\n",
       "   'AAT',\n",
       "   'algorithm',\n",
       "   '\\n',\n",
       "   'shares',\n",
       "   'similarities',\n",
       "   ',',\n",
       "   'but',\n",
       "   'also',\n",
       "   'has',\n",
       "   'significant',\n",
       "   'differences',\n",
       "   'from',\n",
       "   'what',\n",
       "   'is',\n",
       "   '\\n',\n",
       "   'described',\n",
       "   'in',\n",
       "   'the',\n",
       "   'literature',\n",
       "   '.',\n",
       "   'Through',\n",
       "   '1,471',\n",
       "   'unique',\n",
       "   'tags',\n",
       "   'used',\n",
       "   'by',\n",
       "   'AAT',\n",
       "   '\\n',\n",
       "   'that',\n",
       "   'we',\n",
       "   'found',\n",
       "   'within',\n",
       "   'our',\n",
       "   'dataset',\n",
       "   ',',\n",
       "   'objects',\n",
       "   ',',\n",
       "   'foods',\n",
       "   ',',\n",
       "   'animals',\n",
       "   ',',\n",
       "   'locations',\n",
       "   '\\n',\n",
       "   'are',\n",
       "   'among',\n",
       "   'the',\n",
       "   'most',\n",
       "   'common',\n",
       "   'topics',\n",
       "   'on',\n",
       "   'Instagram',\n",
       "   'posts',\n",
       "   '.',\n",
       "   'We',\n",
       "   'found',\n",
       "   '\\n',\n",
       "   'that',\n",
       "   ...],\n",
       "  'pos_tagger': '',\n",
       "  'lema': ['through',\n",
       "   'the',\n",
       "   'eye',\n",
       "   'of',\n",
       "   'Instagram',\n",
       "   'analyze',\n",
       "   'Image',\n",
       "   'Content',\n",
       "   'utilize',\n",
       "   'Meta',\n",
       "   '’s',\n",
       "   'Automatic',\n",
       "   'Alt',\n",
       "   'text',\n",
       "   'João',\n",
       "   'Francisco',\n",
       "   'Hecksher',\n",
       "   'Olivetti',\n",
       "   'joao.olivetti@ufv.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Viçosa',\n",
       "   'UFV',\n",
       "   'Florestal',\n",
       "   'MG',\n",
       "   'Brasil',\n",
       "   'ABSTRACT',\n",
       "   'Multimedia',\n",
       "   'communication',\n",
       "   'have',\n",
       "   'become',\n",
       "   'an',\n",
       "   'essential',\n",
       "   'part',\n",
       "   'of',\n",
       "   'social',\n",
       "   'medium',\n",
       "   'with',\n",
       "   'image',\n",
       "   'represent',\n",
       "   'a',\n",
       "   'significant',\n",
       "   'part',\n",
       "   'of',\n",
       "   'the',\n",
       "   'content',\n",
       "   'on',\n",
       "   'most',\n",
       "   'platform',\n",
       "   'this',\n",
       "   'study',\n",
       "   'investigate',\n",
       "   'image',\n",
       "   'content',\n",
       "   'on',\n",
       "   'Instagram',\n",
       "   'through',\n",
       "   'Meta',\n",
       "   '’s',\n",
       "   'internal',\n",
       "   'image',\n",
       "   'classification',\n",
       "   'algorithm',\n",
       "   'Automatic',\n",
       "   'Alt',\n",
       "   'Text',\n",
       "   'AAT',\n",
       "   'our',\n",
       "   'approach',\n",
       "   'differ',\n",
       "   'from',\n",
       "   'research',\n",
       "   'on',\n",
       "   'datum',\n",
       "   'from',\n",
       "   'comment',\n",
       "   'and',\n",
       "   'hashtag',\n",
       "   'because',\n",
       "   'of',\n",
       "   'the',\n",
       "   'use',\n",
       "   'of',\n",
       "   'actual',\n",
       "   'visual',\n",
       "   'description',\n",
       "   'as',\n",
       "   'the',\n",
       "   'mean',\n",
       "   'of',\n",
       "   'understand',\n",
       "   'the',\n",
       "   'kind',\n",
       "   'of',\n",
       "   'the',\n",
       "   'content',\n",
       "   'publish',\n",
       "   'on',\n",
       "   'the',\n",
       "   'network',\n",
       "   'our',\n",
       "   'analysis',\n",
       "   'of',\n",
       "   '200k',\n",
       "   'post',\n",
       "   'reveal',\n",
       "   '1,471',\n",
       "   'unique',\n",
       "   'tag',\n",
       "   'be',\n",
       "   'use',\n",
       "   'to',\n",
       "   'characterize',\n",
       "   'image',\n",
       "   'content',\n",
       "   'on',\n",
       "   'Instagram',\n",
       "   'represent',\n",
       "   'mostly',\n",
       "   'object',\n",
       "   'food',\n",
       "   'animal',\n",
       "   'location',\n",
       "   'and',\n",
       "   'other',\n",
       "   'common',\n",
       "   'component',\n",
       "   'of',\n",
       "   'social',\n",
       "   'medium',\n",
       "   'photo',\n",
       "   'notably',\n",
       "   'we',\n",
       "   'find',\n",
       "   'that',\n",
       "   'content',\n",
       "   'about',\n",
       "   'personal',\n",
       "   'aesthetic',\n",
       "   'be',\n",
       "   'highly',\n",
       "   'popular',\n",
       "   'on',\n",
       "   'the',\n",
       "   'platform',\n",
       "   'with',\n",
       "   'person',\n",
       "   'and',\n",
       "   'selfie',\n",
       "   'be',\n",
       "   'respectively',\n",
       "   'some',\n",
       "   'of',\n",
       "   'the',\n",
       "   'top',\n",
       "   'two',\n",
       "   'most',\n",
       "   'common',\n",
       "   'tag',\n",
       "   'and',\n",
       "   'post',\n",
       "   'category',\n",
       "   'be',\n",
       "   'also',\n",
       "   'highly',\n",
       "   'related',\n",
       "   'to',\n",
       "   'other',\n",
       "   'tag',\n",
       "   'such',\n",
       "   'as',\n",
       "   'makeup',\n",
       "   'lipstick',\n",
       "   'and',\n",
       "   'eyeliner',\n",
       "   'furthermore',\n",
       "   'we',\n",
       "   'explore',\n",
       "   'the',\n",
       "   'connection',\n",
       "   'between',\n",
       "   'tag',\n",
       "   'represent',\n",
       "   'very',\n",
       "   'popular',\n",
       "   'content',\n",
       "   'trend',\n",
       "   'within',\n",
       "   'the',\n",
       "   'network',\n",
       "   'finally',\n",
       "   'we',\n",
       "   'uncover',\n",
       "   'substantial',\n",
       "   'difference',\n",
       "   'in',\n",
       "   'post',\n",
       "   'behavior',\n",
       "   'of',\n",
       "   'influencer',\n",
       "   'and',\n",
       "   'news',\n",
       "   'page',\n",
       "   'when',\n",
       "   'compare',\n",
       "   'to',\n",
       "   'regular',\n",
       "   'user',\n",
       "   'observe',\n",
       "   'they',\n",
       "   'post',\n",
       "   'more',\n",
       "   'frequently',\n",
       "   'and',\n",
       "   'about',\n",
       "   'more',\n",
       "   'specific',\n",
       "   'content',\n",
       "   'suggest',\n",
       "   'what',\n",
       "   'may',\n",
       "   'attract',\n",
       "   'more',\n",
       "   'engagement',\n",
       "   'on',\n",
       "   'Instagram',\n",
       "   'keyword',\n",
       "   'Instagram',\n",
       "   'alt',\n",
       "   'text',\n",
       "   'social',\n",
       "   'medium',\n",
       "   'image',\n",
       "   'classification',\n",
       "   'image',\n",
       "   'tagging',\n",
       "   'complex',\n",
       "   'network',\n",
       "   'influencer',\n",
       "   '1',\n",
       "   'introduction',\n",
       "   'the',\n",
       "   'web',\n",
       "   'have',\n",
       "   'become',\n",
       "   'an',\n",
       "   'integral',\n",
       "   'part',\n",
       "   'of',\n",
       "   'daily',\n",
       "   'life',\n",
       "   'for',\n",
       "   'a',\n",
       "   'significant',\n",
       "   'portion',\n",
       "   'of',\n",
       "   'the',\n",
       "   'global',\n",
       "   'population',\n",
       "   'with',\n",
       "   'the',\n",
       "   'increase',\n",
       "   'prevalence',\n",
       "   'of',\n",
       "   'online',\n",
       "   'social',\n",
       "   'network',\n",
       "   'and',\n",
       "   'the',\n",
       "   'multimedia',\n",
       "   'nature',\n",
       "   'of',\n",
       "   'communi',\n",
       "   'cation',\n",
       "   'online',\n",
       "   'visual',\n",
       "   'content',\n",
       "   'understanding',\n",
       "   'be',\n",
       "   'crucial',\n",
       "   'to',\n",
       "   'enhance',\n",
       "   'accessibility',\n",
       "   'various',\n",
       "   'tool',\n",
       "   'have',\n",
       "   'be',\n",
       "   'implement',\n",
       "   'to',\n",
       "   'assist',\n",
       "   'disabled',\n",
       "   'user',\n",
       "   'in',\n",
       "   'this',\n",
       "   'task',\n",
       "   'one',\n",
       "   'notable',\n",
       "   'example',\n",
       "   'be',\n",
       "   'alt',\n",
       "   'text',\n",
       "   'alternative',\n",
       "   'text',\n",
       "   'a',\n",
       "   'textual',\n",
       "   'description',\n",
       "   'of',\n",
       "   'an',\n",
       "   'image',\n",
       "   'include',\n",
       "   'in',\n",
       "   'the',\n",
       "   'HTML',\n",
       "   'code',\n",
       "   'of',\n",
       "   'a',\n",
       "   'webpage',\n",
       "   'it',\n",
       "   'describe',\n",
       "   'the',\n",
       "   'appearance',\n",
       "   'and',\n",
       "   'function',\n",
       "   'of',\n",
       "   'an',\n",
       "   'image',\n",
       "   'to',\n",
       "   'user',\n",
       "   'who',\n",
       "   'can',\n",
       "   'not',\n",
       "   'see',\n",
       "   'it',\n",
       "   'those',\n",
       "   'use',\n",
       "   'screen',\n",
       "   'reader',\n",
       "   'or',\n",
       "   'when',\n",
       "   'the',\n",
       "   'image',\n",
       "   'fail',\n",
       "   'to',\n",
       "   'load',\n",
       "   'in',\n",
       "   '2017',\n",
       "   'Instagram',\n",
       "   'and',\n",
       "   'Facebook',\n",
       "   'implement',\n",
       "   'Meta',\n",
       "   '’s',\n",
       "   'Automatic',\n",
       "   'Alt',\n",
       "   'text',\n",
       "   'AAT',\n",
       "   'algorithm',\n",
       "   'which',\n",
       "   'automatically',\n",
       "   'extract',\n",
       "   'feature',\n",
       "   'from',\n",
       "   'image',\n",
       "   'and',\n",
       "   'photo',\n",
       "   'and',\n",
       "   'provide',\n",
       "   'tag',\n",
       "   'and',\n",
       "   'textual',\n",
       "   'description',\n",
       "   'of',\n",
       "   'visual',\n",
       "   'content',\n",
       "   'Meta',\n",
       "   '’s',\n",
       "   'AAT',\n",
       "   'have',\n",
       "   'prove',\n",
       "   'effective',\n",
       "   'in',\n",
       "   'aid',\n",
       "   'blind',\n",
       "   'in',\n",
       "   'proceeding',\n",
       "   'of',\n",
       "   'the',\n",
       "   'Brazilian',\n",
       "   'Symposium',\n",
       "   'on',\n",
       "   'Multimedia',\n",
       "   'and',\n",
       "   'the',\n",
       "   'web',\n",
       "   'WebMe',\n",
       "   'dia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Porto',\n",
       "   'Alegre',\n",
       "   'Brazilian',\n",
       "   'Computer',\n",
       "   'Society',\n",
       "   '2024',\n",
       "   '©',\n",
       "   '2024',\n",
       "   'sbc',\n",
       "   'Sociedade',\n",
       "   'Brasileira',\n",
       "   'de',\n",
       "   'Computação',\n",
       "   'ISSN',\n",
       "   '2966',\n",
       "   '2753',\n",
       "   'Philipe',\n",
       "   'de',\n",
       "   'Freitas',\n",
       "   'Melo',\n",
       "   'philipe.freitas@ufv.br',\n",
       "   'Universidade',\n",
       "   'Federal',\n",
       "   'de',\n",
       "   'Viçosa',\n",
       "   'UFV',\n",
       "   'Florestal',\n",
       "   'MG',\n",
       "   'Brasil',\n",
       "   'user',\n",
       "   'to',\n",
       "   'navigate',\n",
       "   'these',\n",
       "   'platform',\n",
       "   'and',\n",
       "   'understand',\n",
       "   'the',\n",
       "   'image',\n",
       "   'content',\n",
       "   '29',\n",
       "   'furthermore',\n",
       "   'recent',\n",
       "   'extension',\n",
       "   'to',\n",
       "   'the',\n",
       "   'AAT',\n",
       "   'system',\n",
       "   'create',\n",
       "   'a',\n",
       "   'large',\n",
       "   'scale',\n",
       "   'description',\n",
       "   'of',\n",
       "   'content',\n",
       "   'on',\n",
       "   'Instagram',\n",
       "   'as',\n",
       "   'this',\n",
       "   'feature',\n",
       "   'provide',\n",
       "   'valuable',\n",
       "   'detail',\n",
       "   'to',\n",
       "   'the',\n",
       "   'entire',\n",
       "   'platform',\n",
       "   'beyond',\n",
       "   'just',\n",
       "   'assist',\n",
       "   'visually',\n",
       "   'impair',\n",
       "   'user',\n",
       "   'image',\n",
       "   'analysis',\n",
       "   'pose',\n",
       "   'significant',\n",
       "   'challenge',\n",
       "   'for',\n",
       "   'researcher',\n",
       "   'due',\n",
       "   'to',\n",
       "   'the',\n",
       "   'inherent',\n",
       "   'difficulty',\n",
       "   'of',\n",
       "   'both',\n",
       "   'manual',\n",
       "   'and',\n",
       "   'automate',\n",
       "   'process',\n",
       "   'to',\n",
       "   'categorize',\n",
       "   'its',\n",
       "   'content',\n",
       "   'however',\n",
       "   'it',\n",
       "   'be',\n",
       "   'a',\n",
       "   'key',\n",
       "   'social',\n",
       "   'currency',\n",
       "   'on',\n",
       "   'the',\n",
       "   'web',\n",
       "   '13',\n",
       "   'and',\n",
       "   'substantial',\n",
       "   'insight',\n",
       "   'can',\n",
       "   'be',\n",
       "   'gain',\n",
       "   'from',\n",
       "   'analyze',\n",
       "   'image',\n",
       "   'on',\n",
       "   'Instagram',\n",
       "   'range',\n",
       "   'from',\n",
       "   'social',\n",
       "   'behavior',\n",
       "   '5',\n",
       "   'to',\n",
       "   'political',\n",
       "   'campaign',\n",
       "   '22',\n",
       "   'and',\n",
       "   'even',\n",
       "   'typification',\n",
       "   'of',\n",
       "   'fake',\n",
       "   'profile',\n",
       "   '30',\n",
       "   'Most',\n",
       "   'exist',\n",
       "   'study',\n",
       "   'though',\n",
       "   'rely',\n",
       "   'on',\n",
       "   'traditional',\n",
       "   'analysis',\n",
       "   'of',\n",
       "   'textual',\n",
       "   'comment',\n",
       "   'or',\n",
       "   'hashtag',\n",
       "   'from',\n",
       "   'post',\n",
       "   'which',\n",
       "   'may',\n",
       "   'not',\n",
       "   'accurately',\n",
       "   'represent',\n",
       "   'the',\n",
       "   'actual',\n",
       "   'content',\n",
       "   'portray',\n",
       "   'in',\n",
       "   'picture',\n",
       "   'this',\n",
       "   'study',\n",
       "   'aim',\n",
       "   'to',\n",
       "   'analyze',\n",
       "   'the',\n",
       "   'structure',\n",
       "   'of',\n",
       "   'Meta',\n",
       "   '’s',\n",
       "   'Automatic',\n",
       "   'Alt',\n",
       "   'Text',\n",
       "   'AAT',\n",
       "   'system',\n",
       "   'in',\n",
       "   'categorize',\n",
       "   'Instagram',\n",
       "   'image',\n",
       "   'we',\n",
       "   'seek',\n",
       "   'to',\n",
       "   'answer',\n",
       "   'the',\n",
       "   'follow',\n",
       "   'research',\n",
       "   'question',\n",
       "   'rq1',\n",
       "   'what',\n",
       "   'be',\n",
       "   'the',\n",
       "   'most',\n",
       "   'popular',\n",
       "   'content',\n",
       "   'tag',\n",
       "   'on',\n",
       "   'different',\n",
       "   'Instagram',\n",
       "   'user',\n",
       "   'class',\n",
       "   'RQ2',\n",
       "   'what',\n",
       "   'be',\n",
       "   'the',\n",
       "   'relationship',\n",
       "   'between',\n",
       "   'these',\n",
       "   'content',\n",
       "   'RQ3',\n",
       "   'be',\n",
       "   'there',\n",
       "   'a',\n",
       "   'substantial',\n",
       "   'difference',\n",
       "   'between',\n",
       "   'the',\n",
       "   'content',\n",
       "   'post',\n",
       "   'by',\n",
       "   'different',\n",
       "   'type',\n",
       "   'of',\n",
       "   'user',\n",
       "   'to',\n",
       "   'achieve',\n",
       "   'this',\n",
       "   'we',\n",
       "   'utilize',\n",
       "   'a',\n",
       "   'custom',\n",
       "   'build',\n",
       "   'datum',\n",
       "   'scraper',\n",
       "   'to',\n",
       "   'collect',\n",
       "   '61,944',\n",
       "   'Instagram',\n",
       "   'account',\n",
       "   'and',\n",
       "   '198,623',\n",
       "   'post',\n",
       "   'this',\n",
       "   'effort',\n",
       "   'extract',\n",
       "   'approximately',\n",
       "   '550k',\n",
       "   'alt',\n",
       "   'text',\n",
       "   'tag',\n",
       "   'describe',\n",
       "   'the',\n",
       "   'content',\n",
       "   'of',\n",
       "   'Instagram',\n",
       "   'image',\n",
       "   'after',\n",
       "   'that',\n",
       "   'by',\n",
       "   'apply',\n",
       "   'complex',\n",
       "   'network',\n",
       "   'analysis',\n",
       "   'association',\n",
       "   'rule',\n",
       "   'and',\n",
       "   'rank',\n",
       "   'metric',\n",
       "   'we',\n",
       "   'conduct',\n",
       "   'a',\n",
       "   'broad',\n",
       "   'analysis',\n",
       "   'to',\n",
       "   'well',\n",
       "   'understand',\n",
       "   'and',\n",
       "   'categorize',\n",
       "   'the',\n",
       "   'content',\n",
       "   'post',\n",
       "   'on',\n",
       "   'Instagram',\n",
       "   'as',\n",
       "   'also',\n",
       "   'the',\n",
       "   'relation',\n",
       "   'of',\n",
       "   'these',\n",
       "   'tag',\n",
       "   'among',\n",
       "   'the',\n",
       "   'user',\n",
       "   'and',\n",
       "   'its',\n",
       "   'popularity',\n",
       "   'our',\n",
       "   'result',\n",
       "   'indicate',\n",
       "   'that',\n",
       "   'the',\n",
       "   'current',\n",
       "   'state',\n",
       "   'of',\n",
       "   'the',\n",
       "   'AAT',\n",
       "   'algorithm',\n",
       "   'share',\n",
       "   'similarity',\n",
       "   'but',\n",
       "   'also',\n",
       "   'have',\n",
       "   'significant',\n",
       "   'difference',\n",
       "   'from',\n",
       "   'what',\n",
       "   'be',\n",
       "   'describe',\n",
       "   'in',\n",
       "   'the',\n",
       "   'literature',\n",
       "   'through',\n",
       "   '1,471',\n",
       "   'unique',\n",
       "   'tag',\n",
       "   'use',\n",
       "   'by',\n",
       "   'AAT',\n",
       "   'that',\n",
       "   'we',\n",
       "   'find',\n",
       "   'within',\n",
       "   'our',\n",
       "   'dataset',\n",
       "   'object',\n",
       "   'food',\n",
       "   'animal',\n",
       "   'location',\n",
       "   'be',\n",
       "   'among',\n",
       "   'the',\n",
       "   'most',\n",
       "   'common',\n",
       "   'topic',\n",
       "   'on',\n",
       "   'Instagram',\n",
       "   'post',\n",
       "   'we',\n",
       "   'find',\n",
       "   'that',\n",
       "   'personal',\n",
       "   'aesthetic',\n",
       "   'content',\n",
       "   'be',\n",
       "   'highly',\n",
       "   'popular',\n",
       "   'on',\n",
       "   'Instagram',\n",
       "   'often',\n",
       "   'link',\n",
       "   'to',\n",
       "   'beauty',\n",
       "   'tag',\n",
       "   'such',\n",
       "   'as',\n",
       "   'makeup',\n",
       "   'lipstick',\n",
       "   'and',\n",
       "   'eyeliner',\n",
       "   'lastly',\n",
       "   'we',\n",
       "   'observe',\n",
       "   'substantial',\n",
       "   'difference',\n",
       "   'in',\n",
       "   'post',\n",
       "   'behavior',\n",
       "   'between',\n",
       "   'influencer',\n",
       "   'news',\n",
       "   'page',\n",
       "   'and',\n",
       "   'regular',\n",
       "   'user',\n",
       "   '2',\n",
       "   'background',\n",
       "   'and',\n",
       "   'relate',\n",
       "   'work',\n",
       "   'this',\n",
       "   'section',\n",
       "   'show',\n",
       "   'similar',\n",
       "   'adjacent',\n",
       "   'or',\n",
       "   'related',\n",
       "   'research',\n",
       "   'to',\n",
       "   'the',\n",
       "   'present',\n",
       "   'work',\n",
       "   'a',\n",
       "   'discussion',\n",
       "   'on',\n",
       "   'what',\n",
       "   'alt',\n",
       "   'text',\n",
       "   'be',\n",
       "   'also',\n",
       "   'include',\n",
       "   'as',\n",
       "   'context',\n",
       "   'on',\n",
       "   'this',\n",
       "   'paper',\n",
       "   'main',\n",
       "   'datapoint',\n",
       "   '2.1',\n",
       "   'Automatic',\n",
       "   'Alt',\n",
       "   'text',\n",
       "   'alt',\n",
       "   'text',\n",
       "   'be',\n",
       "   'the',\n",
       "   'content',\n",
       "   'present',\n",
       "   'in',\n",
       "   'the',\n",
       "   'alt',\n",
       "   'attribute',\n",
       "   'of',\n",
       "   'HTML',\n",
       "   'document',\n",
       "   'its',\n",
       "   'original',\n",
       "   'usage',\n",
       "   'be',\n",
       "   'to',\n",
       "   'show',\n",
       "   'a',\n",
       "   'brief',\n",
       "   'description',\n",
       "   'of',\n",
       "   'an',\n",
       "   'object',\n",
       "   'that',\n",
       "   'could',\n",
       "   'not',\n",
       "   'be',\n",
       "   'load',\n",
       "   'nowadays',\n",
       "   'it',\n",
       "   'be',\n",
       "   'use',\n",
       "   'mainly',\n",
       "   'for',\n",
       "   '275',\n",
       "   'WebMedia’2024',\n",
       "   'Juiz',\n",
       "   'de',\n",
       "   'Fora',\n",
       "   'Brazil',\n",
       "   'Olivetti',\n",
       "   'and',\n",
       "   'Melo',\n",
       "   'accessibility',\n",
       "   'purpose',\n",
       "   'allow',\n",
       "   'screen',\n",
       "   'reader',\n",
       "   'to',\n",
       "   'describe',\n",
       "   'visual',\n",
       "   'content',\n",
       "   'to',\n",
       "   'visually',\n",
       "   'impair',\n",
       "   'user',\n",
       "   'in',\n",
       "   'social',\n",
       "   'medium',\n",
       "   'as',\n",
       "   'content',\n",
       "   'be',\n",
       "   'user',\n",
       "   'upload',\n",
       "   'most',\n",
       "   'image',\n",
       "   'end',\n",
       "   'up',\n",
       "   'have',\n",
       "   'no',\n",
       "   'alt',\n",
       "   'text',\n",
       "   'associate',\n",
       "   'with',\n",
       "   'they',\n",
       "   'this',\n",
       "   'have',\n",
       "   'lead',\n",
       "   'many',\n",
       "   'Online',\n",
       "   'Social',\n",
       "   'Networks',\n",
       "   'to',\n",
       "   'allow',\n",
       "   'user',\n",
       "   'to',\n",
       "   'add',\n",
       "   'their',\n",
       "   'own',\n",
       "   'description',\n",
       "   'of',\n",
       "   'the',\n",
       "   'image',\n",
       "   'this',\n",
       "   'feature',\n",
       "   'however',\n",
       "   'remain',\n",
       "   'very',\n",
       "   'underutilized',\n",
       "   'there',\n",
       "   'have',\n",
       "   'be',\n",
       "   'an',\n",
       "   'extensive',\n",
       "   'body',\n",
       "   'of',\n",
       "   'work',\n",
       "   'focus',\n",
       "   'on',\n",
       "   'alt',\n",
       "   'text',\n",
       "   'include',\n",
       "   'analysis',\n",
       "   'on',\n",
       "   'its',\n",
       "   'manual',\n",
       "   'and',\n",
       "   'automated',\n",
       "   'construction',\n",
       "   '11',\n",
       "   'pre',\n",
       "   'process',\n",
       "   'image',\n",
       "   'description',\n",
       "   'pair',\n",
       "   'dataset',\n",
       "   '27',\n",
       "   'inquiry',\n",
       "   'on',\n",
       "   'what',\n",
       "   'Blind',\n",
       "   'or',\n",
       "   'Low',\n",
       "   'Vision',\n",
       "   'BLVs',\n",
       "   'people',\n",
       "   'want',\n",
       "   'from',\n",
       "   'these',\n",
       "   'tag',\n",
       "   '28',\n",
       "   'as',\n",
       "   'well',\n",
       "   'as',\n",
       "   'what',\n",
       "   'harm',\n",
       "   'can',\n",
       "   'arise',\n",
       "   'from',\n",
       "   'alt',\n",
       "   'text',\n",
       "   'tagging[16',\n",
       "   'this',\n",
       "   'have',\n",
       "   'motivate',\n",
       "   'Meta',\n",
       "   '’s',\n",
       "   'by',\n",
       "   'the',\n",
       "   'time',\n",
       "   'call',\n",
       "   'Facebook',\n",
       "   'researcher',\n",
       "   'to',\n",
       "   'develop',\n",
       "   'the',\n",
       "   'technology',\n",
       "   'know',\n",
       "   'as',\n",
       "   'Automatic',\n",
       "   'Alt',\n",
       "   'Text',\n",
       "   'AAT',\n",
       "   'with',\n",
       "   'its',\n",
       "   'original',\n",
       "   'development',\n",
       "   'be',\n",
       "   'discuss',\n",
       "   ...]}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Título:\tTemplate JSON para CORPUS\n",
    "Conteúdo:\n",
    "{\n",
    "\n",
    "            \"titulo\": \"Título do Artigo\",\n",
    "\n",
    "            \"informacoes_url\": \"link pro artigo\",\n",
    "\n",
    "            \"idioma\": \"Inglês/Português\",\n",
    "\n",
    "            \"storage_key\": \"files/article_x.pdf\",\n",
    "\n",
    "            \"autores\": [\n",
    "\n",
    "                {\n",
    "\n",
    "                    \"nome\": \"autor 1\",\n",
    "\n",
    "                    \"afiliacao\": \"Universidade x\",\n",
    "\n",
    "                    \"orcid\": \"http://orcid.org/xxxx-xxxx-xxxx-xxxx\"\n",
    "\n",
    "                },\n",
    "\n",
    "                {\n",
    "\n",
    "                    \"nome\": \"autor 2\",\n",
    "\n",
    "                    \"afiliacao\": \"Universidade x\",\n",
    "\n",
    "                    \"orcid\": \"http://orcid.org/xxxx-xxxx-xxxx-xxxx\"\n",
    "\n",
    "                }\n",
    "\n",
    "            ],\n",
    "\n",
    "            \"data_publicacao\": \"xx/xx/xxxx\",\n",
    "\n",
    "            \"resumo\": \"Abstract do artigo\",\n",
    "\n",
    "            \"keywords\": [\n",
    "\n",
    "                \"keyword1\",\n",
    "\n",
    "                \"keyword2\",\n",
    "\n",
    "                \"keyword3\"\n",
    "\n",
    "            ],\n",
    "\n",
    "            \"referencias\": [\n",
    "\n",
    "                \"ref1\",\n",
    "\n",
    "                \"ref2\",\n",
    "\n",
    "                \"ref3\"\n",
    "\n",
    "                ],\n",
    "\n",
    "            \"artigo_completo\": \"Artigo\"\n",
    "\n",
    "            \"artigo_tokenizado\": [\"token1\",\"token2\",...,\"tokenN\"]\n",
    "\n",
    "            \"pos_tagger\": [\"TAG_token1\",\"TAG_token2\",...,\"TAG_tokenN]\n",
    "\n",
    "            \"lema\": [\"Lema_1\",\"Lema_2\",...\"Lema_N\"]\n",
    "\n",
    "\n",
    "\n",
    "        }\n"
   ],
   "id": "8542e778f823ab55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c392368fd198150"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
