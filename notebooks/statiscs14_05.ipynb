{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-13T01:01:56.852865Z",
     "start_time": "2025-05-13T01:01:56.446044Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"pt_core_news_sm\")"
   ],
   "outputs": [],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T00:28:50.133580Z",
     "start_time": "2025-05-13T00:28:50.131570Z"
    }
   },
   "cell_type": "code",
   "source": "# from process_text import get_lemmas, tokenize_text, pos_tag_text",
   "id": "2e111561b01451c1",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T00:25:29.092538Z",
     "start_time": "2025-05-13T00:25:28.956703Z"
    }
   },
   "cell_type": "code",
   "source": "# corpus = pd.read_json('corpus.json')",
   "id": "870dbd6a3b57c14d",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# tokenizado = corpus.apply(lambda x: tokenize_text(str(x['resumo'] + x['text']), 'portuguese'))",
   "id": "1014157d410e07c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T00:45:43.335704Z",
     "start_time": "2025-05-13T00:45:11.468271Z"
    }
   },
   "cell_type": "code",
   "source": "# post_tag = corpus.apply(lambda x: pos_tag_text(str(x['resumo']) + ' ' + str(x['text']), 'portuguese'), axis=1)",
   "id": "1d45e3cce57b2084",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T00:58:53.762771Z",
     "start_time": "2025-05-13T00:58:53.760737Z"
    }
   },
   "cell_type": "code",
   "source": "# lemmas = corpus.apply(lambda x: get_lemmas(str(x['resumo']) + ' ' + str(x['text']), 'portuguese'), axis=1)",
   "id": "c01e431657b568c4",
   "outputs": [],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T00:58:33.070016Z",
     "start_time": "2025-05-13T00:58:33.068011Z"
    }
   },
   "cell_type": "code",
   "source": "# tokenizado",
   "id": "487836f24c5dd176",
   "outputs": [],
   "execution_count": 122
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T00:47:44.981405Z",
     "start_time": "2025-05-13T00:47:44.978630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# corpus['lema'] = lemmas\n",
    "# corpus['pos_tagger'] = post_tag\n",
    "# corpus['artigo_tokenizado'] = tokenizado"
   ],
   "id": "e17dc08c98b4bfa2",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T00:58:57.834367Z",
     "start_time": "2025-05-13T00:58:57.832059Z"
    }
   },
   "cell_type": "code",
   "source": "# corpus.to_json('corpus.json', orient='records', lines=True)",
   "id": "b307192b8eea0523",
   "outputs": [],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T00:49:33.342669Z",
     "start_time": "2025-05-13T00:49:33.214Z"
    }
   },
   "cell_type": "code",
   "source": "corpus = pd.read_json('corpus.json', lines=True)",
   "id": "9e26db9c93a54f3",
   "outputs": [],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T01:30:04.822861Z",
     "start_time": "2025-05-13T01:28:53.626172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_valid_tokens(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc\n",
    "            if not token.is_stop and not token.is_punct and not token.is_space]\n",
    "\n",
    "# Aplica a extração por linha\n",
    "vocab = corpus['artigo_tokenizado'].apply(lambda x: extract_valid_tokens(str(x)))\n",
    "\n",
    "# Flatten + removendo duplicados com set()\n",
    "all_tokens = [token for tokens in vocab for token in tokens]\n",
    "vocabulario = sorted(set(all_tokens))  # agora vai remover duplicados corretamente\n",
    "\n",
    "print(f\"Total de termos únicos no vocabulário: {len(vocabulario)}\")"
   ],
   "id": "a83398a595acc4ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de termos únicos no vocabulário: 14938\n"
     ]
    }
   ],
   "execution_count": 146
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T01:35:43.290324Z",
     "start_time": "2025-05-13T01:35:42.619295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Baixar stopwords se necessário\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Combinar colunas de texto relevantes\n",
    "corpus['texto_completo'] = corpus['resumo'] + ' ' + corpus['keywords'] + ' ' + corpus['text']\n",
    "\n",
    "# Função de pré-processamento\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "\n",
    "    # Converter para minúsculas\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remover pontuação\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remover números\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "    # Remover stopwords\n",
    "    stop_words = set(stopwords.words('portuguese'))  # assumindo que o idioma é português\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Aplicar pré-processamento\n",
    "corpus['texto_processado'] = corpus['texto_completo'].apply(preprocess_text)"
   ],
   "id": "2c32dc2e8dd714aa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/eduardo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 150
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T01:37:29.559877Z",
     "start_time": "2025-05-13T01:37:29.465685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Criar vetorizador\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Ajustar e transformar os dados\n",
    "X = vectorizer.fit_transform(corpus['texto_processado'])\n",
    "\n",
    "# Obter o vocabulário\n",
    "vocabulario_bow = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Tamanho do vocabulário BoW: {len(vocabulario_bow)}\")\n",
    "print(\"Exemplos de termos:\", vocabulario_bow[:20])"
   ],
   "id": "43e855d5e7e3b7fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário BoW: 14225\n",
      "Exemplos de termos: ['aae' 'aaeexpr' 'aaes' 'aat' 'abaixo' 'abdômen' 'aberta' 'aberto'\n",
      " 'abertos' 'abertura' 'abid' 'ablação' 'aborda' 'abordada' 'abordadas'\n",
      " 'abordado' 'abordados' 'abordag' 'abordagem' 'abordagens']\n"
     ]
    }
   ],
   "execution_count": 157
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T01:39:31.706169Z",
     "start_time": "2025-05-13T01:39:31.634253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Criar vetorizador TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()  # ajuste conforme necessário\n",
    "\n",
    "# Ajustar e transformar os dados\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus['texto_processado'])\n",
    "\n",
    "# Obter o vocabulário\n",
    "vocabulario_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Tamanho do vocabulário TF-IDF: {len(vocabulario_tfidf)}\")"
   ],
   "id": "47f1a3ad95595058",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário TF-IDF: 14225\n"
     ]
    }
   ],
   "execution_count": 167
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T01:39:41.734209Z",
     "start_time": "2025-05-13T01:39:41.708366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Criar vocabulário manual com base em todas as palavras\n",
    "all_words = ' '.join(corpus['texto_processado']).split()\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Definir um mínimo de frequência\n",
    "min_freq = 1  # ajuste conforme necessário\n",
    "vocabulario_manual = [word for word, count in word_counts.items() if count >= min_freq]\n",
    "\n",
    "print(f\"Tamanho do vocabulário manual: {len(vocabulario_manual)}\")"
   ],
   "id": "d8dc54e8ac5d93dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário manual: 14456\n"
     ]
    }
   ],
   "execution_count": 170
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5628034490ee6882"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
