{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb06a50e",
   "metadata": {},
   "source": [
    "Faz as importações neccessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6ce15ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import spacy\n",
    "from PyPDF2 import PdfReader\n",
    "from googletrans import Translator\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "translator = Translator(service_urls=['translate.googleapis.com'])\n",
    "\n",
    "EMAIL_RE = re.compile(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,4}\\b', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7800a3",
   "metadata": {},
   "source": [
    "Funções para extrair dados dos textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "875dca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    Remove: references style[1], \\n\n",
    "    :param text:\n",
    "    :return:\n",
    "    '''\n",
    "    pattern = r'\\d*https?://\\S+|[\\w.+-]+@[\\w-]+\\.[\\w.-]+'\n",
    "    text = text.replace(\"\\u200b\", \"\")\n",
    "    text = text.replace('- ', '')\n",
    "    text = text.replace(' -', '')\n",
    "    text = text.replace('\\n', '')\n",
    "    text = re.sub(r'([A-Za-z])-\\s+([A-Za-z])', r'\\1\\2', text)\n",
    "    text = re.sub(r'\\[[^\\]]+\\]', '', text)\n",
    "    text = re.sub(r'\\d*https?://\\S+', '', text)\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    text = re.sub(pattern, '', text)\n",
    "    text = re.sub(r'(?:REFER[EÊ]NCIAS?|REFERENCES).*$', '', text, flags=re.DOTALL).strip()\n",
    "    # text = re.sub(r'\\d+', '', text)\n",
    "    return text\n",
    "\n",
    "def extract_title(text: str) -> str:\n",
    "    lines = text.strip().split(\"\\n\")\n",
    "    lines = [line.strip() for line in lines if line.strip()]\n",
    "    return lines[0] if lines else \"\"\n",
    "\n",
    "def extract_authors(text: str) -> list:\n",
    "    lines = text.split('\\n')\n",
    "    # mantém linhas não-vazias\n",
    "    lines = [l.strip() for l in lines if l.strip()]\n",
    "\n",
    "    # 1) usa spaCy NER para extrair todos os PERSON\n",
    "    doc = nlp(text)\n",
    "    persons = [ent.text for ent in doc.ents if ent.label_ == \"PER\"]\n",
    "    # remove duplicatas, mantendo ordem\n",
    "    seen = set()\n",
    "    persons = [p for p in persons if not (p in seen or seen.add(p))]\n",
    "\n",
    "    authors = []\n",
    "    for name in persons:\n",
    "        # acha índice da linha onde aparece esse nome\n",
    "        idx = next((i for i,line in enumerate(lines) if name in line), None)\n",
    "        afiliacao = \"\"\n",
    "        email = \"\"\n",
    "        if idx is not None:\n",
    "            # olha nas próximas 3 linhas por afiliação e e-mail\n",
    "            for j in range(idx+1, min(idx+4, len(lines))):\n",
    "                ln = lines[j]\n",
    "                # se contiver “universidade” ou “departamento”\n",
    "                if \"universidade\" in ln.lower() or \"departamento\" in ln.lower():\n",
    "                    afiliacao = ln\n",
    "                # se encontrar e-mail\n",
    "                m = EMAIL_RE.search(ln)\n",
    "                if m:\n",
    "                    email = m.group(0)\n",
    "        authors.append({\n",
    "            \"nome\": name,\n",
    "            \"afiliacao\": afiliacao,\n",
    "            \"orcid\": email\n",
    "        })\n",
    "    return authors\n",
    "\n",
    "def extract_abstract(text: str) -> str:\n",
    "    # Agora busca por \"RESUMO\" (depois da tradução)\n",
    "    m = re.search(r'(?i)resumo\\s*[:\\-]?\\s*(.+?)\\s*(palavras[- ]?chave|1 introdução|introdução)', text, re.DOTALL)\n",
    "    return m.group(1).strip() if m else \"\"\n",
    "\n",
    "def extract_keywords(text: str) -> list:\n",
    "    m = re.search(r'(?i)palavras[- ]?chave\\s*[:\\-]?\\s*(.+?)\\s*(1 introdução|introdução)', text, re.DOTALL)\n",
    "    if not m:\n",
    "        return []\n",
    "    keywords_raw = m.group(1).strip()\n",
    "    return [kw.strip() for kw in re.split(r'[;,]', keywords_raw) if kw.strip()]\n",
    "\n",
    "def extract_referencias(text: str) -> list:\n",
    "    m = re.search(r'(?i)referências\\s*(.+)', text, re.DOTALL)\n",
    "    if not m:\n",
    "        return []\n",
    "    refs = [r.strip() for r in re.split(r'\\n+', m.group(1)) if r.strip()]\n",
    "    return refs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b1670cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def translate_long_text(text, dest='en', chunk_size=4500):\n",
    "    \"\"\"\n",
    "    Translates text using Google Translate API directly\n",
    "    \"\"\"\n",
    "    translated = []\n",
    "\n",
    "    for start in range(0, len(text), chunk_size):\n",
    "        piece = text[start:start + chunk_size]\n",
    "\n",
    "        url = \"https://translate.googleapis.com/translate_a/single\"\n",
    "        params = {\n",
    "            \"client\": \"gtx\",\n",
    "            \"sl\": \"auto\",\n",
    "            \"tl\": dest,\n",
    "            \"dt\": \"t\",\n",
    "            \"q\": piece\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            translations = response.json()[0]\n",
    "            translated_piece = ''.join([t[0] for t in translations if t[0]])\n",
    "            translated.append(translated_piece)\n",
    "\n",
    "    return ' '.join(translated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154b55d9",
   "metadata": {},
   "source": [
    "Função para processar os pdf e extrair o corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d76fe57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_pdfs(input_dir: str, output_dir: str):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    files = [f for f in os.listdir(input_dir) if f.lower().endswith('.pdf')]\n",
    "\n",
    "    print(f\"Encontrados {len(files)} arquivos para processar.\")\n",
    "\n",
    "    corpus = []\n",
    "\n",
    "    for fname in files:\n",
    "        try:\n",
    "            path = os.path.join(input_dir, fname)\n",
    "            print(f\"\\n Processando: {path}\")\n",
    "\n",
    "            name = os.path.splitext(fname)[0]\n",
    "\n",
    "            reader = PdfReader(path)\n",
    "            full_text = []\n",
    "            for page in reader.pages:\n",
    "                full_text.append(page.extract_text())\n",
    "\n",
    "            full_text = '\\n'.join(full_text)  \n",
    "            full_text = translate_long_text(full_text, dest='pt')\n",
    "        \n",
    "            cleaned_text = clean_text(full_text)\n",
    "\n",
    "            doc = nlp(cleaned_text)\n",
    "\n",
    "            tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "            pos_tags = [token.pos_ for token in doc if not token.is_punct and not token.is_space]\n",
    "            lemmas = [token.lemma_ for token in doc if not token.is_punct and not token.is_space]\n",
    "\n",
    "            artigo_json = {\n",
    "                \"titulo\": extract_title(full_text),\n",
    "                \"informacoes_url\": \"\",\n",
    "                \"idioma\": \"Português\",\n",
    "                \"storage_key\": f\"files/{name}.pdf\",\n",
    "                \"autores\": extract_authors(full_text),\n",
    "                \"data_publicacao\": \"2024\",  # Ajuste manual se precisar\n",
    "                \"resumo\": extract_abstract(full_text),\n",
    "                \"keywords\": extract_keywords(full_text),\n",
    "                \"referencias\": extract_referencias(full_text),\n",
    "                \"artigo_completo\": cleaned_text,\n",
    "                \"artigo_tokenizado\": tokens,\n",
    "                \"pos_tagger\": pos_tags,\n",
    "                \"lema\": lemmas\n",
    "            }\n",
    "\n",
    "            corpus.append(artigo_json)\n",
    "            print(f\"Processado {fname} com sucesso!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar {fname}: {str(e)}\")\n",
    "\n",
    "     # Salvar JSON\n",
    "    out_path = os.path.join(output_dir, f\"corpus.json\")\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(corpus, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"\\n Processamento concluído!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fad5a05",
   "metadata": {},
   "source": [
    "Chamar função que vai exibir processar os pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e6e62cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrados 40 arquivos para processar.\n",
      "\n",
      " Processando: ./files/985-24770-1-10-20240923.pdf\n",
      "Processado 985-24770-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24763-2-10-20241004.pdf\n",
      "Processado 985-24763-2-10-20241004.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24772-1-10-20240923.pdf\n",
      "Processado 985-24772-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24742-1-10-20240923.pdf\n",
      "Processado 985-24742-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24756-1-10-20240923.pdf\n",
      "Processado 985-24756-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24758-1-10-20240923.pdf\n",
      "Processado 985-24758-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24766-1-10-20240923.pdf\n",
      "Processado 985-24766-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24753-2-10-20241004.pdf\n",
      "Processado 985-24753-2-10-20241004.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24769-1-10-20240923.pdf\n",
      "Processado 985-24769-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24760-1-10-20240923.pdf\n",
      "Processado 985-24760-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24750-1-10-20240923.pdf\n",
      "Processado 985-24750-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24774-1-10-20240923.pdf\n",
      "Processado 985-24774-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24744-1-10-20240923.pdf\n",
      "Processado 985-24744-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24748-1-10-20240923.pdf\n",
      "Processado 985-24748-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24757-1-10-20240923.pdf\n",
      "Processado 985-24757-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24738-1-10-20240923.pdf\n",
      "Processado 985-24738-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24759-1-10-20240923.pdf\n",
      "Processado 985-24759-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24751-1-10-20240923.pdf\n",
      "Processado 985-24751-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24776-1-10-20240923.pdf\n",
      "Processado 985-24776-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24746-1-10-20240923.pdf\n",
      "Processado 985-24746-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24737-1-10-20240923.pdf\n",
      "Processado 985-24737-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24739-1-10-20240923.pdf\n",
      "Processado 985-24739-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24741-1-10-20240923.pdf\n",
      "Processado 985-24741-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24775-1-10-20240923.pdf\n",
      "Processado 985-24775-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24773-1-10-20240923.pdf\n",
      "Processado 985-24773-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24743-1-10-20240923.pdf\n",
      "Processado 985-24743-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24762-1-10-20240923.pdf\n",
      "Processado 985-24762-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24740-1-10-20240923.pdf\n",
      "Processado 985-24740-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24768-1-10-20240923.pdf\n",
      "Processado 985-24768-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24755-1-10-20240923.pdf\n",
      "Processado 985-24755-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24764-1-10-20240923.pdf\n",
      "Processado 985-24764-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24752-1-10-20240923.pdf\n",
      "Processado 985-24752-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24747-1-10-20240923.pdf\n",
      "Processado 985-24747-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24754-1-10-20240923.pdf\n",
      "Processado 985-24754-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24765-1-10-20240923.pdf\n",
      "Processado 985-24765-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24745-1-10-20240923.pdf\n",
      "Processado 985-24745-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24767-1-10-20240923.pdf\n",
      "Processado 985-24767-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24749-1-10-20240923.pdf\n",
      "Processado 985-24749-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24771-2-10-20240925.pdf\n",
      "Processado 985-24771-2-10-20240925.pdf com sucesso!\n",
      "\n",
      " Processando: ./files/985-24761-1-10-20240923.pdf\n",
      "Processado 985-24761-1-10-20240923.pdf com sucesso!\n",
      "\n",
      " Processamento concluído!\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"./files\"       # Pasta onde estão seus PDFs\n",
    "output_dir = \"./jsons\"     # Pasta onde vai salvar os JSONs\n",
    "\n",
    "process_all_pdfs(input_dir, output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
